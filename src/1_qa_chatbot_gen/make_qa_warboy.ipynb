{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA 데이터셋 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field, HttpUrl\n",
    "\n",
    "class Page(BaseModel):\n",
    "    id: str = Field(..., description=\"ID of the Page\")\n",
    "    link: HttpUrl = Field(description=\"Url link of the page\")\n",
    "    name: str = Field(description=\"Name of the page\")\n",
    "    parent: str = Field(default=\"\", description=\"ID of the parent page\")\n",
    "    child: List[str] = Field(default=[], description=\"List of ids of the child pages\")\n",
    "    description: str = Field(default=\"\", description=\"Description of the page\")\n",
    "    description_clean: str = Field(default=\"\", description=\"Content markdown\")\n",
    "    html_content: str = Field(default=\"\", description=\"HTML code of the main content in the page\")\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.link, self.name))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Page):\n",
    "            return False\n",
    "        return (self.link, self.name) == (other.link, other.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB 가지고 오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_dir = '/Users/jwlee-pro/Documents/Workspace_2025/projects/llm-rag-chatbot/data/db-warboy_sdk_v1.json'\n",
    "\n",
    "with open(data_dir, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    final_pages = [Page.model_validate_json(page) for page in data[\"sdk\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Page(id='cf227685-cc4e-420e-b21a-e7da166093e5', link=HttpUrl('https://furiosa-ai.github.io/docs/latest/en/software/vm_support.html'), name='vm_support', parent='', child=[], description='\\n\\n\\n* Configuring Warboy Pass-through for Virtual Machine\\n* [View page source](../_sources/software/vm_support.rst.txt)\\n\\n---\\n\\n\\n\\nConfiguring Warboy Pass-through for Virtual Machine\\n[\\uf0c1](#configuring-warboy-pass-through-for-virtual-machine \"Permalink to this heading\")\\n=========================================================================================================================================\\n\\nThis section describes how to enable Warboy pass-through for a virtual machine.\\nThe example of this section is based on a specific VM tool\\n`QEMU-KVM`\\n,\\nbut it also works in other VM tools. The environment used in the example is as follows:\\n\\n* Host OS: CentOS 8\\n* Guest OS: Ubuntu 20.04\\n* Virtual Machine: QEMU-KVM\\n\\nPrerequisites\\n[\\uf0c1](#prerequisites \"Permalink to this heading\")\\n-------------------------------------------------------------\\n\\n* IOMMU and VT-x should be enabled in BIOS.\\n* `qemu-kvm`\\n  ,\\n  `libvirt`\\n  ,\\n  `virt-install`\\n  should be installed in a host machine.\\n\\nSetup Instruction\\n[\\uf0c1](#setup-instruction \"Permalink to this heading\")\\n---------------------------------------------------------------------\\n\\n### 1. Enabling IOMMU in BIOS and Linux OS [\\uf0c1](#enabling-iommu-in-bios-and-linux-os \"Permalink to this heading\")\\n\\nFirst of all, you need to enable IOMMU in BIOS and Linux OS.\\nThe following command shows if IOMMU is enabled.\\n\\n```\\ndmesg | grep -e DMAR -e IOMMU\\n\\n```\\n\\nYou will be able to see some messages related to DMAR or IOMMU if IOMMU is enabled.\\nIf you cannot find any messages related to DMAR or IOMMU, you need to enable IOMMU\\nin BIOS, Linux OS or both.\\n\\nThe ways to enable IOMMU in BIOS may depend on server or motherboard models.\\nPlease refer to the manufacturer’s manual.\\n\\nYou check if IOMMU is enabled in Linux OS as follows:\\n\\n```\\ngrep GRUB_CMDLINE_LINUX /etc/default/grub | grep iommu\\n\\n```\\n\\nIf you cannot find any messages related to IOMMU,\\nplease add\\n`intel_iommu=on`\\nfor Intel CPU or\\n`amd_iommu=on`\\nfor AMD CPU\\nto\\n`GRUB_CMDLINE_LINUX`\\nin\\n`/etc/default/grub`\\nand apply the changes by rebooting the machine.\\n\\nIf you use a legacy BIOS boot mode or UEFI boot mode, the way to enable IOMMU\\nin Linux OS can be different.\\n\\n* Legacy BIOS boot mode:\\n  `grub2-mkconfig\\n  \\n  -o\\n  \\n  /boot/grub2/grub.cfg`\\n* UEFI boot mode,\\n  `grub2-mkconfig\\n  \\n  -o\\n  \\n  /boot/efi/EFI/{linux_distrib}/grub.cfg`\\n  .\\n\\nPlease replace\\n`{linux_distrib}`\\nwith a Linux OS name, such as\\n`centos`\\n,\\n`redhat`\\n, or\\n`ubuntu`\\n.\\n\\n\\n### 2. Loading `vfio-pci` module [\\uf0c1](#loading-vfio-pci-module \"Permalink to this heading\")\\n\\nPlease make sure if the kernel module\\n`vfio-pci`\\nis loaded.\\n\\n> ```\\n> [root@localhost ~]# lsmod | grep vfio_pci\\n> vfio_pci               61440  0\\n> vfio_virqfd            16384  1 vfio_pci\\n> vfio_iommu_type1       36864  0\\n> vfio                   36864  2 vfio_iommu_type1,vfio_pci\\n> irqbypass              16384  2 vfio_pci,kvm\\n> \\n> ```\\n\\nIf\\n`vfio_pci`\\nis not loaded yet, please run\\n`modprobe\\n\\nvfio-pci`\\nto load the module.\\nIn some OS environments, you don’t have to load\\n`vfio-pci`\\n.\\nTo make sure, please refer to the OS manual.\\n\\n\\n### 3. Checking if a virtual machine tool is ready [\\uf0c1](#checking-if-a-virtual-machine-tool-is-ready \"Permalink to this heading\")\\n\\nPlease check if a virtual machine tool is ready to run as follows.\\nIf\\n`virt-host-validate`\\nis not found,\\nplease install the prerequisite packages described in\\n[Prerequisites](#vmsupport-prerequisites)\\n\\n> ```\\n> [root@localhost ~]# virt-host-validate\\n>   QEMU: Checking for hardware virtualization                                 : PASS\\n> \\n>   QEMU: Checking for device assignment IOMMU support                         : PASS\\n>   QEMU: Checking if IOMMU is enabled by kernel                               : PASS\\n> \\n> ```\\n\\nIf check items are PASSED, the virtual machine tool is ready.\\n\\n\\n### 4. Finding Warboy’s PCIe device name [\\uf0c1](#finding-warboy-s-pcie-device-name \"Permalink to this heading\")\\n\\nPCI BDF (Bus, Device, Function)\\nis a unique identifier assigned to every PCIe device connected to a machine.\\nPlease find a PCI BDF of a Warboy card that you want to pass through to a virtual machine.\\n\\n> ```\\n> [root@localhost ~]# lspci -nD | grep 1ed2\\n> 0000:01:00.0 1200: 1ed2:0000 (rev 01)\\n> \\n> ```\\n\\n`1ed2`\\nis the PCI vendor ID of FursioaAI Inc.\\n`01:00.0`\\nis the PCI BDF of a Warboy card in the above example.\\nYour PCI BDF will be different according to motherboard model, server model, and PCIe slot.\\n\\nAlternatively, you can use\\n`lspci\\n\\n-DD`\\ncommand to show a PCI BDF list\\nwith vendor names and find a Warboy card from the list.\\nThe vendor names depend on PCIe ID database in OS. If the database is outdated in OS,\\nthe command will show\\n`Device\\n\\n1ed2:0000`\\ninstead of\\n`FuriosaAI,\\n\\nInc.\\n\\nWarboy`\\n.\\nYou can update outdated PCIe ID database by running\\n`update-pciids`\\nin shell.\\n\\nOnce you find the PCIe BDB name, you can find a PCIe device name accepted by a virtual machine tool\\nas follows:\\n\\n> ```\\n> [root@localhost ~]# virsh nodedev-list | grep pci\\n> ...\\n> \\n> pci_0000_01_00_0\\n> \\n> ```\\n\\nA PCIe device name consists of\\n`pci_`\\nand a PCI BDF concatnated with\\n`_`\\n.\\nIn the above example,\\n`pci_0000_01_00_0`\\nis the PCIe device name of a Warboy card.\\n\\n\\n### 5. Creating a virtual machine [\\uf0c1](#creating-a-virtual-machine \"Permalink to this heading\")\\n\\nIf you reach here, you are ready to create a virtual machine with a Warboy passthrough device.\\nPlease create a virtual machine as follows.\\n\\n> ```\\n> virt-install --name ubuntu-vm \\\\\\n>   --os-variant ubuntu20.04 \\\\\\n>   --vcpus 2 \\\\\\n>   --memory 4096 \\\\\\n>   --location /var/lib/libvirt/images/ubuntu-20.04.5-live-server-amd64.iso,kernel=casper/vmlinuz,initrd=casper/initrd \\\\\\n>   --network bridge=br0,model=virtio \\\\\\n>   --disk size=50 \\\\\\n>   --graphics none \\\\\\n>   --host-device=pci_0000_01_00_0\\n> \\n> ```\\n\\nPlease note the option\\n`--host-device`\\nwith the PCIe device name\\nthat we found in the previous step.\\nAlso, you can add more options to the command for your use cases.\\n\\nIn the above example, we set the guest OS image.\\nSo, it will start the guest OS installation step once the virtual machine starts.\\nUbuntu 20.04 or above is recommended for a guest OS.\\nYou can find recommended OS distributions for FuriosaAI SDK at\\n[Minimum requirements for SDK installation](installation.html#minimumrequirements)\\n.\\n\\n\\n### 6. Checking the availability of a Warboy device in VM [\\uf0c1](#checking-the-availability-of-a-warboy-device-in-vm \"Permalink to this heading\")\\n\\nPlease make sure if the Warboy device is available on the virtual machine.\\n`lspci`\\nwill shows all PCIe devices available on the virtual machine as follows.\\n\\n> ```\\n> furiosa@ubuntu-vm:~$ lspci\\n> ...\\n> 05:00.0 Processing accelerators: Device 1ed2:0000 (rev 01)\\n> ...\\n> \\n> furiosa@ubuntu-vm:~$ sudo update-pciids\\n> \\n> furiosa@ubuntu-vm:~$ lspci | grep Furiosa\\n> 05:00.0 Processing accelerators: FuriosaAI, Inc. Warboy (rev 01)\\n> \\n> ```\\n\\n\\n### 7. SDK installation [\\uf0c1](#sdk-installation \"Permalink to this heading\")\\n\\nOnce you confirm that Warboy is available in a virtual machine,\\nplease install\\n[Driver, Firmware, and Runtime Installation](installation.html#requiredpackages)\\nto install SDK and move forward next steps.\\n\\n\\n\\n\\n\\n\\n[Previous](kubernetes_support.html \"Kubernetes Support\")\\n[Next](tutorials.html \"Tutorial and Code Examples\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Configuring Warboy Pass-through for Virtual Machine * [View page source](../_sources/software/vm_support.rst.txt)\\n---\\nConfiguring Warboy Pass-through for Virtual Machine [\\uf0c1](#configuring-warboy-pass-through-for-virtual-machine \"Permalink to this heading\") =========================================================================================================================================\\nThis section describes how to enable Warboy pass-through for a virtual machine. The example of this section is based on a specific VM tool `QEMU-KVM` , but it also works in other VM tools. The environment used in the example is as follows:\\n* Host OS: CentOS 8 * Guest OS: Ubuntu 20.04 * Virtual Machine: QEMU-KVM\\nPrerequisites [\\uf0c1](#prerequisites \"Permalink to this heading\") -------------------------------------------------------------\\n* IOMMU and VT-x should be enabled in BIOS. * `qemu-kvm`   ,   `libvirt`   ,   `virt-install`   should be installed in a host machine.\\nSetup Instruction [\\uf0c1](#setup-instruction \"Permalink to this heading\") ---------------------------------------------------------------------\\n### 1. Enabling IOMMU in BIOS and Linux OS [\\uf0c1](#enabling-iommu-in-bios-and-linux-os \"Permalink to this heading\")\\nFirst of all, you need to enable IOMMU in BIOS and Linux OS. The following command shows if IOMMU is enabled.\\n``` dmesg | grep -e DMAR -e IOMMU\\n```\\nYou will be able to see some messages related to DMAR or IOMMU if IOMMU is enabled. If you cannot find any messages related to DMAR or IOMMU, you need to enable IOMMU in BIOS, Linux OS or both.\\nThe ways to enable IOMMU in BIOS may depend on server or motherboard models. Please refer to the manufacturer’s manual.\\nYou check if IOMMU is enabled in Linux OS as follows:\\n``` grep GRUB_CMDLINE_LINUX /etc/default/grub | grep iommu\\n```\\nIf you cannot find any messages related to IOMMU, please add `intel_iommu=on` for Intel CPU or `amd_iommu=on` for AMD CPU to `GRUB_CMDLINE_LINUX` in `/etc/default/grub` and apply the changes by rebooting the machine.\\nIf you use a legacy BIOS boot mode or UEFI boot mode, the way to enable IOMMU in Linux OS can be different.\\n* Legacy BIOS boot mode:   `grub2-mkconfig      -o      /boot/grub2/grub.cfg` * UEFI boot mode,   `grub2-mkconfig      -o      /boot/efi/EFI/{linux_distrib}/grub.cfg`   .\\nPlease replace `{linux_distrib}` with a Linux OS name, such as `centos` , `redhat` , or `ubuntu` .\\n### 2. Loading `vfio-pci` module [\\uf0c1](#loading-vfio-pci-module \"Permalink to this heading\")\\nPlease make sure if the kernel module `vfio-pci` is loaded.\\n> ``` > [root@localhost ~]# lsmod | grep vfio_pci > vfio_pci               61440  0 > vfio_virqfd            16384  1 vfio_pci > vfio_iommu_type1       36864  0 > vfio                   36864  2 vfio_iommu_type1,vfio_pci > irqbypass              16384  2 vfio_pci,kvm >  > ```\\nIf `vfio_pci` is not loaded yet, please run `modprobe\\nvfio-pci` to load the module. In some OS environments, you don’t have to load `vfio-pci` . To make sure, please refer to the OS manual.\\n### 3. Checking if a virtual machine tool is ready [\\uf0c1](#checking-if-a-virtual-machine-tool-is-ready \"Permalink to this heading\")\\nPlease check if a virtual machine tool is ready to run as follows. If `virt-host-validate` is not found, please install the prerequisite packages described in [Prerequisites](#vmsupport-prerequisites)\\n> ``` > [root@localhost ~]# virt-host-validate >   QEMU: Checking for hardware virtualization                                 : PASS >  >   QEMU: Checking for device assignment IOMMU support                         : PASS >   QEMU: Checking if IOMMU is enabled by kernel                               : PASS >  > ```\\nIf check items are PASSED, the virtual machine tool is ready.\\n### 4. Finding Warboy’s PCIe device name [\\uf0c1](#finding-warboy-s-pcie-device-name \"Permalink to this heading\")\\nPCI BDF (Bus, Device, Function) is a unique identifier assigned to every PCIe device connected to a machine. Please find a PCI BDF of a Warboy card that you want to pass through to a virtual machine.\\n> ``` > [root@localhost ~]# lspci -nD | grep 1ed2 > 0000:01:00.0 1200: 1ed2:0000 (rev 01) >  > ```  `1ed2` is the PCI vendor ID of FursioaAI Inc. `01:00.0` is the PCI BDF of a Warboy card in the above example. Your PCI BDF will be different according to motherboard model, server model, and PCIe slot.\\nAlternatively, you can use `lspci\\n-DD` command to show a PCI BDF list with vendor names and find a Warboy card from the list. The vendor names depend on PCIe ID database in OS. If the database is outdated in OS, the command will show `Device\\n1ed2:0000` instead of `FuriosaAI,\\nInc.\\nWarboy` .\\nYou can update outdated PCIe ID database by running `update-pciids` in shell.\\nOnce you find the PCIe BDB name, you can find a PCIe device name accepted by a virtual machine tool as follows:\\n> ``` > [root@localhost ~]# virsh nodedev-list | grep pci > ... >  > pci_0000_01_00_0 >  > ```\\nA PCIe device name consists of `pci_` and a PCI BDF concatnated with `_` . In the above example, `pci_0000_01_00_0` is the PCIe device name of a Warboy card.\\n### 5. Creating a virtual machine [\\uf0c1](#creating-a-virtual-machine \"Permalink to this heading\")\\nIf you reach here, you are ready to create a virtual machine with a Warboy passthrough device. Please create a virtual machine as follows.\\n> ``` > virt-install --name ubuntu-vm \\\\ >   --os-variant ubuntu20.04 \\\\ >   --vcpus 2 \\\\ >   --memory 4096 \\\\ >   --location /var/lib/libvirt/images/ubuntu-20.04.5-live-server-amd64.iso,kernel=casper/vmlinuz,initrd=casper/initrd \\\\ >   --network bridge=br0,model=virtio \\\\ >   --disk size=50 \\\\ >   --graphics none \\\\ >   --host-device=pci_0000_01_00_0 >  > ```\\nPlease note the option `--host-device` with the PCIe device name that we found in the previous step. Also, you can add more options to the command for your use cases.\\nIn the above example, we set the guest OS image. So, it will start the guest OS installation step once the virtual machine starts. Ubuntu 20.04 or above is recommended for a guest OS. You can find recommended OS distributions for FuriosaAI SDK at [Minimum requirements for SDK installation](installation.html#minimumrequirements) .\\n### 6. Checking the availability of a Warboy device in VM [\\uf0c1](#checking-the-availability-of-a-warboy-device-in-vm \"Permalink to this heading\")\\nPlease make sure if the Warboy device is available on the virtual machine. `lspci` will shows all PCIe devices available on the virtual machine as follows.\\n> ``` > furiosa@ubuntu-vm:~$ lspci > ... > 05:00.0 Processing accelerators: Device 1ed2:0000 (rev 01) > ... >  > furiosa@ubuntu-vm:~$ sudo update-pciids >  > furiosa@ubuntu-vm:~$ lspci | grep Furiosa > 05:00.0 Processing accelerators: FuriosaAI, Inc. Warboy (rev 01) >  > ```\\n### 7. SDK installation [\\uf0c1](#sdk-installation \"Permalink to this heading\")\\nOnce you confirm that Warboy is available in a virtual machine, please install [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) to install SDK and move forward next steps.\\n[Previous](kubernetes_support.html \"Kubernetes Support\") [Next](tutorials.html \"Tutorial and Code Examples\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Configuring Warboy Pass-through for Virtual Machine\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/vm_support.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"configuring-warboy-pass-through-for-virtual-machine\">\\n     <span id=\"vmsupport\">\\n     </span>\\n     <h1>\\n      Configuring Warboy Pass-through for Virtual Machine\\n      <a class=\"headerlink\" href=\"#configuring-warboy-pass-through-for-virtual-machine\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      This section describes how to enable Warboy pass-through for a virtual machine.\\nThe example of this section is based on a specific VM tool\\n      <code class=\"docutils literal notranslate\">\\n       <span class=\"pre\">\\n        QEMU-KVM\\n       </span>\\n      </code>\\n      ,\\nbut it also works in other VM tools. The environment used in the example is as follows:\\n     </p>\\n     <ul class=\"simple\">\\n      <li>\\n       <p>\\n        Host OS: CentOS 8\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Guest OS: Ubuntu 20.04\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Virtual Machine: QEMU-KVM\\n       </p>\\n      </li>\\n     </ul>\\n     <section id=\"prerequisites\">\\n      <span id=\"vmsupport-prerequisites\">\\n      </span>\\n      <h2>\\n       Prerequisites\\n       <a class=\"headerlink\" href=\"#prerequisites\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         IOMMU and VT-x should be enabled in BIOS.\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           qemu-kvm\\n          </span>\\n         </code>\\n         ,\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           libvirt\\n          </span>\\n         </code>\\n         ,\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           virt-install\\n          </span>\\n         </code>\\n         should be installed in a host machine.\\n        </p>\\n       </li>\\n      </ul>\\n     </section>\\n     <section id=\"setup-instruction\">\\n      <h2>\\n       Setup Instruction\\n       <a class=\"headerlink\" href=\"#setup-instruction\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <section id=\"enabling-iommu-in-bios-and-linux-os\">\\n       <h3>\\n        1. Enabling IOMMU in BIOS and Linux OS\\n        <a class=\"headerlink\" href=\"#enabling-iommu-in-bios-and-linux-os\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        First of all, you need to enable IOMMU in BIOS and Linux OS.\\nThe following command shows if IOMMU is enabled.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>dmesg<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>grep<span class=\"w\"> </span>-e<span class=\"w\"> </span>DMAR<span class=\"w\"> </span>-e<span class=\"w\"> </span>IOMMU\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        You will be able to see some messages related to DMAR or IOMMU if IOMMU is enabled.\\nIf you cannot find any messages related to DMAR or IOMMU, you need to enable IOMMU\\nin BIOS, Linux OS or both.\\n       </p>\\n       <p>\\n        The ways to enable IOMMU in BIOS may depend on server or motherboard models.\\nPlease refer to the manufacturer’s manual.\\n       </p>\\n       <p>\\n        You check if IOMMU is enabled in Linux OS as follows:\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>grep<span class=\"w\"> </span>GRUB_CMDLINE_LINUX<span class=\"w\"> </span>/etc/default/grub<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>grep<span class=\"w\"> </span>iommu\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        If you cannot find any messages related to IOMMU,\\nplease add\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          intel_iommu=on\\n         </span>\\n        </code>\\n        for Intel CPU or\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          amd_iommu=on\\n         </span>\\n        </code>\\n        for AMD CPU\\nto\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          GRUB_CMDLINE_LINUX\\n         </span>\\n        </code>\\n        in\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          /etc/default/grub\\n         </span>\\n        </code>\\n        and apply the changes by rebooting the machine.\\n       </p>\\n       <p>\\n        If you use a legacy BIOS boot mode or UEFI boot mode, the way to enable IOMMU\\nin Linux OS can be different.\\n       </p>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          Legacy BIOS boot mode:\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            grub2-mkconfig\\n           </span>\\n           <span class=\"pre\">\\n            -o\\n           </span>\\n           <span class=\"pre\">\\n            /boot/grub2/grub.cfg\\n           </span>\\n          </code>\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          UEFI boot mode,\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            grub2-mkconfig\\n           </span>\\n           <span class=\"pre\">\\n            -o\\n           </span>\\n           <span class=\"pre\">\\n            /boot/efi/EFI/{linux_distrib}/grub.cfg\\n           </span>\\n          </code>\\n          .\\n         </p>\\n        </li>\\n       </ul>\\n       <p>\\n        Please replace\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          {linux_distrib}\\n         </span>\\n        </code>\\n        with a Linux OS name, such as\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          centos\\n         </span>\\n        </code>\\n        ,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          redhat\\n         </span>\\n        </code>\\n        , or\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          ubuntu\\n         </span>\\n        </code>\\n        .\\n       </p>\\n      </section>\\n      <section id=\"loading-vfio-pci-module\">\\n       <h3>\\n        2. Loading\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          vfio-pci\\n         </span>\\n        </code>\\n        module\\n        <a class=\"headerlink\" href=\"#loading-vfio-pci-module\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Please make sure if the kernel module\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          vfio-pci\\n         </span>\\n        </code>\\n        is loaded.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"p\">[</span><span class=\"n\">root</span><span class=\"nd\">@localhost</span> <span class=\"o\">~</span><span class=\"p\">]</span><span class=\"c1\"># lsmod | grep vfio_pci</span>\\n<span class=\"n\">vfio_pci</span>               <span class=\"mi\">61440</span>  <span class=\"mi\">0</span>\\n<span class=\"n\">vfio_virqfd</span>            <span class=\"mi\">16384</span>  <span class=\"mi\">1</span> <span class=\"n\">vfio_pci</span>\\n<span class=\"n\">vfio_iommu_type1</span>       <span class=\"mi\">36864</span>  <span class=\"mi\">0</span>\\n<span class=\"n\">vfio</span>                   <span class=\"mi\">36864</span>  <span class=\"mi\">2</span> <span class=\"n\">vfio_iommu_type1</span><span class=\"p\">,</span><span class=\"n\">vfio_pci</span>\\n<span class=\"n\">irqbypass</span>              <span class=\"mi\">16384</span>  <span class=\"mi\">2</span> <span class=\"n\">vfio_pci</span><span class=\"p\">,</span><span class=\"n\">kvm</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        If\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          vfio_pci\\n         </span>\\n        </code>\\n        is not loaded yet, please run\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          modprobe\\n         </span>\\n         <span class=\"pre\">\\n          vfio-pci\\n         </span>\\n        </code>\\n        to load the module.\\nIn some OS environments, you don’t have to load\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          vfio-pci\\n         </span>\\n        </code>\\n        .\\nTo make sure, please refer to the OS manual.\\n       </p>\\n      </section>\\n      <section id=\"checking-if-a-virtual-machine-tool-is-ready\">\\n       <h3>\\n        3. Checking if a virtual machine tool is ready\\n        <a class=\"headerlink\" href=\"#checking-if-a-virtual-machine-tool-is-ready\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Please check if a virtual machine tool is ready to run as follows.\\nIf\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          virt-host-validate\\n         </span>\\n        </code>\\n        is not found,\\nplease install the prerequisite packages described in\\n        <a class=\"reference internal\" href=\"#vmsupport-prerequisites\">\\n         <span class=\"std std-ref\">\\n          Prerequisites\\n         </span>\\n        </a>\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"p\">[</span><span class=\"n\">root</span><span class=\"nd\">@localhost</span> <span class=\"o\">~</span><span class=\"p\">]</span><span class=\"c1\"># virt-host-validate</span>\\n  <span class=\"n\">QEMU</span><span class=\"p\">:</span> <span class=\"n\">Checking</span> <span class=\"k\">for</span> <span class=\"n\">hardware</span> <span class=\"n\">virtualization</span>                                 <span class=\"p\">:</span> <span class=\"n\">PASS</span>\\n\\n  <span class=\"n\">QEMU</span><span class=\"p\">:</span> <span class=\"n\">Checking</span> <span class=\"k\">for</span> <span class=\"n\">device</span> <span class=\"n\">assignment</span> <span class=\"n\">IOMMU</span> <span class=\"n\">support</span>                         <span class=\"p\">:</span> <span class=\"n\">PASS</span>\\n  <span class=\"n\">QEMU</span><span class=\"p\">:</span> <span class=\"n\">Checking</span> <span class=\"k\">if</span> <span class=\"n\">IOMMU</span> <span class=\"ow\">is</span> <span class=\"n\">enabled</span> <span class=\"n\">by</span> <span class=\"n\">kernel</span>                               <span class=\"p\">:</span> <span class=\"n\">PASS</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        If check items are PASSED, the virtual machine tool is ready.\\n       </p>\\n      </section>\\n      <section id=\"finding-warboy-s-pcie-device-name\">\\n       <h3>\\n        4. Finding Warboy’s PCIe device name\\n        <a class=\"headerlink\" href=\"#finding-warboy-s-pcie-device-name\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        PCI BDF (Bus, Device, Function)\\nis a unique identifier assigned to every PCIe device connected to a machine.\\nPlease find a PCI BDF of a Warboy card that you want to pass through to a virtual machine.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"p\">[</span><span class=\"n\">root</span><span class=\"nd\">@localhost</span> <span class=\"o\">~</span><span class=\"p\">]</span><span class=\"c1\"># lspci -nD | grep 1ed2</span>\\n<span class=\"mi\">0000</span><span class=\"p\">:</span><span class=\"mi\">01</span><span class=\"p\">:</span><span class=\"mf\">00.0</span> <span class=\"mi\">1200</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"n\">ed2</span><span class=\"p\">:</span><span class=\"mi\">0000</span> <span class=\"p\">(</span><span class=\"n\">rev</span> <span class=\"mi\">01</span><span class=\"p\">)</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          1ed2\\n         </span>\\n        </code>\\n        is the PCI vendor ID of FursioaAI Inc.\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          01:00.0\\n         </span>\\n        </code>\\n        is the PCI BDF of a Warboy card in the above example.\\nYour PCI BDF will be different according to motherboard model, server model, and PCIe slot.\\n       </p>\\n       <p>\\n        Alternatively, you can use\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          lspci\\n         </span>\\n         <span class=\"pre\">\\n          -DD\\n         </span>\\n        </code>\\n        command to show a PCI BDF list\\nwith vendor names and find a Warboy card from the list.\\nThe vendor names depend on PCIe ID database in OS. If the database is outdated in OS,\\nthe command will show\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Device\\n         </span>\\n         <span class=\"pre\">\\n          1ed2:0000\\n         </span>\\n        </code>\\n        instead of\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          FuriosaAI,\\n         </span>\\n         <span class=\"pre\">\\n          Inc.\\n         </span>\\n         <span class=\"pre\">\\n          Warboy\\n         </span>\\n        </code>\\n        .\\nYou can update outdated PCIe ID database by running\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          update-pciids\\n         </span>\\n        </code>\\n        in shell.\\n       </p>\\n       <p>\\n        Once you find the PCIe BDB name, you can find a PCIe device name accepted by a virtual machine tool\\nas follows:\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"p\">[</span><span class=\"n\">root</span><span class=\"nd\">@localhost</span> <span class=\"o\">~</span><span class=\"p\">]</span><span class=\"c1\"># virsh nodedev-list | grep pci</span>\\n<span class=\"o\">...</span>\\n\\n<span class=\"n\">pci_0000_01_00_0</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        A PCIe device name consists of\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          pci_\\n         </span>\\n        </code>\\n        and a PCI BDF concatnated with\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          _\\n         </span>\\n        </code>\\n        .\\nIn the above example,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          pci_0000_01_00_0\\n         </span>\\n        </code>\\n        is the PCIe device name of a Warboy card.\\n       </p>\\n      </section>\\n      <section id=\"creating-a-virtual-machine\">\\n       <h3>\\n        5. Creating a virtual machine\\n        <a class=\"headerlink\" href=\"#creating-a-virtual-machine\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        If you reach here, you are ready to create a virtual machine with a Warboy passthrough device.\\nPlease create a virtual machine as follows.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"n\">virt</span><span class=\"o\">-</span><span class=\"n\">install</span> <span class=\"o\">--</span><span class=\"n\">name</span> <span class=\"n\">ubuntu</span><span class=\"o\">-</span><span class=\"n\">vm</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">os</span><span class=\"o\">-</span><span class=\"n\">variant</span> <span class=\"n\">ubuntu20</span><span class=\"mf\">.04</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">vcpus</span> <span class=\"mi\">2</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">memory</span> <span class=\"mi\">4096</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">location</span> <span class=\"o\">/</span><span class=\"n\">var</span><span class=\"o\">/</span><span class=\"n\">lib</span><span class=\"o\">/</span><span class=\"n\">libvirt</span><span class=\"o\">/</span><span class=\"n\">images</span><span class=\"o\">/</span><span class=\"n\">ubuntu</span><span class=\"o\">-</span><span class=\"mf\">20.04.5</span><span class=\"o\">-</span><span class=\"n\">live</span><span class=\"o\">-</span><span class=\"n\">server</span><span class=\"o\">-</span><span class=\"n\">amd64</span><span class=\"o\">.</span><span class=\"n\">iso</span><span class=\"p\">,</span><span class=\"n\">kernel</span><span class=\"o\">=</span><span class=\"n\">casper</span><span class=\"o\">/</span><span class=\"n\">vmlinuz</span><span class=\"p\">,</span><span class=\"n\">initrd</span><span class=\"o\">=</span><span class=\"n\">casper</span><span class=\"o\">/</span><span class=\"n\">initrd</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">network</span> <span class=\"n\">bridge</span><span class=\"o\">=</span><span class=\"n\">br0</span><span class=\"p\">,</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">virtio</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">disk</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"mi\">50</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">graphics</span> <span class=\"n\">none</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">host</span><span class=\"o\">-</span><span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">pci_0000_01_00_0</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        Please note the option\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          --host-device\\n         </span>\\n        </code>\\n        with the PCIe device name\\nthat we found in the previous step.\\nAlso, you can add more options to the command for your use cases.\\n       </p>\\n       <p>\\n        In the above example, we set the guest OS image.\\nSo, it will start the guest OS installation step once the virtual machine starts.\\nUbuntu 20.04 or above is recommended for a guest OS.\\nYou can find recommended OS distributions for FuriosaAI SDK at\\n        <a class=\"reference internal\" href=\"installation.html#minimumrequirements\">\\n         <span class=\"std std-ref\">\\n          Minimum requirements for SDK installation\\n         </span>\\n        </a>\\n        .\\n       </p>\\n      </section>\\n      <section id=\"checking-the-availability-of-a-warboy-device-in-vm\">\\n       <h3>\\n        6. Checking the availability of a Warboy device in VM\\n        <a class=\"headerlink\" href=\"#checking-the-availability-of-a-warboy-device-in-vm\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Please make sure if the Warboy device is available on the virtual machine.\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          lspci\\n         </span>\\n        </code>\\n        will shows all PCIe devices available on the virtual machine as follows.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span>furiosa@ubuntu-vm:~$ lspci\\n...\\n05:00.0 Processing accelerators: Device 1ed2:0000 (rev 01)\\n...\\n\\nfuriosa@ubuntu-vm:~$ sudo update-pciids\\n\\nfuriosa@ubuntu-vm:~$ lspci | grep Furiosa\\n05:00.0 Processing accelerators: FuriosaAI, Inc. Warboy (rev 01)\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n      </section>\\n      <section id=\"sdk-installation\">\\n       <h3>\\n        7. SDK installation\\n        <a class=\"headerlink\" href=\"#sdk-installation\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Once you confirm that Warboy is available in a virtual machine,\\nplease install\\n        <a class=\"reference internal\" href=\"installation.html#requiredpackages\">\\n         <span class=\"std std-ref\">\\n          Driver, Firmware, and Runtime Installation\\n         </span>\\n        </a>\\n        to install SDK and move forward next steps.\\n       </p>\\n      </section>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"kubernetes_support.html\" rel=\"prev\" title=\"Kubernetes Support\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"tutorials.html\" rel=\"next\" title=\"Tutorial and Code Examples\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='cf227685-cc4e-420e-b21a-e7da166093e5' link=HttpUrl('https://furiosa-ai.github.io/docs/latest/en/software/vm_support.html') name='vm_support' parent='' child=[] description='\\n\\n\\n* Configuring Warboy Pass-through for Virtual Machine\\n* [View page source](../_sources/software/vm_support.rst.txt)\\n\\n---\\n\\n\\n\\nConfiguring Warboy Pass-through for Virtual Machine\\n[\\uf0c1](#configuring-warboy-pass-through-for-virtual-machine \"Permalink to this heading\")\\n=========================================================================================================================================\\n\\nThis section describes how to enable Warboy pass-through for a virtual machine.\\nThe example of this section is based on a specific VM tool\\n`QEMU-KVM`\\n,\\nbut it also works in other VM tools. The environment used in the example is as follows:\\n\\n* Host OS: CentOS 8\\n* Guest OS: Ubuntu 20.04\\n* Virtual Machine: QEMU-KVM\\n\\nPrerequisites\\n[\\uf0c1](#prerequisites \"Permalink to this heading\")\\n-------------------------------------------------------------\\n\\n* IOMMU and VT-x should be enabled in BIOS.\\n* `qemu-kvm`\\n  ,\\n  `libvirt`\\n  ,\\n  `virt-install`\\n  should be installed in a host machine.\\n\\nSetup Instruction\\n[\\uf0c1](#setup-instruction \"Permalink to this heading\")\\n---------------------------------------------------------------------\\n\\n### 1. Enabling IOMMU in BIOS and Linux OS [\\uf0c1](#enabling-iommu-in-bios-and-linux-os \"Permalink to this heading\")\\n\\nFirst of all, you need to enable IOMMU in BIOS and Linux OS.\\nThe following command shows if IOMMU is enabled.\\n\\n```\\ndmesg | grep -e DMAR -e IOMMU\\n\\n```\\n\\nYou will be able to see some messages related to DMAR or IOMMU if IOMMU is enabled.\\nIf you cannot find any messages related to DMAR or IOMMU, you need to enable IOMMU\\nin BIOS, Linux OS or both.\\n\\nThe ways to enable IOMMU in BIOS may depend on server or motherboard models.\\nPlease refer to the manufacturer’s manual.\\n\\nYou check if IOMMU is enabled in Linux OS as follows:\\n\\n```\\ngrep GRUB_CMDLINE_LINUX /etc/default/grub | grep iommu\\n\\n```\\n\\nIf you cannot find any messages related to IOMMU,\\nplease add\\n`intel_iommu=on`\\nfor Intel CPU or\\n`amd_iommu=on`\\nfor AMD CPU\\nto\\n`GRUB_CMDLINE_LINUX`\\nin\\n`/etc/default/grub`\\nand apply the changes by rebooting the machine.\\n\\nIf you use a legacy BIOS boot mode or UEFI boot mode, the way to enable IOMMU\\nin Linux OS can be different.\\n\\n* Legacy BIOS boot mode:\\n  `grub2-mkconfig\\n  \\n  -o\\n  \\n  /boot/grub2/grub.cfg`\\n* UEFI boot mode,\\n  `grub2-mkconfig\\n  \\n  -o\\n  \\n  /boot/efi/EFI/{linux_distrib}/grub.cfg`\\n  .\\n\\nPlease replace\\n`{linux_distrib}`\\nwith a Linux OS name, such as\\n`centos`\\n,\\n`redhat`\\n, or\\n`ubuntu`\\n.\\n\\n\\n### 2. Loading `vfio-pci` module [\\uf0c1](#loading-vfio-pci-module \"Permalink to this heading\")\\n\\nPlease make sure if the kernel module\\n`vfio-pci`\\nis loaded.\\n\\n> ```\\n> [root@localhost ~]# lsmod | grep vfio_pci\\n> vfio_pci               61440  0\\n> vfio_virqfd            16384  1 vfio_pci\\n> vfio_iommu_type1       36864  0\\n> vfio                   36864  2 vfio_iommu_type1,vfio_pci\\n> irqbypass              16384  2 vfio_pci,kvm\\n> \\n> ```\\n\\nIf\\n`vfio_pci`\\nis not loaded yet, please run\\n`modprobe\\n\\nvfio-pci`\\nto load the module.\\nIn some OS environments, you don’t have to load\\n`vfio-pci`\\n.\\nTo make sure, please refer to the OS manual.\\n\\n\\n### 3. Checking if a virtual machine tool is ready [\\uf0c1](#checking-if-a-virtual-machine-tool-is-ready \"Permalink to this heading\")\\n\\nPlease check if a virtual machine tool is ready to run as follows.\\nIf\\n`virt-host-validate`\\nis not found,\\nplease install the prerequisite packages described in\\n[Prerequisites](#vmsupport-prerequisites)\\n\\n> ```\\n> [root@localhost ~]# virt-host-validate\\n>   QEMU: Checking for hardware virtualization                                 : PASS\\n> \\n>   QEMU: Checking for device assignment IOMMU support                         : PASS\\n>   QEMU: Checking if IOMMU is enabled by kernel                               : PASS\\n> \\n> ```\\n\\nIf check items are PASSED, the virtual machine tool is ready.\\n\\n\\n### 4. Finding Warboy’s PCIe device name [\\uf0c1](#finding-warboy-s-pcie-device-name \"Permalink to this heading\")\\n\\nPCI BDF (Bus, Device, Function)\\nis a unique identifier assigned to every PCIe device connected to a machine.\\nPlease find a PCI BDF of a Warboy card that you want to pass through to a virtual machine.\\n\\n> ```\\n> [root@localhost ~]# lspci -nD | grep 1ed2\\n> 0000:01:00.0 1200: 1ed2:0000 (rev 01)\\n> \\n> ```\\n\\n`1ed2`\\nis the PCI vendor ID of FursioaAI Inc.\\n`01:00.0`\\nis the PCI BDF of a Warboy card in the above example.\\nYour PCI BDF will be different according to motherboard model, server model, and PCIe slot.\\n\\nAlternatively, you can use\\n`lspci\\n\\n-DD`\\ncommand to show a PCI BDF list\\nwith vendor names and find a Warboy card from the list.\\nThe vendor names depend on PCIe ID database in OS. If the database is outdated in OS,\\nthe command will show\\n`Device\\n\\n1ed2:0000`\\ninstead of\\n`FuriosaAI,\\n\\nInc.\\n\\nWarboy`\\n.\\nYou can update outdated PCIe ID database by running\\n`update-pciids`\\nin shell.\\n\\nOnce you find the PCIe BDB name, you can find a PCIe device name accepted by a virtual machine tool\\nas follows:\\n\\n> ```\\n> [root@localhost ~]# virsh nodedev-list | grep pci\\n> ...\\n> \\n> pci_0000_01_00_0\\n> \\n> ```\\n\\nA PCIe device name consists of\\n`pci_`\\nand a PCI BDF concatnated with\\n`_`\\n.\\nIn the above example,\\n`pci_0000_01_00_0`\\nis the PCIe device name of a Warboy card.\\n\\n\\n### 5. Creating a virtual machine [\\uf0c1](#creating-a-virtual-machine \"Permalink to this heading\")\\n\\nIf you reach here, you are ready to create a virtual machine with a Warboy passthrough device.\\nPlease create a virtual machine as follows.\\n\\n> ```\\n> virt-install --name ubuntu-vm \\\\\\n>   --os-variant ubuntu20.04 \\\\\\n>   --vcpus 2 \\\\\\n>   --memory 4096 \\\\\\n>   --location /var/lib/libvirt/images/ubuntu-20.04.5-live-server-amd64.iso,kernel=casper/vmlinuz,initrd=casper/initrd \\\\\\n>   --network bridge=br0,model=virtio \\\\\\n>   --disk size=50 \\\\\\n>   --graphics none \\\\\\n>   --host-device=pci_0000_01_00_0\\n> \\n> ```\\n\\nPlease note the option\\n`--host-device`\\nwith the PCIe device name\\nthat we found in the previous step.\\nAlso, you can add more options to the command for your use cases.\\n\\nIn the above example, we set the guest OS image.\\nSo, it will start the guest OS installation step once the virtual machine starts.\\nUbuntu 20.04 or above is recommended for a guest OS.\\nYou can find recommended OS distributions for FuriosaAI SDK at\\n[Minimum requirements for SDK installation](installation.html#minimumrequirements)\\n.\\n\\n\\n### 6. Checking the availability of a Warboy device in VM [\\uf0c1](#checking-the-availability-of-a-warboy-device-in-vm \"Permalink to this heading\")\\n\\nPlease make sure if the Warboy device is available on the virtual machine.\\n`lspci`\\nwill shows all PCIe devices available on the virtual machine as follows.\\n\\n> ```\\n> furiosa@ubuntu-vm:~$ lspci\\n> ...\\n> 05:00.0 Processing accelerators: Device 1ed2:0000 (rev 01)\\n> ...\\n> \\n> furiosa@ubuntu-vm:~$ sudo update-pciids\\n> \\n> furiosa@ubuntu-vm:~$ lspci | grep Furiosa\\n> 05:00.0 Processing accelerators: FuriosaAI, Inc. Warboy (rev 01)\\n> \\n> ```\\n\\n\\n### 7. SDK installation [\\uf0c1](#sdk-installation \"Permalink to this heading\")\\n\\nOnce you confirm that Warboy is available in a virtual machine,\\nplease install\\n[Driver, Firmware, and Runtime Installation](installation.html#requiredpackages)\\nto install SDK and move forward next steps.\\n\\n\\n\\n\\n\\n\\n[Previous](kubernetes_support.html \"Kubernetes Support\")\\n[Next](tutorials.html \"Tutorial and Code Examples\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n' description_clean='* Configuring Warboy Pass-through for Virtual Machine * [View page source](../_sources/software/vm_support.rst.txt)\\n---\\nConfiguring Warboy Pass-through for Virtual Machine [\\uf0c1](#configuring-warboy-pass-through-for-virtual-machine \"Permalink to this heading\") =========================================================================================================================================\\nThis section describes how to enable Warboy pass-through for a virtual machine. The example of this section is based on a specific VM tool `QEMU-KVM` , but it also works in other VM tools. The environment used in the example is as follows:\\n* Host OS: CentOS 8 * Guest OS: Ubuntu 20.04 * Virtual Machine: QEMU-KVM\\nPrerequisites [\\uf0c1](#prerequisites \"Permalink to this heading\") -------------------------------------------------------------\\n* IOMMU and VT-x should be enabled in BIOS. * `qemu-kvm`   ,   `libvirt`   ,   `virt-install`   should be installed in a host machine.\\nSetup Instruction [\\uf0c1](#setup-instruction \"Permalink to this heading\") ---------------------------------------------------------------------\\n### 1. Enabling IOMMU in BIOS and Linux OS [\\uf0c1](#enabling-iommu-in-bios-and-linux-os \"Permalink to this heading\")\\nFirst of all, you need to enable IOMMU in BIOS and Linux OS. The following command shows if IOMMU is enabled.\\n``` dmesg | grep -e DMAR -e IOMMU\\n```\\nYou will be able to see some messages related to DMAR or IOMMU if IOMMU is enabled. If you cannot find any messages related to DMAR or IOMMU, you need to enable IOMMU in BIOS, Linux OS or both.\\nThe ways to enable IOMMU in BIOS may depend on server or motherboard models. Please refer to the manufacturer’s manual.\\nYou check if IOMMU is enabled in Linux OS as follows:\\n``` grep GRUB_CMDLINE_LINUX /etc/default/grub | grep iommu\\n```\\nIf you cannot find any messages related to IOMMU, please add `intel_iommu=on` for Intel CPU or `amd_iommu=on` for AMD CPU to `GRUB_CMDLINE_LINUX` in `/etc/default/grub` and apply the changes by rebooting the machine.\\nIf you use a legacy BIOS boot mode or UEFI boot mode, the way to enable IOMMU in Linux OS can be different.\\n* Legacy BIOS boot mode:   `grub2-mkconfig      -o      /boot/grub2/grub.cfg` * UEFI boot mode,   `grub2-mkconfig      -o      /boot/efi/EFI/{linux_distrib}/grub.cfg`   .\\nPlease replace `{linux_distrib}` with a Linux OS name, such as `centos` , `redhat` , or `ubuntu` .\\n### 2. Loading `vfio-pci` module [\\uf0c1](#loading-vfio-pci-module \"Permalink to this heading\")\\nPlease make sure if the kernel module `vfio-pci` is loaded.\\n> ``` > [root@localhost ~]# lsmod | grep vfio_pci > vfio_pci               61440  0 > vfio_virqfd            16384  1 vfio_pci > vfio_iommu_type1       36864  0 > vfio                   36864  2 vfio_iommu_type1,vfio_pci > irqbypass              16384  2 vfio_pci,kvm >  > ```\\nIf `vfio_pci` is not loaded yet, please run `modprobe\\nvfio-pci` to load the module. In some OS environments, you don’t have to load `vfio-pci` . To make sure, please refer to the OS manual.\\n### 3. Checking if a virtual machine tool is ready [\\uf0c1](#checking-if-a-virtual-machine-tool-is-ready \"Permalink to this heading\")\\nPlease check if a virtual machine tool is ready to run as follows. If `virt-host-validate` is not found, please install the prerequisite packages described in [Prerequisites](#vmsupport-prerequisites)\\n> ``` > [root@localhost ~]# virt-host-validate >   QEMU: Checking for hardware virtualization                                 : PASS >  >   QEMU: Checking for device assignment IOMMU support                         : PASS >   QEMU: Checking if IOMMU is enabled by kernel                               : PASS >  > ```\\nIf check items are PASSED, the virtual machine tool is ready.\\n### 4. Finding Warboy’s PCIe device name [\\uf0c1](#finding-warboy-s-pcie-device-name \"Permalink to this heading\")\\nPCI BDF (Bus, Device, Function) is a unique identifier assigned to every PCIe device connected to a machine. Please find a PCI BDF of a Warboy card that you want to pass through to a virtual machine.\\n> ``` > [root@localhost ~]# lspci -nD | grep 1ed2 > 0000:01:00.0 1200: 1ed2:0000 (rev 01) >  > ```  `1ed2` is the PCI vendor ID of FursioaAI Inc. `01:00.0` is the PCI BDF of a Warboy card in the above example. Your PCI BDF will be different according to motherboard model, server model, and PCIe slot.\\nAlternatively, you can use `lspci\\n-DD` command to show a PCI BDF list with vendor names and find a Warboy card from the list. The vendor names depend on PCIe ID database in OS. If the database is outdated in OS, the command will show `Device\\n1ed2:0000` instead of `FuriosaAI,\\nInc.\\nWarboy` .\\nYou can update outdated PCIe ID database by running `update-pciids` in shell.\\nOnce you find the PCIe BDB name, you can find a PCIe device name accepted by a virtual machine tool as follows:\\n> ``` > [root@localhost ~]# virsh nodedev-list | grep pci > ... >  > pci_0000_01_00_0 >  > ```\\nA PCIe device name consists of `pci_` and a PCI BDF concatnated with `_` . In the above example, `pci_0000_01_00_0` is the PCIe device name of a Warboy card.\\n### 5. Creating a virtual machine [\\uf0c1](#creating-a-virtual-machine \"Permalink to this heading\")\\nIf you reach here, you are ready to create a virtual machine with a Warboy passthrough device. Please create a virtual machine as follows.\\n> ``` > virt-install --name ubuntu-vm \\\\ >   --os-variant ubuntu20.04 \\\\ >   --vcpus 2 \\\\ >   --memory 4096 \\\\ >   --location /var/lib/libvirt/images/ubuntu-20.04.5-live-server-amd64.iso,kernel=casper/vmlinuz,initrd=casper/initrd \\\\ >   --network bridge=br0,model=virtio \\\\ >   --disk size=50 \\\\ >   --graphics none \\\\ >   --host-device=pci_0000_01_00_0 >  > ```\\nPlease note the option `--host-device` with the PCIe device name that we found in the previous step. Also, you can add more options to the command for your use cases.\\nIn the above example, we set the guest OS image. So, it will start the guest OS installation step once the virtual machine starts. Ubuntu 20.04 or above is recommended for a guest OS. You can find recommended OS distributions for FuriosaAI SDK at [Minimum requirements for SDK installation](installation.html#minimumrequirements) .\\n### 6. Checking the availability of a Warboy device in VM [\\uf0c1](#checking-the-availability-of-a-warboy-device-in-vm \"Permalink to this heading\")\\nPlease make sure if the Warboy device is available on the virtual machine. `lspci` will shows all PCIe devices available on the virtual machine as follows.\\n> ``` > furiosa@ubuntu-vm:~$ lspci > ... > 05:00.0 Processing accelerators: Device 1ed2:0000 (rev 01) > ... >  > furiosa@ubuntu-vm:~$ sudo update-pciids >  > furiosa@ubuntu-vm:~$ lspci | grep Furiosa > 05:00.0 Processing accelerators: FuriosaAI, Inc. Warboy (rev 01) >  > ```\\n### 7. SDK installation [\\uf0c1](#sdk-installation \"Permalink to this heading\")\\nOnce you confirm that Warboy is available in a virtual machine, please install [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) to install SDK and move forward next steps.\\n[Previous](kubernetes_support.html \"Kubernetes Support\") [Next](tutorials.html \"Tutorial and Code Examples\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .' html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Configuring Warboy Pass-through for Virtual Machine\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/vm_support.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"configuring-warboy-pass-through-for-virtual-machine\">\\n     <span id=\"vmsupport\">\\n     </span>\\n     <h1>\\n      Configuring Warboy Pass-through for Virtual Machine\\n      <a class=\"headerlink\" href=\"#configuring-warboy-pass-through-for-virtual-machine\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      This section describes how to enable Warboy pass-through for a virtual machine.\\nThe example of this section is based on a specific VM tool\\n      <code class=\"docutils literal notranslate\">\\n       <span class=\"pre\">\\n        QEMU-KVM\\n       </span>\\n      </code>\\n      ,\\nbut it also works in other VM tools. The environment used in the example is as follows:\\n     </p>\\n     <ul class=\"simple\">\\n      <li>\\n       <p>\\n        Host OS: CentOS 8\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Guest OS: Ubuntu 20.04\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Virtual Machine: QEMU-KVM\\n       </p>\\n      </li>\\n     </ul>\\n     <section id=\"prerequisites\">\\n      <span id=\"vmsupport-prerequisites\">\\n      </span>\\n      <h2>\\n       Prerequisites\\n       <a class=\"headerlink\" href=\"#prerequisites\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         IOMMU and VT-x should be enabled in BIOS.\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           qemu-kvm\\n          </span>\\n         </code>\\n         ,\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           libvirt\\n          </span>\\n         </code>\\n         ,\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           virt-install\\n          </span>\\n         </code>\\n         should be installed in a host machine.\\n        </p>\\n       </li>\\n      </ul>\\n     </section>\\n     <section id=\"setup-instruction\">\\n      <h2>\\n       Setup Instruction\\n       <a class=\"headerlink\" href=\"#setup-instruction\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <section id=\"enabling-iommu-in-bios-and-linux-os\">\\n       <h3>\\n        1. Enabling IOMMU in BIOS and Linux OS\\n        <a class=\"headerlink\" href=\"#enabling-iommu-in-bios-and-linux-os\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        First of all, you need to enable IOMMU in BIOS and Linux OS.\\nThe following command shows if IOMMU is enabled.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>dmesg<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>grep<span class=\"w\"> </span>-e<span class=\"w\"> </span>DMAR<span class=\"w\"> </span>-e<span class=\"w\"> </span>IOMMU\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        You will be able to see some messages related to DMAR or IOMMU if IOMMU is enabled.\\nIf you cannot find any messages related to DMAR or IOMMU, you need to enable IOMMU\\nin BIOS, Linux OS or both.\\n       </p>\\n       <p>\\n        The ways to enable IOMMU in BIOS may depend on server or motherboard models.\\nPlease refer to the manufacturer’s manual.\\n       </p>\\n       <p>\\n        You check if IOMMU is enabled in Linux OS as follows:\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>grep<span class=\"w\"> </span>GRUB_CMDLINE_LINUX<span class=\"w\"> </span>/etc/default/grub<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>grep<span class=\"w\"> </span>iommu\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        If you cannot find any messages related to IOMMU,\\nplease add\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          intel_iommu=on\\n         </span>\\n        </code>\\n        for Intel CPU or\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          amd_iommu=on\\n         </span>\\n        </code>\\n        for AMD CPU\\nto\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          GRUB_CMDLINE_LINUX\\n         </span>\\n        </code>\\n        in\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          /etc/default/grub\\n         </span>\\n        </code>\\n        and apply the changes by rebooting the machine.\\n       </p>\\n       <p>\\n        If you use a legacy BIOS boot mode or UEFI boot mode, the way to enable IOMMU\\nin Linux OS can be different.\\n       </p>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          Legacy BIOS boot mode:\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            grub2-mkconfig\\n           </span>\\n           <span class=\"pre\">\\n            -o\\n           </span>\\n           <span class=\"pre\">\\n            /boot/grub2/grub.cfg\\n           </span>\\n          </code>\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          UEFI boot mode,\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            grub2-mkconfig\\n           </span>\\n           <span class=\"pre\">\\n            -o\\n           </span>\\n           <span class=\"pre\">\\n            /boot/efi/EFI/{linux_distrib}/grub.cfg\\n           </span>\\n          </code>\\n          .\\n         </p>\\n        </li>\\n       </ul>\\n       <p>\\n        Please replace\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          {linux_distrib}\\n         </span>\\n        </code>\\n        with a Linux OS name, such as\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          centos\\n         </span>\\n        </code>\\n        ,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          redhat\\n         </span>\\n        </code>\\n        , or\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          ubuntu\\n         </span>\\n        </code>\\n        .\\n       </p>\\n      </section>\\n      <section id=\"loading-vfio-pci-module\">\\n       <h3>\\n        2. Loading\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          vfio-pci\\n         </span>\\n        </code>\\n        module\\n        <a class=\"headerlink\" href=\"#loading-vfio-pci-module\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Please make sure if the kernel module\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          vfio-pci\\n         </span>\\n        </code>\\n        is loaded.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"p\">[</span><span class=\"n\">root</span><span class=\"nd\">@localhost</span> <span class=\"o\">~</span><span class=\"p\">]</span><span class=\"c1\"># lsmod | grep vfio_pci</span>\\n<span class=\"n\">vfio_pci</span>               <span class=\"mi\">61440</span>  <span class=\"mi\">0</span>\\n<span class=\"n\">vfio_virqfd</span>            <span class=\"mi\">16384</span>  <span class=\"mi\">1</span> <span class=\"n\">vfio_pci</span>\\n<span class=\"n\">vfio_iommu_type1</span>       <span class=\"mi\">36864</span>  <span class=\"mi\">0</span>\\n<span class=\"n\">vfio</span>                   <span class=\"mi\">36864</span>  <span class=\"mi\">2</span> <span class=\"n\">vfio_iommu_type1</span><span class=\"p\">,</span><span class=\"n\">vfio_pci</span>\\n<span class=\"n\">irqbypass</span>              <span class=\"mi\">16384</span>  <span class=\"mi\">2</span> <span class=\"n\">vfio_pci</span><span class=\"p\">,</span><span class=\"n\">kvm</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        If\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          vfio_pci\\n         </span>\\n        </code>\\n        is not loaded yet, please run\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          modprobe\\n         </span>\\n         <span class=\"pre\">\\n          vfio-pci\\n         </span>\\n        </code>\\n        to load the module.\\nIn some OS environments, you don’t have to load\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          vfio-pci\\n         </span>\\n        </code>\\n        .\\nTo make sure, please refer to the OS manual.\\n       </p>\\n      </section>\\n      <section id=\"checking-if-a-virtual-machine-tool-is-ready\">\\n       <h3>\\n        3. Checking if a virtual machine tool is ready\\n        <a class=\"headerlink\" href=\"#checking-if-a-virtual-machine-tool-is-ready\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Please check if a virtual machine tool is ready to run as follows.\\nIf\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          virt-host-validate\\n         </span>\\n        </code>\\n        is not found,\\nplease install the prerequisite packages described in\\n        <a class=\"reference internal\" href=\"#vmsupport-prerequisites\">\\n         <span class=\"std std-ref\">\\n          Prerequisites\\n         </span>\\n        </a>\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"p\">[</span><span class=\"n\">root</span><span class=\"nd\">@localhost</span> <span class=\"o\">~</span><span class=\"p\">]</span><span class=\"c1\"># virt-host-validate</span>\\n  <span class=\"n\">QEMU</span><span class=\"p\">:</span> <span class=\"n\">Checking</span> <span class=\"k\">for</span> <span class=\"n\">hardware</span> <span class=\"n\">virtualization</span>                                 <span class=\"p\">:</span> <span class=\"n\">PASS</span>\\n\\n  <span class=\"n\">QEMU</span><span class=\"p\">:</span> <span class=\"n\">Checking</span> <span class=\"k\">for</span> <span class=\"n\">device</span> <span class=\"n\">assignment</span> <span class=\"n\">IOMMU</span> <span class=\"n\">support</span>                         <span class=\"p\">:</span> <span class=\"n\">PASS</span>\\n  <span class=\"n\">QEMU</span><span class=\"p\">:</span> <span class=\"n\">Checking</span> <span class=\"k\">if</span> <span class=\"n\">IOMMU</span> <span class=\"ow\">is</span> <span class=\"n\">enabled</span> <span class=\"n\">by</span> <span class=\"n\">kernel</span>                               <span class=\"p\">:</span> <span class=\"n\">PASS</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        If check items are PASSED, the virtual machine tool is ready.\\n       </p>\\n      </section>\\n      <section id=\"finding-warboy-s-pcie-device-name\">\\n       <h3>\\n        4. Finding Warboy’s PCIe device name\\n        <a class=\"headerlink\" href=\"#finding-warboy-s-pcie-device-name\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        PCI BDF (Bus, Device, Function)\\nis a unique identifier assigned to every PCIe device connected to a machine.\\nPlease find a PCI BDF of a Warboy card that you want to pass through to a virtual machine.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"p\">[</span><span class=\"n\">root</span><span class=\"nd\">@localhost</span> <span class=\"o\">~</span><span class=\"p\">]</span><span class=\"c1\"># lspci -nD | grep 1ed2</span>\\n<span class=\"mi\">0000</span><span class=\"p\">:</span><span class=\"mi\">01</span><span class=\"p\">:</span><span class=\"mf\">00.0</span> <span class=\"mi\">1200</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"n\">ed2</span><span class=\"p\">:</span><span class=\"mi\">0000</span> <span class=\"p\">(</span><span class=\"n\">rev</span> <span class=\"mi\">01</span><span class=\"p\">)</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          1ed2\\n         </span>\\n        </code>\\n        is the PCI vendor ID of FursioaAI Inc.\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          01:00.0\\n         </span>\\n        </code>\\n        is the PCI BDF of a Warboy card in the above example.\\nYour PCI BDF will be different according to motherboard model, server model, and PCIe slot.\\n       </p>\\n       <p>\\n        Alternatively, you can use\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          lspci\\n         </span>\\n         <span class=\"pre\">\\n          -DD\\n         </span>\\n        </code>\\n        command to show a PCI BDF list\\nwith vendor names and find a Warboy card from the list.\\nThe vendor names depend on PCIe ID database in OS. If the database is outdated in OS,\\nthe command will show\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Device\\n         </span>\\n         <span class=\"pre\">\\n          1ed2:0000\\n         </span>\\n        </code>\\n        instead of\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          FuriosaAI,\\n         </span>\\n         <span class=\"pre\">\\n          Inc.\\n         </span>\\n         <span class=\"pre\">\\n          Warboy\\n         </span>\\n        </code>\\n        .\\nYou can update outdated PCIe ID database by running\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          update-pciids\\n         </span>\\n        </code>\\n        in shell.\\n       </p>\\n       <p>\\n        Once you find the PCIe BDB name, you can find a PCIe device name accepted by a virtual machine tool\\nas follows:\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"p\">[</span><span class=\"n\">root</span><span class=\"nd\">@localhost</span> <span class=\"o\">~</span><span class=\"p\">]</span><span class=\"c1\"># virsh nodedev-list | grep pci</span>\\n<span class=\"o\">...</span>\\n\\n<span class=\"n\">pci_0000_01_00_0</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        A PCIe device name consists of\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          pci_\\n         </span>\\n        </code>\\n        and a PCI BDF concatnated with\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          _\\n         </span>\\n        </code>\\n        .\\nIn the above example,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          pci_0000_01_00_0\\n         </span>\\n        </code>\\n        is the PCIe device name of a Warboy card.\\n       </p>\\n      </section>\\n      <section id=\"creating-a-virtual-machine\">\\n       <h3>\\n        5. Creating a virtual machine\\n        <a class=\"headerlink\" href=\"#creating-a-virtual-machine\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        If you reach here, you are ready to create a virtual machine with a Warboy passthrough device.\\nPlease create a virtual machine as follows.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"n\">virt</span><span class=\"o\">-</span><span class=\"n\">install</span> <span class=\"o\">--</span><span class=\"n\">name</span> <span class=\"n\">ubuntu</span><span class=\"o\">-</span><span class=\"n\">vm</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">os</span><span class=\"o\">-</span><span class=\"n\">variant</span> <span class=\"n\">ubuntu20</span><span class=\"mf\">.04</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">vcpus</span> <span class=\"mi\">2</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">memory</span> <span class=\"mi\">4096</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">location</span> <span class=\"o\">/</span><span class=\"n\">var</span><span class=\"o\">/</span><span class=\"n\">lib</span><span class=\"o\">/</span><span class=\"n\">libvirt</span><span class=\"o\">/</span><span class=\"n\">images</span><span class=\"o\">/</span><span class=\"n\">ubuntu</span><span class=\"o\">-</span><span class=\"mf\">20.04.5</span><span class=\"o\">-</span><span class=\"n\">live</span><span class=\"o\">-</span><span class=\"n\">server</span><span class=\"o\">-</span><span class=\"n\">amd64</span><span class=\"o\">.</span><span class=\"n\">iso</span><span class=\"p\">,</span><span class=\"n\">kernel</span><span class=\"o\">=</span><span class=\"n\">casper</span><span class=\"o\">/</span><span class=\"n\">vmlinuz</span><span class=\"p\">,</span><span class=\"n\">initrd</span><span class=\"o\">=</span><span class=\"n\">casper</span><span class=\"o\">/</span><span class=\"n\">initrd</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">network</span> <span class=\"n\">bridge</span><span class=\"o\">=</span><span class=\"n\">br0</span><span class=\"p\">,</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">virtio</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">disk</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"mi\">50</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">graphics</span> <span class=\"n\">none</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">host</span><span class=\"o\">-</span><span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">pci_0000_01_00_0</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        Please note the option\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          --host-device\\n         </span>\\n        </code>\\n        with the PCIe device name\\nthat we found in the previous step.\\nAlso, you can add more options to the command for your use cases.\\n       </p>\\n       <p>\\n        In the above example, we set the guest OS image.\\nSo, it will start the guest OS installation step once the virtual machine starts.\\nUbuntu 20.04 or above is recommended for a guest OS.\\nYou can find recommended OS distributions for FuriosaAI SDK at\\n        <a class=\"reference internal\" href=\"installation.html#minimumrequirements\">\\n         <span class=\"std std-ref\">\\n          Minimum requirements for SDK installation\\n         </span>\\n        </a>\\n        .\\n       </p>\\n      </section>\\n      <section id=\"checking-the-availability-of-a-warboy-device-in-vm\">\\n       <h3>\\n        6. Checking the availability of a Warboy device in VM\\n        <a class=\"headerlink\" href=\"#checking-the-availability-of-a-warboy-device-in-vm\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Please make sure if the Warboy device is available on the virtual machine.\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          lspci\\n         </span>\\n        </code>\\n        will shows all PCIe devices available on the virtual machine as follows.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span>furiosa@ubuntu-vm:~$ lspci\\n...\\n05:00.0 Processing accelerators: Device 1ed2:0000 (rev 01)\\n...\\n\\nfuriosa@ubuntu-vm:~$ sudo update-pciids\\n\\nfuriosa@ubuntu-vm:~$ lspci | grep Furiosa\\n05:00.0 Processing accelerators: FuriosaAI, Inc. Warboy (rev 01)\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n      </section>\\n      <section id=\"sdk-installation\">\\n       <h3>\\n        7. SDK installation\\n        <a class=\"headerlink\" href=\"#sdk-installation\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Once you confirm that Warboy is available in a virtual machine,\\nplease install\\n        <a class=\"reference internal\" href=\"installation.html#requiredpackages\">\\n         <span class=\"std std-ref\">\\n          Driver, Firmware, and Runtime Installation\\n         </span>\\n        </a>\\n        to install SDK and move forward next steps.\\n       </p>\\n      </section>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"kubernetes_support.html\" rel=\"prev\" title=\"Kubernetes Support\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"tutorials.html\" rel=\"next\" title=\"Tutorial and Code Examples\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'\n",
      "Total pages loaded: 23\n"
     ]
    }
   ],
   "source": [
    "# final_pages의 첫 번째 객체를 출력\n",
    "print(final_pages[0])\n",
    "\n",
    "# final_pages 전체 크기 확인\n",
    "print(f\"Total pages loaded: {len(final_pages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA 데이터셋 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# openAI key \n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markdownify import markdownify as md\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "\n",
    "class QAPair(BaseModel):\n",
    "    question: str = Field(description=\"Question generated by llm\")\n",
    "    answer: str = Field(description=\"Answer generated by llm\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=QAPair)\n",
    "\n",
    "\n",
    "# LLM 및 Prompt 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"As an expert in creating educational Question-Answer datasets, your task is to generate one high-quality QA pair based on the provided markdown content. \n",
    "\n",
    "Instructions:\n",
    "1. Carefully analyze the markdown content in CONTENT section, identifying key concepts, details, and information.\n",
    "2. Imagine you are a first-time visitor to a website and aim to create a challenging, abstract question that encourages deep engagement with the content.\n",
    "3. Ensure that the question is specific enough that it can only be answered by referencing the given markdown.\n",
    "4. Generate a concise, direct answer without introductory phrases like \"The markdown says\" or \"Here is...\".\n",
    "5. Output **only one** QA pair.\n",
    "6. {instructions}\n",
    "\n",
    "Desired Format:\n",
    "- Question: [Your abstract, content-specific question]\n",
    "- Answer: [Your precise, context-reliant answer]\n",
    "\n",
    "CONTENT:\n",
    "{content}\n",
    "\"\"\",\n",
    "    partial_variables={\"instructions\": output_parser.get_format_instructions()},\n",
    "    input_variables=[\"content\"],\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "def generate_qa(pages, qa_per_page=10):\n",
    "    \"\"\"\n",
    "    Generate QA pairs for each page and save them into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        pages (List[Page]): List of Page objects.\n",
    "        qa_per_page (int): Number of QA pairs to generate per page.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing generated QA pairs.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for page in pages:\n",
    "        markdown_content = md(page.html_content, strip=[\"img\"])  # Convert HTML to markdown, stripping <img> tags\n",
    "\n",
    "        # Generate QA pairs for each page\n",
    "        for _ in range(qa_per_page):\n",
    "            try:\n",
    "                qa_pair = chain.invoke({\"content\": markdown_content})\n",
    "                data.append({\n",
    "                    \"page_id\": page.id,\n",
    "                    \"link\": str(page.link),\n",
    "                    \"question\": qa_pair.question,\n",
    "                    \"answer\": qa_pair.answer,\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating QA for page {page.id}: {e}\")\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version #3\n",
    "- 중복 방지 \n",
    "- html -> 마크다운 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markdownify import markdownify as md\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class QAPair(BaseModel):\n",
    "    question: str = Field(description=\"Question generated by llm\")\n",
    "    answer: str = Field(description=\"Answer generated by llm\")\n",
    "\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=QAPair)\n",
    "\n",
    "# LLM 및 Prompt 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "prompt = PromptTemplate(template=\"\"\"\n",
    "As an expert in creating educational Question-Answer datasets, your task is to generate one high-quality QA pair based on the provided markdown content.\n",
    "\n",
    "Instructions:\n",
    "1. Carefully analyze the markdown content in CONTENT section, identifying key concepts, details, and information.\n",
    "2. Avoid generating a question that is similar to any previously generated questions listed in PREVIOUS_QUESTIONS.\n",
    "3. Imagine you are a first-time visitor to a website and aim to create a challenging, abstract question that encourages deep engagement with the content.\n",
    "4. Ensure that the question is specific enough that it can only be answered by referencing the given markdown.\n",
    "5. Generate a concise, direct answer without introductory phrases like \"The markdown says\" or \"Here is...\".\n",
    "6. Output the result in the following JSON format:\n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"question\": \"Your abstract, content-specific question\",\n",
    "    \"answer\": \"Your precise, context-reliant answer\"\n",
    "}}\n",
    "                        \n",
    "CONTENT:\n",
    "{content}\n",
    "\n",
    "PREVIOUS_QUESTIONS:\n",
    "{previous_questions}\n",
    "\"\"\",\n",
    "    input_variables=[\"content\", \"previous_questions\"],\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "def generate_qa(pages, qa_per_page=10):\n",
    "    \"\"\"\n",
    "    Generate QA pairs for each page and save them into a DataFrame.\n",
    "    Args:\n",
    "        pages (List[Page]): List of Page objects.\n",
    "        qa_per_page (int): Number of QA pairs to generate per page.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing generated QA pairs.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for page in pages:\n",
    "        markdown_content = md(page.html_content, strip=[\"img\"])  # Convert HTML to markdown, stripping <img> tags\n",
    "        previous_questions = set()\n",
    "\n",
    "        # Generate QA pairs for each page\n",
    "        for _ in range(qa_per_page):\n",
    "            try:\n",
    "                previous_questions_str = \"\\n\".join(previous_questions) if previous_questions else \"None\"\n",
    "                qa_pair = chain.invoke({\n",
    "                    \"content\": markdown_content,\n",
    "                    \"previous_questions\": previous_questions_str,\n",
    "                })\n",
    "\n",
    "                # 중복된 질문 방지\n",
    "                if qa_pair.question in previous_questions:\n",
    "                    print(f\"Duplicate question detected and skipped: {qa_pair.question}\")\n",
    "                    continue\n",
    "\n",
    "                # 데이터 추가\n",
    "                data.append({\n",
    "                    \"page_id\": page.id,\n",
    "                    \"link\": str(page.link),\n",
    "                    \"question\": qa_pair.question,\n",
    "                    \"answer\": qa_pair.answer,\n",
    "                })\n",
    "\n",
    "                # 현재 질문 저장\n",
    "                previous_questions.add(qa_pair.question)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating QA for page {page.id}: {e}\")\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자 이제 QA 데이터셋 생성해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>link</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cf227685-cc4e-420e-b21a-e7da166093e5</td>\n",
       "      <td>https://furiosa-ai.github.io/docs/latest/en/so...</td>\n",
       "      <td>What steps are necessary to ensure that a Warb...</td>\n",
       "      <td>First, enable IOMMU in both BIOS and Linux OS....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cf227685-cc4e-420e-b21a-e7da166093e5</td>\n",
       "      <td>https://furiosa-ai.github.io/docs/latest/en/so...</td>\n",
       "      <td>What is the significance of the PCI BDF in con...</td>\n",
       "      <td>The PCI BDF (Bus, Device, Function) is a uniqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cf227685-cc4e-420e-b21a-e7da166093e5</td>\n",
       "      <td>https://furiosa-ai.github.io/docs/latest/en/so...</td>\n",
       "      <td>What are the specific BIOS and Linux OS config...</td>\n",
       "      <td>In BIOS, IOMMU and VT-x should be enabled. In ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333851a4-2ea4-4903-87a2-0d50943faf1f</td>\n",
       "      <td>https://furiosa-ai.github.io/docs/latest/en/so...</td>\n",
       "      <td>How does the optimization of the 'Quantize' op...</td>\n",
       "      <td>The optimization of the 'Quantize' operator us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>333851a4-2ea4-4903-87a2-0d50943faf1f</td>\n",
       "      <td>https://furiosa-ai.github.io/docs/latest/en/so...</td>\n",
       "      <td>What are the key considerations and strategies...</td>\n",
       "      <td>Optimizing inference performance in a producti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>f6a8c152-6f4c-4ac0-98ae-fbe3093522c6</td>\n",
       "      <td>https://furiosa-ai.github.io/docs/latest/en/cu...</td>\n",
       "      <td>What steps should be taken if a model compilat...</td>\n",
       "      <td>File a bug report at the FuriosaAI customer se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>f6a8c152-6f4c-4ac0-98ae-fbe3093522c6</td>\n",
       "      <td>https://furiosa-ai.github.io/docs/latest/en/cu...</td>\n",
       "      <td>What is the default action recommended by Furi...</td>\n",
       "      <td>File a report to the Bug Report section of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>ca538c67-ce79-412f-bd1b-c0f73dd818e7</td>\n",
       "      <td>https://furiosa-ai.github.io/docs/latest/en/re...</td>\n",
       "      <td>What new support features were added to the Fu...</td>\n",
       "      <td>The 0.7.0 release added Linear and Nearest mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>ca538c67-ce79-412f-bd1b-c0f73dd818e7</td>\n",
       "      <td>https://furiosa-ai.github.io/docs/latest/en/re...</td>\n",
       "      <td>What functionality does the compiler cache int...</td>\n",
       "      <td>The compiler cache stores compiled binaries in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>ca538c67-ce79-412f-bd1b-c0f73dd818e7</td>\n",
       "      <td>https://furiosa-ai.github.io/docs/latest/en/re...</td>\n",
       "      <td>What improvements have been made to the quanti...</td>\n",
       "      <td>The quantization tools now allow inference of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 page_id  \\\n",
       "0   cf227685-cc4e-420e-b21a-e7da166093e5   \n",
       "1   cf227685-cc4e-420e-b21a-e7da166093e5   \n",
       "2   cf227685-cc4e-420e-b21a-e7da166093e5   \n",
       "3   333851a4-2ea4-4903-87a2-0d50943faf1f   \n",
       "4   333851a4-2ea4-4903-87a2-0d50943faf1f   \n",
       "..                                   ...   \n",
       "64  f6a8c152-6f4c-4ac0-98ae-fbe3093522c6   \n",
       "65  f6a8c152-6f4c-4ac0-98ae-fbe3093522c6   \n",
       "66  ca538c67-ce79-412f-bd1b-c0f73dd818e7   \n",
       "67  ca538c67-ce79-412f-bd1b-c0f73dd818e7   \n",
       "68  ca538c67-ce79-412f-bd1b-c0f73dd818e7   \n",
       "\n",
       "                                                 link  \\\n",
       "0   https://furiosa-ai.github.io/docs/latest/en/so...   \n",
       "1   https://furiosa-ai.github.io/docs/latest/en/so...   \n",
       "2   https://furiosa-ai.github.io/docs/latest/en/so...   \n",
       "3   https://furiosa-ai.github.io/docs/latest/en/so...   \n",
       "4   https://furiosa-ai.github.io/docs/latest/en/so...   \n",
       "..                                                ...   \n",
       "64  https://furiosa-ai.github.io/docs/latest/en/cu...   \n",
       "65  https://furiosa-ai.github.io/docs/latest/en/cu...   \n",
       "66  https://furiosa-ai.github.io/docs/latest/en/re...   \n",
       "67  https://furiosa-ai.github.io/docs/latest/en/re...   \n",
       "68  https://furiosa-ai.github.io/docs/latest/en/re...   \n",
       "\n",
       "                                             question  \\\n",
       "0   What steps are necessary to ensure that a Warb...   \n",
       "1   What is the significance of the PCI BDF in con...   \n",
       "2   What are the specific BIOS and Linux OS config...   \n",
       "3   How does the optimization of the 'Quantize' op...   \n",
       "4   What are the key considerations and strategies...   \n",
       "..                                                ...   \n",
       "64  What steps should be taken if a model compilat...   \n",
       "65  What is the default action recommended by Furi...   \n",
       "66  What new support features were added to the Fu...   \n",
       "67  What functionality does the compiler cache int...   \n",
       "68  What improvements have been made to the quanti...   \n",
       "\n",
       "                                               answer  \n",
       "0   First, enable IOMMU in both BIOS and Linux OS....  \n",
       "1   The PCI BDF (Bus, Device, Function) is a uniqu...  \n",
       "2   In BIOS, IOMMU and VT-x should be enabled. In ...  \n",
       "3   The optimization of the 'Quantize' operator us...  \n",
       "4   Optimizing inference performance in a producti...  \n",
       "..                                                ...  \n",
       "64  File a bug report at the FuriosaAI customer se...  \n",
       "65  File a report to the Bug Report section of the...  \n",
       "66  The 0.7.0 release added Linear and Nearest mod...  \n",
       "67  The compiler cache stores compiled binaries in...  \n",
       "68  The quantization tools now allow inference of ...  \n",
       "\n",
       "[69 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Usage\n",
    "qa_per_page = 3  # Number of QA pairs per page\n",
    "qa_df = generate_qa(final_pages, qa_per_page)\n",
    "qa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '/Users/jwlee-pro/Documents/Workspace_2025/projects/llm-rag-chatbot/data'\n",
    "# file_dir = f'{data_dir}/qa-rngd_sdk.csv'\n",
    "# qa_df.to_csv(file_dir, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_duplicates(qa_df):\n",
    "#     # 중복된 질문 제거\n",
    "#     qa_df = qa_df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
    "#     return qa_df\n",
    "\n",
    "# remove_duplicates(qa_df)\n",
    "# file_dir = f'{data_dir}/qa-rngd_sdk_v2.csv'\n",
    "# qa_df.to_csv(file_dir, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/jwlee-pro/Documents/Workspace_2025/projects/llm-rag-chatbot/data'\n",
    "file_dir = f'{data_dir}/qa-warboy_sdk_v3.csv'\n",
    "qa_df.to_csv(file_dir, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate QA of card page with RAGAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: I think generating QA pair with RAGAS has low quality (warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"FuriosaAI RNGD Ragas QA Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CustomDocument(id_='3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/intro.html', 'title': 'intro', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../_sources/furiosa_llm/intro.rst \"Download source file\") * .pdf\\nFuriosa LLM ===========\\nFuriosa LLM [#](#furiosa-llm \"Link to this heading\") ====================================================\\nFuriosa LLM provides a high-performance inference engine for LLM models and Multi-Modal LLM models, Furiosa LLM is designed to provide the state-of-the-art serving optimization. The features of Furiosa LLM includes:\\n* vLLM-compatible API * Efficient KV cache management with PagedAttention * Continuous batching of incoming requests in serving * Quantization: INT4, INT8, FP8, GPTQ, AWQ * Data Parallelism and Pipeline Parallelism across multiple NPUs * Tensor Parallelism (planned in release 2024.2) across multiple NPUs * OpenAI-compatible API server * Various decoding algorithms, greedy search, beam search, top-k/top-p, speculative decoding (planned) * HuggingFace model integration and hub support * HuggingFace PEFT support (planned)\\n[previous\\nRunning MLPerf™ Inference Benchmark](../getting_started/furiosa_mlperf.html \"previous page\") [next\\nReferences](references.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../_sources/furiosa_llm/intro.rst \"Download source file\") * .pdf\\nFuriosa LLM ===========\\nFuriosa LLM [#](#furiosa-llm \"Link to this heading\") ====================================================\\nFuriosa LLM provides a high-performance inference engine for LLM models and Multi-Modal LLM models, Furiosa LLM is designed to provide the state-of-the-art serving optimization. The features of Furiosa LLM includes:\\n* vLLM-compatible API * Efficient KV cache management with PagedAttention * Continuous batching of incoming requests in serving * Quantization: INT4, INT8, FP8, GPTQ, AWQ * Data Parallelism and Pipeline Parallelism across multiple NPUs * Tensor Parallelism (planned in release 2024.2) across multiple NPUs * OpenAI-compatible API server * Various decoding algorithms, greedy search, beam search, top-k/top-p, speculative decoding (planned) * HuggingFace model integration and hub support * HuggingFace PEFT support (planned)\\n[previous\\nRunning MLPerf™ Inference Benchmark](../getting_started/furiosa_mlperf.html \"previous page\") [next\\nReferences](references.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='dcd59fbc-fb76-4f34-b6ec-ea88a833b047', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/device_plugin.html', 'title': 'device_plugin', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../../_sources/cloud_native_toolkit/kubernetes/device_plugin.rst \"Download source file\") * .pdf\\nInstalling Furiosa Device Plugin ================================\\nContents --------\\n* [Furiosa Device Plugin](#furiosa-device-plugin)   + [Configuration](#configuration)   + [Deploying Furiosa Device Plugin with Helm](#deploying-furiosa-device-plugin-with-helm)\\nInstalling Furiosa Device Plugin [#](#installing-furiosa-device-plugin \"Link to this heading\") ==============================================================================================\\nFuriosa Device Plugin [#](#furiosa-device-plugin \"Link to this heading\") ------------------------------------------------------------------------\\nThe Furiosa device plugin implements the [Kubernetes Device Plugin](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) interface for FuriosaAI NPU devices, and its features are as follows:\\n* discovering the Furiosa NPU devices and register to a Kubernetes cluster. * tracking the health of the devices and report to a Kubernetes cluster. * running AI workload on the top of the Furiosa NPU devices within a Kubernetes cluster.\\n### Configuration [#](#configuration \"Link to this heading\")\\nThe Furiosa NPU can be integrated into the Kubernetes cluster in various configurations. A single NPU card can either be exposed as a single resource or partitioned into multiple resources. Partitioning into multiple resources allows for more granular control.\\nThe following table shows the available resource strategy:\\nResource Strategy\\n[#](#id1 \"Link to this table\")\\n| NPU Configuration | Resource Name | Resource Count Per Card | | --- | --- | --- | | legacy | beta.furiosa.ai/npu | 1 | | generic | furiosa.ai/rngd | 1 |\\nThe helm chart of Furiosa device plugin is available at [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) .\\nFollowing shows default values of the helm chart.\\n``` config:   resourceStrategy: generic   debugMode: false   disabledDeviceUUIDListMap:\\n```\\n### Deploying Furiosa Device Plugin with Helm [#](#deploying-furiosa-device-plugin-with-helm \"Link to this heading\")\\nThe Furiosa device plugin helm chart is available at [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) . To configure deployment as you need, you can modify `charts/furiosa-device-plugin/values.yaml` .\\n* If resourceStrategy is not specified, the default value is   `\"generic\"`   . * If debugMode is not specified, the default value is   `false`   . * If disabledDeviceUUIDListMap is not specified, the default value is empty list   `[]`   .\\nYou can deploy the Furiosa Device Plugin by running the following commands:\\n``` helm repo add furiosa https://furiosa-ai.github.io/helm-charts helm repo update helm install furiosa-device-plugin furiosa/furiosa-device-plugin -n kube-system\\n```\\n[previous\\nInstalling Furiosa Feature Discovery](feature_discovery.html \"previous page\") [next\\nInstalling Furiosa Metrics Exporter](metrics_exporter.html \"next page\")\\nContents\\n* [Furiosa Device Plugin](#furiosa-device-plugin)   + [Configuration](#configuration)   + [Deploying Furiosa Device Plugin with Helm](#deploying-furiosa-device-plugin-with-helm)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../../_sources/cloud_native_toolkit/kubernetes/device_plugin.rst \"Download source file\") * .pdf\\nInstalling Furiosa Device Plugin ================================\\nContents --------\\n* [Furiosa Device Plugin](#furiosa-device-plugin)   + [Configuration](#configuration)   + [Deploying Furiosa Device Plugin with Helm](#deploying-furiosa-device-plugin-with-helm)\\nInstalling Furiosa Device Plugin [#](#installing-furiosa-device-plugin \"Link to this heading\") ==============================================================================================\\nFuriosa Device Plugin [#](#furiosa-device-plugin \"Link to this heading\") ------------------------------------------------------------------------\\nThe Furiosa device plugin implements the [Kubernetes Device Plugin](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) interface for FuriosaAI NPU devices, and its features are as follows:\\n* discovering the Furiosa NPU devices and register to a Kubernetes cluster. * tracking the health of the devices and report to a Kubernetes cluster. * running AI workload on the top of the Furiosa NPU devices within a Kubernetes cluster.\\n### Configuration [#](#configuration \"Link to this heading\")\\nThe Furiosa NPU can be integrated into the Kubernetes cluster in various configurations. A single NPU card can either be exposed as a single resource or partitioned into multiple resources. Partitioning into multiple resources allows for more granular control.\\nThe following table shows the available resource strategy:\\nResource Strategy\\n[#](#id1 \"Link to this table\")\\n| NPU Configuration | Resource Name | Resource Count Per Card | | --- | --- | --- | | legacy | beta.furiosa.ai/npu | 1 | | generic | furiosa.ai/rngd | 1 |\\nThe helm chart of Furiosa device plugin is available at [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) .\\nFollowing shows default values of the helm chart.\\n``` config:   resourceStrategy: generic   debugMode: false   disabledDeviceUUIDListMap:\\n```\\n### Deploying Furiosa Device Plugin with Helm [#](#deploying-furiosa-device-plugin-with-helm \"Link to this heading\")\\nThe Furiosa device plugin helm chart is available at [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) . To configure deployment as you need, you can modify `charts/furiosa-device-plugin/values.yaml` .\\n* If resourceStrategy is not specified, the default value is   `\"generic\"`   . * If debugMode is not specified, the default value is   `false`   . * If disabledDeviceUUIDListMap is not specified, the default value is empty list   `[]`   .\\nYou can deploy the Furiosa Device Plugin by running the following commands:\\n``` helm repo add furiosa https://furiosa-ai.github.io/helm-charts helm repo update helm install furiosa-device-plugin furiosa/furiosa-device-plugin -n kube-system\\n```\\n[previous\\nInstalling Furiosa Feature Discovery](feature_discovery.html \"previous page\") [next\\nInstalling Furiosa Metrics Exporter](metrics_exporter.html \"next page\")\\nContents\\n* [Furiosa Device Plugin](#furiosa-device-plugin)   + [Configuration](#configuration)   + [Deploying Furiosa Device Plugin with Helm](#deploying-furiosa-device-plugin-with-helm)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='3bb3bd0a-7bdc-45a7-8e3e-e556a52d7eda', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/furiosa_mlperf.html', 'title': 'furiosa_mlperf', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../_sources/getting_started/furiosa_mlperf.rst \"Download source file\") * .pdf\\nRunning MLPerf™ Inference Benchmark ===================================\\nContents --------\\n* [Installing   `furiosa-mlperf`   command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1)   + [SYNOPSIS](#synopsis)   + [Examples](#examples) * [Running   `furiosa-mlperf`   in container environment](#running-furiosa-mlperf-in-container-environment)\\nRunning MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark \"Link to this heading\") ===================================================================================================\\nMLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems.\\nFuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack.\\nNote  `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1.\\nThe only exception is that we replaced the Llama2 benchmark with one using Llama 3.1.\\nInstalling `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command \"Link to this heading\") --------------------------------------------------------------------------------------------------\\nTo install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following:\\nThe minimum requirements for `furiosa-mlperf` are as follows:\\n* Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers (   [Setting up APT server](prerequisites.html#aptsetup)   ) * About 100GB storage space (only for the Llama 3.1 70B)\\nThen, please install the `furiosa-mlperf` package as follows:\\n``` sudo apt update sudo apt install -y furiosa-mlperf\\n```\\nThis command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` .\\nRunning MLPerf Inference Benchmark [#](#id1 \"Link to this heading\") -------------------------------------------------------------------\\n### SYNOPSIS [#](#synopsis \"Link to this heading\")\\nThe `furiosa-mlperf` command provides the following subcommands:\\n``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z)\\nUsage: furiosa-mlperf <COMMAND>\\nCommands:   bert-offline       Run BERT benchmark with offline scenario   bert-server        Run BERT benchmark with server scenario   gpt-j-offline      Run GPT-J benchmark with offline scenario   gpt-j-server       Run GPT-J benchmark with server scenario   llama-3.1-offline  Run Llama 3.1 benchmark with offline scenario   llama-3.1-server   Run Llama 3.1 benchmark with server scenario   help               Print this message or the help of the given subcommand(s)\\nOptions:   -h, --help     Print help   -V, --version  Print version\\n```\\n### Examples [#](#examples \"Link to this heading\")\\n* BERT benchmark      The BERT benchmark is based on running with a single RNGD.      + Server Scenario          To run BERT Large serving inference benchmark, you can use the following command:          ```     furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh \"npu:0:*\"          ```   + Offline Scenario          To run BERT Large offline inference benchmark, you can use the following command:          ```     furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh \"npu:0:*\"          ``` * GPT-J benchmark      The GPT-J benchmark is based on running with a single RNGD.      + Server Scenario          To run GPT-J 6B serving inference benchmark, you can use the following command:          ```     furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result          ```   + Offline Scenario          To run GPT-J 6B offline inference benchmark, you can use the following command:          ```     furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result          ``` * Llama 3.1 benchmark      The Llama 3.1 benchmark is based on running with four RNGDs.      + Server Scenario          To run Llama 3.1 70B serving inference benchmark, you can use the following command:          ```     furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result          ```   + Offline Scenario          To run Llama 3.1 70B offline inference benchmark, you can use the following command:          ```     furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result          ``` * Common      Once the process completes, it writes the results to a file in the specified results directory.   You can open this file to view a summary of the results.      ```   cat gpt-j-offline-result/mlperf_log_summary.txt      ```         ```   ================================================   MLPerf Results Summary   ================================================   SUT name : GPT-J SUT   Scenario : Offline   Mode     : PerformanceOnly   Samples per second: 11.842   Tokens per second (inferred): 817.095   Result is : VALID     Min duration satisfied : Yes     Min queries satisfied : Yes     Early stopping satisfied: Yes      ```\\nRunning `furiosa-mlperf` in container environment [#](#running-furiosa-mlperf-in-container-environment \"Link to this heading\") ------------------------------------------------------------------------------------------------------------------------------\\nFuriosaAI provides a containerized version of the `furiosa-mlperf` command. If you use the containerized version, you can run the `furiosa-mlperf` command without installing the FuriosaAI Software Stack on your host system or you can run the command on Kubernetes environment.\\nTo run the `furiosa-mlperf` container, you can use the following command:\\n(Assumes model artifacts exist in `/opt/gpt-j-6b` directory)\\n``` $ docker run -it --rm --privileged \\\\   -v /opt/gpt-j-6b:gpt-j-6b furiosa-mlperf:2024.1.0 bash\\n(container) # furiosa-mlperf gpt-j-offline /gpt-j-6b /gpt-j-offline-result\\n```\\nTo run in a containerized environment, refer to the examples provided in this document.\\nWarning\\nThe example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit)\\n[previous\\nQuick Start with Furiosa LLM](furiosa_llm.html \"previous page\") [next\\nFuriosa LLM](../furiosa_llm/intro.html \"next page\")\\nContents\\n* [Installing   `furiosa-mlperf`   command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1)   + [SYNOPSIS](#synopsis)   + [Examples](#examples) * [Running   `furiosa-mlperf`   in container environment](#running-furiosa-mlperf-in-container-environment)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../_sources/getting_started/furiosa_mlperf.rst \"Download source file\") * .pdf\\nRunning MLPerf™ Inference Benchmark ===================================\\nContents --------\\n* [Installing   `furiosa-mlperf`   command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1)   + [SYNOPSIS](#synopsis)   + [Examples](#examples) * [Running   `furiosa-mlperf`   in container environment](#running-furiosa-mlperf-in-container-environment)\\nRunning MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark \"Link to this heading\") ===================================================================================================\\nMLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems.\\nFuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack.\\nNote  `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1.\\nThe only exception is that we replaced the Llama2 benchmark with one using Llama 3.1.\\nInstalling `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command \"Link to this heading\") --------------------------------------------------------------------------------------------------\\nTo install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following:\\nThe minimum requirements for `furiosa-mlperf` are as follows:\\n* Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers (   [Setting up APT server](prerequisites.html#aptsetup)   ) * About 100GB storage space (only for the Llama 3.1 70B)\\nThen, please install the `furiosa-mlperf` package as follows:\\n``` sudo apt update sudo apt install -y furiosa-mlperf\\n```\\nThis command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` .\\nRunning MLPerf Inference Benchmark [#](#id1 \"Link to this heading\") -------------------------------------------------------------------\\n### SYNOPSIS [#](#synopsis \"Link to this heading\")\\nThe `furiosa-mlperf` command provides the following subcommands:\\n``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z)\\nUsage: furiosa-mlperf <COMMAND>\\nCommands:   bert-offline       Run BERT benchmark with offline scenario   bert-server        Run BERT benchmark with server scenario   gpt-j-offline      Run GPT-J benchmark with offline scenario   gpt-j-server       Run GPT-J benchmark with server scenario   llama-3.1-offline  Run Llama 3.1 benchmark with offline scenario   llama-3.1-server   Run Llama 3.1 benchmark with server scenario   help               Print this message or the help of the given subcommand(s)\\nOptions:   -h, --help     Print help   -V, --version  Print version\\n```\\n### Examples [#](#examples \"Link to this heading\")\\n* BERT benchmark      The BERT benchmark is based on running with a single RNGD.      + Server Scenario          To run BERT Large serving inference benchmark, you can use the following command:          ```     furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh \"npu:0:*\"          ```   + Offline Scenario          To run BERT Large offline inference benchmark, you can use the following command:          ```     furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh \"npu:0:*\"          ``` * GPT-J benchmark      The GPT-J benchmark is based on running with a single RNGD.      + Server Scenario          To run GPT-J 6B serving inference benchmark, you can use the following command:          ```     furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result          ```   + Offline Scenario          To run GPT-J 6B offline inference benchmark, you can use the following command:          ```     furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result          ``` * Llama 3.1 benchmark      The Llama 3.1 benchmark is based on running with four RNGDs.      + Server Scenario          To run Llama 3.1 70B serving inference benchmark, you can use the following command:          ```     furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result          ```   + Offline Scenario          To run Llama 3.1 70B offline inference benchmark, you can use the following command:          ```     furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result          ``` * Common      Once the process completes, it writes the results to a file in the specified results directory.   You can open this file to view a summary of the results.      ```   cat gpt-j-offline-result/mlperf_log_summary.txt      ```         ```   ================================================   MLPerf Results Summary   ================================================   SUT name : GPT-J SUT   Scenario : Offline   Mode     : PerformanceOnly   Samples per second: 11.842   Tokens per second (inferred): 817.095   Result is : VALID     Min duration satisfied : Yes     Min queries satisfied : Yes     Early stopping satisfied: Yes      ```\\nRunning `furiosa-mlperf` in container environment [#](#running-furiosa-mlperf-in-container-environment \"Link to this heading\") ------------------------------------------------------------------------------------------------------------------------------\\nFuriosaAI provides a containerized version of the `furiosa-mlperf` command. If you use the containerized version, you can run the `furiosa-mlperf` command without installing the FuriosaAI Software Stack on your host system or you can run the command on Kubernetes environment.\\nTo run the `furiosa-mlperf` container, you can use the following command:\\n(Assumes model artifacts exist in `/opt/gpt-j-6b` directory)\\n``` $ docker run -it --rm --privileged \\\\   -v /opt/gpt-j-6b:gpt-j-6b furiosa-mlperf:2024.1.0 bash\\n(container) # furiosa-mlperf gpt-j-offline /gpt-j-6b /gpt-j-offline-result\\n```\\nTo run in a containerized environment, refer to the examples provided in this document.\\nWarning\\nThe example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit)\\n[previous\\nQuick Start with Furiosa LLM](furiosa_llm.html \"previous page\") [next\\nFuriosa LLM](../furiosa_llm/intro.html \"next page\")\\nContents\\n* [Installing   `furiosa-mlperf`   command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1)   + [SYNOPSIS](#synopsis)   + [Examples](#examples) * [Running   `furiosa-mlperf`   in container environment](#running-furiosa-mlperf-in-container-environment)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='a3d94379-304a-4dbc-8300-39169378bfd5', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/furiosa-llm-serve.html', 'title': 'furiosa-llm-serve', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../_sources/furiosa_llm/furiosa-llm-serve.rst \"Download source file\") * .pdf\\nOpenAI Compatible Server ========================\\nContents --------\\n* [Preparing Chat Templates](#preparing-chat-templates) * [Launching the Server](#launching-the-server)   + [Arguments for the serve command](#arguments-for-the-serve-command) * [Examples](#examples)   + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds)   + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api)\\nOpenAI Compatible Server [#](#openai-compatible-server \"Link to this heading\") ==============================================================================\\nThe `furiosa-llm` package includes an OpenAI-compatible server that can interact with OpenAI clients (and, of course, HTTP clients as well), supporting the `/v1/chat/completions` and `/v1/completions` APIs. In this section, we will explain how to launch the OpenAI-compatible furiosa-llm\\nserver.\\nTip\\nYou can learn more about the OpenAI API at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) .\\nTo launch the server, you must prepare: (1) the FuriosaAI LLM Engine artifact and (2) a chat template for the model. To download the FuriosaAI LLM Engine artifact, follow the instructions provided through our distribution channels or contact our sales team. To prepare the chat template, follow the instructions in the following section.\\nPreparing Chat Templates [#](#preparing-chat-templates \"Link to this heading\") ------------------------------------------------------------------------------\\nFuriosa SDK 2024.1.0 (alpha) uses Transformers v4.31.0, which does not include a chat template. Therefore, to support `/v1/chat/completions` , you must provide a chat template yourself. This constraint will be removed in future releases.\\nWarning\\nThis document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future.\\nIf you have access to the Llama repositories on Hugging Face, you can obtain the chat template for Llama using the following commands:\\n``` # Prerequisite: create a separate environment to install the latest Transformers version pip install \"transformers>=4.34.0\" python - <<EOF from transformers import AutoTokenizer tok = AutoTokenizer.from_pretrained(\\'meta-llama/Meta-Llama-3.1-70B-Instruct\\') with open(\\'chat_template.tpl\\', \\'w\\') as f:     f.write(tok.chat_template) EOF\\n```\\nLaunching the Server [#](#launching-the-server \"Link to this heading\") ----------------------------------------------------------------------\\nYou can launch the server using the furiosa-llm serve\\ncommand.\\n### Arguments for the serve command [#](#arguments-for-the-serve-command \"Link to this heading\")\\n``` usage: furiosa-llm serve [-h] --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} --artifact ARTIFACT [--host HOST] [--port PORT]                      --chat-template CHAT_TEMPLATE [--response-role RESPONSE_ROLE] [-pp PIPELINE_PARALLEL_SIZE] [-tp TENSOR_PARALLEL_SIZE] [--devices DEVICES]\\noptions: -h, --help            show this help message and exit --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm}                         The model to use. Currently only one model is supported per server. --artifact ARTIFACT   Path to Furiosa LLM Engine artifact --host HOST           Host to bind the server to --port PORT           Port to bind the server to --chat-template CHAT_TEMPLATE                         Path to chat template file (must be a jinja2 template) --response-role RESPONSE_ROLE                         Response role for /v1/chat/completions API (default: \\'assistant\\') -pp PIPELINE_PARALLEL_SIZE, --pipeline-parallel-size PIPELINE_PARALLEL_SIZE                         Number of pipeline stages. -tp TENSOR_PARALLEL_SIZE, --tensor-parallel-size TENSOR_PARALLEL_SIZE                         Number of tensor parallel replicas. --devices DEVICES     Devices to use (e.g. \"npu:0:*,npu:1:*\"). If unspecified, all available devices from the host will be used.\\n```\\nExamples [#](#examples \"Link to this heading\") ----------------------------------------------\\n### LLaMA-3.1-70B with 4 RNGDs [#](#llama-3-1-70b-with-4-rngds \"Link to this heading\")\\n``` furiosa-llm serve \\\\ --model {path to mlperf-llama-3-1-fp8-pp4} \\\\ -tp 4 -pp 4 --devices \"npu:0:*,npu:1:*,npu:2:*,npu:3:*\" \\\\ --chat-template {path to chat template}\\n```\\n### LLaMA-3.1-8B with Single RNGD [#](#llama-3-1-8b-with-single-rngd \"Link to this heading\")\\n``` furiosa-llm serve \\\\ --model {path to mlperf-llama-3-1-8b-fp8} \\\\ -tp 4 -pp 1 --devices \"npu:0:*\" \\\\  # You may choose arbitrary device index, if multiple devices are on host --chat-template {path to chat template}\\n```\\nUsing OpenAI Client [#](#using-openai-client \"Link to this heading\") --------------------------------------------------------------------\\nYou can use two APIs: `client.chat.completions` and `client.completions` . You can also set `stream=True` to receive a streaming response.\\nTip\\nYou can install the OpenAI Python client using the following command:\\n``` pip install openai\\n```\\n``` import openai\\nHOST = \"localhost:8000\" openai.api_base = f\"http://{HOST}/v1\" openai.api_key = \"0000\"\\nstream_chat_completion = openai.ChatCompletion.create(     model=\"\",     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},         {\"role\": \"user\", \"content\": \"What is the largest animal in the world?\"},     ],     stream=True, )\\nfor completion in stream_chat_completion:     content = completion.choices[0].delta.get(\"content\")     if content:         print(content, end=\"\")\\n```\\nThe compatibility with OpenAI API [#](#the-compatibility-with-openai-api \"Link to this heading\") ------------------------------------------------------------------------------------------------\\nCurrently, `furiosa\\nserve` supports the following OpenAI API parameters: You can find more about each parameter at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) .\\nWarning\\nPlease note that using `use_beam_search` with `stream` is not allowed because the beam search cannot determine the tokens until the end of the sequence.\\nIn 2024.1 release, `n` works only for beam search and it will be fixed in the next release.\\n* `n` * `temperature` * `top_p` * `top_k` * `early_stopping` * `length_penalty` * `max_tokens` * `min_tokens` * `use_beam_search` * `best_of` \\n[previous\\nFuriosa LLM](intro.html \"previous page\") [next\\nReferences](references.html \"next page\")\\nContents\\n* [Preparing Chat Templates](#preparing-chat-templates) * [Launching the Server](#launching-the-server)   + [Arguments for the serve command](#arguments-for-the-serve-command) * [Examples](#examples)   + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds)   + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../_sources/furiosa_llm/furiosa-llm-serve.rst \"Download source file\") * .pdf\\nOpenAI Compatible Server ========================\\nContents --------\\n* [Preparing Chat Templates](#preparing-chat-templates) * [Launching the Server](#launching-the-server)   + [Arguments for the serve command](#arguments-for-the-serve-command) * [Examples](#examples)   + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds)   + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api)\\nOpenAI Compatible Server [#](#openai-compatible-server \"Link to this heading\") ==============================================================================\\nThe `furiosa-llm` package includes an OpenAI-compatible server that can interact with OpenAI clients (and, of course, HTTP clients as well), supporting the `/v1/chat/completions` and `/v1/completions` APIs. In this section, we will explain how to launch the OpenAI-compatible furiosa-llm\\nserver.\\nTip\\nYou can learn more about the OpenAI API at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) .\\nTo launch the server, you must prepare: (1) the FuriosaAI LLM Engine artifact and (2) a chat template for the model. To download the FuriosaAI LLM Engine artifact, follow the instructions provided through our distribution channels or contact our sales team. To prepare the chat template, follow the instructions in the following section.\\nPreparing Chat Templates [#](#preparing-chat-templates \"Link to this heading\") ------------------------------------------------------------------------------\\nFuriosa SDK 2024.1.0 (alpha) uses Transformers v4.31.0, which does not include a chat template. Therefore, to support `/v1/chat/completions` , you must provide a chat template yourself. This constraint will be removed in future releases.\\nWarning\\nThis document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future.\\nIf you have access to the Llama repositories on Hugging Face, you can obtain the chat template for Llama using the following commands:\\n``` # Prerequisite: create a separate environment to install the latest Transformers version pip install \"transformers>=4.34.0\" python - <<EOF from transformers import AutoTokenizer tok = AutoTokenizer.from_pretrained(\\'meta-llama/Meta-Llama-3.1-70B-Instruct\\') with open(\\'chat_template.tpl\\', \\'w\\') as f:     f.write(tok.chat_template) EOF\\n```\\nLaunching the Server [#](#launching-the-server \"Link to this heading\") ----------------------------------------------------------------------\\nYou can launch the server using the furiosa-llm serve\\ncommand.\\n### Arguments for the serve command [#](#arguments-for-the-serve-command \"Link to this heading\")\\n``` usage: furiosa-llm serve [-h] --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} --artifact ARTIFACT [--host HOST] [--port PORT]                      --chat-template CHAT_TEMPLATE [--response-role RESPONSE_ROLE] [-pp PIPELINE_PARALLEL_SIZE] [-tp TENSOR_PARALLEL_SIZE] [--devices DEVICES]\\noptions: -h, --help            show this help message and exit --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm}                         The model to use. Currently only one model is supported per server. --artifact ARTIFACT   Path to Furiosa LLM Engine artifact --host HOST           Host to bind the server to --port PORT           Port to bind the server to --chat-template CHAT_TEMPLATE                         Path to chat template file (must be a jinja2 template) --response-role RESPONSE_ROLE                         Response role for /v1/chat/completions API (default: \\'assistant\\') -pp PIPELINE_PARALLEL_SIZE, --pipeline-parallel-size PIPELINE_PARALLEL_SIZE                         Number of pipeline stages. -tp TENSOR_PARALLEL_SIZE, --tensor-parallel-size TENSOR_PARALLEL_SIZE                         Number of tensor parallel replicas. --devices DEVICES     Devices to use (e.g. \"npu:0:*,npu:1:*\"). If unspecified, all available devices from the host will be used.\\n```\\nExamples [#](#examples \"Link to this heading\") ----------------------------------------------\\n### LLaMA-3.1-70B with 4 RNGDs [#](#llama-3-1-70b-with-4-rngds \"Link to this heading\")\\n``` furiosa-llm serve \\\\ --model {path to mlperf-llama-3-1-fp8-pp4} \\\\ -tp 4 -pp 4 --devices \"npu:0:*,npu:1:*,npu:2:*,npu:3:*\" \\\\ --chat-template {path to chat template}\\n```\\n### LLaMA-3.1-8B with Single RNGD [#](#llama-3-1-8b-with-single-rngd \"Link to this heading\")\\n``` furiosa-llm serve \\\\ --model {path to mlperf-llama-3-1-8b-fp8} \\\\ -tp 4 -pp 1 --devices \"npu:0:*\" \\\\  # You may choose arbitrary device index, if multiple devices are on host --chat-template {path to chat template}\\n```\\nUsing OpenAI Client [#](#using-openai-client \"Link to this heading\") --------------------------------------------------------------------\\nYou can use two APIs: `client.chat.completions` and `client.completions` . You can also set `stream=True` to receive a streaming response.\\nTip\\nYou can install the OpenAI Python client using the following command:\\n``` pip install openai\\n```\\n``` import openai\\nHOST = \"localhost:8000\" openai.api_base = f\"http://{HOST}/v1\" openai.api_key = \"0000\"\\nstream_chat_completion = openai.ChatCompletion.create(     model=\"\",     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},         {\"role\": \"user\", \"content\": \"What is the largest animal in the world?\"},     ],     stream=True, )\\nfor completion in stream_chat_completion:     content = completion.choices[0].delta.get(\"content\")     if content:         print(content, end=\"\")\\n```\\nThe compatibility with OpenAI API [#](#the-compatibility-with-openai-api \"Link to this heading\") ------------------------------------------------------------------------------------------------\\nCurrently, `furiosa\\nserve` supports the following OpenAI API parameters: You can find more about each parameter at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) .\\nWarning\\nPlease note that using `use_beam_search` with `stream` is not allowed because the beam search cannot determine the tokens until the end of the sequence.\\nIn 2024.1 release, `n` works only for beam search and it will be fixed in the next release.\\n* `n` * `temperature` * `top_p` * `top_k` * `early_stopping` * `length_penalty` * `max_tokens` * `min_tokens` * `use_beam_search` * `best_of` \\n[previous\\nFuriosa LLM](intro.html \"previous page\") [next\\nReferences](references.html \"next page\")\\nContents\\n* [Preparing Chat Templates](#preparing-chat-templates) * [Launching the Server](#launching-the-server)   + [Arguments for the serve command](#arguments-for-the-serve-command) * [Examples](#examples)   + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds)   + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='13853744-dc18-4c98-b1c2-4f5806b28514', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/', 'title': '', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](_sources/index.rst \"Download source file\") * .pdf\\nFuriosaAI Developer Center ==========================\\nContents --------\\n* [Overview](#overview) * [Getting Started](#getting-started) * [Cloud Native Toolkit](#cloud-native-toolkit) * [Device Management](#device-management) * [Customer Support](#customer-support)\\nFuriosaAI Developer Center [#](#furiosaai-developer-center \"Link to this heading\") ==================================================================================\\nWelcome to the FuriosaAI Developer Center. FuriosaAI provides an streamlined software stack for deep learning model inference on FuriosaAI NPUs. This document provides a guide to easily perform the entire workflow of writing inference applications, from starting with PyTorch model to model quantization, serving, and production deployment.\\nWarning\\nThis document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future.\\nOverview [#](#overview \"Link to this heading\") ----------------------------------------------\\n* [FuriosaAI RNGD](overview/rngd.html#rngd)   : RNGD Hardware Specification, and features * [FuriosaAI’s Software Stack](overview/software_stack.html#softwarestack)   : An overview of the FuriosaAI software stack * [Supported Models](overview/supported_models.html#supportedmodels)   : A list of supported models * [What’s New](whatsnew/index.html#whatsnew)   : New features and changes in the latest release * [Roadmap](overview/roadmap.html#roadmap)   : The future roadmap of FuriosaAI Software Stack\\nGetting Started [#](#getting-started \"Link to this heading\") ------------------------------------------------------------\\n* [Installing Prerequisites](getting_started/prerequisites.html#installingprerequisites)   : How to install the prerequisites for FuriosaAI Software Stack * [Quick Start with Furiosa LLM](getting_started/furiosa_llm.html#gettingstartedfuriosallm) * [Running MLPerf™ Inference Benchmark](getting_started/furiosa_mlperf.html#gettingstartedfuriosamlperf)\\nCloud Native Toolkit [#](#cloud-native-toolkit \"Link to this heading\") ----------------------------------------------------------------------\\n* [Cloud Native Toolkit](cloud_native_toolkit/intro.html#cloudnativetoolkit)   : An overview of the Cloud Native Toolkit * [Kubernetes Support](cloud_native_toolkit/kubernetes.html#kubernetes)   : An overview of the Kubernetes Support\\nDevice Management [#](#device-management \"Link to this heading\") ----------------------------------------------------------------\\n* [furiosa-smi](device_management/furiosa_smi.html#furiosasmi)   : A command line utility for managing FuriosaAI NPUs\\nCustomer Support [#](#customer-support \"Link to this heading\") --------------------------------------------------------------\\n* [FuriosaAI Homepage](https://furiosa.ai) * [FuriosaAI Forum](https://furiosa-ai.discourse.group/) * [FuriosaAI Customer Portal](https://furiosa-ai.atlassian.net/servicedesk/customer/portals/) * [FuriosaAI Warboy SDK Document](https://furiosa-ai.github.io/docs/latest/en/)\\n[next\\nFuriosaAI RNGD](overview/rngd.html \"next page\")\\nContents\\n* [Overview](#overview) * [Getting Started](#getting-started) * [Cloud Native Toolkit](#cloud-native-toolkit) * [Device Management](#device-management) * [Customer Support](#customer-support)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](_sources/index.rst \"Download source file\") * .pdf\\nFuriosaAI Developer Center ==========================\\nContents --------\\n* [Overview](#overview) * [Getting Started](#getting-started) * [Cloud Native Toolkit](#cloud-native-toolkit) * [Device Management](#device-management) * [Customer Support](#customer-support)\\nFuriosaAI Developer Center [#](#furiosaai-developer-center \"Link to this heading\") ==================================================================================\\nWelcome to the FuriosaAI Developer Center. FuriosaAI provides an streamlined software stack for deep learning model inference on FuriosaAI NPUs. This document provides a guide to easily perform the entire workflow of writing inference applications, from starting with PyTorch model to model quantization, serving, and production deployment.\\nWarning\\nThis document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future.\\nOverview [#](#overview \"Link to this heading\") ----------------------------------------------\\n* [FuriosaAI RNGD](overview/rngd.html#rngd)   : RNGD Hardware Specification, and features * [FuriosaAI’s Software Stack](overview/software_stack.html#softwarestack)   : An overview of the FuriosaAI software stack * [Supported Models](overview/supported_models.html#supportedmodels)   : A list of supported models * [What’s New](whatsnew/index.html#whatsnew)   : New features and changes in the latest release * [Roadmap](overview/roadmap.html#roadmap)   : The future roadmap of FuriosaAI Software Stack\\nGetting Started [#](#getting-started \"Link to this heading\") ------------------------------------------------------------\\n* [Installing Prerequisites](getting_started/prerequisites.html#installingprerequisites)   : How to install the prerequisites for FuriosaAI Software Stack * [Quick Start with Furiosa LLM](getting_started/furiosa_llm.html#gettingstartedfuriosallm) * [Running MLPerf™ Inference Benchmark](getting_started/furiosa_mlperf.html#gettingstartedfuriosamlperf)\\nCloud Native Toolkit [#](#cloud-native-toolkit \"Link to this heading\") ----------------------------------------------------------------------\\n* [Cloud Native Toolkit](cloud_native_toolkit/intro.html#cloudnativetoolkit)   : An overview of the Cloud Native Toolkit * [Kubernetes Support](cloud_native_toolkit/kubernetes.html#kubernetes)   : An overview of the Kubernetes Support\\nDevice Management [#](#device-management \"Link to this heading\") ----------------------------------------------------------------\\n* [furiosa-smi](device_management/furiosa_smi.html#furiosasmi)   : A command line utility for managing FuriosaAI NPUs\\nCustomer Support [#](#customer-support \"Link to this heading\") --------------------------------------------------------------\\n* [FuriosaAI Homepage](https://furiosa.ai) * [FuriosaAI Forum](https://furiosa-ai.discourse.group/) * [FuriosaAI Customer Portal](https://furiosa-ai.atlassian.net/servicedesk/customer/portals/) * [FuriosaAI Warboy SDK Document](https://furiosa-ai.github.io/docs/latest/en/)\\n[next\\nFuriosaAI RNGD](overview/rngd.html \"next page\")\\nContents\\n* [Overview](#overview) * [Getting Started](#getting-started) * [Cloud Native Toolkit](#cloud-native-toolkit) * [Device Management](#device-management) * [Customer Support](#customer-support)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='2b294227-f255-492c-b642-46e100e59705', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes.html', 'title': 'kubernetes', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../_sources/cloud_native_toolkit/kubernetes.rst \"Download source file\") * .pdf\\nKubernetes Support ==================\\nKubernetes Support [#](#kubernetes-support \"Link to this heading\") ==================================================================\\nWe do support the following versions of Kubernetes and CRI runtime:\\n* Kubernetes: v1.24.0 or later * helm v3.0.0 or later * CRI Runtime:   [containerd](https://github.com/containerd/containerd)   or   [CRI-O](https://github.com/cri-o/cri-o)\\nNote\\nDocker is officially deprecated as a container runtime in Kubernetes. It is recommended to use containerd or CRI-O as a container runtime. Otherwise you may face unexpected issues with the device plugin. For more information, see [here](https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/) .\\nKubernetes Support\\n* [Installing Furiosa Feature Discovery](kubernetes/feature_discovery.html) * [Installing Furiosa Device Plugin](kubernetes/device_plugin.html) * [Installing Furiosa Metrics Exporter](kubernetes/metrics_exporter.html) * [Scheduling NPUs](kubernetes/scheduling_npus.html)\\n[previous\\nCloud Native Toolkit](intro.html \"previous page\") [next\\nInstalling Furiosa Feature Discovery](kubernetes/feature_discovery.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../_sources/cloud_native_toolkit/kubernetes.rst \"Download source file\") * .pdf\\nKubernetes Support ==================\\nKubernetes Support [#](#kubernetes-support \"Link to this heading\") ==================================================================\\nWe do support the following versions of Kubernetes and CRI runtime:\\n* Kubernetes: v1.24.0 or later * helm v3.0.0 or later * CRI Runtime:   [containerd](https://github.com/containerd/containerd)   or   [CRI-O](https://github.com/cri-o/cri-o)\\nNote\\nDocker is officially deprecated as a container runtime in Kubernetes. It is recommended to use containerd or CRI-O as a container runtime. Otherwise you may face unexpected issues with the device plugin. For more information, see [here](https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/) .\\nKubernetes Support\\n* [Installing Furiosa Feature Discovery](kubernetes/feature_discovery.html) * [Installing Furiosa Device Plugin](kubernetes/device_plugin.html) * [Installing Furiosa Metrics Exporter](kubernetes/metrics_exporter.html) * [Scheduling NPUs](kubernetes/scheduling_npus.html)\\n[previous\\nCloud Native Toolkit](intro.html \"previous page\") [next\\nInstalling Furiosa Feature Discovery](kubernetes/feature_discovery.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='d4927eaf-a1a0-40bd-9624-79db4213c5fc', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/roadmap.html', 'title': 'roadmap', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../_sources/overview/roadmap.rst \"Download source file\") * .pdf\\nRoadmap =======\\nContents --------\\n* [Latest Recent Release](#latest-recent-release) * [Future Releases](#future-releases)   + [2024.2.0 (beta 0) - November, 2024](#beta-0-november-2024)   + [2024.3.0 (beta 1) - December, 2024](#beta-1-december-2024)\\nRoadmap [#](#roadmap \"Link to this heading\") ============================================\\nFurisaAI strives to deliver the releases for each month, while offering patch releases. This page shows the forward-looking roadmap of ongoing & upcoming projects and when they are expected to land, broken down by areas on [our software stack](software_stack.html#softwarestack) .\\nLatest Recent Release [#](#latest-recent-release \"Link to this heading\") ------------------------------------------------------------------------\\nThe latest release is 2024.1.0 (alpha) on October 11, 2024. You can find the release notes [here](../whatsnew/index.html#whatsnew) .\\nFuture Releases [#](#future-releases \"Link to this heading\") ------------------------------------------------------------\\nNote\\nThe roadmap is subject to change and may not reflect the final product.\\n### 2024.2.0 (beta 0) - November, 2024 [#](#beta-0-november-2024 \"Link to this heading\")\\n* Model support:      + Language Models: CodeLLaaMA2, Vicuna, Solar, EXAONE-3.0   + Vision Models: MobileNetV1, MobileNetV2, ResNet152, ResNet50, EfficientNet, YOLOv8m, .. * (Furiosa LLM) Tensor Parallelism support Phase 1: Intra-chip * Torch 2.4.1 support * CPU memory swapping in Furiosa LLM\\n### 2024.3.0 (beta 1) - December, 2024 [#](#beta-1-december-2024 \"Link to this heading\")\\n* Model support: TBD * (Furiosa LLM) Tensor Parallelism support Phase 2: Inter-chip * `torch.compile()`   backend * Huggingface Optimum integration\\n[previous\\nWhat’s New](../whatsnew/index.html \"previous page\") [next\\nInstalling Prerequisites](../getting_started/prerequisites.html \"next page\")\\nContents\\n* [Latest Recent Release](#latest-recent-release) * [Future Releases](#future-releases)   + [2024.2.0 (beta 0) - November, 2024](#beta-0-november-2024)   + [2024.3.0 (beta 1) - December, 2024](#beta-1-december-2024)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../_sources/overview/roadmap.rst \"Download source file\") * .pdf\\nRoadmap =======\\nContents --------\\n* [Latest Recent Release](#latest-recent-release) * [Future Releases](#future-releases)   + [2024.2.0 (beta 0) - November, 2024](#beta-0-november-2024)   + [2024.3.0 (beta 1) - December, 2024](#beta-1-december-2024)\\nRoadmap [#](#roadmap \"Link to this heading\") ============================================\\nFurisaAI strives to deliver the releases for each month, while offering patch releases. This page shows the forward-looking roadmap of ongoing & upcoming projects and when they are expected to land, broken down by areas on [our software stack](software_stack.html#softwarestack) .\\nLatest Recent Release [#](#latest-recent-release \"Link to this heading\") ------------------------------------------------------------------------\\nThe latest release is 2024.1.0 (alpha) on October 11, 2024. You can find the release notes [here](../whatsnew/index.html#whatsnew) .\\nFuture Releases [#](#future-releases \"Link to this heading\") ------------------------------------------------------------\\nNote\\nThe roadmap is subject to change and may not reflect the final product.\\n### 2024.2.0 (beta 0) - November, 2024 [#](#beta-0-november-2024 \"Link to this heading\")\\n* Model support:      + Language Models: CodeLLaaMA2, Vicuna, Solar, EXAONE-3.0   + Vision Models: MobileNetV1, MobileNetV2, ResNet152, ResNet50, EfficientNet, YOLOv8m, .. * (Furiosa LLM) Tensor Parallelism support Phase 1: Intra-chip * Torch 2.4.1 support * CPU memory swapping in Furiosa LLM\\n### 2024.3.0 (beta 1) - December, 2024 [#](#beta-1-december-2024 \"Link to this heading\")\\n* Model support: TBD * (Furiosa LLM) Tensor Parallelism support Phase 2: Inter-chip * `torch.compile()`   backend * Huggingface Optimum integration\\n[previous\\nWhat’s New](../whatsnew/index.html \"previous page\") [next\\nInstalling Prerequisites](../getting_started/prerequisites.html \"next page\")\\nContents\\n* [Latest Recent Release](#latest-recent-release) * [Future Releases](#future-releases)   + [2024.2.0 (beta 0) - November, 2024](#beta-0-november-2024)   + [2024.3.0 (beta 1) - December, 2024](#beta-1-december-2024)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='4f19d9ef-0eb7-40bf-a693-6e2d80ddb54a', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/supported_models.html', 'title': 'supported_models', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../_sources/overview/supported_models.rst \"Download source file\") * .pdf\\nSupported Models ================\\nContents --------\\n* [Decoder-only Models](#decoder-only-models) * [Encoder-only Models](#encoder-only-models)\\nSupported Models [#](#supported-models \"Link to this heading\") ==============================================================\\nFuriosaAI Software Stack supports a variety of Transformer-based models in HuggingFace Hub. The following is the list of model architectures that are currently supported by Furiosa SDK. If your model is based on the following architectures, you can use Furiosa SDK to compile, quantize, and run the model on FuriosaAI RNGD.\\nDecoder-only Models [#](#decoder-only-models \"Link to this heading\") --------------------------------------------------------------------\\nThe following models are supported for decoding only:\\nDecoder-only Models\\n[#](#id1 \"Link to this table\")\\n| Architecture | Model Name | Example HuggingFace Models | | --- | --- | --- | | `LlamaForCausalLM` | Llama 2, Llama 3.1 | `meta-llama/Llama-2-70b-hf` , `meta-llama/Meta-Llama-3.1-70B` , `meta-llama/Meta-Llama-3-70B-Instruct` , `meta-llama/Meta-Llama-3.1-8B` , `meta-llama/Llama-3.1-8B-Instruct` , .. | | `GPTJForCausalLM` | GPT-J | `EleutherAI/gpt-j-6b` |\\nEncoder-only Models [#](#encoder-only-models \"Link to this heading\") --------------------------------------------------------------------\\nEncoder-only Models\\n[#](#id2 \"Link to this table\")\\n| Architecture | Model Name | Example HuggingFace Models | | --- | --- | --- | | `BertForQuestionAnswering` | Bert | `google-bert/bert-large-uncased` , `google-bert/bert-base-uncased` , .. |\\n[previous\\nFuriosaAI’s Software Stack](software_stack.html \"previous page\") [next\\nWhat’s New](../whatsnew/index.html \"next page\")\\nContents\\n* [Decoder-only Models](#decoder-only-models) * [Encoder-only Models](#encoder-only-models)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../_sources/overview/supported_models.rst \"Download source file\") * .pdf\\nSupported Models ================\\nContents --------\\n* [Decoder-only Models](#decoder-only-models) * [Encoder-only Models](#encoder-only-models)\\nSupported Models [#](#supported-models \"Link to this heading\") ==============================================================\\nFuriosaAI Software Stack supports a variety of Transformer-based models in HuggingFace Hub. The following is the list of model architectures that are currently supported by Furiosa SDK. If your model is based on the following architectures, you can use Furiosa SDK to compile, quantize, and run the model on FuriosaAI RNGD.\\nDecoder-only Models [#](#decoder-only-models \"Link to this heading\") --------------------------------------------------------------------\\nThe following models are supported for decoding only:\\nDecoder-only Models\\n[#](#id1 \"Link to this table\")\\n| Architecture | Model Name | Example HuggingFace Models | | --- | --- | --- | | `LlamaForCausalLM` | Llama 2, Llama 3.1 | `meta-llama/Llama-2-70b-hf` , `meta-llama/Meta-Llama-3.1-70B` , `meta-llama/Meta-Llama-3-70B-Instruct` , `meta-llama/Meta-Llama-3.1-8B` , `meta-llama/Llama-3.1-8B-Instruct` , .. | | `GPTJForCausalLM` | GPT-J | `EleutherAI/gpt-j-6b` |\\nEncoder-only Models [#](#encoder-only-models \"Link to this heading\") --------------------------------------------------------------------\\nEncoder-only Models\\n[#](#id2 \"Link to this table\")\\n| Architecture | Model Name | Example HuggingFace Models | | --- | --- | --- | | `BertForQuestionAnswering` | Bert | `google-bert/bert-large-uncased` , `google-bert/bert-base-uncased` , .. |\\n[previous\\nFuriosaAI’s Software Stack](software_stack.html \"previous page\") [next\\nWhat’s New](../whatsnew/index.html \"next page\")\\nContents\\n* [Decoder-only Models](#decoder-only-models) * [Encoder-only Models](#encoder-only-models)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='0acdfa06-7dff-4603-9a5c-dbc4e3310580', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/software_stack.html', 'title': 'software_stack', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../_sources/overview/software_stack.rst \"Download source file\") * .pdf\\nFuriosaAI’s Software Stack ==========================\\nContents --------\\n* [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support)\\nFuriosaAI’s Software Stack [#](#furiosaai-s-software-stack \"Link to this heading\") ==================================================================================\\nFuriosaAI provides the streamlined software stack to allow FuriosaAI NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials. The above diagram demonstrates the SW stack provided by FuriosaAI, by layers.\\nThe following outlines the key components.\\nKernel Driver, Firmware, and PE Runtime [#](#kernel-driver-firmware-and-pe-runtime \"Link to this heading\") ----------------------------------------------------------------------------------------------------------\\nThe kernel device driver enables the Linux operating system to recognize NPU devices and expose them as Linux device files. The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE). PERT is responsible for communicating with the host’s runtime and scheduling, managing the resources of PEs to execute NPU tasks.\\nFuriosa Compiler [#](#furiosa-compiler \"Link to this heading\") --------------------------------------------------------------\\nFuriosa compiler analyzes, optimizes a model graph, and generates a NPU executable program. The operation passes involve graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. When `torch.compile()` backend, `FuriosaBackend` is used or `furiosa-llm` is used, the Furiosa Compiler is transparently used to generate NPU executable programs for Runtime.\\nFuriosa Runtime [#](#furiosa-runtime \"Link to this heading\") ------------------------------------------------------------\\nRuntime loads multiple executable NPU programs generated by Furiosa compiler, and run them on the NPU. A single model can be compiled into multiple executable programs according to model architectures and applications. Runtime is responsible for scheduling NPU programs and managing computation and memory resource on NPUs and CPUs. Also, Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs.\\nFuriosa Model Compressor (Quantizer) [#](#furiosa-model-compressor-quantizer \"Link to this heading\") ----------------------------------------------------------------------------------------------------\\nFuriosa Model Compressor is a library as well as toolkit for model calibration and quantization. Model quantization is a powerful technique to reduce memory footprint, computation cost, inference latency and power consumption. Furiosa Model Compressor provides post-training quantization methods, such as\\n* BF16 (W16A16) * INT8 Weight-Only (W8A16) * FP8 (W8A8) * INT8 SmoothQuant (W8A8) * INT4 Weight-Only (W4A16 AWQ / GPTQ) (Planned in release 2024.2)\\nFuriosa LLM [#](#furiosa-llm \"Link to this heading\") ----------------------------------------------------\\nFuriosa LLM provides a high-performance inference engine for LLM models, such as Llama 3.1 70B, 8B, GPT-J, and Bert. Furiosa LLM is designed to provide the state-of-the-art serving optimization for LLM models. The key features of Furiosa LLM include vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and OpenAI-compatible API server. You can find further information at [Furiosa LLM](../furiosa_llm/intro.html#furiosallm) .\\nKubernetes Support [#](#kubernetes-support \"Link to this heading\") ------------------------------------------------------------------\\nKubernetes, an open-source platform designed to manage containerized applications and services, is extensively adopted by various companies due to its robust capabilities for deploying, scaling, and automating containerized workloads. FuriosaAI software stack also offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment.\\nFuriosaAI’s device plugin enables Kubernetes clusters to recognize FuriosaAI’s NPUs and allows NPUs to be scheduled for workloads and services that require them. This feature allows users to easily deploy AI workloads with FuriosaAI NPUs on Kubernetes, enabling efficient resource utilization and scaling.\\nYou can find more information about Kubernetes support in the [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) .\\n[previous\\nFuriosaAI RNGD](rngd.html \"previous page\") [next\\nSupported Models](supported_models.html \"next page\")\\nContents\\n* [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../_sources/overview/software_stack.rst \"Download source file\") * .pdf\\nFuriosaAI’s Software Stack ==========================\\nContents --------\\n* [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support)\\nFuriosaAI’s Software Stack [#](#furiosaai-s-software-stack \"Link to this heading\") ==================================================================================\\nFuriosaAI provides the streamlined software stack to allow FuriosaAI NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials. The above diagram demonstrates the SW stack provided by FuriosaAI, by layers.\\nThe following outlines the key components.\\nKernel Driver, Firmware, and PE Runtime [#](#kernel-driver-firmware-and-pe-runtime \"Link to this heading\") ----------------------------------------------------------------------------------------------------------\\nThe kernel device driver enables the Linux operating system to recognize NPU devices and expose them as Linux device files. The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE). PERT is responsible for communicating with the host’s runtime and scheduling, managing the resources of PEs to execute NPU tasks.\\nFuriosa Compiler [#](#furiosa-compiler \"Link to this heading\") --------------------------------------------------------------\\nFuriosa compiler analyzes, optimizes a model graph, and generates a NPU executable program. The operation passes involve graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. When `torch.compile()` backend, `FuriosaBackend` is used or `furiosa-llm` is used, the Furiosa Compiler is transparently used to generate NPU executable programs for Runtime.\\nFuriosa Runtime [#](#furiosa-runtime \"Link to this heading\") ------------------------------------------------------------\\nRuntime loads multiple executable NPU programs generated by Furiosa compiler, and run them on the NPU. A single model can be compiled into multiple executable programs according to model architectures and applications. Runtime is responsible for scheduling NPU programs and managing computation and memory resource on NPUs and CPUs. Also, Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs.\\nFuriosa Model Compressor (Quantizer) [#](#furiosa-model-compressor-quantizer \"Link to this heading\") ----------------------------------------------------------------------------------------------------\\nFuriosa Model Compressor is a library as well as toolkit for model calibration and quantization. Model quantization is a powerful technique to reduce memory footprint, computation cost, inference latency and power consumption. Furiosa Model Compressor provides post-training quantization methods, such as\\n* BF16 (W16A16) * INT8 Weight-Only (W8A16) * FP8 (W8A8) * INT8 SmoothQuant (W8A8) * INT4 Weight-Only (W4A16 AWQ / GPTQ) (Planned in release 2024.2)\\nFuriosa LLM [#](#furiosa-llm \"Link to this heading\") ----------------------------------------------------\\nFuriosa LLM provides a high-performance inference engine for LLM models, such as Llama 3.1 70B, 8B, GPT-J, and Bert. Furiosa LLM is designed to provide the state-of-the-art serving optimization for LLM models. The key features of Furiosa LLM include vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and OpenAI-compatible API server. You can find further information at [Furiosa LLM](../furiosa_llm/intro.html#furiosallm) .\\nKubernetes Support [#](#kubernetes-support \"Link to this heading\") ------------------------------------------------------------------\\nKubernetes, an open-source platform designed to manage containerized applications and services, is extensively adopted by various companies due to its robust capabilities for deploying, scaling, and automating containerized workloads. FuriosaAI software stack also offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment.\\nFuriosaAI’s device plugin enables Kubernetes clusters to recognize FuriosaAI’s NPUs and allows NPUs to be scheduled for workloads and services that require them. This feature allows users to easily deploy AI workloads with FuriosaAI NPUs on Kubernetes, enabling efficient resource utilization and scaling.\\nYou can find more information about Kubernetes support in the [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) .\\n[previous\\nFuriosaAI RNGD](rngd.html \"previous page\") [next\\nSupported Models](supported_models.html \"next page\")\\nContents\\n* [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='da255daf-c8d3-430e-b976-ad096f3a9ad7', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/references/sampling_params.html', 'title': 'sampling_params', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../../_sources/furiosa_llm/references/sampling_params.rst \"Download source file\") * .pdf\\nSamplingParams ==============\\nContents --------\\n* [`SamplingParams`](#furiosa_llm.SamplingParams)\\nSamplingParams [#](#samplingparams \"Link to this heading\") ==========================================================\\n*class* furiosa\\\\_llm.\\nSamplingParams\\n(\\n*\\\\** ,\\n*n\\n:\\nint\\n=\\n1* ,\\n*best\\\\_of\\n:\\nint\\n|\\nNone\\n=\\nNone* ,\\n*temperature\\n:\\nfloat\\n=\\n1.0* ,\\n*top\\\\_p\\n:\\nfloat\\n=\\n1.0* ,\\n*top\\\\_k\\n:\\nint\\n=\\n-1* ,\\n*use\\\\_beam\\\\_search\\n:\\nbool\\n=\\nFalse* ,\\n*length\\\\_penalty\\n:\\nfloat\\n=\\n1.0* ,\\n*early\\\\_stopping\\n:\\nbool\\n|\\nstr\\n=\\nFalse* ,\\n*max\\\\_tokens\\n:\\nint\\n=\\n16* ,\\n*min\\\\_tokens\\n:\\nint\\n=\\n0* )\\n[[source]](../../_modules/furiosa_llm/sampling_params.html#SamplingParams) [#](#furiosa_llm.SamplingParams \"Link to this definition\")\\nBases: `object`  Sampling parameters for text generation.\\nThe default parameters represents greedy search.\\nParameters :\\n* **n**   – Number of output sequences to return for the given prompt. * **best\\\\_of**   – Number of output sequences that are generated from the prompt.   From these   best\\\\_of      sequences, the top   n      sequences are returned.   best\\\\_of      must be greater than or equal to   n      . This is treated as   the beam width when   use\\\\_beam\\\\_search      is True. By default,   best\\\\_of      is set to   n      . * **temperature**   – Float that controls the randomness of the sampling. Lower   values make the model more deterministic, while higher values make   the model more random. Zero means greedy sampling. * **top\\\\_p**   – Float that controls the cumulative probability of the top tokens   to consider. Must be in (0, 1]. Set to 1 to consider all tokens. * **top\\\\_k**   – Integer that controls the number of top tokens to consider. Set   to -1 to consider all tokens. * **use\\\\_beam\\\\_search**   – Whether to use beam search instead of sampling. * **length\\\\_penalty**   – Float that penalizes sequences based on their length.   Used in beam search. * **early\\\\_stopping**   – Controls the stopping condition for beam search. It   accepts the following values:   True      , where the generation stops as   soon as there are   best\\\\_of      complete candidates;   False      , where an   heuristic is applied and the generation stops when is it very   unlikely to find better candidates;   “never”      , where the beam search   procedure only stops when there cannot be better candidates   (canonical beam search algorithm). * **max\\\\_tokens**   – Maximum number of tokens to generate per output sequence. * **min\\\\_tokens**   – Minimum number of tokens to generate per output sequence   before EOS or stop\\\\_token\\\\_ids can be generated\\n[previous\\nLLM class](llm.html \"previous page\") [next\\nCloud Native Toolkit](../../cloud_native_toolkit/intro.html \"next page\")\\nContents\\n* [`SamplingParams`](#furiosa_llm.SamplingParams)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../../_sources/furiosa_llm/references/sampling_params.rst \"Download source file\") * .pdf\\nSamplingParams ==============\\nContents --------\\n* [`SamplingParams`](#furiosa_llm.SamplingParams)\\nSamplingParams [#](#samplingparams \"Link to this heading\") ==========================================================\\n*class* furiosa\\\\_llm.\\nSamplingParams\\n(\\n*\\\\** ,\\n*n\\n:\\nint\\n=\\n1* ,\\n*best\\\\_of\\n:\\nint\\n|\\nNone\\n=\\nNone* ,\\n*temperature\\n:\\nfloat\\n=\\n1.0* ,\\n*top\\\\_p\\n:\\nfloat\\n=\\n1.0* ,\\n*top\\\\_k\\n:\\nint\\n=\\n-1* ,\\n*use\\\\_beam\\\\_search\\n:\\nbool\\n=\\nFalse* ,\\n*length\\\\_penalty\\n:\\nfloat\\n=\\n1.0* ,\\n*early\\\\_stopping\\n:\\nbool\\n|\\nstr\\n=\\nFalse* ,\\n*max\\\\_tokens\\n:\\nint\\n=\\n16* ,\\n*min\\\\_tokens\\n:\\nint\\n=\\n0* )\\n[[source]](../../_modules/furiosa_llm/sampling_params.html#SamplingParams) [#](#furiosa_llm.SamplingParams \"Link to this definition\")\\nBases: `object`  Sampling parameters for text generation.\\nThe default parameters represents greedy search.\\nParameters :\\n* **n**   – Number of output sequences to return for the given prompt. * **best\\\\_of**   – Number of output sequences that are generated from the prompt.   From these   best\\\\_of      sequences, the top   n      sequences are returned.   best\\\\_of      must be greater than or equal to   n      . This is treated as   the beam width when   use\\\\_beam\\\\_search      is True. By default,   best\\\\_of      is set to   n      . * **temperature**   – Float that controls the randomness of the sampling. Lower   values make the model more deterministic, while higher values make   the model more random. Zero means greedy sampling. * **top\\\\_p**   – Float that controls the cumulative probability of the top tokens   to consider. Must be in (0, 1]. Set to 1 to consider all tokens. * **top\\\\_k**   – Integer that controls the number of top tokens to consider. Set   to -1 to consider all tokens. * **use\\\\_beam\\\\_search**   – Whether to use beam search instead of sampling. * **length\\\\_penalty**   – Float that penalizes sequences based on their length.   Used in beam search. * **early\\\\_stopping**   – Controls the stopping condition for beam search. It   accepts the following values:   True      , where the generation stops as   soon as there are   best\\\\_of      complete candidates;   False      , where an   heuristic is applied and the generation stops when is it very   unlikely to find better candidates;   “never”      , where the beam search   procedure only stops when there cannot be better candidates   (canonical beam search algorithm). * **max\\\\_tokens**   – Maximum number of tokens to generate per output sequence. * **min\\\\_tokens**   – Minimum number of tokens to generate per output sequence   before EOS or stop\\\\_token\\\\_ids can be generated\\n[previous\\nLLM class](llm.html \"previous page\") [next\\nCloud Native Toolkit](../../cloud_native_toolkit/intro.html \"next page\")\\nContents\\n* [`SamplingParams`](#furiosa_llm.SamplingParams)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='edfa6fb4-e88c-4fa8-a6aa-9986213aa6ab', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/device_management/furiosa_smi.html', 'title': 'furiosa_smi', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../_sources/device_management/furiosa_smi.rst \"Download source file\") * .pdf\\nfuriosa-smi ===========\\nContents --------\\n* [Installing   `furiosa-smi`   command](#installing-furiosa-smi-command)   + [Synopsis](#synopsis)   + [`furiosa-smi          info`](#furiosa-smi-info)   + [`furiosa-smi          status`](#furiosa-smi-status)   + [`furiosa-smi          ps`](#furiosa-smi-ps)   + [`furiosa-smi          topo`](#furiosa-smi-topo)\\nfuriosa-smi [#](#furiosa-smi \"Link to this heading\") ====================================================\\nThe `furiosa-smi` command provides a variety of subcommands and has the ability to obtain information or control the device.\\nInstalling `furiosa-smi` command [#](#installing-furiosa-smi-command \"Link to this heading\") --------------------------------------------------------------------------------------------\\nTo install the `furiosa-smi` command, you need to install `furiosa-smi` as following:\\nThe minimum requirements for `furiosa-smi` are as follows:\\n* Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers (   [Setting up APT server](../getting_started/prerequisites.html#aptsetup)   )\\nThen, please install the `furiosa-smi` package as follows:\\n``` sudo apt update sudo apt install -y furiosa-smi\\n```\\nThis command installs packages `furiosa-libsmi` and `furiosa-smi` .\\n### Synopsis [#](#synopsis \"Link to this heading\")\\n``` furiosa-smi <sub command> [option] ..\\n```\\n### `furiosa-smi info` [#](#furiosa-smi-info \"Link to this heading\")\\nAfter installing the kernel driver, you can use the `furiosa-smi` command to check whether the NPU device is recognized. Currently, this command provides the `furiosa-smi\\ninfo` command to output temperature, power consumption and PCI information of the NPU device. If the device is not visible with this command after mounting it on the machine, please install the driver. If you add the `--full` option to the `info` command, you can see the device’s UUID and serial number information together.\\n``` $ furiosa-smi info +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware       | Temp.   | Power   | PCI-BDF      | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0   | 0.0.15+af1daaa | 30.18°C | 53.00 W | 0000:17:00.0 | +------+--------+----------------+---------+---------+--------------+ | rngd | npu1   | 0.0.15+af1daaa | 29.25°C | 53.00 W | 0000:2a:00.0 | +------+--------+----------------+---------+---------+--------------+\\n$ furiosa-smi info --format full +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ | Arch | Device | UUID                                 | S/N        | Firmware       | Temp.   | Power   | Clock | PCI-BDF      | PCI-DEV | +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ | rngd | npu0   | 3E93AE7C-E8EA-4C62-BED6-AD2EC0461AE8 | RNGDXXXXXX | 0.0.15+af1daaa | 30.18°C | 53.00 W |   N/A | 0000:17:00.0 | 508:0   | +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ | rngd | npu1   | 176DAD0F-1510-475C-91D8-5F79551CF718 | RNGDXXXXXY | 0.0.15+af1daaa | 29.44°C | 53.00 W |   N/A | 0000:2a:00.0 | 506:0   | +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+\\n```\\n### `furiosa-smi status` [#](#furiosa-smi-status \"Link to this heading\")\\nThe `status` subcommand provides information about the device files available on the NPU device. You can also check whether each core present in the NPU is in use or idle.\\n``` $ furiosa-smi status +------+--------+---------------+------------------+ | Arch | Device | Cores         | Core Utilization | +------+--------+---------------+------------------+ |      |        | 0 (occupied), | Core 0: 0.00%,   | |      |        | 1 (occupied), | Core 1: 0.00%,   | |      |        | 2 (occupied), | Core 2: 0.00%,   | | rngd | npu0   | 3 (occupied), | Core 3: 0.00%,   | |      |        | 4 (occupied), | Core 4: 0.00%,   | |      |        | 5 (occupied), | Core 5: 0.00%,   | |      |        | 6 (occupied), | Core 6: 0.00%,   | |      |        | 7 (occupied)  | Core 7: 0.00%    | +------+--------+---------------+------------------+ |      |        | 0 (occupied), | Core 0: 0.00%,   | |      |        | 1 (occupied), | Core 1: 0.00%,   | |      |        | 2 (occupied), | Core 2: 0.00%,   | | rngd | npu1   | 3 (occupied), | Core 3: 0.00%,   | |      |        | 4 (occupied), | Core 4: 0.00%,   | |      |        | 5 (occupied), | Core 5: 0.00%,   | |      |        | 6 (occupied), | Core 6: 0.00%,   | |      |        | 7 (occupied)  | Core 7: 0.00%    | +------+--------+---------------+------------------+\\n```\\n### `furiosa-smi ps` [#](#furiosa-smi-ps \"Link to this heading\")\\nThe `ps` subcommand prints information about the OS process currently occupying the NPU device.\\n``` $ furiosa-smi ps +-----------+--------+------------------------------------------------------------+ | NPU       | PID    | CMD                                                        | +-----------+--------+------------------------------------------------------------+ | npu0pe0-3 | 132529 | /usr/bin/python3 /usr/local/bin/uvicorn gptj:app           | +-----------+--------+------------------------------------------------------------+\\n```\\n### `furiosa-smi topo` [#](#furiosa-smi-topo \"Link to this heading\")\\nThe `topo` subcommand shows the topology of the NPU device and its NUMA node.\\n``` $ furiosa-smi topo +--------+--------------+--------------+-----------+ | Device | npu0         | npu1         | NUMA node | +--------+--------------+--------------+-----------+ | npu0   | Noc          | Interconnect | 0         | +--------+--------------+--------------+-----------+ | npu1   | Interconnect | Noc          | 0         | +--------+--------------+--------------+-----------+\\nLegend:\\n  Noc          = Connection within the same npu chip   Bridge       = Devices communicating via one or more PCIe switches   Cpu          = Devices communicating exclusively within a single CPU socket   Interconnect = Devices communicating via inter-socket links (e.g., QPI, GMI)   Unknown      = Connection type is unidentified\\n```\\n[previous\\nCloud Native Toolkit](../cloud_native_toolkit/intro.html \"previous page\")\\nContents\\n* [Installing   `furiosa-smi`   command](#installing-furiosa-smi-command)   + [Synopsis](#synopsis)   + [`furiosa-smi          info`](#furiosa-smi-info)   + [`furiosa-smi          status`](#furiosa-smi-status)   + [`furiosa-smi          ps`](#furiosa-smi-ps)   + [`furiosa-smi          topo`](#furiosa-smi-topo)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../_sources/device_management/furiosa_smi.rst \"Download source file\") * .pdf\\nfuriosa-smi ===========\\nContents --------\\n* [Installing   `furiosa-smi`   command](#installing-furiosa-smi-command)   + [Synopsis](#synopsis)   + [`furiosa-smi          info`](#furiosa-smi-info)   + [`furiosa-smi          status`](#furiosa-smi-status)   + [`furiosa-smi          ps`](#furiosa-smi-ps)   + [`furiosa-smi          topo`](#furiosa-smi-topo)\\nfuriosa-smi [#](#furiosa-smi \"Link to this heading\") ====================================================\\nThe `furiosa-smi` command provides a variety of subcommands and has the ability to obtain information or control the device.\\nInstalling `furiosa-smi` command [#](#installing-furiosa-smi-command \"Link to this heading\") --------------------------------------------------------------------------------------------\\nTo install the `furiosa-smi` command, you need to install `furiosa-smi` as following:\\nThe minimum requirements for `furiosa-smi` are as follows:\\n* Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers (   [Setting up APT server](../getting_started/prerequisites.html#aptsetup)   )\\nThen, please install the `furiosa-smi` package as follows:\\n``` sudo apt update sudo apt install -y furiosa-smi\\n```\\nThis command installs packages `furiosa-libsmi` and `furiosa-smi` .\\n### Synopsis [#](#synopsis \"Link to this heading\")\\n``` furiosa-smi <sub command> [option] ..\\n```\\n### `furiosa-smi info` [#](#furiosa-smi-info \"Link to this heading\")\\nAfter installing the kernel driver, you can use the `furiosa-smi` command to check whether the NPU device is recognized. Currently, this command provides the `furiosa-smi\\ninfo` command to output temperature, power consumption and PCI information of the NPU device. If the device is not visible with this command after mounting it on the machine, please install the driver. If you add the `--full` option to the `info` command, you can see the device’s UUID and serial number information together.\\n``` $ furiosa-smi info +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware       | Temp.   | Power   | PCI-BDF      | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0   | 0.0.15+af1daaa | 30.18°C | 53.00 W | 0000:17:00.0 | +------+--------+----------------+---------+---------+--------------+ | rngd | npu1   | 0.0.15+af1daaa | 29.25°C | 53.00 W | 0000:2a:00.0 | +------+--------+----------------+---------+---------+--------------+\\n$ furiosa-smi info --format full +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ | Arch | Device | UUID                                 | S/N        | Firmware       | Temp.   | Power   | Clock | PCI-BDF      | PCI-DEV | +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ | rngd | npu0   | 3E93AE7C-E8EA-4C62-BED6-AD2EC0461AE8 | RNGDXXXXXX | 0.0.15+af1daaa | 30.18°C | 53.00 W |   N/A | 0000:17:00.0 | 508:0   | +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ | rngd | npu1   | 176DAD0F-1510-475C-91D8-5F79551CF718 | RNGDXXXXXY | 0.0.15+af1daaa | 29.44°C | 53.00 W |   N/A | 0000:2a:00.0 | 506:0   | +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+\\n```\\n### `furiosa-smi status` [#](#furiosa-smi-status \"Link to this heading\")\\nThe `status` subcommand provides information about the device files available on the NPU device. You can also check whether each core present in the NPU is in use or idle.\\n``` $ furiosa-smi status +------+--------+---------------+------------------+ | Arch | Device | Cores         | Core Utilization | +------+--------+---------------+------------------+ |      |        | 0 (occupied), | Core 0: 0.00%,   | |      |        | 1 (occupied), | Core 1: 0.00%,   | |      |        | 2 (occupied), | Core 2: 0.00%,   | | rngd | npu0   | 3 (occupied), | Core 3: 0.00%,   | |      |        | 4 (occupied), | Core 4: 0.00%,   | |      |        | 5 (occupied), | Core 5: 0.00%,   | |      |        | 6 (occupied), | Core 6: 0.00%,   | |      |        | 7 (occupied)  | Core 7: 0.00%    | +------+--------+---------------+------------------+ |      |        | 0 (occupied), | Core 0: 0.00%,   | |      |        | 1 (occupied), | Core 1: 0.00%,   | |      |        | 2 (occupied), | Core 2: 0.00%,   | | rngd | npu1   | 3 (occupied), | Core 3: 0.00%,   | |      |        | 4 (occupied), | Core 4: 0.00%,   | |      |        | 5 (occupied), | Core 5: 0.00%,   | |      |        | 6 (occupied), | Core 6: 0.00%,   | |      |        | 7 (occupied)  | Core 7: 0.00%    | +------+--------+---------------+------------------+\\n```\\n### `furiosa-smi ps` [#](#furiosa-smi-ps \"Link to this heading\")\\nThe `ps` subcommand prints information about the OS process currently occupying the NPU device.\\n``` $ furiosa-smi ps +-----------+--------+------------------------------------------------------------+ | NPU       | PID    | CMD                                                        | +-----------+--------+------------------------------------------------------------+ | npu0pe0-3 | 132529 | /usr/bin/python3 /usr/local/bin/uvicorn gptj:app           | +-----------+--------+------------------------------------------------------------+\\n```\\n### `furiosa-smi topo` [#](#furiosa-smi-topo \"Link to this heading\")\\nThe `topo` subcommand shows the topology of the NPU device and its NUMA node.\\n``` $ furiosa-smi topo +--------+--------------+--------------+-----------+ | Device | npu0         | npu1         | NUMA node | +--------+--------------+--------------+-----------+ | npu0   | Noc          | Interconnect | 0         | +--------+--------------+--------------+-----------+ | npu1   | Interconnect | Noc          | 0         | +--------+--------------+--------------+-----------+\\nLegend:\\n  Noc          = Connection within the same npu chip   Bridge       = Devices communicating via one or more PCIe switches   Cpu          = Devices communicating exclusively within a single CPU socket   Interconnect = Devices communicating via inter-socket links (e.g., QPI, GMI)   Unknown      = Connection type is unidentified\\n```\\n[previous\\nCloud Native Toolkit](../cloud_native_toolkit/intro.html \"previous page\")\\nContents\\n* [Installing   `furiosa-smi`   command](#installing-furiosa-smi-command)   + [Synopsis](#synopsis)   + [`furiosa-smi          info`](#furiosa-smi-info)   + [`furiosa-smi          status`](#furiosa-smi-status)   + [`furiosa-smi          ps`](#furiosa-smi-ps)   + [`furiosa-smi          topo`](#furiosa-smi-topo)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='e08e3845-eb5a-4b6f-8530-fb63f81ef7d0', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/references/llm.html', 'title': 'llm', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../../_sources/furiosa_llm/references/llm.rst \"Download source file\") * .pdf\\nLLM class =========\\nContents --------\\n* [`LLM`](#furiosa_llm.LLM)   + [`LLM.generate()`](#furiosa_llm.LLM.generate)   + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms)\\nLLM class [#](#llm-class \"Link to this heading\") ================================================\\n*class* furiosa\\\\_llm.\\nLLM\\n(\\n*pretrained\\\\_id\\n:\\nstr* ,\\n*task\\\\_type\\n:\\nstr\\n|\\nNone\\n=\\nNone* ,\\n*llm\\\\_config\\n:\\nLLMConfig\\n|\\nNone\\n=\\nNone* ,\\n*qformat\\\\_path\\n:\\nPathLike\\n|\\nNone\\n=\\nNone* ,\\n*qparam\\\\_path\\n:\\nPathLike\\n|\\nNone\\n=\\nNone* ,\\n*prefill\\\\_quant\\\\_bin\\\\_path\\n:\\nPathLike\\n|\\nNone\\n=\\nNone* ,\\n*decode\\\\_quant\\\\_bin\\\\_path\\n:\\nPathLike\\n|\\nNone\\n=\\nNone* ,\\n*config\\n:\\nDict\\n[\\nstr\\n,\\nAny\\n]\\n=\\n{}* ,\\n*bucket\\\\_config\\n:\\nBucketConfig\\n|\\nNone\\n=\\nNone* ,\\n*max\\\\_seq\\\\_len\\\\_to\\\\_capture\\n:\\nint\\n=\\n2048* ,\\n*tensor\\\\_parallel\\\\_size\\n:\\nint\\n=\\n4* ,\\n*pipeline\\\\_parallel\\\\_size\\n:\\nint\\n=\\n1* ,\\n*data\\\\_parallel\\\\_size\\n:\\nint\\n|\\nNone\\n=\\nNone* ,\\n*tokenizer\\n:\\nPreTrainedTokenizer\\n|\\nPreTrainedTokenizerFast\\n|\\nNone\\n=\\nNone* ,\\n*tokenizer\\\\_mode\\n:\\nLiteral\\n[\\n\\'auto\\'\\n,\\n\\'slow\\'\\n]\\n=\\n\\'auto\\'* ,\\n*seed\\n:\\nint\\n|\\nNone\\n=\\nNone* ,\\n*devices\\n:\\nstr\\n|\\nSequence\\n[\\nDevice\\n]\\n|\\nNone\\n=\\nNone* ,\\n*param\\\\_file\\\\_path\\n:\\nPathLike\\n|\\nNone\\n=\\nNone* ,\\n*param\\\\_saved\\\\_format\\n:\\nLiteral\\n[\\n\\'safetensors\\'\\n,\\n\\'pt\\'\\n]\\n=\\n\\'safetensors\\'* ,\\n*do\\\\_decompositions\\\\_for\\\\_model\\\\_rewrite\\n:\\nbool\\n=\\nFalse* ,\\n*comp\\\\_supertask\\\\_kind\\n:\\nLiteral\\n[\\n\\'edf\\'\\n,\\n\\'dfg\\'\\n,\\n\\'fx\\'\\n]\\n|\\nNone\\n=\\nNone* ,\\n*cache\\\\_dir\\n:\\nPathLike\\n|\\nNone\\n=\\nPosixPath(\\'/home/hyunsik/.cache/furiosa/llm\\')* ,\\n*backend\\n:\\nLLMBackend\\n|\\nNone\\n=\\nNone* ,\\n*use\\\\_blockwise\\\\_compile\\n:\\nbool\\n=\\nTrue* ,\\n*num\\\\_blocks\\\\_per\\\\_supertask\\n:\\nint\\n=\\n1* ,\\n*embed\\\\_all\\\\_constants\\\\_into\\\\_graph\\n:\\nbool\\n=\\nFalse* ,\\n*paged\\\\_attention\\\\_num\\\\_blocks\\n:\\nint\\n|\\nNone\\n=\\nNone* ,\\n*paged\\\\_attention\\\\_block\\\\_size\\n:\\nint\\n=\\n1* ,\\n*kv\\\\_cache\\\\_sharing\\\\_across\\\\_beams\\\\_config\\n:\\nKvCacheSharingAcrossBeamsConfig\\n|\\nNone\\n=\\nNone* ,\\n*scheduler\\\\_config\\n:\\nSchedulerConfig\\n=\\nSchedulerConfig(npu\\\\_queue\\\\_limit=2,\\nmax\\\\_processing\\\\_samples=65536,\\nspare\\\\_blocks\\\\_ratio=0.2,\\nis\\\\_offline=False)* ,\\n*packing\\\\_type\\n:\\nLiteral\\n[\\n\\'IDENTITY\\'\\n]\\n=\\n\\'IDENTITY\\'* ,\\n*compiler\\\\_config\\\\_overrides\\n:\\nMapping\\n|\\nNone\\n=\\nNone* ,\\n*use\\\\_random\\\\_weight\\n:\\nbool\\n=\\nFalse* ,\\n*num\\\\_pipeline\\\\_builder\\\\_workers\\n:\\nint\\n=\\n1* ,\\n*num\\\\_compile\\\\_workers\\n:\\nint\\n=\\n1* ,\\n*skip\\\\_engine\\n:\\nbool\\n=\\nFalse* ,\\n*\\\\** ,\\n*\\\\_cleanup\\n:\\nbool\\n=\\nTrue* ,\\n*\\\\*\\\\*\\nkwargs* )\\n[[source]](../../_modules/furiosa_llm/api.html#LLM) [#](#furiosa_llm.LLM \"Link to this definition\")\\nBases: `object`  An LLM for generating texts from given prompts and sampling parameters.\\nParameters :\\n* **pretrained\\\\_id**   – The name of the pretrained model. This corresponds to   pretrained\\\\_model\\\\_name\\\\_or\\\\_path in HuggingFace Transformers. * **task\\\\_type**   – The type of the task. This corresponds to task in HuggingFace Transformers.   See   <https://huggingface.co/docs/transformers/main/en/quicktour#pipeline>   for more   details. * **llm\\\\_config**   – The configuration for the LLM. This includes quantization and optimization   configurations. * **qformat\\\\_path**   – The path to the quantization format file. * **qparam\\\\_path**   – The path to the quantization parameter file. * **prefill\\\\_quant\\\\_bin\\\\_path**   – The path to the quantziation prefill bin file. * **decode\\\\_quant\\\\_bin\\\\_path**   – The path to the quantziation decode bin file. * **config**   – The configuration for the HuggingFace Transformers model. This is a dictionary   that includes the configuration for the model. * **bucket\\\\_config**   – Config for bucket generating policy. If not given, the model will use single one batch,   max\\\\_seq\\\\_len\\\\_to\\\\_capture      attention size bucket per   each phase. * **max\\\\_seq\\\\_len\\\\_to\\\\_capture**   – Maximum sequence length covered by LLM engine. Sequence with larger context than this will not be covered.   The default is 2048. * **tensor\\\\_parallel\\\\_size**   – The number of PEs for each tensor parallelism group. The default is 4. * **pipeline\\\\_parallel\\\\_size**   – The number of pipeline stages for pipeline parallelism. The default is 1,   which means no pipeline parallelism. * **data\\\\_parallel\\\\_size**   – The size of the data parallelism group. If not given, it will be inferred from   total avaialble PEs and other parallelism degrees. * **tokenizer**   – The name or path of a HuggingFace Transformers tokenizer. * **tokenizer\\\\_mode**   – The tokenizer mode. “auto” will use the fast tokenizer   if available, and “slow” will always use the slow tokenizer. * **seed**   – The seed to initialize the random number generator for sampling. * **devices**   – The devices to run the model. It can be a single device or a list of devices.   Each device can be either “cpu:X” or “cuda:X” where X is a specific device index.   The default is “cpu:0”. * **param\\\\_file\\\\_path**   – The path to the parameter file to use for pipeline generation.   If not specified, the parameters will be saved in a temporary file which will be   used for pipeline generation. * **param\\\\_saved\\\\_format**   – The format of the parameter file. Only possible value is “safetensors” now.   The default is “safetensors”. * **do\\\\_decompositions\\\\_for\\\\_model\\\\_rewrite**   – Whether to decompose some ops to describe various parallelism strategies   with mppp config. When the value is True, mppp config that matches with the decomposed FX graph should be given. * **comp\\\\_supertask\\\\_kind**   – The format that pipeline’s supertask will be represented as.   Possible values are “fx”,”dfg”, and “edf”, and the default is “fx”. * **cache\\\\_dir**   – The cache directory for all generated files for this LLM instance.   When its value is   `None`   , caching is disabled. The default is “$HOME/.cache/furiosa/llm”. * **backend**   – The backend implementation to run forward() of a model for the LLM.   The default is LLMBackend.TORCH\\\\_PIPELINE\\\\_RUNNER. * **use\\\\_blockwise\\\\_compile**   – If True, each task will be compiled in the unit of transformer block,   and compilation result for transformer block is generated once and reused. * **num\\\\_blocks\\\\_per\\\\_supertask**   – The number of transformer blocks that will be merged into one supertask. This option is valid   only when   use\\\\_blockwise\\\\_compile=True      . The default is 1. * **embed\\\\_all\\\\_constants\\\\_into\\\\_graph**   – Whether to embed constant tensors into graph or make them as input of the graph and save them as separate files. * **paged\\\\_attention\\\\_num\\\\_blocks**   – The maximum number of blocks that each k/v storage per layer can store. This argument must be given   if model uses paged attention. * **paged\\\\_attention\\\\_block\\\\_size**   – The maximum number of tokens that can be stored in a single paged attention block. This argument must be given   if model uses paged attention. * **kv\\\\_cache\\\\_sharing\\\\_across\\\\_beams\\\\_config**   – Configuration for sharing kv cache across beams. This argument must be given if and only if   the model is optimized to share kv cache across beams. If this argument is given, decode phase buckets with batch size of   `batch_size`   \\\\*   `kv_cache_sharing_across_beams_config.beam_width`   will be created. * **scheduler\\\\_config**   – Configuration for the scheduler, allowing to maximum number of tasks which can be queued to HW, maximum number of samples   that can be processed by the scheduler, and ratio of spare blocks that are reserved by scheduler. * **packing\\\\_type**   – Packing algorithm. Possible values are “IDENTITY” only for now * **compiler\\\\_config\\\\_overrides**   – Overrides for the compiler config. This is a dictionary that includes the configuration for the compiler. * **use\\\\_random\\\\_weight**   – If True, the model will be initialized with random weights. * **num\\\\_pipeline\\\\_builder\\\\_workers**   – number of workers used for building pipelines (except for compilation). The default is 1 (no parallelism).   Setting this value larger than 1 reduces pipeline building time, especially for large models, but requires much more memory. * **num\\\\_compile\\\\_workers**   – number of workers used for compilation. The default is 1 (no parallelism). * **skip\\\\_engine**   – If True, the native runtime engine will not be initialized. This is useful when you need   the pipelines for other purposes than running them with the engine.\\ngenerate\\n(\\n*prompts:\\nstr\\n|\\n~typing.List[str],\\nsampling\\\\_params:\\n~furiosa\\\\_llm.sampling\\\\_params.SamplingParams\\n=\\nSamplingParams(n=1,\\nbest\\\\_of=1,\\ntemperature=1.0,\\ntop\\\\_p=1.0,\\ntop\\\\_k=-1,\\nuse\\\\_beam\\\\_search=False,\\nlength\\\\_penalty=1.0,\\nearly\\\\_stopping=False,\\nmax\\\\_tokens=16min\\\\_tokens=0,\\n,\\nprompt\\\\_token\\\\_ids:\\n~typing.List[int]\\n|\\n~typing.List[~typing.List[int]]\\n|\\nNone\\n=\\nNone* )\\n→\\nRequestOutput\\n|\\nList\\n[\\nRequestOutput\\n]\\n[[source]](../../_modules/furiosa_llm/api.html#LLM.generate) [#](#furiosa_llm.LLM.generate \"Link to this definition\")\\nGenerate texts from given prompts and sampling parameters.\\nParameters :\\n* **prompts**   – The prompts to generate texts. * **sampling\\\\_params**   – The sampling parameters for generating texts. * **prompt\\\\_token\\\\_ids**   – The token ids of the prompts. If not given, the token ids are   generated from the prompts using the tokenizer.\\nReturns :\\nA list of RequestOutput\\nobjects containing the generated completions in the same order as the input prompts.\\nget\\\\_splitted\\\\_gms\\n(\\n*get\\\\_input\\\\_constants\\n:\\nbool\\n=\\nFalse* )\\n→\\nDict\\n[\\nstr\\n,\\nTuple\\n[\\nGraphModule\\n,\\n...\\n]\\n|\\nTuple\\n[\\nTuple\\n[\\nGraphModule\\n,\\nTuple\\n[\\nTensor\\n|\\nNone\\n,\\n...\\n]\\n]\\n,\\n...\\n]\\n]\\n[[source]](../../_modules/furiosa_llm/api.html#LLM.get_splitted_gms) [#](#furiosa_llm.LLM.get_splitted_gms \"Link to this definition\")\\nGet sub GraphModules for each pipeline.\\nReturns :\\nDictionary whose key is the pipeline name and value is the tuple containing `GraphModule``s\\n(computation\\nsupertasks)\\nand\\nsome\\nadditional\\ninformation\\nif\\nnecessary.\\nif\\n``get_input_constants==False` , each value is just a tuple of `GraphModule``s\\nin\\nthe\\npipeline.\\nOtherwise,\\neach\\nvalue\\nis\\na\\ntuple\\nwhose\\nelement\\nis\\n``GraphModule` in the pipeline and list of input constant tensors, which were originally constant tensors, but converted to input. The list of input constant tensors has same length as corresponding `GraphModule` ’s number of inputs with each element exactly corresponding to the input of the `GraphModule` with same index, but elements with original input tensor indexes are `None` .\\nReturn type :\\nDict[str, Union[Tuple[GraphModule, …], Tuple[Tuple[GraphModule, Tuple[Optional[torch.Tensor], …]], …],],]\\n[previous\\nReferences](../references.html \"previous page\") [next\\nSamplingParams](sampling_params.html \"next page\")\\nContents\\n* [`LLM`](#furiosa_llm.LLM)   + [`LLM.generate()`](#furiosa_llm.LLM.generate)   + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../../_sources/furiosa_llm/references/llm.rst \"Download source file\") * .pdf\\nLLM class =========\\nContents --------\\n* [`LLM`](#furiosa_llm.LLM)   + [`LLM.generate()`](#furiosa_llm.LLM.generate)   + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms)\\nLLM class [#](#llm-class \"Link to this heading\") ================================================\\n*class* furiosa\\\\_llm.\\nLLM\\n(\\n*pretrained\\\\_id\\n:\\nstr* ,\\n*task\\\\_type\\n:\\nstr\\n|\\nNone\\n=\\nNone* ,\\n*llm\\\\_config\\n:\\nLLMConfig\\n|\\nNone\\n=\\nNone* ,\\n*qformat\\\\_path\\n:\\nPathLike\\n|\\nNone\\n=\\nNone* ,\\n*qparam\\\\_path\\n:\\nPathLike\\n|\\nNone\\n=\\nNone* ,\\n*prefill\\\\_quant\\\\_bin\\\\_path\\n:\\nPathLike\\n|\\nNone\\n=\\nNone* ,\\n*decode\\\\_quant\\\\_bin\\\\_path\\n:\\nPathLike\\n|\\nNone\\n=\\nNone* ,\\n*config\\n:\\nDict\\n[\\nstr\\n,\\nAny\\n]\\n=\\n{}* ,\\n*bucket\\\\_config\\n:\\nBucketConfig\\n|\\nNone\\n=\\nNone* ,\\n*max\\\\_seq\\\\_len\\\\_to\\\\_capture\\n:\\nint\\n=\\n2048* ,\\n*tensor\\\\_parallel\\\\_size\\n:\\nint\\n=\\n4* ,\\n*pipeline\\\\_parallel\\\\_size\\n:\\nint\\n=\\n1* ,\\n*data\\\\_parallel\\\\_size\\n:\\nint\\n|\\nNone\\n=\\nNone* ,\\n*tokenizer\\n:\\nPreTrainedTokenizer\\n|\\nPreTrainedTokenizerFast\\n|\\nNone\\n=\\nNone* ,\\n*tokenizer\\\\_mode\\n:\\nLiteral\\n[\\n\\'auto\\'\\n,\\n\\'slow\\'\\n]\\n=\\n\\'auto\\'* ,\\n*seed\\n:\\nint\\n|\\nNone\\n=\\nNone* ,\\n*devices\\n:\\nstr\\n|\\nSequence\\n[\\nDevice\\n]\\n|\\nNone\\n=\\nNone* ,\\n*param\\\\_file\\\\_path\\n:\\nPathLike\\n|\\nNone\\n=\\nNone* ,\\n*param\\\\_saved\\\\_format\\n:\\nLiteral\\n[\\n\\'safetensors\\'\\n,\\n\\'pt\\'\\n]\\n=\\n\\'safetensors\\'* ,\\n*do\\\\_decompositions\\\\_for\\\\_model\\\\_rewrite\\n:\\nbool\\n=\\nFalse* ,\\n*comp\\\\_supertask\\\\_kind\\n:\\nLiteral\\n[\\n\\'edf\\'\\n,\\n\\'dfg\\'\\n,\\n\\'fx\\'\\n]\\n|\\nNone\\n=\\nNone* ,\\n*cache\\\\_dir\\n:\\nPathLike\\n|\\nNone\\n=\\nPosixPath(\\'/home/hyunsik/.cache/furiosa/llm\\')* ,\\n*backend\\n:\\nLLMBackend\\n|\\nNone\\n=\\nNone* ,\\n*use\\\\_blockwise\\\\_compile\\n:\\nbool\\n=\\nTrue* ,\\n*num\\\\_blocks\\\\_per\\\\_supertask\\n:\\nint\\n=\\n1* ,\\n*embed\\\\_all\\\\_constants\\\\_into\\\\_graph\\n:\\nbool\\n=\\nFalse* ,\\n*paged\\\\_attention\\\\_num\\\\_blocks\\n:\\nint\\n|\\nNone\\n=\\nNone* ,\\n*paged\\\\_attention\\\\_block\\\\_size\\n:\\nint\\n=\\n1* ,\\n*kv\\\\_cache\\\\_sharing\\\\_across\\\\_beams\\\\_config\\n:\\nKvCacheSharingAcrossBeamsConfig\\n|\\nNone\\n=\\nNone* ,\\n*scheduler\\\\_config\\n:\\nSchedulerConfig\\n=\\nSchedulerConfig(npu\\\\_queue\\\\_limit=2,\\nmax\\\\_processing\\\\_samples=65536,\\nspare\\\\_blocks\\\\_ratio=0.2,\\nis\\\\_offline=False)* ,\\n*packing\\\\_type\\n:\\nLiteral\\n[\\n\\'IDENTITY\\'\\n]\\n=\\n\\'IDENTITY\\'* ,\\n*compiler\\\\_config\\\\_overrides\\n:\\nMapping\\n|\\nNone\\n=\\nNone* ,\\n*use\\\\_random\\\\_weight\\n:\\nbool\\n=\\nFalse* ,\\n*num\\\\_pipeline\\\\_builder\\\\_workers\\n:\\nint\\n=\\n1* ,\\n*num\\\\_compile\\\\_workers\\n:\\nint\\n=\\n1* ,\\n*skip\\\\_engine\\n:\\nbool\\n=\\nFalse* ,\\n*\\\\** ,\\n*\\\\_cleanup\\n:\\nbool\\n=\\nTrue* ,\\n*\\\\*\\\\*\\nkwargs* )\\n[[source]](../../_modules/furiosa_llm/api.html#LLM) [#](#furiosa_llm.LLM \"Link to this definition\")\\nBases: `object`  An LLM for generating texts from given prompts and sampling parameters.\\nParameters :\\n* **pretrained\\\\_id**   – The name of the pretrained model. This corresponds to   pretrained\\\\_model\\\\_name\\\\_or\\\\_path in HuggingFace Transformers. * **task\\\\_type**   – The type of the task. This corresponds to task in HuggingFace Transformers.   See   <https://huggingface.co/docs/transformers/main/en/quicktour#pipeline>   for more   details. * **llm\\\\_config**   – The configuration for the LLM. This includes quantization and optimization   configurations. * **qformat\\\\_path**   – The path to the quantization format file. * **qparam\\\\_path**   – The path to the quantization parameter file. * **prefill\\\\_quant\\\\_bin\\\\_path**   – The path to the quantziation prefill bin file. * **decode\\\\_quant\\\\_bin\\\\_path**   – The path to the quantziation decode bin file. * **config**   – The configuration for the HuggingFace Transformers model. This is a dictionary   that includes the configuration for the model. * **bucket\\\\_config**   – Config for bucket generating policy. If not given, the model will use single one batch,   max\\\\_seq\\\\_len\\\\_to\\\\_capture      attention size bucket per   each phase. * **max\\\\_seq\\\\_len\\\\_to\\\\_capture**   – Maximum sequence length covered by LLM engine. Sequence with larger context than this will not be covered.   The default is 2048. * **tensor\\\\_parallel\\\\_size**   – The number of PEs for each tensor parallelism group. The default is 4. * **pipeline\\\\_parallel\\\\_size**   – The number of pipeline stages for pipeline parallelism. The default is 1,   which means no pipeline parallelism. * **data\\\\_parallel\\\\_size**   – The size of the data parallelism group. If not given, it will be inferred from   total avaialble PEs and other parallelism degrees. * **tokenizer**   – The name or path of a HuggingFace Transformers tokenizer. * **tokenizer\\\\_mode**   – The tokenizer mode. “auto” will use the fast tokenizer   if available, and “slow” will always use the slow tokenizer. * **seed**   – The seed to initialize the random number generator for sampling. * **devices**   – The devices to run the model. It can be a single device or a list of devices.   Each device can be either “cpu:X” or “cuda:X” where X is a specific device index.   The default is “cpu:0”. * **param\\\\_file\\\\_path**   – The path to the parameter file to use for pipeline generation.   If not specified, the parameters will be saved in a temporary file which will be   used for pipeline generation. * **param\\\\_saved\\\\_format**   – The format of the parameter file. Only possible value is “safetensors” now.   The default is “safetensors”. * **do\\\\_decompositions\\\\_for\\\\_model\\\\_rewrite**   – Whether to decompose some ops to describe various parallelism strategies   with mppp config. When the value is True, mppp config that matches with the decomposed FX graph should be given. * **comp\\\\_supertask\\\\_kind**   – The format that pipeline’s supertask will be represented as.   Possible values are “fx”,”dfg”, and “edf”, and the default is “fx”. * **cache\\\\_dir**   – The cache directory for all generated files for this LLM instance.   When its value is   `None`   , caching is disabled. The default is “$HOME/.cache/furiosa/llm”. * **backend**   – The backend implementation to run forward() of a model for the LLM.   The default is LLMBackend.TORCH\\\\_PIPELINE\\\\_RUNNER. * **use\\\\_blockwise\\\\_compile**   – If True, each task will be compiled in the unit of transformer block,   and compilation result for transformer block is generated once and reused. * **num\\\\_blocks\\\\_per\\\\_supertask**   – The number of transformer blocks that will be merged into one supertask. This option is valid   only when   use\\\\_blockwise\\\\_compile=True      . The default is 1. * **embed\\\\_all\\\\_constants\\\\_into\\\\_graph**   – Whether to embed constant tensors into graph or make them as input of the graph and save them as separate files. * **paged\\\\_attention\\\\_num\\\\_blocks**   – The maximum number of blocks that each k/v storage per layer can store. This argument must be given   if model uses paged attention. * **paged\\\\_attention\\\\_block\\\\_size**   – The maximum number of tokens that can be stored in a single paged attention block. This argument must be given   if model uses paged attention. * **kv\\\\_cache\\\\_sharing\\\\_across\\\\_beams\\\\_config**   – Configuration for sharing kv cache across beams. This argument must be given if and only if   the model is optimized to share kv cache across beams. If this argument is given, decode phase buckets with batch size of   `batch_size`   \\\\*   `kv_cache_sharing_across_beams_config.beam_width`   will be created. * **scheduler\\\\_config**   – Configuration for the scheduler, allowing to maximum number of tasks which can be queued to HW, maximum number of samples   that can be processed by the scheduler, and ratio of spare blocks that are reserved by scheduler. * **packing\\\\_type**   – Packing algorithm. Possible values are “IDENTITY” only for now * **compiler\\\\_config\\\\_overrides**   – Overrides for the compiler config. This is a dictionary that includes the configuration for the compiler. * **use\\\\_random\\\\_weight**   – If True, the model will be initialized with random weights. * **num\\\\_pipeline\\\\_builder\\\\_workers**   – number of workers used for building pipelines (except for compilation). The default is 1 (no parallelism).   Setting this value larger than 1 reduces pipeline building time, especially for large models, but requires much more memory. * **num\\\\_compile\\\\_workers**   – number of workers used for compilation. The default is 1 (no parallelism). * **skip\\\\_engine**   – If True, the native runtime engine will not be initialized. This is useful when you need   the pipelines for other purposes than running them with the engine.\\ngenerate\\n(\\n*prompts:\\nstr\\n|\\n~typing.List[str],\\nsampling\\\\_params:\\n~furiosa\\\\_llm.sampling\\\\_params.SamplingParams\\n=\\nSamplingParams(n=1,\\nbest\\\\_of=1,\\ntemperature=1.0,\\ntop\\\\_p=1.0,\\ntop\\\\_k=-1,\\nuse\\\\_beam\\\\_search=False,\\nlength\\\\_penalty=1.0,\\nearly\\\\_stopping=False,\\nmax\\\\_tokens=16min\\\\_tokens=0,\\n,\\nprompt\\\\_token\\\\_ids:\\n~typing.List[int]\\n|\\n~typing.List[~typing.List[int]]\\n|\\nNone\\n=\\nNone* )\\n→\\nRequestOutput\\n|\\nList\\n[\\nRequestOutput\\n]\\n[[source]](../../_modules/furiosa_llm/api.html#LLM.generate) [#](#furiosa_llm.LLM.generate \"Link to this definition\")\\nGenerate texts from given prompts and sampling parameters.\\nParameters :\\n* **prompts**   – The prompts to generate texts. * **sampling\\\\_params**   – The sampling parameters for generating texts. * **prompt\\\\_token\\\\_ids**   – The token ids of the prompts. If not given, the token ids are   generated from the prompts using the tokenizer.\\nReturns :\\nA list of RequestOutput\\nobjects containing the generated completions in the same order as the input prompts.\\nget\\\\_splitted\\\\_gms\\n(\\n*get\\\\_input\\\\_constants\\n:\\nbool\\n=\\nFalse* )\\n→\\nDict\\n[\\nstr\\n,\\nTuple\\n[\\nGraphModule\\n,\\n...\\n]\\n|\\nTuple\\n[\\nTuple\\n[\\nGraphModule\\n,\\nTuple\\n[\\nTensor\\n|\\nNone\\n,\\n...\\n]\\n]\\n,\\n...\\n]\\n]\\n[[source]](../../_modules/furiosa_llm/api.html#LLM.get_splitted_gms) [#](#furiosa_llm.LLM.get_splitted_gms \"Link to this definition\")\\nGet sub GraphModules for each pipeline.\\nReturns :\\nDictionary whose key is the pipeline name and value is the tuple containing `GraphModule``s\\n(computation\\nsupertasks)\\nand\\nsome\\nadditional\\ninformation\\nif\\nnecessary.\\nif\\n``get_input_constants==False` , each value is just a tuple of `GraphModule``s\\nin\\nthe\\npipeline.\\nOtherwise,\\neach\\nvalue\\nis\\na\\ntuple\\nwhose\\nelement\\nis\\n``GraphModule` in the pipeline and list of input constant tensors, which were originally constant tensors, but converted to input. The list of input constant tensors has same length as corresponding `GraphModule` ’s number of inputs with each element exactly corresponding to the input of the `GraphModule` with same index, but elements with original input tensor indexes are `None` .\\nReturn type :\\nDict[str, Union[Tuple[GraphModule, …], Tuple[Tuple[GraphModule, Tuple[Optional[torch.Tensor], …]], …],],]\\n[previous\\nReferences](../references.html \"previous page\") [next\\nSamplingParams](sampling_params.html \"next page\")\\nContents\\n* [`LLM`](#furiosa_llm.LLM)   + [`LLM.generate()`](#furiosa_llm.LLM.generate)   + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='85efd72a-7fd9-4895-a0c7-090891c1e2cf', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/references.html', 'title': 'references', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../_sources/furiosa_llm/references.rst \"Download source file\") * .pdf\\nReferences ==========\\nReferences [#](#references \"Link to this heading\") ==================================================\\nReferences\\n* [LLM class](references/llm.html) * [SamplingParams](references/sampling_params.html)\\n[previous\\nOpenAI Compatible Server](furiosa-llm-serve.html \"previous page\") [next\\nLLM class](references/llm.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../_sources/furiosa_llm/references.rst \"Download source file\") * .pdf\\nReferences ==========\\nReferences [#](#references \"Link to this heading\") ==================================================\\nReferences\\n* [LLM class](references/llm.html) * [SamplingParams](references/sampling_params.html)\\n[previous\\nOpenAI Compatible Server](furiosa-llm-serve.html \"previous page\") [next\\nLLM class](references/llm.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='fe9686e2-f00f-4e68-87fa-80abbaf03e2b', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/metrics_exporter.html', 'title': 'metrics_exporter', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../../_sources/cloud_native_toolkit/kubernetes/metrics_exporter.rst \"Download source file\") * .pdf\\nInstalling Furiosa Metrics Exporter ===================================\\nContents --------\\n* [Furiosa Metrics Exporter](#furiosa-metrics-exporter)   + [Metrics](#metrics)   + [Deploying Furiosa Metrics Exporter with Helm](#deploying-furiosa-metrics-exporter-with-helm)\\nInstalling Furiosa Metrics Exporter [#](#installing-furiosa-metrics-exporter \"Link to this heading\") ====================================================================================================\\nFuriosa Metrics Exporter [#](#furiosa-metrics-exporter \"Link to this heading\") ------------------------------------------------------------------------------\\nThe Furiosa metrics exporter exposes collection of metrics related to FuriosaAI NPU devices in [Prometheus](https://prometheus.io/) format. In a Kubernetes cluster, you can scrape the metrics provided by furiosa-metrics-exporter using Prometheus and visualize them with a Grafana dashboard. This can be easily set up using the [Prometheus Chart](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus) and [Grafana](https://github.com/grafana/helm-charts/tree/main/charts/grafana) Helm charts, along with the furiosa-metrics-exporter Helm chart.\\n### Metrics [#](#metrics \"Link to this heading\")\\nThe exporter is composed of chain of collectors, each collector is responsible for collecting specific metrics from the Furiosa NPU devices. The following table shows the available collectors and metrics:\\nNPU Metrics\\n[#](#id1 \"Link to this table\")\\n| Collector Name | Metric | Type | Metric Labels | Description | | --- | --- | --- | --- | --- | | Liveness | furiosa\\\\_npu\\\\_alive | guage | arch, core, device, uuid, kubernetes\\\\_node\\\\_name | The liveness of the Furiosa NPU device. | | Error | furiosa\\\\_npu\\\\_error | guage | arch, core, device, uuid, kubernetes\\\\_node\\\\_name, label | The error count of the Furiosa NPU device. | | Temperature | furiosa\\\\_npu\\\\_hw\\\\_temperature | guage | arch, core, device, uuid, kubernetes\\\\_node\\\\_name, label | The temperature of the Furiosa NPU device. | | Power | furiosa\\\\_npu\\\\_hw\\\\_power | guage | arch, core, device, uuid, kubernetes\\\\_node\\\\_name, label | The power consumption of the Furiosa NPU device. | | Core Utilization | furiosa\\\\_npu\\\\_core\\\\_utilization | guage | arch, core, device, uuid, kubernetes\\\\_node\\\\_name | The core utilization of the Furiosa NPU device. |\\nAll metrics share common metric labels such as arch, core, device, kubernetes\\\\_node\\\\_name, and uuid. The following table describes the common metric labels:\\nCommon NPU Metrics Label\\n[#](#id2 \"Link to this table\")\\n| Common Metric Label | Description | | --- | --- | | arch | The architecture of the Furiosa NPU device. e.g. warboy, rngd | | core | The core number of the Furiosa NPU device. e.g. 0, 1, 2, 3, 4, 5, 6, 7, 0-1, 2-3, 0-3, 4-5, 6-7, 4-7, 0-7 | | device | The device name of the Furiosa NPU device. e.g. npu0 | | kubernetes\\\\_node\\\\_name | The name of the Kubernetes node where the exporter is running, this attribute can be missing if the exporter is running on the host machine or in a naked container. | | uuid | The UUID of the Furiosa NPU device. |\\nThe metric label “label” is used to describe additional attributes specific to each metric. This approach helps avoid having too many metric definitions and effectively aggregates metrics that share common characteristics.\\nNPU Metrics Type\\n[#](#id3 \"Link to this table\")\\n| Metric Type | Label Attribute | Description | | --- | --- | --- | | Error | axi\\\\_post\\\\_error | Indicates count of axi post error. | | Error | axi\\\\_fetch\\\\_error | Indicates count of axi fetch error. | | Error | axi\\\\_discard\\\\_error | Indicates count of axi discard error. | | Error | axi\\\\_doorbell\\\\_done | Indicates count of axi doorbell done error. | | Error | pcie\\\\_post\\\\_error | Indicates count of PCIe post error. | | Error | pcie\\\\_fetch\\\\_error | Indicates count of PCIe fetch error. | | Error | pcie\\\\_discard\\\\_error | Indicates count of PCIe discard error. | | Error | pcie\\\\_doorbell\\\\_done | Indicates count of PCIe doorbell done error. | | Error | device\\\\_error | Total count of device error. | | Temperature | peak | The highest temperature observed from SoC sensors | | Temperature | ambient | The temperature observed from sensors attached to the board | | Power | rms | Root Mean Square (RMS) value of the power consumed by the device, providing an average power consumption metric over a period of time. |\\nThe following shows real-world example of the metrics:\\n``` #liveness furiosa_npu_alive{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 1\\n#error furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"axi_post_error\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"axi_fetch_error\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"axi_discard_error\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"axi_doorbell_done\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"pcie_post_error\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"pcie_fetch_error\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"pcie_discard_error\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"pcie_doorbell_done\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"device_error\",uuid=\"uuid\"} 0\\n#temperature furiosa_npu_hw_temperature{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"peak\",uuid=\"uuid\"} 39 furiosa_npu_hw_temperature{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"ambient\",uuid=\"uuid\"} 35\\n#power furiosa_npu_hw_power{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"rms\",uuid=\"uuid\"} 4795000\\n#core utilization furiosa_npu_core_utilization{arch=\"rngd\",core=\"0\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90 furiosa_npu_core_utilization{arch=\"rngd\",core=\"1\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90 furiosa_npu_core_utilization{arch=\"rngd\",core=\"2\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90 furiosa_npu_core_utilization{arch=\"rngd\",core=\"3\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90 furiosa_npu_core_utilization{arch=\"rngd\",core=\"4\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90 furiosa_npu_core_utilization{arch=\"rngd\",core=\"5\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90 furiosa_npu_core_utilization{arch=\"rngd\",core=\"6\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90 furiosa_npu_core_utilization{arch=\"rngd\",core=\"7\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90\\n```\\n### Deploying Furiosa Metrics Exporter with Helm [#](#deploying-furiosa-metrics-exporter-with-helm \"Link to this heading\")\\nThe Furiosa metrics exporter helm chart is available at [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) . To configure deployment as you need, you can modify `charts/furiosa-metrics-exporter/values.yaml` . For example, the Furiosa metrics exporter Helm chart automatically creates a Service Object with Prometheus annotations to enable metric scraping automatically. You can modify the values.yaml to change the port or disable the Prometheus annotations if needed. You can deploy the Furiosa Metrics Exporter by running the following commands:\\n``` helm repo add furiosa https://furiosa-ai.github.io/helm-charts helm repo update helm install furiosa-metrics-exporter furiosa/furiosa-metrics-exporter -n kube-system\\n```\\n[previous\\nInstalling Furiosa Device Plugin](device_plugin.html \"previous page\") [next\\nScheduling NPUs](scheduling_npus.html \"next page\")\\nContents\\n* [Furiosa Metrics Exporter](#furiosa-metrics-exporter)   + [Metrics](#metrics)   + [Deploying Furiosa Metrics Exporter with Helm](#deploying-furiosa-metrics-exporter-with-helm)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../../_sources/cloud_native_toolkit/kubernetes/metrics_exporter.rst \"Download source file\") * .pdf\\nInstalling Furiosa Metrics Exporter ===================================\\nContents --------\\n* [Furiosa Metrics Exporter](#furiosa-metrics-exporter)   + [Metrics](#metrics)   + [Deploying Furiosa Metrics Exporter with Helm](#deploying-furiosa-metrics-exporter-with-helm)\\nInstalling Furiosa Metrics Exporter [#](#installing-furiosa-metrics-exporter \"Link to this heading\") ====================================================================================================\\nFuriosa Metrics Exporter [#](#furiosa-metrics-exporter \"Link to this heading\") ------------------------------------------------------------------------------\\nThe Furiosa metrics exporter exposes collection of metrics related to FuriosaAI NPU devices in [Prometheus](https://prometheus.io/) format. In a Kubernetes cluster, you can scrape the metrics provided by furiosa-metrics-exporter using Prometheus and visualize them with a Grafana dashboard. This can be easily set up using the [Prometheus Chart](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus) and [Grafana](https://github.com/grafana/helm-charts/tree/main/charts/grafana) Helm charts, along with the furiosa-metrics-exporter Helm chart.\\n### Metrics [#](#metrics \"Link to this heading\")\\nThe exporter is composed of chain of collectors, each collector is responsible for collecting specific metrics from the Furiosa NPU devices. The following table shows the available collectors and metrics:\\nNPU Metrics\\n[#](#id1 \"Link to this table\")\\n| Collector Name | Metric | Type | Metric Labels | Description | | --- | --- | --- | --- | --- | | Liveness | furiosa\\\\_npu\\\\_alive | guage | arch, core, device, uuid, kubernetes\\\\_node\\\\_name | The liveness of the Furiosa NPU device. | | Error | furiosa\\\\_npu\\\\_error | guage | arch, core, device, uuid, kubernetes\\\\_node\\\\_name, label | The error count of the Furiosa NPU device. | | Temperature | furiosa\\\\_npu\\\\_hw\\\\_temperature | guage | arch, core, device, uuid, kubernetes\\\\_node\\\\_name, label | The temperature of the Furiosa NPU device. | | Power | furiosa\\\\_npu\\\\_hw\\\\_power | guage | arch, core, device, uuid, kubernetes\\\\_node\\\\_name, label | The power consumption of the Furiosa NPU device. | | Core Utilization | furiosa\\\\_npu\\\\_core\\\\_utilization | guage | arch, core, device, uuid, kubernetes\\\\_node\\\\_name | The core utilization of the Furiosa NPU device. |\\nAll metrics share common metric labels such as arch, core, device, kubernetes\\\\_node\\\\_name, and uuid. The following table describes the common metric labels:\\nCommon NPU Metrics Label\\n[#](#id2 \"Link to this table\")\\n| Common Metric Label | Description | | --- | --- | | arch | The architecture of the Furiosa NPU device. e.g. warboy, rngd | | core | The core number of the Furiosa NPU device. e.g. 0, 1, 2, 3, 4, 5, 6, 7, 0-1, 2-3, 0-3, 4-5, 6-7, 4-7, 0-7 | | device | The device name of the Furiosa NPU device. e.g. npu0 | | kubernetes\\\\_node\\\\_name | The name of the Kubernetes node where the exporter is running, this attribute can be missing if the exporter is running on the host machine or in a naked container. | | uuid | The UUID of the Furiosa NPU device. |\\nThe metric label “label” is used to describe additional attributes specific to each metric. This approach helps avoid having too many metric definitions and effectively aggregates metrics that share common characteristics.\\nNPU Metrics Type\\n[#](#id3 \"Link to this table\")\\n| Metric Type | Label Attribute | Description | | --- | --- | --- | | Error | axi\\\\_post\\\\_error | Indicates count of axi post error. | | Error | axi\\\\_fetch\\\\_error | Indicates count of axi fetch error. | | Error | axi\\\\_discard\\\\_error | Indicates count of axi discard error. | | Error | axi\\\\_doorbell\\\\_done | Indicates count of axi doorbell done error. | | Error | pcie\\\\_post\\\\_error | Indicates count of PCIe post error. | | Error | pcie\\\\_fetch\\\\_error | Indicates count of PCIe fetch error. | | Error | pcie\\\\_discard\\\\_error | Indicates count of PCIe discard error. | | Error | pcie\\\\_doorbell\\\\_done | Indicates count of PCIe doorbell done error. | | Error | device\\\\_error | Total count of device error. | | Temperature | peak | The highest temperature observed from SoC sensors | | Temperature | ambient | The temperature observed from sensors attached to the board | | Power | rms | Root Mean Square (RMS) value of the power consumed by the device, providing an average power consumption metric over a period of time. |\\nThe following shows real-world example of the metrics:\\n``` #liveness furiosa_npu_alive{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 1\\n#error furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"axi_post_error\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"axi_fetch_error\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"axi_discard_error\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"axi_doorbell_done\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"pcie_post_error\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"pcie_fetch_error\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"pcie_discard_error\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"pcie_doorbell_done\",uuid=\"uuid\"} 0 furiosa_npu_error{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"device_error\",uuid=\"uuid\"} 0\\n#temperature furiosa_npu_hw_temperature{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"peak\",uuid=\"uuid\"} 39 furiosa_npu_hw_temperature{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"ambient\",uuid=\"uuid\"} 35\\n#power furiosa_npu_hw_power{arch=\"rngd\",core=\"0-7\",device=\"npu0\",kubernetes_node_name=\"node\",label=\"rms\",uuid=\"uuid\"} 4795000\\n#core utilization furiosa_npu_core_utilization{arch=\"rngd\",core=\"0\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90 furiosa_npu_core_utilization{arch=\"rngd\",core=\"1\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90 furiosa_npu_core_utilization{arch=\"rngd\",core=\"2\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90 furiosa_npu_core_utilization{arch=\"rngd\",core=\"3\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90 furiosa_npu_core_utilization{arch=\"rngd\",core=\"4\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90 furiosa_npu_core_utilization{arch=\"rngd\",core=\"5\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90 furiosa_npu_core_utilization{arch=\"rngd\",core=\"6\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90 furiosa_npu_core_utilization{arch=\"rngd\",core=\"7\",device=\"npu0\",kubernetes_node_name=\"node\",uuid=\"uuid\"} 90\\n```\\n### Deploying Furiosa Metrics Exporter with Helm [#](#deploying-furiosa-metrics-exporter-with-helm \"Link to this heading\")\\nThe Furiosa metrics exporter helm chart is available at [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) . To configure deployment as you need, you can modify `charts/furiosa-metrics-exporter/values.yaml` . For example, the Furiosa metrics exporter Helm chart automatically creates a Service Object with Prometheus annotations to enable metric scraping automatically. You can modify the values.yaml to change the port or disable the Prometheus annotations if needed. You can deploy the Furiosa Metrics Exporter by running the following commands:\\n``` helm repo add furiosa https://furiosa-ai.github.io/helm-charts helm repo update helm install furiosa-metrics-exporter furiosa/furiosa-metrics-exporter -n kube-system\\n```\\n[previous\\nInstalling Furiosa Device Plugin](device_plugin.html \"previous page\") [next\\nScheduling NPUs](scheduling_npus.html \"next page\")\\nContents\\n* [Furiosa Metrics Exporter](#furiosa-metrics-exporter)   + [Metrics](#metrics)   + [Deploying Furiosa Metrics Exporter with Helm](#deploying-furiosa-metrics-exporter-with-helm)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='a1f21dc2-1473-410f-a3ae-e46d451a9c0e', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/feature_discovery.html', 'title': 'feature_discovery', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../../_sources/cloud_native_toolkit/kubernetes/feature_discovery.rst \"Download source file\") * .pdf\\nInstalling Furiosa Feature Discovery ====================================\\nContents --------\\n* [Furiosa Feature discovery and NFD](#furiosa-feature-discovery-and-nfd)   + [Labels](#labels)   + [Deploying Furiosa Feature Discovery with Helm](#deploying-furiosa-feature-discovery-with-helm)\\nInstalling Furiosa Feature Discovery [#](#installing-furiosa-feature-discovery \"Link to this heading\") ======================================================================================================\\nFuriosa Feature discovery and NFD [#](#furiosa-feature-discovery-and-nfd \"Link to this heading\") ------------------------------------------------------------------------------------------------\\nThe Furiosa feature discovery automatically labels Kubernetes nodes with information about FuriosaAI NPU properties, such as the NPU family, count, and driver versions. Using these labels, you can schedule your Kubernetes workloads based on specific NPU requirements.\\nThe Furiosa feature Discovery leverage NFD(Node Feature Discovery) which is a tool that detects hardware features and labels Kubernetes nodes. It is recommended to use NFD and Furiosa Feature Discovery to ensure that the Cloud Native Toolkit is deployed only on nodes equipped with FuriosaAI NPUs.\\n### Labels [#](#labels \"Link to this heading\")\\nThe followings are the labels that the Furiosa Feature Discovery attaches and what they mean.\\nLabels\\n[#](#id1 \"Link to this table\")\\n| Label | Value | Description | | --- | --- | --- | | furiosa.ai/npu.count | n | # of NPU devices | | furiosa.ai/npu.family | warboy, rngd | Chip family | | furiosa.ai/npu.product | warboy, rngd, rngd-s, rngd-max | Chip product name | | furiosa.ai/npu.driver.version | x.y.z | NPU device driver version | | furiosa.ai/npu.driver.version.major | x | NPU device driver version major part | | furiosa.ai/npu.driver.version.minor | y | NPU device driver version minor part | | furiosa.ai/npu.driver.version.patch | z | NPU device driver version patch part | | furiosa.ai/npu.driver.version.metadata | abcxyz | NPU device driver version metadata |\\n### Deploying Furiosa Feature Discovery with Helm [#](#deploying-furiosa-feature-discovery-with-helm \"Link to this heading\")\\nWith the helm chart you can easily install Furiosa feature discovery and NFD into your Kubernetes cluster. Following command shows how to install them. The Furiosa device plugin helm chart is available at [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) . To configure deployment as you need, you can modify `charts/furiosa-feature-discovery/values.yaml` .\\n``` helm repo add furiosa https://furiosa-ai.github.io/helm-charts helm repo update helm install furiosa-feature-discovery furiosa/furiosa-feature-discovery -n kube-system\\n```\\n[previous\\nKubernetes Support](../kubernetes.html \"previous page\") [next\\nInstalling Furiosa Device Plugin](device_plugin.html \"next page\")\\nContents\\n* [Furiosa Feature discovery and NFD](#furiosa-feature-discovery-and-nfd)   + [Labels](#labels)   + [Deploying Furiosa Feature Discovery with Helm](#deploying-furiosa-feature-discovery-with-helm)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../../_sources/cloud_native_toolkit/kubernetes/feature_discovery.rst \"Download source file\") * .pdf\\nInstalling Furiosa Feature Discovery ====================================\\nContents --------\\n* [Furiosa Feature discovery and NFD](#furiosa-feature-discovery-and-nfd)   + [Labels](#labels)   + [Deploying Furiosa Feature Discovery with Helm](#deploying-furiosa-feature-discovery-with-helm)\\nInstalling Furiosa Feature Discovery [#](#installing-furiosa-feature-discovery \"Link to this heading\") ======================================================================================================\\nFuriosa Feature discovery and NFD [#](#furiosa-feature-discovery-and-nfd \"Link to this heading\") ------------------------------------------------------------------------------------------------\\nThe Furiosa feature discovery automatically labels Kubernetes nodes with information about FuriosaAI NPU properties, such as the NPU family, count, and driver versions. Using these labels, you can schedule your Kubernetes workloads based on specific NPU requirements.\\nThe Furiosa feature Discovery leverage NFD(Node Feature Discovery) which is a tool that detects hardware features and labels Kubernetes nodes. It is recommended to use NFD and Furiosa Feature Discovery to ensure that the Cloud Native Toolkit is deployed only on nodes equipped with FuriosaAI NPUs.\\n### Labels [#](#labels \"Link to this heading\")\\nThe followings are the labels that the Furiosa Feature Discovery attaches and what they mean.\\nLabels\\n[#](#id1 \"Link to this table\")\\n| Label | Value | Description | | --- | --- | --- | | furiosa.ai/npu.count | n | # of NPU devices | | furiosa.ai/npu.family | warboy, rngd | Chip family | | furiosa.ai/npu.product | warboy, rngd, rngd-s, rngd-max | Chip product name | | furiosa.ai/npu.driver.version | x.y.z | NPU device driver version | | furiosa.ai/npu.driver.version.major | x | NPU device driver version major part | | furiosa.ai/npu.driver.version.minor | y | NPU device driver version minor part | | furiosa.ai/npu.driver.version.patch | z | NPU device driver version patch part | | furiosa.ai/npu.driver.version.metadata | abcxyz | NPU device driver version metadata |\\n### Deploying Furiosa Feature Discovery with Helm [#](#deploying-furiosa-feature-discovery-with-helm \"Link to this heading\")\\nWith the helm chart you can easily install Furiosa feature discovery and NFD into your Kubernetes cluster. Following command shows how to install them. The Furiosa device plugin helm chart is available at [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) . To configure deployment as you need, you can modify `charts/furiosa-feature-discovery/values.yaml` .\\n``` helm repo add furiosa https://furiosa-ai.github.io/helm-charts helm repo update helm install furiosa-feature-discovery furiosa/furiosa-feature-discovery -n kube-system\\n```\\n[previous\\nKubernetes Support](../kubernetes.html \"previous page\") [next\\nInstalling Furiosa Device Plugin](device_plugin.html \"next page\")\\nContents\\n* [Furiosa Feature discovery and NFD](#furiosa-feature-discovery-and-nfd)   + [Labels](#labels)   + [Deploying Furiosa Feature Discovery with Helm](#deploying-furiosa-feature-discovery-with-helm)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='a53038c2-0668-4963-875e-79abe9c99e2c', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/whatsnew/index.html', 'title': 'index', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../_sources/whatsnew/index.rst \"Download source file\") * .pdf\\nWhat’s New ==========\\nContents --------\\n* [Furiosa SDK 2024.1.0 (2024-10-11)](#furiosa-sdk-2024-1-0-2024-10-11)   + [Highlights](#highlights)\\nWhat’s New [#](#what-s-new \"Link to this heading\") ==================================================\\nThis page describes the changes and functionality available in in the latest releases of Furiosa SDK 2024.1.0.\\nFuriosa SDK 2024.1.0 (2024-10-11) [#](#furiosa-sdk-2024-1-0-2024-10-11 \"Link to this heading\") ----------------------------------------------------------------------------------------------\\n2024.1.0 is the first SDK release for RNGD. This release is alpha release, and the features and APIs described in this document may change in the future.\\n### Highlights [#](#highlights \"Link to this heading\")\\n* Model Support: LLaMA 3.1 8B/70B, BERT Large, GPT-J 6B * Furiosa Quantizer supports the following quantization methods:      + BF16 (W16A16)   + INT8 Weight-Only (W8A16)   + FP8 (W8A8)   + INT8 SmoothQuant (W8A8) * Furiosa LLM      + Efficient KV cache management with PagedAttention   + Continuous batching support in serving   + OpenAI-compatible API server   + Greedy search and beam search   + Pipeline Parallelism and Data Parallelism across multiple NPUs * `furiosa-mlperf`   command      + Server and Offline scenarios   + BERT, GPT-J, LLaMA 3.1 benchmarks * System Management Interface      + System Management Interface Library and CLI for Furiosa NPU family * Cloud Native Toolkit      + Kubernetes integration for managing and monitoring the Furiosa NPU family\\nComponent version\\n[#](#id1 \"Link to this table\")\\n| Package name | Version | | --- | --- | | furiosa-compiler | 2024.1.0 | | furiosa-device-plugin | 2024.1.0 | | furiosa-driver-rngd | 2024.1.0 | | furiosa-feature-discovery | 2024.1.0 | | furiosa-firmware-image-tools | 2024.1.0 | | furiosa-firmware-image-rngd | 0.0.19 | | furiosa-libsmi | 2024.1.0 | | furiosa-llm | 2024.1.0 | | furiosa-llm-models | 2024.1.0 | | furiosa-mlperf | 2024.1.0 | | furiosa-mlperf-resources | 2024.1.0 | | furiosa-model-compressor | 2024.1.0 | | furiosa-model-compressor-impl | 2024.1.0 | | furiosa-native-compiler | 2024.1.0 | | furiosa-native-runtime | 2024.1.0 | | furiosa-smi | 2024.1.0 | | furiosa-torch-ext | 2024.1.0 |\\n[previous\\nSupported Models](../overview/supported_models.html \"previous page\") [next\\nRoadmap](../overview/roadmap.html \"next page\")\\nContents\\n* [Furiosa SDK 2024.1.0 (2024-10-11)](#furiosa-sdk-2024-1-0-2024-10-11)   + [Highlights](#highlights)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../_sources/whatsnew/index.rst \"Download source file\") * .pdf\\nWhat’s New ==========\\nContents --------\\n* [Furiosa SDK 2024.1.0 (2024-10-11)](#furiosa-sdk-2024-1-0-2024-10-11)   + [Highlights](#highlights)\\nWhat’s New [#](#what-s-new \"Link to this heading\") ==================================================\\nThis page describes the changes and functionality available in in the latest releases of Furiosa SDK 2024.1.0.\\nFuriosa SDK 2024.1.0 (2024-10-11) [#](#furiosa-sdk-2024-1-0-2024-10-11 \"Link to this heading\") ----------------------------------------------------------------------------------------------\\n2024.1.0 is the first SDK release for RNGD. This release is alpha release, and the features and APIs described in this document may change in the future.\\n### Highlights [#](#highlights \"Link to this heading\")\\n* Model Support: LLaMA 3.1 8B/70B, BERT Large, GPT-J 6B * Furiosa Quantizer supports the following quantization methods:      + BF16 (W16A16)   + INT8 Weight-Only (W8A16)   + FP8 (W8A8)   + INT8 SmoothQuant (W8A8) * Furiosa LLM      + Efficient KV cache management with PagedAttention   + Continuous batching support in serving   + OpenAI-compatible API server   + Greedy search and beam search   + Pipeline Parallelism and Data Parallelism across multiple NPUs * `furiosa-mlperf`   command      + Server and Offline scenarios   + BERT, GPT-J, LLaMA 3.1 benchmarks * System Management Interface      + System Management Interface Library and CLI for Furiosa NPU family * Cloud Native Toolkit      + Kubernetes integration for managing and monitoring the Furiosa NPU family\\nComponent version\\n[#](#id1 \"Link to this table\")\\n| Package name | Version | | --- | --- | | furiosa-compiler | 2024.1.0 | | furiosa-device-plugin | 2024.1.0 | | furiosa-driver-rngd | 2024.1.0 | | furiosa-feature-discovery | 2024.1.0 | | furiosa-firmware-image-tools | 2024.1.0 | | furiosa-firmware-image-rngd | 0.0.19 | | furiosa-libsmi | 2024.1.0 | | furiosa-llm | 2024.1.0 | | furiosa-llm-models | 2024.1.0 | | furiosa-mlperf | 2024.1.0 | | furiosa-mlperf-resources | 2024.1.0 | | furiosa-model-compressor | 2024.1.0 | | furiosa-model-compressor-impl | 2024.1.0 | | furiosa-native-compiler | 2024.1.0 | | furiosa-native-runtime | 2024.1.0 | | furiosa-smi | 2024.1.0 | | furiosa-torch-ext | 2024.1.0 |\\n[previous\\nSupported Models](../overview/supported_models.html \"previous page\") [next\\nRoadmap](../overview/roadmap.html \"next page\")\\nContents\\n* [Furiosa SDK 2024.1.0 (2024-10-11)](#furiosa-sdk-2024-1-0-2024-10-11)   + [Highlights](#highlights)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='56ae0c48-fbd4-4d43-b6f7-395542108fa7', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/furiosa_llm.html', 'title': 'furiosa_llm', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../_sources/getting_started/furiosa_llm.rst \"Download source file\") * .pdf\\nQuick Start with Furiosa LLM ============================\\nContents --------\\n* [Installing Furiosa LLM](#installing-furiosa-llm)   + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment)\\nQuick Start with Furiosa LLM [#](#quick-start-with-furiosa-llm \"Link to this heading\") ======================================================================================\\nFuriosa LLM is a serving framework for LLM models that utilizes FuriosaAI’s NPU. It provides a Python API compatible with vLLM and a server compatible with the OpenAI API. This document explains how to install and use Furiosa LLM.\\nWarning\\nThis document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future.\\nInstalling Furiosa LLM [#](#installing-furiosa-llm \"Link to this heading\") --------------------------------------------------------------------------\\nThe minimum requirements for Furiosa LLM are as follows:\\n* Ubuntu 20.04 LTS (Debian bullseye) or later * Administrator privileges on system (root) * [Setting up APT server](prerequisites.html#aptsetup)   and   [Installing Prerequisites](prerequisites.html#installingprerequisites) * Python 3.8, 3.9, or 3.10 * Enough storage space for model weights; e.g., about 100GB for Llama 3.1 70B model\\nThen, please install the `furiosa-compiler` package as follows:\\n``` sudo apt install -y furiosa-compiler\\n```\\nAlso, you need to create a Python virtual environment depending on your environment.\\nNote\\nNote that some models, such as meta-llama/Meta-Llama-3.1-8B require you to accept their license, hence, you need to create a HuggingFace account, accept the model’s license, and generate a token. Usually, you can create your token at <https://huggingface.co/settings/tokens> .\\nOnce you get a token, you can authenticate on the HuggingFace Hub as following:\\n``` huggingface-cli login --token $HF_TOKEN\\n```\\nThen, you can install the Furiosa LLM with the following command:\\n``` pip install furiosa-llm\\n```\\n### Offline Batch Inference with Furiosa LLM [#](#offline-batch-inference-with-furiosa-llm \"Link to this heading\")\\nIn this section, we will explain how to perform offline LLM inference using the Python API of Furiosa LLM. First, import the `LLM` class and `SamplingParams` from the furiosa\\\\_llm module. `LLM` class is used to load LLM models and provides the core API for LLM inference. `SamplingParams` allows to specify various parameters for text generation.\\n``` from furiosa_llm import LLM, SamplingParams\\n```\\nNext, we will load an LLM model using the LLM class. The following example loads the meta-llama/Meta-Llama-3.1-8B model from the HuggingFace Hub and performs quantization. The model will be loaded into the memory and ready for inference.\\n``` llm = LLM(model=\"meta-llama/Llama-3.1-8B-Instruct\")\\n```\\nAfter loading the model, you can perform LLM inference by calling the `generate` method.\\n``` prompts = [   \"\" ]\\nsampling_params = SamplingParams(temperature=0.0)\\n```\\nLaunching the OpenAI-Compatible Server [#](#launching-the-openai-compatible-server \"Link to this heading\") ----------------------------------------------------------------------------------------------------------\\nYou can find more details at [OpenAI Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) .\\nRunning Furiosa LLM in container environment [#](#running-furiosa-llm-in-container-environment \"Link to this heading\") ----------------------------------------------------------------------------------------------------------------------\\nFuriosaAI provides an image for running `furiosa-llm` in a containerized environment. By using the containerized version, you can run environments that utilize `furiosa-llm` without installing the FuriosaAI Software Stack on your host system, or you can execute it within a Kubernetes environment.\\nTo run the `furiosa-llm` container, you can use the following command:\\n``` $ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash\\n(container) # python\\n```\\nWarning\\nThe example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit)\\n[previous\\nInstalling Prerequisites](prerequisites.html \"previous page\") [next\\nRunning MLPerf™ Inference Benchmark](furiosa_mlperf.html \"next page\")\\nContents\\n* [Installing Furiosa LLM](#installing-furiosa-llm)   + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../_sources/getting_started/furiosa_llm.rst \"Download source file\") * .pdf\\nQuick Start with Furiosa LLM ============================\\nContents --------\\n* [Installing Furiosa LLM](#installing-furiosa-llm)   + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment)\\nQuick Start with Furiosa LLM [#](#quick-start-with-furiosa-llm \"Link to this heading\") ======================================================================================\\nFuriosa LLM is a serving framework for LLM models that utilizes FuriosaAI’s NPU. It provides a Python API compatible with vLLM and a server compatible with the OpenAI API. This document explains how to install and use Furiosa LLM.\\nWarning\\nThis document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future.\\nInstalling Furiosa LLM [#](#installing-furiosa-llm \"Link to this heading\") --------------------------------------------------------------------------\\nThe minimum requirements for Furiosa LLM are as follows:\\n* Ubuntu 20.04 LTS (Debian bullseye) or later * Administrator privileges on system (root) * [Setting up APT server](prerequisites.html#aptsetup)   and   [Installing Prerequisites](prerequisites.html#installingprerequisites) * Python 3.8, 3.9, or 3.10 * Enough storage space for model weights; e.g., about 100GB for Llama 3.1 70B model\\nThen, please install the `furiosa-compiler` package as follows:\\n``` sudo apt install -y furiosa-compiler\\n```\\nAlso, you need to create a Python virtual environment depending on your environment.\\nNote\\nNote that some models, such as meta-llama/Meta-Llama-3.1-8B require you to accept their license, hence, you need to create a HuggingFace account, accept the model’s license, and generate a token. Usually, you can create your token at <https://huggingface.co/settings/tokens> .\\nOnce you get a token, you can authenticate on the HuggingFace Hub as following:\\n``` huggingface-cli login --token $HF_TOKEN\\n```\\nThen, you can install the Furiosa LLM with the following command:\\n``` pip install furiosa-llm\\n```\\n### Offline Batch Inference with Furiosa LLM [#](#offline-batch-inference-with-furiosa-llm \"Link to this heading\")\\nIn this section, we will explain how to perform offline LLM inference using the Python API of Furiosa LLM. First, import the `LLM` class and `SamplingParams` from the furiosa\\\\_llm module. `LLM` class is used to load LLM models and provides the core API for LLM inference. `SamplingParams` allows to specify various parameters for text generation.\\n``` from furiosa_llm import LLM, SamplingParams\\n```\\nNext, we will load an LLM model using the LLM class. The following example loads the meta-llama/Meta-Llama-3.1-8B model from the HuggingFace Hub and performs quantization. The model will be loaded into the memory and ready for inference.\\n``` llm = LLM(model=\"meta-llama/Llama-3.1-8B-Instruct\")\\n```\\nAfter loading the model, you can perform LLM inference by calling the `generate` method.\\n``` prompts = [   \"\" ]\\nsampling_params = SamplingParams(temperature=0.0)\\n```\\nLaunching the OpenAI-Compatible Server [#](#launching-the-openai-compatible-server \"Link to this heading\") ----------------------------------------------------------------------------------------------------------\\nYou can find more details at [OpenAI Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) .\\nRunning Furiosa LLM in container environment [#](#running-furiosa-llm-in-container-environment \"Link to this heading\") ----------------------------------------------------------------------------------------------------------------------\\nFuriosaAI provides an image for running `furiosa-llm` in a containerized environment. By using the containerized version, you can run environments that utilize `furiosa-llm` without installing the FuriosaAI Software Stack on your host system, or you can execute it within a Kubernetes environment.\\nTo run the `furiosa-llm` container, you can use the following command:\\n``` $ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash\\n(container) # python\\n```\\nWarning\\nThe example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit)\\n[previous\\nInstalling Prerequisites](prerequisites.html \"previous page\") [next\\nRunning MLPerf™ Inference Benchmark](furiosa_mlperf.html \"next page\")\\nContents\\n* [Installing Furiosa LLM](#installing-furiosa-llm)   + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='79971985-74d0-445e-8c78-a80a93ef215b', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/intro.html', 'title': 'intro', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../_sources/cloud_native_toolkit/intro.rst \"Download source file\") * .pdf\\nCloud Native Toolkit ====================\\nCloud Native Toolkit [#](#cloud-native-toolkit \"Link to this heading\") ======================================================================\\nFuriosaAI Cloud Native Toolkit is a software stack to enable FuriosaAI’s NPU product in Kubernetes and Container ecosystem.\\n[previous\\nSamplingParams](../furiosa_llm/references/sampling_params.html \"previous page\") [next\\nKubernetes Support](kubernetes.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../_sources/cloud_native_toolkit/intro.rst \"Download source file\") * .pdf\\nCloud Native Toolkit ====================\\nCloud Native Toolkit [#](#cloud-native-toolkit \"Link to this heading\") ======================================================================\\nFuriosaAI Cloud Native Toolkit is a software stack to enable FuriosaAI’s NPU product in Kubernetes and Container ecosystem.\\n[previous\\nSamplingParams](../furiosa_llm/references/sampling_params.html \"previous page\") [next\\nKubernetes Support](kubernetes.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='ccf0c765-aa95-4b97-bf6b-fe5650ff8858', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/rngd.html', 'title': 'rngd', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../_sources/overview/rngd.rst \"Download source file\") * .pdf\\nFuriosaAI RNGD ==============\\nFuriosaAI RNGD [#](#furiosaai-rngd \"Link to this heading\") ==========================================================\\nFuriosaAI’s second-generation Neural Processing Unit (NPU), RNGD, is a chip designed for deep learning inference, supporting high-performance Large Language Models (LLM), Multi-Modal LLM, Vision models, and other deep learning models.\\nRNGD is based the Tensor Contraction Processor (TCP) architecture which utilizes TSMC’s 5nm process node, and operates at 1.0 GHz. It offers 512 TOPS and 1024 TOPS of INT8 and INT4 performance respectively. RNGD is configured with two HBM3 modules providing a memory bandwidth of 1.5 TB/s, and supports PCIe Gen5 x16. For multi-tenant environments like Kubernetes and virtual environment, a single RNGD chip can work as 2, 4, 8 individual NPUs, each fully isolated with its own cores and memory bandwidth. RNGD supports Single Root IO Virtualization (SR-IOV) and virtualization for multi-instance NPUs.\\nPlease refer to the followings to learn more about TCP architecture and RNGD:\\n* [TCP: A Tensor Contraction Processor for AI Workloads, ACM/IEEE ISCA 2024](https://ieeexplore.ieee.org/document/10609575)   (   [PDF](https://furiosa.ai/download/FuriosaAI-tensor-contraction-processor-isca24)   ) * [FuriosaAI RNGD: A Tensor Contraction Processor for Sustainable AI Computing, Hotchips 2024](https://hc2024.hotchips.org/#clip=8jnhm5vdlsow) * [Tensor Contraction Processor: The first future-proof AI chip architecture](https://furiosa.ai/blog/tensor-contraction-processor-ai-chip-architecture)\\nRNGD Hardware Specification\\n[#](#id1 \"Link to this table\")\\n| Architecture | Tensor Contraction Processor | | --- | --- | | Process Node | TSMC 5nm | | Frequency | 1.0GHz | | BF16 | 256TFLOPS | | FP8 | 512TFLOPS | | INT8 | 512TOPS | | INT4 | 1024TOPS | | Memory Bandwidth | HBM3 1.5TB/s | | Memory Capacity | HBM3 48GB | | On-Chip SRAM | 256MB | | Interconnect Interface | PCIe Gen5 x16 | | Thermal Solution | Passive | | Thermal Design Power (TDP) | 150W | | Power Connector | 12VHPWR | | Form Factor | PCIe dual-slot full-height 3/4 Length | | Multi-Instance Support | 8 | | Virtualization Support | Yes | | SR-IOV | 8 Virtual Functions | | ECC Memory Support | Yes | | Secure Boot with Root of Trust | Yes |\\n[previous\\nFuriosaAI Developer Center](../index.html \"previous page\") [next\\nFuriosaAI’s Software Stack](software_stack.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../_sources/overview/rngd.rst \"Download source file\") * .pdf\\nFuriosaAI RNGD ==============\\nFuriosaAI RNGD [#](#furiosaai-rngd \"Link to this heading\") ==========================================================\\nFuriosaAI’s second-generation Neural Processing Unit (NPU), RNGD, is a chip designed for deep learning inference, supporting high-performance Large Language Models (LLM), Multi-Modal LLM, Vision models, and other deep learning models.\\nRNGD is based the Tensor Contraction Processor (TCP) architecture which utilizes TSMC’s 5nm process node, and operates at 1.0 GHz. It offers 512 TOPS and 1024 TOPS of INT8 and INT4 performance respectively. RNGD is configured with two HBM3 modules providing a memory bandwidth of 1.5 TB/s, and supports PCIe Gen5 x16. For multi-tenant environments like Kubernetes and virtual environment, a single RNGD chip can work as 2, 4, 8 individual NPUs, each fully isolated with its own cores and memory bandwidth. RNGD supports Single Root IO Virtualization (SR-IOV) and virtualization for multi-instance NPUs.\\nPlease refer to the followings to learn more about TCP architecture and RNGD:\\n* [TCP: A Tensor Contraction Processor for AI Workloads, ACM/IEEE ISCA 2024](https://ieeexplore.ieee.org/document/10609575)   (   [PDF](https://furiosa.ai/download/FuriosaAI-tensor-contraction-processor-isca24)   ) * [FuriosaAI RNGD: A Tensor Contraction Processor for Sustainable AI Computing, Hotchips 2024](https://hc2024.hotchips.org/#clip=8jnhm5vdlsow) * [Tensor Contraction Processor: The first future-proof AI chip architecture](https://furiosa.ai/blog/tensor-contraction-processor-ai-chip-architecture)\\nRNGD Hardware Specification\\n[#](#id1 \"Link to this table\")\\n| Architecture | Tensor Contraction Processor | | --- | --- | | Process Node | TSMC 5nm | | Frequency | 1.0GHz | | BF16 | 256TFLOPS | | FP8 | 512TFLOPS | | INT8 | 512TOPS | | INT4 | 1024TOPS | | Memory Bandwidth | HBM3 1.5TB/s | | Memory Capacity | HBM3 48GB | | On-Chip SRAM | 256MB | | Interconnect Interface | PCIe Gen5 x16 | | Thermal Solution | Passive | | Thermal Design Power (TDP) | 150W | | Power Connector | 12VHPWR | | Form Factor | PCIe dual-slot full-height 3/4 Length | | Multi-Instance Support | 8 | | Virtualization Support | Yes | | SR-IOV | 8 Virtual Functions | | ECC Memory Support | Yes | | Secure Boot with Root of Trust | Yes |\\n[previous\\nFuriosaAI Developer Center](../index.html \"previous page\") [next\\nFuriosaAI’s Software Stack](software_stack.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='a214fb49-b797-4d38-b877-597b6bb059eb', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/prerequisites.html', 'title': 'prerequisites', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../_sources/getting_started/prerequisites.rst \"Download source file\") * .pdf\\nInstalling Prerequisites ========================\\nContents --------\\n* [Requirements](#requirements) * [Verifying if the system has devices](#verifying-if-the-system-has-devices) * [Setting up APT server](#setting-up-apt-server) * [Installing Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware)\\nInstalling Prerequisites [#](#installing-prerequisites \"Link to this heading\") ==============================================================================\\nWe will explain how to install the prerequisite packages necessary for FuriosaAI software stack. The prerequisite packages include device driver, firmware, and PE Runtime. These packages are available in package format for installation on Debian and Ubuntu systems.\\nRequirements [#](#requirements \"Link to this heading\") ------------------------------------------------------\\nThe minimum requirements are as follows:\\n* Ubuntu 20.04 LTS (or Debian bullseye) or later * Administrator privileges on system (root)\\nVerifying if the system has devices [#](#verifying-if-the-system-has-devices \"Link to this heading\") ----------------------------------------------------------------------------------------------------\\nYou can verify the proper installation of FuriosaAI’s devices on your machine by running the following commands:\\n``` lspci -nn | grep FuriosaAI\\n```\\nIf the device is properly installed, you should see the PCI information as shown below.\\n``` 4e:00.0 Processing accelerators [1200]: FuriosaAI, Inc. Device [1ed2:0001] (rev 01)\\n```\\nIf the `lspci` command is not available, please install the following packages and run the commands to update PCIe ID database:\\n``` sudo apt update sudo apt install -y pciutils sudo update-pciids\\n```\\nSetting up APT server [#](#setting-up-apt-server \"Link to this heading\") ------------------------------------------------------------------------\\nTo use the APT server provided by FuriosaAI, you must configure it on Ubuntu or Debian Linux as outlined below.\\n1. Install the required packages and register the signing key.\\n``` sudo apt update sudo apt install -y curl gnupg curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/cloud.google.gpg\\n```\\n2. Configure the APT server according to the instructions provided for the Linux distribution versions.\\n> ``` > echo \"deb [arch=$(dpkg --print-architecture)] http://asia-northeast3-apt.pkg.dev/projects/furiosa-ai $(. /etc/os-release && echo \"$VERSION_CODENAME\") main\" | sudo tee /etc/apt/sources.list.d/furiosa.list >  > ```\\nInstalling Pre-requisite Packages [#](#installing-pre-requisite-packages \"Link to this heading\") ------------------------------------------------------------------------------------------------\\nIf you have registered the APT server as described above, you will be able to install the required packages: the device driver and PE Runtime.\\n``` sudo apt update sudo apt install furiosa-pert-rngd furiosa-driver-rngd\\n```\\n[furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) is an useful CLI tool for listing and managing FuriosaAI NPUs.\\n``` sudo apt install furiosa-smi\\n```\\nChecking NPU devices [#](#checking-npu-devices \"Link to this heading\") ----------------------------------------------------------------------\\nOnce the device driver and [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) are successfully installed, you can check the list of NPU devices as following command:\\n``` furiosa-smi info\\n```\\nOutput:\\n``` +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware       | Temp.   | Power   | PCI-BDF      | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0   | 0.0.16+b4a67ca | 28.88°C | 38.00 W | 0000:4e:00.0 | +------+--------+----------------+---------+---------+--------------+\\n```\\nPlease refer to [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) to learn more about `furiosa-smi` command.\\nUpgrading Device Firmware [#](#upgrading-device-firmware \"Link to this heading\") --------------------------------------------------------------------------------\\nUpgrading firmware versions can improve the performance and stability of the devices. If there is newer firmware in the latest release, you can upgrade them using the following methods:\\n``` sudo apt install furiosa-firmware-tools-rngd sudo apt install furiosa-firmware-image-rngd\\n```\\nInstalling the `furiosa-firmware-image-rngd` package will automatically upgrade the firmware. The process takes approximately 3 to 5 minutes per device to complete.\\n[previous\\nRoadmap](../overview/roadmap.html \"previous page\") [next\\nQuick Start with Furiosa LLM](furiosa_llm.html \"next page\")\\nContents\\n* [Requirements](#requirements) * [Verifying if the system has devices](#verifying-if-the-system-has-devices) * [Setting up APT server](#setting-up-apt-server) * [Installing Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../_sources/getting_started/prerequisites.rst \"Download source file\") * .pdf\\nInstalling Prerequisites ========================\\nContents --------\\n* [Requirements](#requirements) * [Verifying if the system has devices](#verifying-if-the-system-has-devices) * [Setting up APT server](#setting-up-apt-server) * [Installing Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware)\\nInstalling Prerequisites [#](#installing-prerequisites \"Link to this heading\") ==============================================================================\\nWe will explain how to install the prerequisite packages necessary for FuriosaAI software stack. The prerequisite packages include device driver, firmware, and PE Runtime. These packages are available in package format for installation on Debian and Ubuntu systems.\\nRequirements [#](#requirements \"Link to this heading\") ------------------------------------------------------\\nThe minimum requirements are as follows:\\n* Ubuntu 20.04 LTS (or Debian bullseye) or later * Administrator privileges on system (root)\\nVerifying if the system has devices [#](#verifying-if-the-system-has-devices \"Link to this heading\") ----------------------------------------------------------------------------------------------------\\nYou can verify the proper installation of FuriosaAI’s devices on your machine by running the following commands:\\n``` lspci -nn | grep FuriosaAI\\n```\\nIf the device is properly installed, you should see the PCI information as shown below.\\n``` 4e:00.0 Processing accelerators [1200]: FuriosaAI, Inc. Device [1ed2:0001] (rev 01)\\n```\\nIf the `lspci` command is not available, please install the following packages and run the commands to update PCIe ID database:\\n``` sudo apt update sudo apt install -y pciutils sudo update-pciids\\n```\\nSetting up APT server [#](#setting-up-apt-server \"Link to this heading\") ------------------------------------------------------------------------\\nTo use the APT server provided by FuriosaAI, you must configure it on Ubuntu or Debian Linux as outlined below.\\n1. Install the required packages and register the signing key.\\n``` sudo apt update sudo apt install -y curl gnupg curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/cloud.google.gpg\\n```\\n2. Configure the APT server according to the instructions provided for the Linux distribution versions.\\n> ``` > echo \"deb [arch=$(dpkg --print-architecture)] http://asia-northeast3-apt.pkg.dev/projects/furiosa-ai $(. /etc/os-release && echo \"$VERSION_CODENAME\") main\" | sudo tee /etc/apt/sources.list.d/furiosa.list >  > ```\\nInstalling Pre-requisite Packages [#](#installing-pre-requisite-packages \"Link to this heading\") ------------------------------------------------------------------------------------------------\\nIf you have registered the APT server as described above, you will be able to install the required packages: the device driver and PE Runtime.\\n``` sudo apt update sudo apt install furiosa-pert-rngd furiosa-driver-rngd\\n```\\n[furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) is an useful CLI tool for listing and managing FuriosaAI NPUs.\\n``` sudo apt install furiosa-smi\\n```\\nChecking NPU devices [#](#checking-npu-devices \"Link to this heading\") ----------------------------------------------------------------------\\nOnce the device driver and [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) are successfully installed, you can check the list of NPU devices as following command:\\n``` furiosa-smi info\\n```\\nOutput:\\n``` +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware       | Temp.   | Power   | PCI-BDF      | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0   | 0.0.16+b4a67ca | 28.88°C | 38.00 W | 0000:4e:00.0 | +------+--------+----------------+---------+---------+--------------+\\n```\\nPlease refer to [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) to learn more about `furiosa-smi` command.\\nUpgrading Device Firmware [#](#upgrading-device-firmware \"Link to this heading\") --------------------------------------------------------------------------------\\nUpgrading firmware versions can improve the performance and stability of the devices. If there is newer firmware in the latest release, you can upgrade them using the following methods:\\n``` sudo apt install furiosa-firmware-tools-rngd sudo apt install furiosa-firmware-image-rngd\\n```\\nInstalling the `furiosa-firmware-image-rngd` package will automatically upgrade the firmware. The process takes approximately 3 to 5 minutes per device to complete.\\n[previous\\nRoadmap](../overview/roadmap.html \"previous page\") [next\\nQuick Start with Furiosa LLM](furiosa_llm.html \"next page\")\\nContents\\n* [Requirements](#requirements) * [Verifying if the system has devices](#verifying-if-the-system-has-devices) * [Setting up APT server](#setting-up-apt-server) * [Installing Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'),\n",
       " CustomDocument(id_='1ba93fae-bf2e-42c1-a66d-dabbee880912', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/scheduling_npus.html', 'title': 'scheduling_npus', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../../_sources/cloud_native_toolkit/kubernetes/scheduling_npus.rst \"Download source file\") * .pdf\\nScheduling NPUs ===============\\nContents --------\\n* [Preparing Node](#preparing-node) * [Requesting NPUs](#requesting-npus) * [Scheduling NPUs With Specific Requirements](#scheduling-npus-with-specific-requirements)\\nScheduling NPUs [#](#scheduling-npus \"Link to this heading\") ============================================================\\nThis page describes how administrator prepares node and user can consume NPU in Kubernetes.\\nPreparing Node [#](#preparing-node \"Link to this heading\") ----------------------------------------------------------\\nAs an administrator, you have to install [prerequisites](../../getting_started/prerequisites.html#installingprerequisites) such as driver, firmware on nodes and deploy [Furiosa Device Plugin](device_plugin.html#deviceplugin) .\\nOnce you have installed it, your cluster exposes Furiosa NPUs as schedulable resources, such as `furiosa.ai/rngd` .\\nTo ensure your node is ready, you can examine Capacity and/or Allocatable field of `v1.node` object. Here is an example of node that has 2 RNGD NPUs:\\n``` ... status:   ...   allocatable:     cpu: \"20\"     ephemeral-storage: \"1770585791219\"     furiosa.ai/rngd: \"2\"     hugepages-1Gi: \"0\"     hugepages-2Mi: \"0\"     memory: 527727860Ki     pods: \"110\"   capacity:     cpu: \"20\"     ephemeral-storage: 1921208544Ki     furiosa.ai/rngd: \"2\"     hugepages-1Gi: \"0\"     hugepages-2Mi: \"0\"     memory: 527830260Ki     pods: \"110\" ...\\n```\\nThe following command should show the `Capacity` field of each node in the Kubernetes cluster.\\n``` kubectl get nodes -o json | jq -r \\'.items[] | .metadata.name as $name | .status.capacity | to_entries | map(\"    \\\\(.key): \\\\(.value)\") | $name + \":\\\\n  capacity:\\\\n\" + join(\"\\\\n\")\\'\\n```\\nRequesting NPUs [#](#requesting-npus \"Link to this heading\") ------------------------------------------------------------\\nYou can consume NPUs from your containers in a Pod by requesting NPU resources, the same way you request CPU or memory.\\nHowever, since NPUs are exposed as a custom resource, there are some limitations you should be aware of when requesting NPU resources:\\n* You can specify NPU   `limits`   without specifying   `requests`   , because kubernetes will use limit as request if request is not specified. * You can specify NPU in both   `limits`   and   `requests`   but these two values must be equal. * You cannot specify NPU   `request`   without specifying   `limits`   .\\nHere is an example manifest for a Pod that requests 2 RNGD NPUs:\\n``` apiVersion: v1 kind: Pod metadata:   name: example-npu-request spec:   containers:   - name: furiosa     image: furiosaai/furiosa-smi:latest     imagePullPolicy: IfNotPresent     command: [\"sleep\"]     args: [\"120\"]     resources:       limits:         furiosa.ai/rngd: 2\\n```\\nScheduling NPUs With Specific Requirements [#](#scheduling-npus-with-specific-requirements \"Link to this heading\") ------------------------------------------------------------------------------------------------------------------\\nIn certain cases, user may need to schedule NPU workload on node that meet specific hardware or software requirements, such as particular driver versions. If the [Furiosa Feature Discovery](feature_discovery.html#featurediscovery) is deployed in your cluster, nodes are automatically labelled based on their hardware and software configurations including driver version. This allows user to schedule Pod on nodes that meet specific requirements.\\nFollowing example shows how to use affinity to schedule a Pod that request 2 RNGD NPUs with specific driver version:\\n``` apiVersion: v1 kind: Pod metadata:   name: example-npu-scheduling-with-affinity spec:   containers:   - name: furiosa     image: furiosaai/furiosa-smi:latest     imagePullPolicy: IfNotPresent     command: [\"sleep\"]     args: [\"120\"]     resources:       limits:         furiosa.ai/rngd: 2   affinity:     nodeAffinity:       requiredDuringSchedulingIgnoredDuringExecution:         nodeSelectorTerms:         - matchExpressions:           - key: furiosa.ai/driver-version             operator: In             values:             - \"1.0.12\"\\n```\\n[previous\\nInstalling Furiosa Metrics Exporter](metrics_exporter.html \"previous page\") [next\\nfuriosa-smi](../../device_management/furiosa_smi.html \"next page\")\\nContents\\n* [Preparing Node](#preparing-node) * [Requesting NPUs](#requesting-npus) * [Scheduling NPUs With Specific Requirements](#scheduling-npus-with-specific-requirements)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}', page_content='* [.rst](../../_sources/cloud_native_toolkit/kubernetes/scheduling_npus.rst \"Download source file\") * .pdf\\nScheduling NPUs ===============\\nContents --------\\n* [Preparing Node](#preparing-node) * [Requesting NPUs](#requesting-npus) * [Scheduling NPUs With Specific Requirements](#scheduling-npus-with-specific-requirements)\\nScheduling NPUs [#](#scheduling-npus \"Link to this heading\") ============================================================\\nThis page describes how administrator prepares node and user can consume NPU in Kubernetes.\\nPreparing Node [#](#preparing-node \"Link to this heading\") ----------------------------------------------------------\\nAs an administrator, you have to install [prerequisites](../../getting_started/prerequisites.html#installingprerequisites) such as driver, firmware on nodes and deploy [Furiosa Device Plugin](device_plugin.html#deviceplugin) .\\nOnce you have installed it, your cluster exposes Furiosa NPUs as schedulable resources, such as `furiosa.ai/rngd` .\\nTo ensure your node is ready, you can examine Capacity and/or Allocatable field of `v1.node` object. Here is an example of node that has 2 RNGD NPUs:\\n``` ... status:   ...   allocatable:     cpu: \"20\"     ephemeral-storage: \"1770585791219\"     furiosa.ai/rngd: \"2\"     hugepages-1Gi: \"0\"     hugepages-2Mi: \"0\"     memory: 527727860Ki     pods: \"110\"   capacity:     cpu: \"20\"     ephemeral-storage: 1921208544Ki     furiosa.ai/rngd: \"2\"     hugepages-1Gi: \"0\"     hugepages-2Mi: \"0\"     memory: 527830260Ki     pods: \"110\" ...\\n```\\nThe following command should show the `Capacity` field of each node in the Kubernetes cluster.\\n``` kubectl get nodes -o json | jq -r \\'.items[] | .metadata.name as $name | .status.capacity | to_entries | map(\"    \\\\(.key): \\\\(.value)\") | $name + \":\\\\n  capacity:\\\\n\" + join(\"\\\\n\")\\'\\n```\\nRequesting NPUs [#](#requesting-npus \"Link to this heading\") ------------------------------------------------------------\\nYou can consume NPUs from your containers in a Pod by requesting NPU resources, the same way you request CPU or memory.\\nHowever, since NPUs are exposed as a custom resource, there are some limitations you should be aware of when requesting NPU resources:\\n* You can specify NPU   `limits`   without specifying   `requests`   , because kubernetes will use limit as request if request is not specified. * You can specify NPU in both   `limits`   and   `requests`   but these two values must be equal. * You cannot specify NPU   `request`   without specifying   `limits`   .\\nHere is an example manifest for a Pod that requests 2 RNGD NPUs:\\n``` apiVersion: v1 kind: Pod metadata:   name: example-npu-request spec:   containers:   - name: furiosa     image: furiosaai/furiosa-smi:latest     imagePullPolicy: IfNotPresent     command: [\"sleep\"]     args: [\"120\"]     resources:       limits:         furiosa.ai/rngd: 2\\n```\\nScheduling NPUs With Specific Requirements [#](#scheduling-npus-with-specific-requirements \"Link to this heading\") ------------------------------------------------------------------------------------------------------------------\\nIn certain cases, user may need to schedule NPU workload on node that meet specific hardware or software requirements, such as particular driver versions. If the [Furiosa Feature Discovery](feature_discovery.html#featurediscovery) is deployed in your cluster, nodes are automatically labelled based on their hardware and software configurations including driver version. This allows user to schedule Pod on nodes that meet specific requirements.\\nFollowing example shows how to use affinity to schedule a Pod that request 2 RNGD NPUs with specific driver version:\\n``` apiVersion: v1 kind: Pod metadata:   name: example-npu-scheduling-with-affinity spec:   containers:   - name: furiosa     image: furiosaai/furiosa-smi:latest     imagePullPolicy: IfNotPresent     command: [\"sleep\"]     args: [\"120\"]     resources:       limits:         furiosa.ai/rngd: 2   affinity:     nodeAffinity:       requiredDuringSchedulingIgnoredDuringExecution:         nodeSelectorTerms:         - matchExpressions:           - key: furiosa.ai/driver-version             operator: In             values:             - \"1.0.12\"\\n```\\n[previous\\nInstalling Furiosa Metrics Exporter](metrics_exporter.html \"previous page\") [next\\nfuriosa-smi](../../device_management/furiosa_smi.html \"next page\")\\nContents\\n* [Preparing Node](#preparing-node) * [Requesting NPUs](#requesting-npus) * [Scheduling NPUs With Specific Requirements](#scheduling-npus-with-specific-requirements)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from markdownify import markdownify as md\n",
    "from llama_index.core import Document \n",
    "\n",
    "class CustomDocument(Document):\n",
    "    page_content: str = Field(default=\"\", description=\"Additional content for the document\")\n",
    "\n",
    "def convert_page_to_llama_index_document(page: Page) -> CustomDocument:\n",
    "    return CustomDocument(\n",
    "        doc_id=page.id,\n",
    "        metadata={\n",
    "            \"source\": str(page.link),\n",
    "            \"title\": page.name,\n",
    "            \"parent_doc_id\": page.parent,\n",
    "            \"child_doc_ids\": json.dumps(page.child),\n",
    "        },\n",
    "        text=page.description_clean,  # 기본 text\n",
    "        page_content=page.description_clean,  # 추가 속성\n",
    "    )\n",
    "\n",
    "docs = [convert_page_to_llama_index_document(page) for page in final_pages]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapp the LLMs in LangchainLLMWrapper so that it can be used with ragas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/intro.html', 'title': 'intro', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='* [.rst](../_sources/furiosa_llm/intro.rst \"Download source file\") * .pdf\\nFuriosa LLM ===========\\nFuriosa LLM [#](#furiosa-llm \"Link to this heading\") ====================================================\\nFuriosa LLM provides a high-performance inference engine for LLM models and Multi-Modal LLM models, Furiosa LLM is designed to provide the state-of-the-art serving optimization. The features of Furiosa LLM includes:\\n* vLLM-compatible API * Efficient KV cache management with PagedAttention * Continuous batching of incoming requests in serving * Quantization: INT4, INT8, FP8, GPTQ, AWQ * Data Parallelism and Pipeline Parallelism across multiple NPUs * Tensor Parallelism (planned in release 2024.2) across multiple NPUs * OpenAI-compatible API server * Various decoding algorithms, greedy search, beam search, top-k/top-p, speculative decoding (planned) * HuggingFace model integration and hub support * HuggingFace PEFT support (planned)\\n[previous\\nRunning MLPerf™ Inference Benchmark](../getting_started/furiosa_mlperf.html \"previous page\") [next\\nReferences](references.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying HeadlineSplitter:   0%|          | 0/21 [00:00<?, ?it/s]           unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "Applying SummaryExtractor:  41%|████      | 11/27 [00:03<00:02,  6.53it/s]Property 'summary' already exists in node 'fba6c3'. Skipping!\n",
      "Applying SummaryExtractor:  44%|████▍     | 12/27 [00:03<00:03,  4.75it/s]Property 'summary' already exists in node '5d2c26'. Skipping!\n",
      "Applying SummaryExtractor:  48%|████▊     | 13/27 [00:03<00:03,  4.38it/s]Property 'summary' already exists in node '8bf36e'. Skipping!\n",
      "Property 'summary' already exists in node '4c3e39'. Skipping!\n",
      "Applying SummaryExtractor:  59%|█████▉    | 16/27 [00:04<00:01,  5.89it/s]Property 'summary' already exists in node '0b7164'. Skipping!\n",
      "Property 'summary' already exists in node '15a150'. Skipping!\n",
      "Applying SummaryExtractor:  74%|███████▍  | 20/27 [00:04<00:00,  7.83it/s]Property 'summary' already exists in node 'c931ea'. Skipping!\n",
      "Applying SummaryExtractor:  81%|████████▏ | 22/27 [00:04<00:00,  9.22it/s]Property 'summary' already exists in node 'bbe529'. Skipping!\n",
      "Property 'summary' already exists in node '79cbfc'. Skipping!\n",
      "Applying SummaryExtractor:  89%|████████▉ | 24/27 [00:05<00:00,  4.88it/s]Property 'summary' already exists in node '475491'. Skipping!\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/55 [00:00<?, ?it/s]Property 'summary_embedding' already exists in node '0b7164'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '15a150'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'fba6c3'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '4c3e39'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '5d2c26'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '79cbfc'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '8bf36e'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'c931ea'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '475491'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'bbe529'. Skipping!\n",
      "Generating personas: 100%|██████████| 3/3 [00:01<00:00,  2.47it/s]                                           \n",
      "Generating Scenarios: 100%|██████████| 3/3 [01:03<00:00, 21.18s/it]\n",
      "Generating Samples: 100%|██████████| 52/52 [00:23<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(docs, testset_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "file_dir = f'{data_dir}/qa-warboy_sdk_ragas_v2'\n",
    "\n",
    "dataset_df = dataset.to_pandas()\n",
    "dataset_df.to_parquet(f\"{file_dir}.parquet\")\n",
    "dataset_df.to_csv(f\"{file_dir}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is FuriosaAI's role in running MLPerf Inf...</td>\n",
       "      <td>[* [.rst](../_sources/getting_started/furiosa_...</td>\n",
       "      <td>FuriosaAI Software Stack provides the `furiosa...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Howw doo youu usee Furiosa LLM withh FuriosaAI...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n* [.rst](../_sources/getting_start...</td>\n",
       "      <td>To use Furiosa LLM with FuriosaAI NPU in a con...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the installation requirements for Fur...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nRunning Furiosa LLM in container e...</td>\n",
       "      <td>The installation requirements for Furiosa LLM ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What is FuriosaAI's role in running MLPerf Inf...   \n",
       "1  Howw doo youu usee Furiosa LLM withh FuriosaAI...   \n",
       "2  What are the installation requirements for Fur...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [* [.rst](../_sources/getting_started/furiosa_...   \n",
       "1  [<1-hop>\\n\\n* [.rst](../_sources/getting_start...   \n",
       "2  [<1-hop>\\n\\nRunning Furiosa LLM in container e...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  FuriosaAI Software Stack provides the `furiosa...   \n",
       "1  To use Furiosa LLM with FuriosaAI NPU in a con...   \n",
       "2  The installation requirements for Furiosa LLM ...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "1  multi_hop_abstract_query_synthesizer  \n",
       "2  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ragas.testset.evolutions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtestset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestsetGenerator\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtestset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevolutions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple, reasoning, multi_context, conditional\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI, OpenAIEmbeddings\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# documents = load your documents\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# generator with openai models\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ragas.testset.evolutions'"
     ]
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context, conditional\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# generator with openai models\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "distributions = {\n",
    "    simple: 1,\n",
    "    # multi_context: 1,\n",
    "    # reasoning: 0.35,\n",
    "    # conditional: 0.2,\n",
    "}\n",
    "\n",
    "testset = generator.generate_with_llamaindex_docs(docs, 10, distributions) \n",
    "testset_df = testset.to_pandas()\n",
    "testset_df.to_parquet(\"v2/qa_ragas_2.parquet\")\n",
    "testset_df.to_csv(\"v2/qa_ragas_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-quantize",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
