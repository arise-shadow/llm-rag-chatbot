{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA 데이터셋 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field, HttpUrl\n",
    "\n",
    "class Page(BaseModel):\n",
    "    id: str = Field(..., description=\"ID of the Page\")\n",
    "    link: HttpUrl = Field(description=\"Url link of the page\")\n",
    "    name: str = Field(description=\"Name of the page\")\n",
    "    parent: str = Field(default=\"\", description=\"ID of the parent page\")\n",
    "    child: List[str] = Field(default=[], description=\"List of ids of the child pages\")\n",
    "    description: str = Field(default=\"\", description=\"Description of the page\")\n",
    "    description_clean: str = Field(default=\"\", description=\"Content markdown\")\n",
    "    html_content: str = Field(default=\"\", description=\"HTML code of the main content in the page\")\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.link, self.name))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Page):\n",
    "            return False\n",
    "        return (self.link, self.name) == (other.link, other.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB 가지고 오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_dir = '/Users/jwlee-pro/Documents/Workspace_2025/projects/llm-rag-chatbot/data/db/db-rngd_sdk.json'\n",
    "\n",
    "with open(data_dir, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    final_pages = [Page.model_validate_json(page) for page in data[\"sdk\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Page(id='3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa', link=HttpUrl('https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/intro.html'), name='intro', parent='', child=[], description='\\n\\n\\n\\n\\n* [.rst](../_sources/furiosa_llm/intro.rst \"Download source file\")\\n* .pdf\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFuriosa LLM\\n===========\\n\\n\\n\\n\\n\\nFuriosa LLM\\n[#](#furiosa-llm \"Link to this heading\")\\n====================================================\\n\\nFuriosa LLM provides a high-performance inference engine for LLM models and Multi-Modal LLM models,\\nFuriosa LLM is designed to provide the state-of-the-art serving optimization.\\nThe features of Furiosa LLM includes:\\n\\n* vLLM-compatible API\\n* Efficient KV cache management with PagedAttention\\n* Continuous batching of incoming requests in serving\\n* Quantization: INT4, INT8, FP8, GPTQ, AWQ\\n* Data Parallelism and Pipeline Parallelism across multiple NPUs\\n* Tensor Parallelism (planned in release 2024.2) across multiple NPUs\\n* OpenAI-compatible API server\\n* Various decoding algorithms, greedy search, beam search, top-k/top-p, speculative decoding (planned)\\n* HuggingFace model integration and hub support\\n* HuggingFace PEFT support (planned)\\n\\n\\n[previous\\n\\nRunning MLPerf™ Inference Benchmark](../getting_started/furiosa_mlperf.html \"previous page\")\\n[next\\n\\nReferences](references.html \"next page\")\\n\\n\\n\\n\\nBy FuriosaAI, Inc.\\n\\n\\n© Copyright 2024, FuriosaAI, Inc..\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n', description_clean='* [.rst](../_sources/furiosa_llm/intro.rst \"Download source file\") * .pdf\\nFuriosa LLM ===========\\nFuriosa LLM [#](#furiosa-llm \"Link to this heading\") ====================================================\\nFuriosa LLM provides a high-performance inference engine for LLM models and Multi-Modal LLM models, Furiosa LLM is designed to provide the state-of-the-art serving optimization. The features of Furiosa LLM includes:\\n* vLLM-compatible API * Efficient KV cache management with PagedAttention * Continuous batching of incoming requests in serving * Quantization: INT4, INT8, FP8, GPTQ, AWQ * Data Parallelism and Pipeline Parallelism across multiple NPUs * Tensor Parallelism (planned in release 2024.2) across multiple NPUs * OpenAI-compatible API server * Various decoding algorithms, greedy search, beam search, top-k/top-p, speculative decoding (planned) * HuggingFace model integration and hub support * HuggingFace PEFT support (planned)\\n[previous\\nRunning MLPerf™ Inference Benchmark](../getting_started/furiosa_mlperf.html \"previous page\") [next\\nReferences](references.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', html_content='<main class=\"bd-main\" id=\"main-content\" role=\"main\">\\n <div class=\"sbt-scroll-pixel-helper\">\\n </div>\\n <div class=\"bd-content\">\\n  <div class=\"bd-article-container\">\\n   <div class=\"bd-header-article d-print-none\">\\n    <div class=\"header-article-items header-article__inner\">\\n     <div class=\"header-article-items__start\">\\n      <div class=\"header-article-item\">\\n       <button class=\"sidebar-toggle primary-toggle btn btn-sm\" data-bs-placement=\"bottom\" data-bs-toggle=\"tooltip\" title=\"Toggle primary sidebar\">\\n        <span class=\"fa-solid fa-bars\">\\n        </span>\\n       </button>\\n      </div>\\n     </div>\\n     <div class=\"header-article-items__end\">\\n      <div class=\"header-article-item\">\\n       <div class=\"article-header-buttons\">\\n        <div class=\"dropdown dropdown-download-buttons\">\\n         <button aria-expanded=\"false\" aria-label=\"Download this page\" class=\"btn dropdown-toggle\" data-bs-toggle=\"dropdown\" type=\"button\">\\n          <i class=\"fas fa-download\">\\n          </i>\\n         </button>\\n         <ul class=\"dropdown-menu\">\\n          <li>\\n           <a class=\"btn btn-sm btn-download-source-button dropdown-item\" data-bs-placement=\"left\" data-bs-toggle=\"tooltip\" href=\"../_sources/furiosa_llm/intro.rst\" target=\"_blank\" title=\"Download source file\">\\n            <span class=\"btn__icon-container\">\\n             <i class=\"fas fa-file\">\\n             </i>\\n            </span>\\n            <span class=\"btn__text-container\">\\n             .rst\\n            </span>\\n           </a>\\n          </li>\\n          <li>\\n           <button class=\"btn btn-sm btn-download-pdf-button dropdown-item\" data-bs-placement=\"left\" data-bs-toggle=\"tooltip\" onclick=\"window.print()\" title=\"Print to PDF\">\\n            <span class=\"btn__icon-container\">\\n             <i class=\"fas fa-file-pdf\">\\n             </i>\\n            </span>\\n            <span class=\"btn__text-container\">\\n             .pdf\\n            </span>\\n           </button>\\n          </li>\\n         </ul>\\n        </div>\\n        <button class=\"btn btn-sm btn-fullscreen-button\" data-bs-placement=\"bottom\" data-bs-toggle=\"tooltip\" onclick=\"toggleFullScreen()\" title=\"Fullscreen mode\">\\n         <span class=\"btn__icon-container\">\\n          <i class=\"fas fa-expand\">\\n          </i>\\n         </span>\\n        </button>\\n        <script>\\n         document.write(`\\n  <button class=\"btn btn-sm nav-link pst-navbar-icon theme-switch-button\" title=\"light/dark\" aria-label=\"light/dark\" data-bs-placement=\"bottom\" data-bs-toggle=\"tooltip\">\\n    <i class=\"theme-switch fa-solid fa-sun fa-lg\" data-mode=\"light\"></i>\\n    <i class=\"theme-switch fa-solid fa-moon fa-lg\" data-mode=\"dark\"></i>\\n    <i class=\"theme-switch fa-solid fa-circle-half-stroke fa-lg\" data-mode=\"auto\"></i>\\n  </button>\\n`);\\n        </script>\\n        <script>\\n         document.write(`\\n  <button class=\"btn btn-sm pst-navbar-icon search-button search-button__button\" title=\"Search\" aria-label=\"Search\" data-bs-placement=\"bottom\" data-bs-toggle=\"tooltip\">\\n    <i class=\"fa-solid fa-magnifying-glass fa-lg\"></i>\\n  </button>\\n`);\\n        </script>\\n       </div>\\n      </div>\\n     </div>\\n    </div>\\n   </div>\\n   <div class=\"onlyprint\" id=\"jb-print-docs-body\">\\n    <h1>\\n     Furiosa LLM\\n    </h1>\\n    <!-- Table of contents -->\\n    <div id=\"print-main-content\">\\n     <div id=\"jb-print-toc\">\\n     </div>\\n    </div>\\n   </div>\\n   <div id=\"searchbox\">\\n   </div>\\n   <article class=\"bd-article\">\\n    <section id=\"furiosa-llm\">\\n     <span id=\"furiosallm\">\\n     </span>\\n     <h1>\\n      Furiosa LLM\\n      <a class=\"headerlink\" href=\"#furiosa-llm\" title=\"Link to this heading\">\\n       #\\n      </a>\\n     </h1>\\n     <p>\\n      Furiosa LLM provides a high-performance inference engine for LLM models and Multi-Modal LLM models,\\nFuriosa LLM is designed to provide the state-of-the-art serving optimization.\\nThe features of Furiosa LLM includes:\\n     </p>\\n     <ul class=\"simple\">\\n      <li>\\n       <p>\\n        vLLM-compatible API\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Efficient KV cache management with PagedAttention\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Continuous batching of incoming requests in serving\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Quantization: INT4, INT8, FP8, GPTQ, AWQ\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Data Parallelism and Pipeline Parallelism across multiple NPUs\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Tensor Parallelism (planned in release 2024.2) across multiple NPUs\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        OpenAI-compatible API server\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Various decoding algorithms, greedy search, beam search, top-k/top-p, speculative decoding (planned)\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        HuggingFace model integration and hub support\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        HuggingFace PEFT support (planned)\\n       </p>\\n      </li>\\n     </ul>\\n    </section>\\n   </article>\\n   <footer class=\"prev-next-footer d-print-none\">\\n    <div class=\"prev-next-area\">\\n     <a class=\"left-prev\" href=\"../getting_started/furiosa_mlperf.html\" title=\"previous page\">\\n      <i class=\"fa-solid fa-angle-left\">\\n      </i>\\n      <div class=\"prev-next-info\">\\n       <p class=\"prev-next-subtitle\">\\n        previous\\n       </p>\\n       <p class=\"prev-next-title\">\\n        Running MLPerf™ Inference Benchmark\\n       </p>\\n      </div>\\n     </a>\\n     <a class=\"right-next\" href=\"references.html\" title=\"next page\">\\n      <div class=\"prev-next-info\">\\n       <p class=\"prev-next-subtitle\">\\n        next\\n       </p>\\n       <p class=\"prev-next-title\">\\n        References\\n       </p>\\n      </div>\\n      <i class=\"fa-solid fa-angle-right\">\\n      </i>\\n     </a>\\n    </div>\\n   </footer>\\n  </div>\\n </div>\\n <footer class=\"bd-footer-content\">\\n  <div class=\"bd-footer-content__inner container\">\\n   <div class=\"footer-item\">\\n    <p class=\"component-author\">\\n     By FuriosaAI, Inc.\\n    </p>\\n   </div>\\n   <div class=\"footer-item\">\\n    <p class=\"copyright\">\\n     © Copyright 2024, FuriosaAI, Inc..\\n     <br/>\\n    </p>\\n   </div>\\n   <div class=\"footer-item\">\\n   </div>\\n   <div class=\"footer-item\">\\n   </div>\\n  </div>\\n </footer>\\n</main>\\n')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa' link=HttpUrl('https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/intro.html') name='intro' parent='' child=[] description='\\n\\n\\n\\n\\n* [.rst](../_sources/furiosa_llm/intro.rst \"Download source file\")\\n* .pdf\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFuriosa LLM\\n===========\\n\\n\\n\\n\\n\\nFuriosa LLM\\n[#](#furiosa-llm \"Link to this heading\")\\n====================================================\\n\\nFuriosa LLM provides a high-performance inference engine for LLM models and Multi-Modal LLM models,\\nFuriosa LLM is designed to provide the state-of-the-art serving optimization.\\nThe features of Furiosa LLM includes:\\n\\n* vLLM-compatible API\\n* Efficient KV cache management with PagedAttention\\n* Continuous batching of incoming requests in serving\\n* Quantization: INT4, INT8, FP8, GPTQ, AWQ\\n* Data Parallelism and Pipeline Parallelism across multiple NPUs\\n* Tensor Parallelism (planned in release 2024.2) across multiple NPUs\\n* OpenAI-compatible API server\\n* Various decoding algorithms, greedy search, beam search, top-k/top-p, speculative decoding (planned)\\n* HuggingFace model integration and hub support\\n* HuggingFace PEFT support (planned)\\n\\n\\n[previous\\n\\nRunning MLPerf™ Inference Benchmark](../getting_started/furiosa_mlperf.html \"previous page\")\\n[next\\n\\nReferences](references.html \"next page\")\\n\\n\\n\\n\\nBy FuriosaAI, Inc.\\n\\n\\n© Copyright 2024, FuriosaAI, Inc..\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n' description_clean='* [.rst](../_sources/furiosa_llm/intro.rst \"Download source file\") * .pdf\\nFuriosa LLM ===========\\nFuriosa LLM [#](#furiosa-llm \"Link to this heading\") ====================================================\\nFuriosa LLM provides a high-performance inference engine for LLM models and Multi-Modal LLM models, Furiosa LLM is designed to provide the state-of-the-art serving optimization. The features of Furiosa LLM includes:\\n* vLLM-compatible API * Efficient KV cache management with PagedAttention * Continuous batching of incoming requests in serving * Quantization: INT4, INT8, FP8, GPTQ, AWQ * Data Parallelism and Pipeline Parallelism across multiple NPUs * Tensor Parallelism (planned in release 2024.2) across multiple NPUs * OpenAI-compatible API server * Various decoding algorithms, greedy search, beam search, top-k/top-p, speculative decoding (planned) * HuggingFace model integration and hub support * HuggingFace PEFT support (planned)\\n[previous\\nRunning MLPerf™ Inference Benchmark](../getting_started/furiosa_mlperf.html \"previous page\") [next\\nReferences](references.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..' html_content='<main class=\"bd-main\" id=\"main-content\" role=\"main\">\\n <div class=\"sbt-scroll-pixel-helper\">\\n </div>\\n <div class=\"bd-content\">\\n  <div class=\"bd-article-container\">\\n   <div class=\"bd-header-article d-print-none\">\\n    <div class=\"header-article-items header-article__inner\">\\n     <div class=\"header-article-items__start\">\\n      <div class=\"header-article-item\">\\n       <button class=\"sidebar-toggle primary-toggle btn btn-sm\" data-bs-placement=\"bottom\" data-bs-toggle=\"tooltip\" title=\"Toggle primary sidebar\">\\n        <span class=\"fa-solid fa-bars\">\\n        </span>\\n       </button>\\n      </div>\\n     </div>\\n     <div class=\"header-article-items__end\">\\n      <div class=\"header-article-item\">\\n       <div class=\"article-header-buttons\">\\n        <div class=\"dropdown dropdown-download-buttons\">\\n         <button aria-expanded=\"false\" aria-label=\"Download this page\" class=\"btn dropdown-toggle\" data-bs-toggle=\"dropdown\" type=\"button\">\\n          <i class=\"fas fa-download\">\\n          </i>\\n         </button>\\n         <ul class=\"dropdown-menu\">\\n          <li>\\n           <a class=\"btn btn-sm btn-download-source-button dropdown-item\" data-bs-placement=\"left\" data-bs-toggle=\"tooltip\" href=\"../_sources/furiosa_llm/intro.rst\" target=\"_blank\" title=\"Download source file\">\\n            <span class=\"btn__icon-container\">\\n             <i class=\"fas fa-file\">\\n             </i>\\n            </span>\\n            <span class=\"btn__text-container\">\\n             .rst\\n            </span>\\n           </a>\\n          </li>\\n          <li>\\n           <button class=\"btn btn-sm btn-download-pdf-button dropdown-item\" data-bs-placement=\"left\" data-bs-toggle=\"tooltip\" onclick=\"window.print()\" title=\"Print to PDF\">\\n            <span class=\"btn__icon-container\">\\n             <i class=\"fas fa-file-pdf\">\\n             </i>\\n            </span>\\n            <span class=\"btn__text-container\">\\n             .pdf\\n            </span>\\n           </button>\\n          </li>\\n         </ul>\\n        </div>\\n        <button class=\"btn btn-sm btn-fullscreen-button\" data-bs-placement=\"bottom\" data-bs-toggle=\"tooltip\" onclick=\"toggleFullScreen()\" title=\"Fullscreen mode\">\\n         <span class=\"btn__icon-container\">\\n          <i class=\"fas fa-expand\">\\n          </i>\\n         </span>\\n        </button>\\n        <script>\\n         document.write(`\\n  <button class=\"btn btn-sm nav-link pst-navbar-icon theme-switch-button\" title=\"light/dark\" aria-label=\"light/dark\" data-bs-placement=\"bottom\" data-bs-toggle=\"tooltip\">\\n    <i class=\"theme-switch fa-solid fa-sun fa-lg\" data-mode=\"light\"></i>\\n    <i class=\"theme-switch fa-solid fa-moon fa-lg\" data-mode=\"dark\"></i>\\n    <i class=\"theme-switch fa-solid fa-circle-half-stroke fa-lg\" data-mode=\"auto\"></i>\\n  </button>\\n`);\\n        </script>\\n        <script>\\n         document.write(`\\n  <button class=\"btn btn-sm pst-navbar-icon search-button search-button__button\" title=\"Search\" aria-label=\"Search\" data-bs-placement=\"bottom\" data-bs-toggle=\"tooltip\">\\n    <i class=\"fa-solid fa-magnifying-glass fa-lg\"></i>\\n  </button>\\n`);\\n        </script>\\n       </div>\\n      </div>\\n     </div>\\n    </div>\\n   </div>\\n   <div class=\"onlyprint\" id=\"jb-print-docs-body\">\\n    <h1>\\n     Furiosa LLM\\n    </h1>\\n    <!-- Table of contents -->\\n    <div id=\"print-main-content\">\\n     <div id=\"jb-print-toc\">\\n     </div>\\n    </div>\\n   </div>\\n   <div id=\"searchbox\">\\n   </div>\\n   <article class=\"bd-article\">\\n    <section id=\"furiosa-llm\">\\n     <span id=\"furiosallm\">\\n     </span>\\n     <h1>\\n      Furiosa LLM\\n      <a class=\"headerlink\" href=\"#furiosa-llm\" title=\"Link to this heading\">\\n       #\\n      </a>\\n     </h1>\\n     <p>\\n      Furiosa LLM provides a high-performance inference engine for LLM models and Multi-Modal LLM models,\\nFuriosa LLM is designed to provide the state-of-the-art serving optimization.\\nThe features of Furiosa LLM includes:\\n     </p>\\n     <ul class=\"simple\">\\n      <li>\\n       <p>\\n        vLLM-compatible API\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Efficient KV cache management with PagedAttention\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Continuous batching of incoming requests in serving\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Quantization: INT4, INT8, FP8, GPTQ, AWQ\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Data Parallelism and Pipeline Parallelism across multiple NPUs\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Tensor Parallelism (planned in release 2024.2) across multiple NPUs\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        OpenAI-compatible API server\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Various decoding algorithms, greedy search, beam search, top-k/top-p, speculative decoding (planned)\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        HuggingFace model integration and hub support\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        HuggingFace PEFT support (planned)\\n       </p>\\n      </li>\\n     </ul>\\n    </section>\\n   </article>\\n   <footer class=\"prev-next-footer d-print-none\">\\n    <div class=\"prev-next-area\">\\n     <a class=\"left-prev\" href=\"../getting_started/furiosa_mlperf.html\" title=\"previous page\">\\n      <i class=\"fa-solid fa-angle-left\">\\n      </i>\\n      <div class=\"prev-next-info\">\\n       <p class=\"prev-next-subtitle\">\\n        previous\\n       </p>\\n       <p class=\"prev-next-title\">\\n        Running MLPerf™ Inference Benchmark\\n       </p>\\n      </div>\\n     </a>\\n     <a class=\"right-next\" href=\"references.html\" title=\"next page\">\\n      <div class=\"prev-next-info\">\\n       <p class=\"prev-next-subtitle\">\\n        next\\n       </p>\\n       <p class=\"prev-next-title\">\\n        References\\n       </p>\\n      </div>\\n      <i class=\"fa-solid fa-angle-right\">\\n      </i>\\n     </a>\\n    </div>\\n   </footer>\\n  </div>\\n </div>\\n <footer class=\"bd-footer-content\">\\n  <div class=\"bd-footer-content__inner container\">\\n   <div class=\"footer-item\">\\n    <p class=\"component-author\">\\n     By FuriosaAI, Inc.\\n    </p>\\n   </div>\\n   <div class=\"footer-item\">\\n    <p class=\"copyright\">\\n     © Copyright 2024, FuriosaAI, Inc..\\n     <br/>\\n    </p>\\n   </div>\\n   <div class=\"footer-item\">\\n   </div>\\n   <div class=\"footer-item\">\\n   </div>\\n  </div>\\n </footer>\\n</main>\\n'\n",
      "Total pages loaded: 21\n"
     ]
    }
   ],
   "source": [
    "# final_pages의 첫 번째 객체를 출력\n",
    "print(final_pages[0])\n",
    "\n",
    "# final_pages 전체 크기 확인\n",
    "print(f\"Total pages loaded: {len(final_pages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA 데이터셋 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# openAI key \n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version #1\n",
    "- html -> 마크다운 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markdownify import markdownify as md\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "import pandas as pd\n",
    "\n",
    "# LLM 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "# 번역용 프롬프트 템플릿\n",
    "prompt = PromptTemplate(template=\"\"\"\n",
    "You are an expert translator specializing in technical documentation. Your task is to translate the provided markdown content from English to Korean while retaining the original meaning and structure.\n",
    "\n",
    "Instructions:\n",
    "1. Translate all textual content to Korean.\n",
    "2. Preserve programming code, CLI commands, and technical terms in English for readability.\n",
    "3. Ensure the translated document is well-formatted in markdown style.\n",
    "4. Avoid literal translations for idiomatic expressions; focus on clear, context-aware translations.\n",
    "\n",
    "CONTENT:\n",
    "{content}\n",
    "\"\"\",\n",
    "    input_variables=[\"content\"],\n",
    ")\n",
    "\n",
    "# 번역 함수\n",
    "def translate_documents(pages):\n",
    "    \"\"\"\n",
    "    Translate the content of each page to Korean.\n",
    "\n",
    "    Args:\n",
    "        pages (List[Page]): List of Page objects.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing translated documents.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for page in pages:\n",
    "        try:\n",
    "            # 프롬프트 생성\n",
    "            prompt_message = prompt.format(content=page.description_clean)\n",
    "            message = HumanMessage(content=prompt_message)\n",
    "\n",
    "            # 번역 수행 (llm에 list 형태로 메시지 전달)\n",
    "            translated_content = llm([message]).content\n",
    "\n",
    "            # 데이터 추가\n",
    "            data.append({\n",
    "                \"page_id\": page.id,\n",
    "                \"link\": str(page.link),\n",
    "                \"original_content\": page.description_clean,\n",
    "                \"translated_content\": translated_content,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error translating page {page.id}: {e}\")\n",
    "\n",
    "    # 번역 결과를 DataFrame으로 변환\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 실행하기 \n",
    "translated_df = translate_documents(final_pages)\n",
    "\n",
    "# 번역 결과를 CSV 및 JSON으로 저장\n",
    "data_dir = '/Users/jwlee-pro/Documents/Workspace_2025/projects/llm-rag-chatbot/data/translate'\n",
    "file_dir = f'{data_dir}/qa-rngd_sdk'\n",
    "\n",
    "translated_df.to_csv(f\"{file_dir}.csv\", index=False, encoding=\"utf-8\")\n",
    "translated_df.to_json(f\"{file_dir}.json\", orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [.rst](../../_sources/cloud_native_toolkit/kubernetes/device_plugin.rst \"소스 파일 다운로드\") * .pdf\n",
      "\n",
      "Furiosa 디바이스 플러그인 설치\n",
      "===============================\n",
      "\n",
      "목차\n",
      "--------\n",
      "\n",
      "* [Furiosa 디바이스 플러그인](#furiosa-device-plugin)  \n",
      "  + [설정](#configuration)  \n",
      "  + [Helm을 사용한 Furiosa 디바이스 플러그인 배포](#deploying-furiosa-device-plugin-with-helm)\n",
      "\n",
      "Furiosa 디바이스 플러그인 설치 [#](#installing-furiosa-device-plugin \"이 제목으로 링크\")\n",
      "==============================================================================================\n",
      "\n",
      "Furiosa 디바이스 플러그인 [#](#furiosa-device-plugin \"이 제목으로 링크\")\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "Furiosa 디바이스 플러그인은 FuriosaAI NPU 장치를 위한 [Kubernetes 디바이스 플러그인](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) 인터페이스를 구현하며, 그 기능은 다음과 같습니다:\n",
      "\n",
      "* Furiosa NPU 장치를 발견하고 Kubernetes 클러스터에 등록합니다.\n",
      "* 장치의 상태를 추적하고 Kubernetes 클러스터에 보고합니다.\n",
      "* Kubernetes 클러스터 내에서 Furiosa NPU 장치 위에서 AI 작업을 실행합니다.\n",
      "\n",
      "### 설정 [#](#configuration \"이 제목으로 링크\")\n",
      "\n",
      "Furiosa NPU는 다양한 설정으로 Kubernetes 클러스터에 통합될 수 있습니다. 단일 NPU 카드는 하나의 리소스로 노출되거나 여러 리소스로 분할될 수 있습니다. 여러 리소스로 분할하면 더 세밀한 제어가 가능합니다.\n",
      "\n",
      "다음 표는 사용 가능한 리소스 전략을 보여줍니다:\n",
      "\n",
      "리소스 전략\n",
      "[#](#id1 \"이 표로 링크\")\n",
      "\n",
      "| NPU 설정 | 리소스 이름 | 카드당 리소스 수 |\n",
      "| --- | --- | --- |\n",
      "| legacy | beta.furiosa.ai/npu | 1 |\n",
      "| generic | furiosa.ai/rngd | 1 |\n",
      "\n",
      "Furiosa 디바이스 플러그인의 helm 차트는 [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) 에서 사용할 수 있습니다.\n",
      "\n",
      "다음은 helm 차트의 기본값을 보여줍니다.\n",
      "\n",
      "```yaml\n",
      "config:\n",
      "  resourceStrategy: generic\n",
      "  debugMode: false\n",
      "  disabledDeviceUUIDListMap: []\n",
      "```\n",
      "\n",
      "### Helm을 사용한 Furiosa 디바이스 플러그인 배포 [#](#deploying-furiosa-device-plugin-with-helm \"이 제목으로 링크\")\n",
      "\n",
      "Furiosa 디바이스 플러그인 helm 차트는 [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) 에서 사용할 수 있습니다. 필요한 대로 배포를 구성하려면 `charts/furiosa-device-plugin/values.yaml` 파일을 수정할 수 있습니다.\n",
      "\n",
      "* resourceStrategy가 지정되지 않으면 기본값은 `\"generic\"`입니다.\n",
      "* debugMode가 지정되지 않으면 기본값은 `false`입니다.\n",
      "* disabledDeviceUUIDListMap이 지정되지 않으면 기본값은 빈 리스트 `[]`입니다.\n",
      "\n",
      "다음 명령어를 실행하여 Furiosa 디바이스 플러그인을 배포할 수 있습니다:\n",
      "\n",
      "```bash\n",
      "helm repo add furiosa https://furiosa-ai.github.io/helm-charts\n",
      "helm repo update\n",
      "helm install furiosa-device-plugin furiosa/furiosa-device-plugin -n kube-system\n",
      "```\n",
      "\n",
      "[이전\n",
      "Furiosa 기능 탐지 설치](feature_discovery.html \"이전 페이지\") [다음\n",
      "Furiosa 메트릭스 익스포터 설치](metrics_exporter.html \"다음 페이지\")\n",
      "\n",
      "목차\n",
      "* [Furiosa 디바이스 플러그인](#furiosa-device-plugin)  \n",
      "  + [설정](#configuration)  \n",
      "  + [Helm을 사용한 Furiosa 디바이스 플러그인 배포](#deploying-furiosa-device-plugin-with-helm)\n",
      "\n",
      "By FuriosaAI, Inc.\n",
      "© 저작권 2024, FuriosaAI, Inc.\n"
     ]
    }
   ],
   "source": [
    "print(translated_df['translated_content'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-quantize",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
