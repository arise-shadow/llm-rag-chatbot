{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번역 성능 평가 \n",
    "\n",
    "Kor - Eng - Jpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번역 시스템 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='Hello! How can I assist you today?', additional_kwargs={'model': 'llama3.1:70b', 'created_at': '2024-12-28T06:41:47.65819851Z', 'done': True, 'done_reason': 'stop', 'context': [128009, 128006, 882, 128007, 271, 15339, 128009, 128006, 78191, 128007, 271, 9906, 0, 2650, 649, 358, 7945, 499, 3432, 30], 'total_duration': 823681499, 'load_duration': 42072342, 'prompt_eval_count': 12, 'prompt_eval_duration': 81689000, 'eval_count': 10, 'eval_duration': 654783000}, raw={'model': 'llama3.1:70b', 'created_at': '2024-12-28T06:41:47.65819851Z', 'response': 'Hello! How can I assist you today?', 'done': True, 'done_reason': 'stop', 'context': [128009, 128006, 882, 128007, 271, 15339, 128009, 128006, 78191, 128007, 271, 9906, 0, 2650, 649, 358, 7945, 499, 3432, 30], 'total_duration': 823681499, 'load_duration': 42072342, 'prompt_eval_count': 12, 'prompt_eval_duration': 81689000, 'eval_count': 10, 'eval_duration': 654783000}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define LLM\n",
    "import os\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "llm = Ollama(model=\"llama3.1:70b\", request_timeout=600,temperature=0)\n",
    "llm.complete(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"This is an {source_lang} to {target_lang} translation, please provide the {target_lang} translation for this text in as polite a tone as possible. \\\n",
    "The original text sentence structure must be preserved.\n",
    "Do not provide any explanations or text apart from the translation.\n",
    "The translation result must be written in {target_lang}.\n",
    "\n",
    "{source_lang}: {source_text}\n",
    "\n",
    "{target_lang}:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역함수 \n",
    "def translate(source_text, source_lang, target_lang, prompt):\n",
    "    full_prompt = prompt.format(source_lang=source_lang, target_lang=target_lang, source_text=source_text)\n",
    "    result = llm.complete(full_prompt)\n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning, hello.\n"
     ]
    }
   ],
   "source": [
    "result = translate(\"안녕하세요. 좋은 아침입니다.\", 'kor', 'eng', prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 시스템 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언어별 토크나이저\n",
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Mecab\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "def tokenize(text, lang):\n",
    "    if lang == \"eng\":\n",
    "        # 영어: NLTK word_tokenize\n",
    "        return word_tokenize(text)\n",
    "    elif lang == \"kor\":\n",
    "        # 한국어: Mecab 형태소 분석기\n",
    "        mecab = Mecab()\n",
    "        return mecab.morphs(text)\n",
    "    elif lang == \"jpn\":\n",
    "        # 일본어: Janome 형태소 분석기\n",
    "        tokenizer = Tokenizer()\n",
    "        return [token.surface for token in tokenizer.tokenize(text)]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported language: {lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def evaluate(\n",
    "    reference: str,\n",
    "    candidate: str,\n",
    "    tgt_lang: str,\n",
    "    bert_model=\"microsoft/deberta-xlarge-mnli\",\n",
    "):\n",
    "    # 텍스트를 토큰화\n",
    "    reference_tokens = tokenize(reference, tgt_lang)\n",
    "    candidate_tokens = tokenize(candidate, tgt_lang)\n",
    "\n",
    "    # BLEU 점수 계산 (스무딩 적용)\n",
    "    bleu = sentence_bleu(\n",
    "        [reference_tokens],\n",
    "        candidate_tokens,\n",
    "        smoothing_function=SmoothingFunction().method1,\n",
    "    )\n",
    "\n",
    "    # METEOR 점수 계산\n",
    "    meteor = meteor_score([reference_tokens], candidate_tokens)\n",
    "\n",
    "    # ROUGE 점수 계산\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "\n",
    "    # BERTScore 계산\n",
    "    P, R, F1 = score(\n",
    "        [candidate],\n",
    "        [reference],\n",
    "        lang=tgt_lang,\n",
    "        model_type=bert_model,\n",
    "        device=\"cuda\",  # GPU 사용\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"BLEU\": round(bleu, 4),\n",
    "        \"METEOR\": round(meteor, 4),\n",
    "        \"ROUGE\": round(\n",
    "            np.mean(np.mean([score.precision for score in scores.values()])), 4\n",
    "        ),\n",
    "        \"BERT\": round(F1.item(), 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reference: str, candidate: str, tgt_lang: str, bert_model=\"bert-base-multilingual-cased\"):\n",
    "    # Tokenize Japanese sentences\n",
    "    reference_tokens = mecab_tokenize(reference)\n",
    "    candidate_tokens = mecab_tokenize(candidate)\n",
    "    \n",
    "    # Calculate BLEU\n",
    "    bleu = sentence_bleu([reference_tokens], candidate_tokens)\n",
    "    \n",
    "    # Calculate ROUGE\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=False)\n",
    "    scores = scorer.score(\" \".join(reference_tokens), \" \".join(candidate_tokens))\n",
    "    \n",
    "    # Calculate BERT Score\n",
    "    P, R, F1 = score(\n",
    "        [candidate],\n",
    "        [reference],\n",
    "        lang=tgt_lang,\n",
    "        model_type=bert_model,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"BLEU\": round(bleu, 4),\n",
    "        \"ROUGE1\": round(scores[\"rouge1\"].fmeasure, 4),\n",
    "        \"ROUGEL\": round(scores[\"rougeL\"].fmeasure, 4),\n",
    "        \"BERT\": round(F1.item(), 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 FLORES-Plus 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.\n",
      "\"그는 \"\"현재 4개월 된 당뇨병에서 치료된 생쥐가 있다\"\"고 덧붙였다.\"\n",
      "「我々が飼っている生後4か月のマウスはかつて糖尿病でしたが現在は糖尿病ではない、」と彼は付け加えました。\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 파일 로더\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "# 데이터 위치 \n",
    "data_dir = \"/home/dudaji/Jun/llm-rag-chatbot/data/flores/\"\n",
    "\n",
    "data_eng = load_text_file(f\"{data_dir}/devtest.eng_Latn\")\n",
    "data_kor = load_text_file(f\"{data_dir}/devtest.kor_Hang\")\n",
    "data_jpn = load_text_file(f\"{data_dir}/devtest.jpn_Jpan\")\n",
    "\n",
    "print(data_eng[0])\n",
    "print(data_kor[0])\n",
    "print(data_jpn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: \"그는 \"\"현재 4개월 된 당뇨병에서 치료된 생쥐가 있다\"\"고 덧붙였다.\"\n",
      "translated: \"He added that there are currently four-month-old mice cured of diabetes.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dudaji/miniconda3/envs/langserve/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BLEU': 0.0163, 'METEOR': 0.3156, 'ROUGE': 0.4615, 'BERT': 0.7634}\n"
     ]
    }
   ],
   "source": [
    "print(f'original: {data_kor[0]}')\n",
    "result = translate(data_kor[0], 'kor', 'eng', prompt)\n",
    "print(f'translated: {result}')\n",
    "\n",
    "eval_result = evaluate(data_eng[0], result, 'eng')\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: \"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.\n",
      "translated: \"우리는 이제 당뇨병이었던 것이 아닌, 당뇨병이 아니었던 4개월 된 생쥐들을 가지고 있습니다.\"\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Install MeCab in order to use it: http://konlpy.org/en/latest/install/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/langserve/lib/python3.11/site-packages/konlpy/tag/_mecab.py:77\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagger \u001b[38;5;241m=\u001b[39m \u001b[43mTagger\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-d \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dicpath)\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagset \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/data/tagset/mecab.json\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m utils\u001b[38;5;241m.\u001b[39minstallpath)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tagger' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m result \u001b[38;5;241m=\u001b[39m translate(data_eng[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkor\u001b[39m\u001b[38;5;124m'\u001b[39m, prompt)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_eng\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_result)\n",
      "Cell \u001b[0;32mIn[40], line 10\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(reference, candidate, tgt_lang, bert_model)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\n\u001b[1;32m      4\u001b[0m     reference: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      5\u001b[0m     candidate: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m ):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# 텍스트를 토큰화\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     reference_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_lang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     candidate_tokens \u001b[38;5;241m=\u001b[39m tokenize(candidate, tgt_lang)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# BLEU 점수 계산 (스무딩 적용)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 12\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(text, lang)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m word_tokenize(text)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# 한국어: Mecab 형태소 분석기\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     mecab \u001b[38;5;241m=\u001b[39m \u001b[43mMecab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mecab\u001b[38;5;241m.\u001b[39mmorphs(text)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjpn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# 일본어: Janome 형태소 분석기\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/langserve/lib/python3.11/site-packages/konlpy/tag/_mecab.py:82\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe MeCab dictionary does not exist at \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Is the dictionary correctly installed?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou can also try entering the dictionary path when initializing the Mecab class: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMecab(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m/some/dic/path\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dicpath)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstall MeCab in order to use it: http://konlpy.org/en/latest/install/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Install MeCab in order to use it: http://konlpy.org/en/latest/install/"
     ]
    }
   ],
   "source": [
    "print(f'original: {data_eng[0]}')\n",
    "result = translate(data_eng[0], 'eng', 'kor', prompt)\n",
    "print(f'translated: {result}')\n",
    "\n",
    "eval_result = evaluate(data_eng[0], result, 'kor')\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning, hello.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CompletionResponse' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m result \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mcomplete(full_prompt)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m----> 5\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m안녕하세요. 좋은 아침입니다.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_result)\n",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(reference, candidate, tgt_lang, bert_model)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\n\u001b[1;32m      3\u001b[0m     reference: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      4\u001b[0m     candidate: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m ):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# BLEU 점수 계산 (스무딩 적용)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     bleu \u001b[38;5;241m=\u001b[39m sentence_bleu(\n\u001b[1;32m     10\u001b[0m         [reference\u001b[38;5;241m.\u001b[39msplit()],\n\u001b[0;32m---> 11\u001b[0m         \u001b[43mcandidate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(),\n\u001b[1;32m     12\u001b[0m         smoothing_function\u001b[38;5;241m=\u001b[39mSmoothingFunction()\u001b[38;5;241m.\u001b[39mmethod1,\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# METEOR 점수 계산\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     meteor \u001b[38;5;241m=\u001b[39m meteor_score([reference], candidate)\n",
      "File \u001b[0;32m~/miniconda3/envs/langserve/lib/python3.11/site-packages/pydantic/main.py:856\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CompletionResponse' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "full_prompt = prompt.format(source_lang='kor', target_lang='eng', source_text=\"안녕하세요. 좋은 아침입니다.\")\n",
    "result = llm.complete(full_prompt)\n",
    "print(result)\n",
    "\n",
    "eval_result = evaluate(\"안녕하세요. 좋은 아침입니다.\", result.text, 'kor')\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good morning, hello.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.\n",
    "\n",
    "evaluate(data_eng[0], data_kor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 수행\n",
    "results = []\n",
    "for _, row in data:  # 각 샘플에 대해 평가 수행\n",
    "    source = row[\"source\"]\n",
    "    reference = row[\"target\"]\n",
    "\n",
    "    # 번역 수행\n",
    "    candidate = translate(source)\n",
    "\n",
    "    # 평가 실행\n",
    "    evaluation = evaluate(reference, candidate, \"ko\")\n",
    "    results.append(\n",
    "        {\n",
    "            \"Source\": source,\n",
    "            \"Reference\": reference,\n",
    "            \"Candidate\": candidate,\n",
    "            **evaluation,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 결과 데이터프레임 생성\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 결과 저장 \n",
    "df.to_csv(\"translation_evaluation_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langserve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
