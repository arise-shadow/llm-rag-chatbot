{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNGD SDK 문서 챗봇 \n",
    "\n",
    "- 작성자: 이준원 (Joonwon Lee)\n",
    "\n",
    "- 날짜: 12/24, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel name: llm-quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field, HttpUrl\n",
    "import json\n",
    "from markdownify import markdownify as md\n",
    "from llama_index.core import Document \n",
    "from llama_index.core.schema import TextNode\n",
    "import os\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Page(BaseModel):\n",
    "    id: str = Field(..., description=\"ID of the Page\")\n",
    "    link: HttpUrl = Field(description=\"Url link of the page\")\n",
    "    name: str = Field(description=\"Name of the page\")\n",
    "    parent: str = Field(default=\"\", description=\"ID of the parent page\")\n",
    "    child: List[str] = Field(default=[], description=\"List of ids of the child pages\")\n",
    "    description: str = Field(default=\"\", description=\"Description of the page\")\n",
    "    description_clean: str = Field(default=\"\", description=\"Content markdown\")\n",
    "    html_content: str = Field(default=\"\", description=\"HTML code of the main content in the page\")\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.link, self.name))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Page):\n",
    "            return False\n",
    "        return (self.link, self.name) == (other.link, other.name)\n",
    "    \n",
    "class CustomDocument(Document):\n",
    "    page_content: str = Field(default=\"\", description=\"Additional content for the document\")\n",
    "\n",
    "def convert_page_to_llama_index_document(page: Page) -> CustomDocument:\n",
    "    return CustomDocument(\n",
    "        doc_id=page.id,\n",
    "        metadata={\n",
    "            \"source\": str(page.link),\n",
    "            \"title\": page.name,\n",
    "            \"parent_doc_id\": page.parent,\n",
    "            \"child_doc_ids\": json.dumps(page.child),\n",
    "        },\n",
    "        text=page.description_clean,  # 기본 text\n",
    "        page_content=page.description_clean,  # 추가 속성\n",
    "    )   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DB 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Page(id='cf227685-cc4e-420e-b21a-e7da166093e5', link=Url('https://furiosa-ai.github.io/docs/latest/en/software/vm_support.html'), name='vm_support', parent='', child=[], description='\\n\\n\\n* Configuring Warboy Pass-through for Virtual Machine\\n* [View page source](../_sources/software/vm_support.rst.txt)\\n\\n---\\n\\n\\n\\nConfiguring Warboy Pass-through for Virtual Machine\\n[\\uf0c1](#configuring-warboy-pass-through-for-virtual-machine \"Permalink to this heading\")\\n=========================================================================================================================================\\n\\nThis section describes how to enable Warboy pass-through for a virtual machine.\\nThe example of this section is based on a specific VM tool\\n`QEMU-KVM`\\n,\\nbut it also works in other VM tools. The environment used in the example is as follows:\\n\\n* Host OS: CentOS 8\\n* Guest OS: Ubuntu 20.04\\n* Virtual Machine: QEMU-KVM\\n\\nPrerequisites\\n[\\uf0c1](#prerequisites \"Permalink to this heading\")\\n-------------------------------------------------------------\\n\\n* IOMMU and VT-x should be enabled in BIOS.\\n* `qemu-kvm`\\n  ,\\n  `libvirt`\\n  ,\\n  `virt-install`\\n  should be installed in a host machine.\\n\\nSetup Instruction\\n[\\uf0c1](#setup-instruction \"Permalink to this heading\")\\n---------------------------------------------------------------------\\n\\n### 1. Enabling IOMMU in BIOS and Linux OS [\\uf0c1](#enabling-iommu-in-bios-and-linux-os \"Permalink to this heading\")\\n\\nFirst of all, you need to enable IOMMU in BIOS and Linux OS.\\nThe following command shows if IOMMU is enabled.\\n\\n```\\ndmesg | grep -e DMAR -e IOMMU\\n\\n```\\n\\nYou will be able to see some messages related to DMAR or IOMMU if IOMMU is enabled.\\nIf you cannot find any messages related to DMAR or IOMMU, you need to enable IOMMU\\nin BIOS, Linux OS or both.\\n\\nThe ways to enable IOMMU in BIOS may depend on server or motherboard models.\\nPlease refer to the manufacturer’s manual.\\n\\nYou check if IOMMU is enabled in Linux OS as follows:\\n\\n```\\ngrep GRUB_CMDLINE_LINUX /etc/default/grub | grep iommu\\n\\n```\\n\\nIf you cannot find any messages related to IOMMU,\\nplease add\\n`intel_iommu=on`\\nfor Intel CPU or\\n`amd_iommu=on`\\nfor AMD CPU\\nto\\n`GRUB_CMDLINE_LINUX`\\nin\\n`/etc/default/grub`\\nand apply the changes by rebooting the machine.\\n\\nIf you use a legacy BIOS boot mode or UEFI boot mode, the way to enable IOMMU\\nin Linux OS can be different.\\n\\n* Legacy BIOS boot mode:\\n  `grub2-mkconfig\\n  \\n  -o\\n  \\n  /boot/grub2/grub.cfg`\\n* UEFI boot mode,\\n  `grub2-mkconfig\\n  \\n  -o\\n  \\n  /boot/efi/EFI/{linux_distrib}/grub.cfg`\\n  .\\n\\nPlease replace\\n`{linux_distrib}`\\nwith a Linux OS name, such as\\n`centos`\\n,\\n`redhat`\\n, or\\n`ubuntu`\\n.\\n\\n\\n### 2. Loading `vfio-pci` module [\\uf0c1](#loading-vfio-pci-module \"Permalink to this heading\")\\n\\nPlease make sure if the kernel module\\n`vfio-pci`\\nis loaded.\\n\\n> ```\\n> [root@localhost ~]# lsmod | grep vfio_pci\\n> vfio_pci               61440  0\\n> vfio_virqfd            16384  1 vfio_pci\\n> vfio_iommu_type1       36864  0\\n> vfio                   36864  2 vfio_iommu_type1,vfio_pci\\n> irqbypass              16384  2 vfio_pci,kvm\\n> \\n> ```\\n\\nIf\\n`vfio_pci`\\nis not loaded yet, please run\\n`modprobe\\n\\nvfio-pci`\\nto load the module.\\nIn some OS environments, you don’t have to load\\n`vfio-pci`\\n.\\nTo make sure, please refer to the OS manual.\\n\\n\\n### 3. Checking if a virtual machine tool is ready [\\uf0c1](#checking-if-a-virtual-machine-tool-is-ready \"Permalink to this heading\")\\n\\nPlease check if a virtual machine tool is ready to run as follows.\\nIf\\n`virt-host-validate`\\nis not found,\\nplease install the prerequisite packages described in\\n[Prerequisites](#vmsupport-prerequisites)\\n\\n> ```\\n> [root@localhost ~]# virt-host-validate\\n>   QEMU: Checking for hardware virtualization                                 : PASS\\n> \\n>   QEMU: Checking for device assignment IOMMU support                         : PASS\\n>   QEMU: Checking if IOMMU is enabled by kernel                               : PASS\\n> \\n> ```\\n\\nIf check items are PASSED, the virtual machine tool is ready.\\n\\n\\n### 4. Finding Warboy’s PCIe device name [\\uf0c1](#finding-warboy-s-pcie-device-name \"Permalink to this heading\")\\n\\nPCI BDF (Bus, Device, Function)\\nis a unique identifier assigned to every PCIe device connected to a machine.\\nPlease find a PCI BDF of a Warboy card that you want to pass through to a virtual machine.\\n\\n> ```\\n> [root@localhost ~]# lspci -nD | grep 1ed2\\n> 0000:01:00.0 1200: 1ed2:0000 (rev 01)\\n> \\n> ```\\n\\n`1ed2`\\nis the PCI vendor ID of FursioaAI Inc.\\n`01:00.0`\\nis the PCI BDF of a Warboy card in the above example.\\nYour PCI BDF will be different according to motherboard model, server model, and PCIe slot.\\n\\nAlternatively, you can use\\n`lspci\\n\\n-DD`\\ncommand to show a PCI BDF list\\nwith vendor names and find a Warboy card from the list.\\nThe vendor names depend on PCIe ID database in OS. If the database is outdated in OS,\\nthe command will show\\n`Device\\n\\n1ed2:0000`\\ninstead of\\n`FuriosaAI,\\n\\nInc.\\n\\nWarboy`\\n.\\nYou can update outdated PCIe ID database by running\\n`update-pciids`\\nin shell.\\n\\nOnce you find the PCIe BDB name, you can find a PCIe device name accepted by a virtual machine tool\\nas follows:\\n\\n> ```\\n> [root@localhost ~]# virsh nodedev-list | grep pci\\n> ...\\n> \\n> pci_0000_01_00_0\\n> \\n> ```\\n\\nA PCIe device name consists of\\n`pci_`\\nand a PCI BDF concatnated with\\n`_`\\n.\\nIn the above example,\\n`pci_0000_01_00_0`\\nis the PCIe device name of a Warboy card.\\n\\n\\n### 5. Creating a virtual machine [\\uf0c1](#creating-a-virtual-machine \"Permalink to this heading\")\\n\\nIf you reach here, you are ready to create a virtual machine with a Warboy passthrough device.\\nPlease create a virtual machine as follows.\\n\\n> ```\\n> virt-install --name ubuntu-vm \\\\\\n>   --os-variant ubuntu20.04 \\\\\\n>   --vcpus 2 \\\\\\n>   --memory 4096 \\\\\\n>   --location /var/lib/libvirt/images/ubuntu-20.04.5-live-server-amd64.iso,kernel=casper/vmlinuz,initrd=casper/initrd \\\\\\n>   --network bridge=br0,model=virtio \\\\\\n>   --disk size=50 \\\\\\n>   --graphics none \\\\\\n>   --host-device=pci_0000_01_00_0\\n> \\n> ```\\n\\nPlease note the option\\n`--host-device`\\nwith the PCIe device name\\nthat we found in the previous step.\\nAlso, you can add more options to the command for your use cases.\\n\\nIn the above example, we set the guest OS image.\\nSo, it will start the guest OS installation step once the virtual machine starts.\\nUbuntu 20.04 or above is recommended for a guest OS.\\nYou can find recommended OS distributions for FuriosaAI SDK at\\n[Minimum requirements for SDK installation](installation.html#minimumrequirements)\\n.\\n\\n\\n### 6. Checking the availability of a Warboy device in VM [\\uf0c1](#checking-the-availability-of-a-warboy-device-in-vm \"Permalink to this heading\")\\n\\nPlease make sure if the Warboy device is available on the virtual machine.\\n`lspci`\\nwill shows all PCIe devices available on the virtual machine as follows.\\n\\n> ```\\n> furiosa@ubuntu-vm:~$ lspci\\n> ...\\n> 05:00.0 Processing accelerators: Device 1ed2:0000 (rev 01)\\n> ...\\n> \\n> furiosa@ubuntu-vm:~$ sudo update-pciids\\n> \\n> furiosa@ubuntu-vm:~$ lspci | grep Furiosa\\n> 05:00.0 Processing accelerators: FuriosaAI, Inc. Warboy (rev 01)\\n> \\n> ```\\n\\n\\n### 7. SDK installation [\\uf0c1](#sdk-installation \"Permalink to this heading\")\\n\\nOnce you confirm that Warboy is available in a virtual machine,\\nplease install\\n[Driver, Firmware, and Runtime Installation](installation.html#requiredpackages)\\nto install SDK and move forward next steps.\\n\\n\\n\\n\\n\\n\\n[Previous](kubernetes_support.html \"Kubernetes Support\")\\n[Next](tutorials.html \"Tutorial and Code Examples\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Configuring Warboy Pass-through for Virtual Machine * [View page source](../_sources/software/vm_support.rst.txt)\\n---\\nConfiguring Warboy Pass-through for Virtual Machine [\\uf0c1](#configuring-warboy-pass-through-for-virtual-machine \"Permalink to this heading\") =========================================================================================================================================\\nThis section describes how to enable Warboy pass-through for a virtual machine. The example of this section is based on a specific VM tool `QEMU-KVM` , but it also works in other VM tools. The environment used in the example is as follows:\\n* Host OS: CentOS 8 * Guest OS: Ubuntu 20.04 * Virtual Machine: QEMU-KVM\\nPrerequisites [\\uf0c1](#prerequisites \"Permalink to this heading\") -------------------------------------------------------------\\n* IOMMU and VT-x should be enabled in BIOS. * `qemu-kvm`   ,   `libvirt`   ,   `virt-install`   should be installed in a host machine.\\nSetup Instruction [\\uf0c1](#setup-instruction \"Permalink to this heading\") ---------------------------------------------------------------------\\n### 1. Enabling IOMMU in BIOS and Linux OS [\\uf0c1](#enabling-iommu-in-bios-and-linux-os \"Permalink to this heading\")\\nFirst of all, you need to enable IOMMU in BIOS and Linux OS. The following command shows if IOMMU is enabled.\\n``` dmesg | grep -e DMAR -e IOMMU\\n```\\nYou will be able to see some messages related to DMAR or IOMMU if IOMMU is enabled. If you cannot find any messages related to DMAR or IOMMU, you need to enable IOMMU in BIOS, Linux OS or both.\\nThe ways to enable IOMMU in BIOS may depend on server or motherboard models. Please refer to the manufacturer’s manual.\\nYou check if IOMMU is enabled in Linux OS as follows:\\n``` grep GRUB_CMDLINE_LINUX /etc/default/grub | grep iommu\\n```\\nIf you cannot find any messages related to IOMMU, please add `intel_iommu=on` for Intel CPU or `amd_iommu=on` for AMD CPU to `GRUB_CMDLINE_LINUX` in `/etc/default/grub` and apply the changes by rebooting the machine.\\nIf you use a legacy BIOS boot mode or UEFI boot mode, the way to enable IOMMU in Linux OS can be different.\\n* Legacy BIOS boot mode:   `grub2-mkconfig      -o      /boot/grub2/grub.cfg` * UEFI boot mode,   `grub2-mkconfig      -o      /boot/efi/EFI/{linux_distrib}/grub.cfg`   .\\nPlease replace `{linux_distrib}` with a Linux OS name, such as `centos` , `redhat` , or `ubuntu` .\\n### 2. Loading `vfio-pci` module [\\uf0c1](#loading-vfio-pci-module \"Permalink to this heading\")\\nPlease make sure if the kernel module `vfio-pci` is loaded.\\n> ``` > [root@localhost ~]# lsmod | grep vfio_pci > vfio_pci               61440  0 > vfio_virqfd            16384  1 vfio_pci > vfio_iommu_type1       36864  0 > vfio                   36864  2 vfio_iommu_type1,vfio_pci > irqbypass              16384  2 vfio_pci,kvm >  > ```\\nIf `vfio_pci` is not loaded yet, please run `modprobe\\nvfio-pci` to load the module. In some OS environments, you don’t have to load `vfio-pci` . To make sure, please refer to the OS manual.\\n### 3. Checking if a virtual machine tool is ready [\\uf0c1](#checking-if-a-virtual-machine-tool-is-ready \"Permalink to this heading\")\\nPlease check if a virtual machine tool is ready to run as follows. If `virt-host-validate` is not found, please install the prerequisite packages described in [Prerequisites](#vmsupport-prerequisites)\\n> ``` > [root@localhost ~]# virt-host-validate >   QEMU: Checking for hardware virtualization                                 : PASS >  >   QEMU: Checking for device assignment IOMMU support                         : PASS >   QEMU: Checking if IOMMU is enabled by kernel                               : PASS >  > ```\\nIf check items are PASSED, the virtual machine tool is ready.\\n### 4. Finding Warboy’s PCIe device name [\\uf0c1](#finding-warboy-s-pcie-device-name \"Permalink to this heading\")\\nPCI BDF (Bus, Device, Function) is a unique identifier assigned to every PCIe device connected to a machine. Please find a PCI BDF of a Warboy card that you want to pass through to a virtual machine.\\n> ``` > [root@localhost ~]# lspci -nD | grep 1ed2 > 0000:01:00.0 1200: 1ed2:0000 (rev 01) >  > ```  `1ed2` is the PCI vendor ID of FursioaAI Inc. `01:00.0` is the PCI BDF of a Warboy card in the above example. Your PCI BDF will be different according to motherboard model, server model, and PCIe slot.\\nAlternatively, you can use `lspci\\n-DD` command to show a PCI BDF list with vendor names and find a Warboy card from the list. The vendor names depend on PCIe ID database in OS. If the database is outdated in OS, the command will show `Device\\n1ed2:0000` instead of `FuriosaAI,\\nInc.\\nWarboy` .\\nYou can update outdated PCIe ID database by running `update-pciids` in shell.\\nOnce you find the PCIe BDB name, you can find a PCIe device name accepted by a virtual machine tool as follows:\\n> ``` > [root@localhost ~]# virsh nodedev-list | grep pci > ... >  > pci_0000_01_00_0 >  > ```\\nA PCIe device name consists of `pci_` and a PCI BDF concatnated with `_` . In the above example, `pci_0000_01_00_0` is the PCIe device name of a Warboy card.\\n### 5. Creating a virtual machine [\\uf0c1](#creating-a-virtual-machine \"Permalink to this heading\")\\nIf you reach here, you are ready to create a virtual machine with a Warboy passthrough device. Please create a virtual machine as follows.\\n> ``` > virt-install --name ubuntu-vm \\\\ >   --os-variant ubuntu20.04 \\\\ >   --vcpus 2 \\\\ >   --memory 4096 \\\\ >   --location /var/lib/libvirt/images/ubuntu-20.04.5-live-server-amd64.iso,kernel=casper/vmlinuz,initrd=casper/initrd \\\\ >   --network bridge=br0,model=virtio \\\\ >   --disk size=50 \\\\ >   --graphics none \\\\ >   --host-device=pci_0000_01_00_0 >  > ```\\nPlease note the option `--host-device` with the PCIe device name that we found in the previous step. Also, you can add more options to the command for your use cases.\\nIn the above example, we set the guest OS image. So, it will start the guest OS installation step once the virtual machine starts. Ubuntu 20.04 or above is recommended for a guest OS. You can find recommended OS distributions for FuriosaAI SDK at [Minimum requirements for SDK installation](installation.html#minimumrequirements) .\\n### 6. Checking the availability of a Warboy device in VM [\\uf0c1](#checking-the-availability-of-a-warboy-device-in-vm \"Permalink to this heading\")\\nPlease make sure if the Warboy device is available on the virtual machine. `lspci` will shows all PCIe devices available on the virtual machine as follows.\\n> ``` > furiosa@ubuntu-vm:~$ lspci > ... > 05:00.0 Processing accelerators: Device 1ed2:0000 (rev 01) > ... >  > furiosa@ubuntu-vm:~$ sudo update-pciids >  > furiosa@ubuntu-vm:~$ lspci | grep Furiosa > 05:00.0 Processing accelerators: FuriosaAI, Inc. Warboy (rev 01) >  > ```\\n### 7. SDK installation [\\uf0c1](#sdk-installation \"Permalink to this heading\")\\nOnce you confirm that Warboy is available in a virtual machine, please install [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) to install SDK and move forward next steps.\\n[Previous](kubernetes_support.html \"Kubernetes Support\") [Next](tutorials.html \"Tutorial and Code Examples\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Configuring Warboy Pass-through for Virtual Machine\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/vm_support.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"configuring-warboy-pass-through-for-virtual-machine\">\\n     <span id=\"vmsupport\">\\n     </span>\\n     <h1>\\n      Configuring Warboy Pass-through for Virtual Machine\\n      <a class=\"headerlink\" href=\"#configuring-warboy-pass-through-for-virtual-machine\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      This section describes how to enable Warboy pass-through for a virtual machine.\\nThe example of this section is based on a specific VM tool\\n      <code class=\"docutils literal notranslate\">\\n       <span class=\"pre\">\\n        QEMU-KVM\\n       </span>\\n      </code>\\n      ,\\nbut it also works in other VM tools. The environment used in the example is as follows:\\n     </p>\\n     <ul class=\"simple\">\\n      <li>\\n       <p>\\n        Host OS: CentOS 8\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Guest OS: Ubuntu 20.04\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Virtual Machine: QEMU-KVM\\n       </p>\\n      </li>\\n     </ul>\\n     <section id=\"prerequisites\">\\n      <span id=\"vmsupport-prerequisites\">\\n      </span>\\n      <h2>\\n       Prerequisites\\n       <a class=\"headerlink\" href=\"#prerequisites\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         IOMMU and VT-x should be enabled in BIOS.\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           qemu-kvm\\n          </span>\\n         </code>\\n         ,\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           libvirt\\n          </span>\\n         </code>\\n         ,\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           virt-install\\n          </span>\\n         </code>\\n         should be installed in a host machine.\\n        </p>\\n       </li>\\n      </ul>\\n     </section>\\n     <section id=\"setup-instruction\">\\n      <h2>\\n       Setup Instruction\\n       <a class=\"headerlink\" href=\"#setup-instruction\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <section id=\"enabling-iommu-in-bios-and-linux-os\">\\n       <h3>\\n        1. Enabling IOMMU in BIOS and Linux OS\\n        <a class=\"headerlink\" href=\"#enabling-iommu-in-bios-and-linux-os\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        First of all, you need to enable IOMMU in BIOS and Linux OS.\\nThe following command shows if IOMMU is enabled.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>dmesg<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>grep<span class=\"w\"> </span>-e<span class=\"w\"> </span>DMAR<span class=\"w\"> </span>-e<span class=\"w\"> </span>IOMMU\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        You will be able to see some messages related to DMAR or IOMMU if IOMMU is enabled.\\nIf you cannot find any messages related to DMAR or IOMMU, you need to enable IOMMU\\nin BIOS, Linux OS or both.\\n       </p>\\n       <p>\\n        The ways to enable IOMMU in BIOS may depend on server or motherboard models.\\nPlease refer to the manufacturer’s manual.\\n       </p>\\n       <p>\\n        You check if IOMMU is enabled in Linux OS as follows:\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>grep<span class=\"w\"> </span>GRUB_CMDLINE_LINUX<span class=\"w\"> </span>/etc/default/grub<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>grep<span class=\"w\"> </span>iommu\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        If you cannot find any messages related to IOMMU,\\nplease add\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          intel_iommu=on\\n         </span>\\n        </code>\\n        for Intel CPU or\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          amd_iommu=on\\n         </span>\\n        </code>\\n        for AMD CPU\\nto\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          GRUB_CMDLINE_LINUX\\n         </span>\\n        </code>\\n        in\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          /etc/default/grub\\n         </span>\\n        </code>\\n        and apply the changes by rebooting the machine.\\n       </p>\\n       <p>\\n        If you use a legacy BIOS boot mode or UEFI boot mode, the way to enable IOMMU\\nin Linux OS can be different.\\n       </p>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          Legacy BIOS boot mode:\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            grub2-mkconfig\\n           </span>\\n           <span class=\"pre\">\\n            -o\\n           </span>\\n           <span class=\"pre\">\\n            /boot/grub2/grub.cfg\\n           </span>\\n          </code>\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          UEFI boot mode,\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            grub2-mkconfig\\n           </span>\\n           <span class=\"pre\">\\n            -o\\n           </span>\\n           <span class=\"pre\">\\n            /boot/efi/EFI/{linux_distrib}/grub.cfg\\n           </span>\\n          </code>\\n          .\\n         </p>\\n        </li>\\n       </ul>\\n       <p>\\n        Please replace\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          {linux_distrib}\\n         </span>\\n        </code>\\n        with a Linux OS name, such as\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          centos\\n         </span>\\n        </code>\\n        ,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          redhat\\n         </span>\\n        </code>\\n        , or\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          ubuntu\\n         </span>\\n        </code>\\n        .\\n       </p>\\n      </section>\\n      <section id=\"loading-vfio-pci-module\">\\n       <h3>\\n        2. Loading\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          vfio-pci\\n         </span>\\n        </code>\\n        module\\n        <a class=\"headerlink\" href=\"#loading-vfio-pci-module\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Please make sure if the kernel module\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          vfio-pci\\n         </span>\\n        </code>\\n        is loaded.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"p\">[</span><span class=\"n\">root</span><span class=\"nd\">@localhost</span> <span class=\"o\">~</span><span class=\"p\">]</span><span class=\"c1\"># lsmod | grep vfio_pci</span>\\n<span class=\"n\">vfio_pci</span>               <span class=\"mi\">61440</span>  <span class=\"mi\">0</span>\\n<span class=\"n\">vfio_virqfd</span>            <span class=\"mi\">16384</span>  <span class=\"mi\">1</span> <span class=\"n\">vfio_pci</span>\\n<span class=\"n\">vfio_iommu_type1</span>       <span class=\"mi\">36864</span>  <span class=\"mi\">0</span>\\n<span class=\"n\">vfio</span>                   <span class=\"mi\">36864</span>  <span class=\"mi\">2</span> <span class=\"n\">vfio_iommu_type1</span><span class=\"p\">,</span><span class=\"n\">vfio_pci</span>\\n<span class=\"n\">irqbypass</span>              <span class=\"mi\">16384</span>  <span class=\"mi\">2</span> <span class=\"n\">vfio_pci</span><span class=\"p\">,</span><span class=\"n\">kvm</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        If\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          vfio_pci\\n         </span>\\n        </code>\\n        is not loaded yet, please run\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          modprobe\\n         </span>\\n         <span class=\"pre\">\\n          vfio-pci\\n         </span>\\n        </code>\\n        to load the module.\\nIn some OS environments, you don’t have to load\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          vfio-pci\\n         </span>\\n        </code>\\n        .\\nTo make sure, please refer to the OS manual.\\n       </p>\\n      </section>\\n      <section id=\"checking-if-a-virtual-machine-tool-is-ready\">\\n       <h3>\\n        3. Checking if a virtual machine tool is ready\\n        <a class=\"headerlink\" href=\"#checking-if-a-virtual-machine-tool-is-ready\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Please check if a virtual machine tool is ready to run as follows.\\nIf\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          virt-host-validate\\n         </span>\\n        </code>\\n        is not found,\\nplease install the prerequisite packages described in\\n        <a class=\"reference internal\" href=\"#vmsupport-prerequisites\">\\n         <span class=\"std std-ref\">\\n          Prerequisites\\n         </span>\\n        </a>\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"p\">[</span><span class=\"n\">root</span><span class=\"nd\">@localhost</span> <span class=\"o\">~</span><span class=\"p\">]</span><span class=\"c1\"># virt-host-validate</span>\\n  <span class=\"n\">QEMU</span><span class=\"p\">:</span> <span class=\"n\">Checking</span> <span class=\"k\">for</span> <span class=\"n\">hardware</span> <span class=\"n\">virtualization</span>                                 <span class=\"p\">:</span> <span class=\"n\">PASS</span>\\n\\n  <span class=\"n\">QEMU</span><span class=\"p\">:</span> <span class=\"n\">Checking</span> <span class=\"k\">for</span> <span class=\"n\">device</span> <span class=\"n\">assignment</span> <span class=\"n\">IOMMU</span> <span class=\"n\">support</span>                         <span class=\"p\">:</span> <span class=\"n\">PASS</span>\\n  <span class=\"n\">QEMU</span><span class=\"p\">:</span> <span class=\"n\">Checking</span> <span class=\"k\">if</span> <span class=\"n\">IOMMU</span> <span class=\"ow\">is</span> <span class=\"n\">enabled</span> <span class=\"n\">by</span> <span class=\"n\">kernel</span>                               <span class=\"p\">:</span> <span class=\"n\">PASS</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        If check items are PASSED, the virtual machine tool is ready.\\n       </p>\\n      </section>\\n      <section id=\"finding-warboy-s-pcie-device-name\">\\n       <h3>\\n        4. Finding Warboy’s PCIe device name\\n        <a class=\"headerlink\" href=\"#finding-warboy-s-pcie-device-name\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        PCI BDF (Bus, Device, Function)\\nis a unique identifier assigned to every PCIe device connected to a machine.\\nPlease find a PCI BDF of a Warboy card that you want to pass through to a virtual machine.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"p\">[</span><span class=\"n\">root</span><span class=\"nd\">@localhost</span> <span class=\"o\">~</span><span class=\"p\">]</span><span class=\"c1\"># lspci -nD | grep 1ed2</span>\\n<span class=\"mi\">0000</span><span class=\"p\">:</span><span class=\"mi\">01</span><span class=\"p\">:</span><span class=\"mf\">00.0</span> <span class=\"mi\">1200</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"n\">ed2</span><span class=\"p\">:</span><span class=\"mi\">0000</span> <span class=\"p\">(</span><span class=\"n\">rev</span> <span class=\"mi\">01</span><span class=\"p\">)</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          1ed2\\n         </span>\\n        </code>\\n        is the PCI vendor ID of FursioaAI Inc.\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          01:00.0\\n         </span>\\n        </code>\\n        is the PCI BDF of a Warboy card in the above example.\\nYour PCI BDF will be different according to motherboard model, server model, and PCIe slot.\\n       </p>\\n       <p>\\n        Alternatively, you can use\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          lspci\\n         </span>\\n         <span class=\"pre\">\\n          -DD\\n         </span>\\n        </code>\\n        command to show a PCI BDF list\\nwith vendor names and find a Warboy card from the list.\\nThe vendor names depend on PCIe ID database in OS. If the database is outdated in OS,\\nthe command will show\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Device\\n         </span>\\n         <span class=\"pre\">\\n          1ed2:0000\\n         </span>\\n        </code>\\n        instead of\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          FuriosaAI,\\n         </span>\\n         <span class=\"pre\">\\n          Inc.\\n         </span>\\n         <span class=\"pre\">\\n          Warboy\\n         </span>\\n        </code>\\n        .\\nYou can update outdated PCIe ID database by running\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          update-pciids\\n         </span>\\n        </code>\\n        in shell.\\n       </p>\\n       <p>\\n        Once you find the PCIe BDB name, you can find a PCIe device name accepted by a virtual machine tool\\nas follows:\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"p\">[</span><span class=\"n\">root</span><span class=\"nd\">@localhost</span> <span class=\"o\">~</span><span class=\"p\">]</span><span class=\"c1\"># virsh nodedev-list | grep pci</span>\\n<span class=\"o\">...</span>\\n\\n<span class=\"n\">pci_0000_01_00_0</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        A PCIe device name consists of\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          pci_\\n         </span>\\n        </code>\\n        and a PCI BDF concatnated with\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          _\\n         </span>\\n        </code>\\n        .\\nIn the above example,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          pci_0000_01_00_0\\n         </span>\\n        </code>\\n        is the PCIe device name of a Warboy card.\\n       </p>\\n      </section>\\n      <section id=\"creating-a-virtual-machine\">\\n       <h3>\\n        5. Creating a virtual machine\\n        <a class=\"headerlink\" href=\"#creating-a-virtual-machine\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        If you reach here, you are ready to create a virtual machine with a Warboy passthrough device.\\nPlease create a virtual machine as follows.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"n\">virt</span><span class=\"o\">-</span><span class=\"n\">install</span> <span class=\"o\">--</span><span class=\"n\">name</span> <span class=\"n\">ubuntu</span><span class=\"o\">-</span><span class=\"n\">vm</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">os</span><span class=\"o\">-</span><span class=\"n\">variant</span> <span class=\"n\">ubuntu20</span><span class=\"mf\">.04</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">vcpus</span> <span class=\"mi\">2</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">memory</span> <span class=\"mi\">4096</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">location</span> <span class=\"o\">/</span><span class=\"n\">var</span><span class=\"o\">/</span><span class=\"n\">lib</span><span class=\"o\">/</span><span class=\"n\">libvirt</span><span class=\"o\">/</span><span class=\"n\">images</span><span class=\"o\">/</span><span class=\"n\">ubuntu</span><span class=\"o\">-</span><span class=\"mf\">20.04.5</span><span class=\"o\">-</span><span class=\"n\">live</span><span class=\"o\">-</span><span class=\"n\">server</span><span class=\"o\">-</span><span class=\"n\">amd64</span><span class=\"o\">.</span><span class=\"n\">iso</span><span class=\"p\">,</span><span class=\"n\">kernel</span><span class=\"o\">=</span><span class=\"n\">casper</span><span class=\"o\">/</span><span class=\"n\">vmlinuz</span><span class=\"p\">,</span><span class=\"n\">initrd</span><span class=\"o\">=</span><span class=\"n\">casper</span><span class=\"o\">/</span><span class=\"n\">initrd</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">network</span> <span class=\"n\">bridge</span><span class=\"o\">=</span><span class=\"n\">br0</span><span class=\"p\">,</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">virtio</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">disk</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"mi\">50</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">graphics</span> <span class=\"n\">none</span> \\\\\\n  <span class=\"o\">--</span><span class=\"n\">host</span><span class=\"o\">-</span><span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">pci_0000_01_00_0</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        Please note the option\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          --host-device\\n         </span>\\n        </code>\\n        with the PCIe device name\\nthat we found in the previous step.\\nAlso, you can add more options to the command for your use cases.\\n       </p>\\n       <p>\\n        In the above example, we set the guest OS image.\\nSo, it will start the guest OS installation step once the virtual machine starts.\\nUbuntu 20.04 or above is recommended for a guest OS.\\nYou can find recommended OS distributions for FuriosaAI SDK at\\n        <a class=\"reference internal\" href=\"installation.html#minimumrequirements\">\\n         <span class=\"std std-ref\">\\n          Minimum requirements for SDK installation\\n         </span>\\n        </a>\\n        .\\n       </p>\\n      </section>\\n      <section id=\"checking-the-availability-of-a-warboy-device-in-vm\">\\n       <h3>\\n        6. Checking the availability of a Warboy device in VM\\n        <a class=\"headerlink\" href=\"#checking-the-availability-of-a-warboy-device-in-vm\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Please make sure if the Warboy device is available on the virtual machine.\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          lspci\\n         </span>\\n        </code>\\n        will shows all PCIe devices available on the virtual machine as follows.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-default notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span>furiosa@ubuntu-vm:~$ lspci\\n...\\n05:00.0 Processing accelerators: Device 1ed2:0000 (rev 01)\\n...\\n\\nfuriosa@ubuntu-vm:~$ sudo update-pciids\\n\\nfuriosa@ubuntu-vm:~$ lspci | grep Furiosa\\n05:00.0 Processing accelerators: FuriosaAI, Inc. Warboy (rev 01)\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n      </section>\\n      <section id=\"sdk-installation\">\\n       <h3>\\n        7. SDK installation\\n        <a class=\"headerlink\" href=\"#sdk-installation\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Once you confirm that Warboy is available in a virtual machine,\\nplease install\\n        <a class=\"reference internal\" href=\"installation.html#requiredpackages\">\\n         <span class=\"std std-ref\">\\n          Driver, Firmware, and Runtime Installation\\n         </span>\\n        </a>\\n        to install SDK and move forward next steps.\\n       </p>\\n      </section>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"kubernetes_support.html\" rel=\"prev\" title=\"Kubernetes Support\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"tutorials.html\" rel=\"next\" title=\"Tutorial and Code Examples\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='333851a4-2ea4-4903-87a2-0d50943faf1f', link=Url('https://furiosa-ai.github.io/docs/latest/en/software/performance.html'), name='performance', parent='', child=[], description='\\n\\n\\n* Performance Optimization\\n* [View page source](../_sources/software/performance.rst.txt)\\n\\n---\\n\\n\\n\\nPerformance Optimization\\n[\\uf0c1](#performance-optimization \"Permalink to this heading\")\\n===================================================================================\\n\\nTo ensure efficient inference serving in production,\\nit’s essential to focus on throughput and latency as key metrics.\\nFuriosa SDK offers two optimization methods for both throughput and latency:\\n\\n* **Model Optimization**\\n  : are ways to optimize models during the phases of model development,\\n  quantization, and compilation. Some optimization techniques may modify the models, leading to\\n  more efficient compiled programs.\\n* **Runtime Optimization**\\n  : are ways to optimize the runtime execution of compiled programs.\\n  They are about how to optimize inference codes through Runtime library depending\\n  on the characteristics of models workloads for higher throughput.\\n\\nIn this section, we will discuss the performance metrics and how to optimize them\\nin both above ways.\\n\\nPerformance Metrics: Latency and Throughput\\n[\\uf0c1](#performance-metrics-latency-and-throughput \"Permalink to this heading\")\\n------------------------------------------------------------------------------------------------------------------------\\n\\n*Latency*\\nis one of the major performance evaluation criteria for model inference.\\nit’s a measure of how long a single inference takes\\nfrom when the input data is passed to the model until the output value is received.\\nWith low latency, users can experience high responsiveness.\\n\\nAnother performance evaluation criterion is throughput.\\nThroughput means the number of inferences that can be processed within a unit of time.\\nThroughput implies that how many requests a system handle simultaneously.\\n\\nA single end-to-end inference consists of three kinds of operations: NPU execution, CPU computation\\nand IO operation between host and NPU device. Three kinds of operations run independently without blocking one another.\\nSo, multiple inferences can run simultaneously while different operations run.\\nWhen we continue to run multiple requests simultaneously,\\nthe longer operation among NPU, CPU, and IO operations is likely to determine the inference time.\\nThis is because the shorter operations will be hidden by other longer operations.\\nThis is a key characteristic to understand how to optimize the performance of a model.\\nYou can find more details at\\n[Concurrency Optimization](#concurrencyoptimization)\\n.\\n\\nNPU utilization is not a performance metrics, but it’s one of the key metrics to indicate\\nhow much the model utilizes a NPU device for inferences.\\nNPU utilization can be defined as the proportion of time the NPU is used during inference.\\nWith NPU utilziation, we can evaluate how well the model is well-optimized for NPU acceleration.\\nSometimes, it may also imply the room for further optimization opportunities.\\nPlease refer to\\n[Toolkit](cli.html#toolkit)\\nfor how to measure NPU utilization.\\n\\n### Performance Profiling [\\uf0c1](#performance-profiling \"Permalink to this heading\")\\n\\nTo analyze the performance of a workload, we need to measure performance metrics as well as\\nwe need to have a closer look at the times of NPU executions, CPU computations, and I/O operations.\\n\\nFor them, there are two useful tools in Furiosa SDK.\\n\\n* [furiosa-bench (Benchmark Tool)](cli.html#furiosabench)\\n  is a tool to measure the performance metrics such as latencies and\\n  throughput (i.e., QPS - queries per second).\\n* [Performance Profiling](profiler.html#profiling)\\n  provides a way to measure the durations of NPU executions and other operations.\\n\\n`furiosa-bench`\\nalso provides\\n`--trace-output`\\noption to generate a trace file.\\nThis is another easy way to measure the durations of NPU executions and other operations\\nfrom a running workload.\\n\\n\\n\\nModel Optimization\\n[\\uf0c1](#model-optimization \"Permalink to this heading\")\\n-----------------------------------------------------------------------\\n\\nIn this seciton, we introduce some model optimization techniques to improve the performance\\nof models. They key idea of the model optimization is to identify the bottleneck parts\\n(usually operators) of the model and to reduce the times of the bottleneck parts or remove them.\\n\\nFor example, if some operators of a model are not accelerated by NPU,\\nthey can be a major bottleneck. If you remove the operators or replace them with other equivalents,\\nthe inference latency can be reduced significantly.\\n\\n### Optimizing `Quantize` Operator [\\uf0c1](#optimizing-quantize-operator \"Permalink to this heading\")\\n\\nFuriosaAI’s first-generation NPU, Warboy, supports only int8 type.\\nAs the majority of deep learning models are built upon floating point types like fp32 and fp16,\\nto execute these models on Warboy,\\na quantization step is necessary to convert the fp32 weights to int8 model weights.\\nIn addition, the quantization step adds\\n`quantize`\\n,\\n`dequantize`\\noperators to the\\ninput and output parts of the model respectively.\\n`quantize`\\nand\\n`dequantize`\\noperators convert fp32 input values to int8 values and vice versa.\\nThose operators are executed on the CPU and are time-consuming.\\n\\nInputs of many CNN-based models are images. In particular, an image is represented as\\nRGB channels. In other words, a single image is composed of three images for each channel of RGB,\\nwhere each image is represented with 8-bit integer values, ranging from 0 to 255.\\n\\nTo feed an image to a model, we need to convert the\\n`int8`\\nvalues of each RGB channel\\nto\\n`fp32`\\nvalues, and\\n`quantize`\\noperator in the model converts\\n`fp32`\\nvalues\\nto\\n`int8`\\nvalues. It’s unnecessary if we can feed RGB images in\\n`int8`\\nto a model directly.\\n\\nTo support this optimization,\\n`furiosa-quantizer`\\nprovides the\\n`ModelEditor`\\nAPI.\\n`ModelEditor`\\ntakes the model optimized by\\n`optimize_model()`\\n.\\n\\n```\\nmodel = onnx.load_model(\"yolox_l.onnx\")\\nmodel = optimize_model(model)\\n\\neditor = ModelEditor(model)\\n\\n```\\n\\n`convert_input_type()`\\nmethod of\\n`ModelEditor`\\ntakes a tensor name and a data type as arguments.\\nIt modifies the data type of the input tensor in the model to be the given arguments.\\nThe target type can be either\\n`INT8`\\nor\\n`UINT8`\\n. You can find the tensor name\\nthrough\\n`get_pure_input_names`\\nmethod.\\n\\n```\\ninput_tensor_name = get_pure_input_names(model)[0]\\n\\n# Convert this input tensor to uint8\\neditor.convert_input_type(input_tensor_name, TensorType.UINT8)\\n\\n```\\n\\nAs you can see in the above example,\\n`convert_input_type`\\nchanges the data type of the input tensor to be\\n`uint8`\\n.\\nThe reason why we use\\n`uint8`\\ninstead of\\n`int8`\\nis that the pixel values are represented as positive values.\\n\\nBefore this model modification,\\n`quantize`\\noperator converts\\n`float32`\\nvalues to\\n`int8`\\nvalues.\\nAfter this model modification, the quantize operator converts\\n`uint8`\\nvalues to\\n`int8`\\nvalues.\\nThis conversion from\\n`uint8`\\nto\\n`int8`\\nis much faster than the conversion from\\n`float32`\\nto\\n`int8`\\n.\\nThe followings are the the benchmark results of before and after the model modification.\\nAlso, the figure shows how the\\n`quantize`\\noperator is changed.\\n\\n\\nQuantization in YOLOX\\\\_L\\n\\n[\\uf0c1](#id2 \"Permalink to this table\")\\n\\n| Input type | `Quantize` execution time |\\n| --- | --- |\\n| float32 | 60.639 ms |\\n| uint8 | 0.277 ms |\\n\\n\\n\\n\\nquantize without\\n`ModelEditor`\\n\\n[\\uf0c1](#id3 \"Permalink to this image\")\\n\\n\\n\\n\\n\\nquantize with\\n`convert_input_type`\\n\\n[\\uf0c1](#id4 \"Permalink to this image\")\\n\\n\\n\\n\\n\\nWarning\\n\\nThis optmization may affect the accurarcy of the model.\\nSince it depends on models and applications,\\nit is recommended to validate the accuracy of the model.\\n\\nThe following is a real example code to use\\n`ModelEditor`\\nAPI with\\n`convert_input_type()`\\n.\\n\\n```\\n#!/usr/bin/env python\\n\\nimport time\\nimport numpy as np\\nimport onnx\\nimport torch\\nimport torchvision\\nfrom torchvision import transforms\\nimport tqdm\\n\\nfrom furiosa.optimizer import optimize_model\\nfrom furiosa.quantizer import get_pure_input_names, quantize, Calibrator, CalibrationMethod, ModelEditor, TensorType\\nfrom furiosa.runtime import session\\nfrom furiosa.runtime.profiler import profile\\n\\n\\ntorch_model = torchvision.models.resnet50(weights=\\'DEFAULT\\')\\ntorch_model = torch_model.eval()\\n\\ndummy_input = (torch.randn(1, 3, 224, 224),)\\n\\ntorch.onnx.export(\\n    torch_model,  # PyTorch model to export\\n    dummy_input,  # model input\\n    \"resnet50.onnx\",  # where to save the exported ONNX model\\n    opset_version=13,  # the ONNX OpSet version to export the model to\\n    do_constant_folding=True,  # whether to execute constant folding for optimization\\n    input_names=[\"input\"],  # the ONNX model\\'s input names\\n    output_names=[\"output\"],  # the ONNX model\\'s output names\\n)\\n\\nonnx_model = onnx.load_model(\"resnet50.onnx\")\\nonnx_model = optimize_model(onnx_model)\\n\\ncalibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM)\\ncalibrator.collect_data([[torch.randn(1, 3, 224, 224).numpy()]])\\nranges = calibrator.compute_range()\\n\\neditor = ModelEditor(onnx_model)\\ninput_tensor_name = get_pure_input_names(onnx_model)[0]\\n\\n# Convert the input type to uint8\\neditor.convert_input_type(input_tensor_name, TensorType.UINT8)\\n\\ngraph = quantize(onnx_model, ranges)\\n\\nwith open(\"trace.json\", \"w\") as trace:\\n    with profile(file=trace) as profiler:\\n        with session.create(graph) as session:\\n            image = torch.randint(256, (1, 3, 224, 224), dtype=torch.uint8)\\n            with profiler.record(\"pre\"):\\n                image = image.numpy()\\n            with profiler.record(\"inf\"):\\n                outputs = session.run(image)\\n            with profiler.record(\"post\"):\\n                prediction = np.argmax(outputs[0].numpy(), axis=1)\\n\\n```\\n\\n\\n\\n### Optimizing `Dequantize` Operator [\\uf0c1](#optimizing-dequantize-operator \"Permalink to this heading\")\\n\\nSimilar to the above\\n`Quantize`\\noperator optimization,\\n`Dequantize`\\noperator also can be optimized in the similar way.\\n\\nIf the model output tensor is\\n`fp32`\\n, the output of\\n`int8`\\nvalues must be converted to f32 values.\\n`Dequantize`\\noperator converts int8 values to fp32 values, and it’s executed on CPU.\\nIf the model output is an RGB image or something else which can be represented as\\n`int8`\\nor\\n`uint8`\\nvalues,\\nwe can skip converting\\n`int8`\\nor\\n`uint8`\\nto\\n`fp32`\\n. It will reduce the inference latency significantly.\\n\\nWe can enable this optimization by using\\n`convert_output_type()`\\nmethod of\\n`ModelEditor`\\n.\\n`convert_output_type()`\\nmethod can modifies a model output by a given tensor name and a target data type.\\nThe target type can be either\\n`INT8`\\nor\\n`UINT8`\\n.\\n\\n\\n\\n\\nquantize with\\n`convert_output_type`\\n\\n[\\uf0c1](#id5 \"Permalink to this image\")\\n\\n\\n\\n\\n\\nquantize with\\n`convert_input_type`\\nand\\n`convert_output_type`\\n\\n[\\uf0c1](#id6 \"Permalink to this image\")\\n\\n\\n\\n\\n\\nNote\\n\\nFuriosa Compiler may automatically apply this optimization\\nto the model even if this optmization is not explicitly applied.\\nIn that case, the optimization by Furiosa Compiler may result in lower latency\\nthan the one manually applied by\\n`ModelEditor`\\n.\\nIt is recommended to do experiments to find the best option.\\n\\n\\nWarning\\n\\nThis optmization may affect the accurarcy of the model.\\nSince it depends on models and applications,\\nit is recommended to validate the accuracy of the model.\\n\\nThe following is an real example code to use\\n`convert_output_type`\\noption.\\n\\n```\\n#!/usr/bin/env python\\n\\nimport time\\nimport numpy as np\\nimport onnx\\nimport torch\\nimport torchvision\\nfrom torchvision import transforms\\nimport tqdm\\n\\nfrom furiosa.optimizer import optimize_model\\nfrom furiosa.quantizer import get_output_names, quantize, Calibrator, CalibrationMethod, ModelEditor, TensorType\\nfrom furiosa.runtime import session\\nfrom furiosa.runtime.profiler import profile\\n\\n\\ntorch_model = torchvision.models.resnet50(weights=\\'DEFAULT\\')\\ntorch_model = torch_model.eval()\\n\\ndummy_input = (torch.randn(1, 3, 224, 224),)\\n\\ntorch.onnx.export(\\n    torch_model,  # PyTorch model to export\\n    dummy_input,  # model input\\n    \"resnet50.onnx\",  # where to save the exported ONNX model\\n    opset_version=13,  # the ONNX OpSet version to export the model to\\n    do_constant_folding=True,  # whether to execute constant folding for optimization\\n    input_names=[\"input\"],  # the ONNX model\\'s input names\\n    output_names=[\"output\"],  # the ONNX model\\'s output names\\n)\\n\\nonnx_model = onnx.load_model(\"resnet50.onnx\")\\nonnx_model = optimize_model(onnx_model)\\n\\ncalibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM)\\ncalibrator.collect_data([[torch.randn(1, 3, 224, 224).numpy()]])\\nranges = calibrator.compute_range()\\n\\neditor = ModelEditor(onnx_model)\\noutput_tensor_name = get_output_names(onnx_model)[0]\\n\\n# output 텐서의 자료형을 int8로 변환\\neditor.convert_output_type(output_tensor_name, TensorType.INT8)\\n\\ngraph = quantize(onnx_model, ranges)\\n\\nwith open(\"trace.json\", \"w\") as trace:\\n    with profile(file=trace) as profiler:\\n        with session.create(graph) as session:\\n            image = torch.randint(256, (1, 3, 224, 224), dtype=torch.uint8)\\n            with profiler.record(\"pre\"):\\n                image = image.numpy()\\n            with profiler.record(\"inf\"):\\n                outputs = session.run(image)\\n            with profiler.record(\"post\"):\\n                prediction = np.argmax(outputs[0].numpy(), axis=1)\\n\\n```\\n\\n\\n\\n### Lower/Unlower Acceleration [\\uf0c1](#lower-unlower-acceleration \"Permalink to this heading\")\\n\\nWarboy internally uses its inherent memory layout to accelerate the computation\\nby leveraging the NPU architecture.\\nFor the memory layout,\\n`Lower`\\noperator reshapes the input tensor to the NPU’s memory layout and\\n`Unlower`\\noperator reshapes the output tensor from the NPU’s memory layout to the original shape.\\nFor them, Furiosa Compiler automatically adds\\n`Lower`\\nand\\n`Unlower`\\noperators to the model.\\n\\nIn many cases,\\n`Lower`\\nand\\n`Unlower`\\nare executed on CPU, causing some overhead\\nof the inference latency.\\nHowever, if the last axis of input or output tensor shape is\\n`width`\\nand\\nthe size of the last axis is a multiple of 32,\\n`Lower`\\nand\\n`Unlower`\\noperators can be accleerated on NPU.\\nThen, the inference latency can be reduced significantly.\\n\\nTherefore, if you are able to specify the shape of the input and output tensors,\\nit’s more optimal to use\\n`NxCxHxW`\\nand specify the width as a multiple of 32.\\nAlso, this optimization can be applied independently to the input and output tensors respectively.\\n\\n\\n### Removal of Pad/Slice [\\uf0c1](#removal-of-pad-slice \"Permalink to this heading\")\\n\\nAs described above, the\\n`Lower`\\n/\\n`Unlower`\\noperations can be accelerated\\nif the last axis of the tensor for either operator is\\nwidth\\n\\nand\\nthe size of the last axis is a multiple of 32.\\n\\nIf the last tensor axis of\\n`Lower`\\nis\\nwidth\\n\\nbut not a multiple of 32,\\nFuriosa Compiler may automatically add\\n`Pad`\\noperator before\\n`Lower`\\noperator\\nto adjust the size of the last axis to a multiple of 32.\\nIn the similar way, Furiosa Compiler may automatically add\\n`Slice`\\noperator after\\n`Unlower`\\noperator to\\nslice data contents from the tensor with the last axis of a multiple of 32 to the original tensor shape.\\n\\nThis optimization gains some performance benefits by accelerating\\n`Lower`\\n/\\n`Unlower`\\noperations.\\nHowever,\\n`Pad`\\nand\\n`Slice`\\nrequires CPU computation.\\nThere’s futher optimization opportunity to remove even\\n`Pad`\\nand\\n`Slice`\\noperators too.\\nIf you can accept the constraints of the input and output tensor shapes,\\nit is strongly recommended using the shape of the tensors\\n`NxCxHxW`\\nand\\na multiple of 32 of the width.\\n\\n\\n### Change the Order of Input Tensor Axes at Compiler Time [\\uf0c1](#change-the-order-of-input-tensor-axes-at-compiler-time \"Permalink to this heading\")\\n\\nAs we discussed above, there are more optimization opportunities\\nif the last axis of the input tensor is\\n`width`\\n.\\nHowever, changing the order of axes requires to modify the models.\\nIt may require some effort to modify the original models in some cases.\\n\\nSo, Furiosa Compiler provides a way to change the order of the input tensor axes\\nat compile time. You can specify\\n`permute_input`\\noption in compiler config\\nto specify the new order of the input tensor axes as follows:\\n\\n* `compiler_config\\n  \\n  =\\n  \\n  {\\n  \\n  \"permute_input\":\\n  \\n  [[0,\\n  \\n  3,\\n  \\n  1,\\n  \\n  2]]\\n  \\n  }`\\n  \\n  > + The parameter of\\n  >   `permute_input`\\n  >   is the same as\\n  >   [torch.permute](https://pytorch.org/docs/stable/generated/torch.permute.html)\\n  >   .\\n  > + For example, the above example code will change\\n  >   `NxHxWxC`\\n  >   to\\n  >   `NxCxHxW`\\n  >   .\\n\\nThe following is a real example code to use\\n`permute_input`\\noption.\\n\\n```\\n#!/usr/bin/env python\\n\\nimport time\\n\\nimport numpy as np\\nimport onnx\\nimport torch\\nimport tqdm\\n\\nfrom furiosa.optimizer import optimize_model\\nfrom furiosa.quantizer import quantize, Calibrator, CalibrationMethod\\nfrom furiosa.runtime import session\\nfrom furiosa.runtime.profiler import profile\\n\\n\\nonnx_model = onnx.load_model(\"model_nhwc.onnx\")\\nonnx_model = optimize_model(onnx_model)\\n\\ncalibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM)\\ncalibrator.collect_data([[torch.randn(1, 512, 512, 3).numpy()]])\\nranges = calibrator.compute_range()\\n\\ngraph = quantize(onnx_model, ranges)\\n\\ncompiler_config = { \"permute_input\": [[0, 3, 1, 2]] }\\n\\nwith open(\"trace.json\", \"w\") as trace:\\n    with profile(file=trace) as profiler:\\n        with session.create(graph, compiler_config=compiler_config) as session:\\n            image = torch.randint(256, (1, 3, 512, 512), dtype=torch.uint8)\\n            with profiler.record(\"pre\"):\\n                image = image.numpy()\\n            with profiler.record(\"inf\"):\\n                outputs = session.run(image)\\n            with profiler.record(\"post\"):\\n                prediction = outputs[0].numpy()\\n\\n```\\n\\nThis is another case to use\\n`permute_input`\\noption.\\nIn some cases, it’s necessary to change the order of the input tensor axes\\nfrom\\n`NxCxHxW`\\nto\\n`NxHxWxC`\\n.\\nPython OpenCV is a popular computer vision library.\\n`cv2.imread()`\\nof OpenCV returns a 3D NumPy array with\\n`HxWxC`\\norder.\\nIf the axes of the input tensors of a model are\\n`NxCxHxW`\\n, it requires to transpose the tensor.\\nThe transpose is a time-consuming operation running in CPU.\\nIn this case, we can remove the transpose operation\\nif we change the order of the input tensor axes of the model to the same as\\nOpenCV’s output; e.g.,\\n`NxHxWxC`\\n. It will reduce the inference latency significantly.\\n\\n\\n### Optimization of Large Input and Output Tensors [\\uf0c1](#optimization-of-large-input-and-output-tensors \"Permalink to this heading\")\\n\\nSome models have large images and as inputs and outputs. For example,\\nDenoising and super resolution models basically take large images as inputs and outputs.\\nDepending on your implementation, those models may be slow in Furiosa SDK and Warboy.\\nFuriosa Compiler optimizes the models with various techniques\\nwhile preserving the semantics of the original models.\\nBasically, Furiosa Compiler handles large tensors as defined by the model.\\nHowever, if the size of tensors is too large, it may exceed SRAM memory of Warboy,\\ncausing more I/O operations between DRAM and SRAM. It may result in poor performance.\\n\\nWe can optimize this case by splitting a large tensor\\ninto a number of smaller tensors and then merging the results.\\nGenerally, we can apply this optimization to denosing and super resolution models\\nbecause the small parts of images can be independently processed and merged to get the final results.\\nThe small parts of images are called patches, and the size of patches is called patch size.\\n\\nTo understand the optimization mechanism, we need to understand how the Furiosa Compiler works.\\nFuriosa Compiler tries to hide IO times between DRAM and SRAM by overlapping\\nthem with NPU executions. In other words, NPU can execute operators while I/O operations are working.\\nIf we split a large tensor into a number of smaller tensors,\\nthe number of I/O operations can be hidden by NPU executions.\\n\\nOnce we decide to use this optimization, the next step is to determine the patch size.\\nHere, one good metric to determine the patch size is the ratio of the time spent on NPU executions.\\nThe smaller the patch size, the more time is spent on NPU computation.\\nIn contrast, the larger the patch size, the more time is spent on I/O operations.\\n\\nAlso, this optimization can be combined with using multiple NPU devices.\\nThe multiple patches can run across multiple NPU devices in parallel.\\n\\n\\n### More Batch, More NPU Utilization [\\uf0c1](#more-batch-more-npu-utilization \"Permalink to this heading\")\\n\\nFor some models with small weights or few layers, the NPU utilization may be low.\\nIn this case, we can increase the batch size to make the NPU utilization higher.\\nWith this optimization, the inference may still have the same latency,\\nbut its throughput can be increased significantly.\\n\\nA batch size can be specified when compiling a model with\\n`--batch-size`\\noption as follows:\\n\\n`furiosa-compiler\\n\\n--batch-size\\n\\n32\\n\\n--target-npu\\n\\nwarboy\\n\\nmnist.dfg\\n\\n-o\\n\\nmnist.enf`\\n\\nA batch size also can be specified when creating a session with\\n`batch_size`\\noption.\\nYou can learn more about the\\n`batch_size`\\noption from\\n[Runner API](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#furiosa.runtime.furiosa.runtime.Runtime.create_runner)\\n.\\n\\n\\n### Single PE vs Fusion PE [\\uf0c1](#single-pe-vs-fusion-pe \"Permalink to this heading\")\\n\\nA single Warboy chip consists of two processing elements (PEs).\\nEach PE of Warboy has its own control unit, and the two PEs can work independently.\\nIn this mode, each PE works with spatially-partitioned memory and processing units.\\nIn contrast, two PEs can also be fused as a single PE.\\nIn this fused mode, two PEs work as a single PE with an unified memory and processing units.\\n\\nThese two modes allow applications to have more flexibility to optimize the performance.\\nFor example, if a model has large weights, we can use a 2PE-fused mode to load the weights\\nfor a lower latency. If a model fits in a single PE, we can use two single PEs separately\\nto run the two model instances for higher throughput.\\n\\nIf a workload is latency-oriented, using a 2PE-fused mode is generally recommended.\\nIf a workload is throughput-oriented, using two single PEs is generally recommended.\\nIt still depends on models and workloads.\\nYou need to find the optimal NPU configuration through experiments.\\n\\nThe followings are example commands to compile a model with a single PE or a fused-PE respectively.\\n\\n* Single PE:\\n  `furiosa-compiler\\n  \\n  --target-npu\\n  \\n  warboy\\n  \\n  resnet50.dfg\\n  \\n  -o\\n  \\n  resnet50.enf`\\n* Fusion PE:\\n  `furiosa-compiler\\n  \\n  --target-npu\\n  \\n  warboy-2pe\\n  \\n  resnet50.dfg\\n  \\n  -o\\n  \\n  resnet50_2pe.enf`\\n\\nThis NPU configuration can also be specified\\nwhen creating a\\n[Runtime](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#furiosa.runtime.Runtime)\\nwith\\n`device`\\noption which are specified by\\n[Device Configuration](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#device-specification)\\n\\n\\n\\nRuntime Optimization\\n[\\uf0c1](#runtime-optimization \"Permalink to this heading\")\\n---------------------------------------------------------------------------\\n\\nSo far, we have discussed the model optimization techniques to reduce the inference latency.\\nAfter we apply the model optimization, we can futher optimize the performance in Runtime level.\\n\\nAs we mentioned above, an end-to-end inference consists of three operations:\\nNPU execution, CPU computation, and IO operation.\\nThree kinds of operations can run independently without blocking one another.\\nThey can be overlapped if we run multiple inferences simultaneously.\\nLeveraging this characteristic is a key idea of the runtime optimization.\\n\\n### More inference concurrency (the number of workers) [\\uf0c1](#more-inference-concurrency-the-number-of-workers \"Permalink to this heading\")\\n\\nWhen we create a session through\\n[Runner API](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#furiosa.runtime.furiosa.runtime.Runtime.create_runner)\\n,\\nwe can specify the number of workers as an option.\\nA single worker is a unit that can run inferences independently sharing NPUs.\\nThis concept is similar to a thread and CPUs.\\n\\nIf there is only one worker, multiple inference requests are processed sequentially through a single worker.\\nWhen one inference is completed, the next inference is processed by the owrker.\\nIn this case, the NPU can be idle while the CPU is working, causing low NPU utilization.\\n\\nHowever, if there are multiple workers, the workers consume requests from the request queue\\nin Runtime. The multiple inferences can be processed simultaneously.\\nIn this case, NPU executions are overlapped with CPU executions, possibly leading to higher NPU utilization.\\n\\nEach worker requires more memory resources to maintain context information for its execution.\\nIf the number of workers is too large, the memory resources may be exhausted.\\nIf the number of workers is too small, the NPU utilization may be low.\\nFinding the optimal number of workers is important to maximize the performance of the model.\\nUsually, we can find the optimal number of workers through experimentation.\\n\\n\\n### Sync API vs Async APIs [\\uf0c1](#sync-api-vs-async-apis \"Permalink to this heading\")\\n\\nThere are two types of runtime APIs: Sync API and Async API.\\nSync API is a blocking API that waits for the completion of the inference.\\nAsync APIs are non-blocking APIs that don’t wait for the completion of the inference.\\nAsync APIs allow to request multiple inferences simultaneously and wait for the results asynchronously.\\n\\n`furiosa.session.create()`\\na creates a syncronous session.\\nAs the below example,\\n`session.run()`\\nis blocked until the result is returned.\\nIt generally is enough for batch workloads with large batch sizes,\\nbut it’s not sufficient for serving workloads that handle multiple current requests simultaneously.\\n\\n```\\nfrom furiosa.runtime import session\\n\\nwith session.create(model) as sess:\\n    input = ...\\n    outputs = sess.run(input) # Wait for completion\\n    ...\\n\\n```\\n\\nTo overcome this limitation, Furiosa SDK provides two types of Async APIs: Queue API and Async/Await API.\\nThey allow to request multiple inferences simultaneously and wait for the results asynchronously.\\nThey are also useful to hide I/O and CPU computation by overlapping them with NPU executions.\\n\\n#### Queue API [\\uf0c1](#queue-api \"Permalink to this heading\")\\n\\n`create_async()`\\ncreates a pair of a submitter and a queue.\\nWith both, we can submit inference requests without waiting for completion and\\nwait for the inference results asynchronously.\\n\\n```\\nimport numpy as np\\nimport random\\n\\nfrom furiosa.runtime import session\\n\\nsubmitter, queue = session.create_async(\"mnist.onnx\",\\n                                        worker_num=2,\\n                                        # Determine how many asynchronous requests you can submit\\n                                        # without blocking.\\n                                        input_queue_size=100,\\n                                        output_queue_size=100)\\n\\nfor i in range(0, 5):\\n    idx = random.randint(0, 59999)\\n    input = np.random.rand(1, 1, 28, 28).astype(np.float32)\\n    submitter.submit(input, context=idx) # non blocking call\\n\\nfor i in range(0, 5):\\n    context, outputs = queue.recv(100) # 100 ms for timeout. If None, queue.recv() will be blocking.\\n    print(outputs[0].numpy())\\n\\nif queue:\\n    queue.close()\\nif submitter:\\n    submitter.close()\\n\\n```\\n\\n\\n\\n#### Using Async/Await syntax [\\uf0c1](#using-async-await-syntax \"Permalink to this heading\")\\n\\nIn the the example below,\\n`NPUModel`\\nof furiosa-server provide an easier way to implement\\na serving application using async/await API.\\n\\n```\\nimport asyncio\\nimport numpy as np\\n\\nfrom furiosa.server.model import NPUModel, NPUModelConfig\\n\\nclass SimpleApplication:\\n    def __init__(self):\\n        self.model = NPUModel(\\n            NPUModelConfig(\\n                name=\"MNIST\",\\n                model=\"mnist.onnx\",\\n            )\\n        )\\n\\n    async def load(self):\\n        await self.model.load()\\n\\n    async def process(self, image):\\n        input = self.preprocess(image)\\n        tensor = await self.model.predict(input)\\n        output = self.postprocess(tensor)\\n        return output\\n\\n    def preprocess(self, image):\\n        # do preprocess\\n        return image\\n\\n    def postprocess(self, tensor):\\n        # do postprocess\\n        return tensor\\n\\n\\nAPP = SimpleApplication()\\n\\nasync def startup():\\n    await APP.load()\\n\\nasync def run(image):\\n    result = await APP.process(image)\\n    return result\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(startup())\\n\\n    image = np.random.rand(1, 1, 28, 28).astype(np.float32)\\n    asyncio.run(run(image))\\n\\n```\\n\\n\\n\\n\\n\\n\\n\\n\\n[Previous](quantization.html \"Model Quantization\")\\n[Next](profiler.html \"Performance Profiling\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Performance Optimization * [View page source](../_sources/software/performance.rst.txt)\\n---\\nPerformance Optimization [\\uf0c1](#performance-optimization \"Permalink to this heading\") ===================================================================================\\nTo ensure efficient inference serving in production, it’s essential to focus on throughput and latency as key metrics. Furiosa SDK offers two optimization methods for both throughput and latency:\\n* **Model Optimization**   : are ways to optimize models during the phases of model development,   quantization, and compilation. Some optimization techniques may modify the models, leading to   more efficient compiled programs. * **Runtime Optimization**   : are ways to optimize the runtime execution of compiled programs.   They are about how to optimize inference codes through Runtime library depending   on the characteristics of models workloads for higher throughput.\\nIn this section, we will discuss the performance metrics and how to optimize them in both above ways.\\nPerformance Metrics: Latency and Throughput [\\uf0c1](#performance-metrics-latency-and-throughput \"Permalink to this heading\") ------------------------------------------------------------------------------------------------------------------------\\n*Latency* is one of the major performance evaluation criteria for model inference. it’s a measure of how long a single inference takes from when the input data is passed to the model until the output value is received. With low latency, users can experience high responsiveness.\\nAnother performance evaluation criterion is throughput. Throughput means the number of inferences that can be processed within a unit of time. Throughput implies that how many requests a system handle simultaneously.\\nA single end-to-end inference consists of three kinds of operations: NPU execution, CPU computation and IO operation between host and NPU device. Three kinds of operations run independently without blocking one another. So, multiple inferences can run simultaneously while different operations run. When we continue to run multiple requests simultaneously, the longer operation among NPU, CPU, and IO operations is likely to determine the inference time. This is because the shorter operations will be hidden by other longer operations. This is a key characteristic to understand how to optimize the performance of a model. You can find more details at [Concurrency Optimization](#concurrencyoptimization) .\\nNPU utilization is not a performance metrics, but it’s one of the key metrics to indicate how much the model utilizes a NPU device for inferences. NPU utilization can be defined as the proportion of time the NPU is used during inference. With NPU utilziation, we can evaluate how well the model is well-optimized for NPU acceleration. Sometimes, it may also imply the room for further optimization opportunities. Please refer to [Toolkit](cli.html#toolkit) for how to measure NPU utilization.\\n### Performance Profiling [\\uf0c1](#performance-profiling \"Permalink to this heading\")\\nTo analyze the performance of a workload, we need to measure performance metrics as well as we need to have a closer look at the times of NPU executions, CPU computations, and I/O operations.\\nFor them, there are two useful tools in Furiosa SDK.\\n* [furiosa-bench (Benchmark Tool)](cli.html#furiosabench)   is a tool to measure the performance metrics such as latencies and   throughput (i.e., QPS - queries per second). * [Performance Profiling](profiler.html#profiling)   provides a way to measure the durations of NPU executions and other operations.  `furiosa-bench` also provides `--trace-output` option to generate a trace file. This is another easy way to measure the durations of NPU executions and other operations from a running workload.\\nModel Optimization [\\uf0c1](#model-optimization \"Permalink to this heading\") -----------------------------------------------------------------------\\nIn this seciton, we introduce some model optimization techniques to improve the performance of models. They key idea of the model optimization is to identify the bottleneck parts (usually operators) of the model and to reduce the times of the bottleneck parts or remove them.\\nFor example, if some operators of a model are not accelerated by NPU, they can be a major bottleneck. If you remove the operators or replace them with other equivalents, the inference latency can be reduced significantly.\\n### Optimizing `Quantize` Operator [\\uf0c1](#optimizing-quantize-operator \"Permalink to this heading\")\\nFuriosaAI’s first-generation NPU, Warboy, supports only int8 type. As the majority of deep learning models are built upon floating point types like fp32 and fp16, to execute these models on Warboy, a quantization step is necessary to convert the fp32 weights to int8 model weights. In addition, the quantization step adds `quantize` , `dequantize` operators to the input and output parts of the model respectively. `quantize` and `dequantize` operators convert fp32 input values to int8 values and vice versa. Those operators are executed on the CPU and are time-consuming.\\nInputs of many CNN-based models are images. In particular, an image is represented as RGB channels. In other words, a single image is composed of three images for each channel of RGB, where each image is represented with 8-bit integer values, ranging from 0 to 255.\\nTo feed an image to a model, we need to convert the `int8` values of each RGB channel to `fp32` values, and `quantize` operator in the model converts `fp32` values to `int8` values. It’s unnecessary if we can feed RGB images in `int8` to a model directly.\\nTo support this optimization, `furiosa-quantizer` provides the `ModelEditor` API. `ModelEditor` takes the model optimized by `optimize_model()` .\\n``` model = onnx.load_model(\"yolox_l.onnx\") model = optimize_model(model)\\neditor = ModelEditor(model)\\n```  `convert_input_type()` method of `ModelEditor` takes a tensor name and a data type as arguments. It modifies the data type of the input tensor in the model to be the given arguments. The target type can be either `INT8` or `UINT8` . You can find the tensor name through `get_pure_input_names` method.\\n``` input_tensor_name = get_pure_input_names(model)[0]\\n# Convert this input tensor to uint8 editor.convert_input_type(input_tensor_name, TensorType.UINT8)\\n```\\nAs you can see in the above example, `convert_input_type` changes the data type of the input tensor to be `uint8` . The reason why we use `uint8` instead of `int8` is that the pixel values are represented as positive values.\\nBefore this model modification, `quantize` operator converts `float32` values to `int8` values. After this model modification, the quantize operator converts `uint8` values to `int8` values. This conversion from `uint8` to `int8` is much faster than the conversion from `float32` to `int8` . The followings are the the benchmark results of before and after the model modification. Also, the figure shows how the `quantize` operator is changed.\\nQuantization in YOLOX\\\\_L\\n[\\uf0c1](#id2 \"Permalink to this table\")\\n| Input type | `Quantize` execution time | | --- | --- | | float32 | 60.639 ms | | uint8 | 0.277 ms |\\nquantize without `ModelEditor`  [\\uf0c1](#id3 \"Permalink to this image\")\\nquantize with `convert_input_type`  [\\uf0c1](#id4 \"Permalink to this image\")\\nWarning\\nThis optmization may affect the accurarcy of the model. Since it depends on models and applications, it is recommended to validate the accuracy of the model.\\nThe following is a real example code to use `ModelEditor` API with `convert_input_type()` .\\n``` #!/usr/bin/env python\\nimport time import numpy as np import onnx import torch import torchvision from torchvision import transforms import tqdm\\nfrom furiosa.optimizer import optimize_model from furiosa.quantizer import get_pure_input_names, quantize, Calibrator, CalibrationMethod, ModelEditor, TensorType from furiosa.runtime import session from furiosa.runtime.profiler import profile\\ntorch_model = torchvision.models.resnet50(weights=\\'DEFAULT\\') torch_model = torch_model.eval()\\ndummy_input = (torch.randn(1, 3, 224, 224),)\\ntorch.onnx.export(     torch_model,  # PyTorch model to export     dummy_input,  # model input     \"resnet50.onnx\",  # where to save the exported ONNX model     opset_version=13,  # the ONNX OpSet version to export the model to     do_constant_folding=True,  # whether to execute constant folding for optimization     input_names=[\"input\"],  # the ONNX model\\'s input names     output_names=[\"output\"],  # the ONNX model\\'s output names )\\nonnx_model = onnx.load_model(\"resnet50.onnx\") onnx_model = optimize_model(onnx_model)\\ncalibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM) calibrator.collect_data([[torch.randn(1, 3, 224, 224).numpy()]]) ranges = calibrator.compute_range()\\neditor = ModelEditor(onnx_model) input_tensor_name = get_pure_input_names(onnx_model)[0]\\n# Convert the input type to uint8 editor.convert_input_type(input_tensor_name, TensorType.UINT8)\\ngraph = quantize(onnx_model, ranges)\\nwith open(\"trace.json\", \"w\") as trace:     with profile(file=trace) as profiler:         with session.create(graph) as session:             image = torch.randint(256, (1, 3, 224, 224), dtype=torch.uint8)             with profiler.record(\"pre\"):                 image = image.numpy()             with profiler.record(\"inf\"):                 outputs = session.run(image)             with profiler.record(\"post\"):                 prediction = np.argmax(outputs[0].numpy(), axis=1)\\n```\\n### Optimizing `Dequantize` Operator [\\uf0c1](#optimizing-dequantize-operator \"Permalink to this heading\")\\nSimilar to the above `Quantize` operator optimization, `Dequantize` operator also can be optimized in the similar way.\\nIf the model output tensor is `fp32` , the output of `int8` values must be converted to f32 values. `Dequantize` operator converts int8 values to fp32 values, and it’s executed on CPU. If the model output is an RGB image or something else which can be represented as `int8` or `uint8` values, we can skip converting `int8` or `uint8` to `fp32` . It will reduce the inference latency significantly.\\nWe can enable this optimization by using `convert_output_type()` method of `ModelEditor` . `convert_output_type()` method can modifies a model output by a given tensor name and a target data type. The target type can be either `INT8` or `UINT8` .\\nquantize with `convert_output_type`  [\\uf0c1](#id5 \"Permalink to this image\")\\nquantize with `convert_input_type` and `convert_output_type`  [\\uf0c1](#id6 \"Permalink to this image\")\\nNote\\nFuriosa Compiler may automatically apply this optimization to the model even if this optmization is not explicitly applied. In that case, the optimization by Furiosa Compiler may result in lower latency than the one manually applied by `ModelEditor` . It is recommended to do experiments to find the best option.\\nWarning\\nThis optmization may affect the accurarcy of the model. Since it depends on models and applications, it is recommended to validate the accuracy of the model.\\nThe following is an real example code to use `convert_output_type` option.\\n``` #!/usr/bin/env python\\nimport time import numpy as np import onnx import torch import torchvision from torchvision import transforms import tqdm\\nfrom furiosa.optimizer import optimize_model from furiosa.quantizer import get_output_names, quantize, Calibrator, CalibrationMethod, ModelEditor, TensorType from furiosa.runtime import session from furiosa.runtime.profiler import profile\\ntorch_model = torchvision.models.resnet50(weights=\\'DEFAULT\\') torch_model = torch_model.eval()\\ndummy_input = (torch.randn(1, 3, 224, 224),)\\ntorch.onnx.export(     torch_model,  # PyTorch model to export     dummy_input,  # model input     \"resnet50.onnx\",  # where to save the exported ONNX model     opset_version=13,  # the ONNX OpSet version to export the model to     do_constant_folding=True,  # whether to execute constant folding for optimization     input_names=[\"input\"],  # the ONNX model\\'s input names     output_names=[\"output\"],  # the ONNX model\\'s output names )\\nonnx_model = onnx.load_model(\"resnet50.onnx\") onnx_model = optimize_model(onnx_model)\\ncalibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM) calibrator.collect_data([[torch.randn(1, 3, 224, 224).numpy()]]) ranges = calibrator.compute_range()\\neditor = ModelEditor(onnx_model) output_tensor_name = get_output_names(onnx_model)[0]\\n# output 텐서의 자료형을 int8로 변환 editor.convert_output_type(output_tensor_name, TensorType.INT8)\\ngraph = quantize(onnx_model, ranges)\\nwith open(\"trace.json\", \"w\") as trace:     with profile(file=trace) as profiler:         with session.create(graph) as session:             image = torch.randint(256, (1, 3, 224, 224), dtype=torch.uint8)             with profiler.record(\"pre\"):                 image = image.numpy()             with profiler.record(\"inf\"):                 outputs = session.run(image)             with profiler.record(\"post\"):                 prediction = np.argmax(outputs[0].numpy(), axis=1)\\n```\\n### Lower/Unlower Acceleration [\\uf0c1](#lower-unlower-acceleration \"Permalink to this heading\")\\nWarboy internally uses its inherent memory layout to accelerate the computation by leveraging the NPU architecture. For the memory layout, `Lower` operator reshapes the input tensor to the NPU’s memory layout and `Unlower` operator reshapes the output tensor from the NPU’s memory layout to the original shape. For them, Furiosa Compiler automatically adds `Lower` and `Unlower` operators to the model.\\nIn many cases, `Lower` and `Unlower` are executed on CPU, causing some overhead of the inference latency. However, if the last axis of input or output tensor shape is `width` and the size of the last axis is a multiple of 32, `Lower` and `Unlower` operators can be accleerated on NPU. Then, the inference latency can be reduced significantly.\\nTherefore, if you are able to specify the shape of the input and output tensors, it’s more optimal to use `NxCxHxW` and specify the width as a multiple of 32. Also, this optimization can be applied independently to the input and output tensors respectively.\\n### Removal of Pad/Slice [\\uf0c1](#removal-of-pad-slice \"Permalink to this heading\")\\nAs described above, the `Lower` / `Unlower` operations can be accelerated if the last axis of the tensor for either operator is width\\nand the size of the last axis is a multiple of 32.\\nIf the last tensor axis of `Lower` is width\\nbut not a multiple of 32, Furiosa Compiler may automatically add `Pad` operator before `Lower` operator to adjust the size of the last axis to a multiple of 32. In the similar way, Furiosa Compiler may automatically add `Slice` operator after `Unlower` operator to slice data contents from the tensor with the last axis of a multiple of 32 to the original tensor shape.\\nThis optimization gains some performance benefits by accelerating `Lower` / `Unlower` operations. However, `Pad` and `Slice` requires CPU computation. There’s futher optimization opportunity to remove even `Pad` and `Slice` operators too. If you can accept the constraints of the input and output tensor shapes, it is strongly recommended using the shape of the tensors `NxCxHxW` and a multiple of 32 of the width.\\n### Change the Order of Input Tensor Axes at Compiler Time [\\uf0c1](#change-the-order-of-input-tensor-axes-at-compiler-time \"Permalink to this heading\")\\nAs we discussed above, there are more optimization opportunities if the last axis of the input tensor is `width` . However, changing the order of axes requires to modify the models. It may require some effort to modify the original models in some cases.\\nSo, Furiosa Compiler provides a way to change the order of the input tensor axes at compile time. You can specify `permute_input` option in compiler config to specify the new order of the input tensor axes as follows:\\n* `compiler_config      =      {      \"permute_input\":      [[0,      3,      1,      2]]      }`      > + The parameter of   >   `permute_input`   >   is the same as   >   [torch.permute](https://pytorch.org/docs/stable/generated/torch.permute.html)   >   .   > + For example, the above example code will change   >   `NxHxWxC`   >   to   >   `NxCxHxW`   >   .\\nThe following is a real example code to use `permute_input` option.\\n``` #!/usr/bin/env python\\nimport time\\nimport numpy as np import onnx import torch import tqdm\\nfrom furiosa.optimizer import optimize_model from furiosa.quantizer import quantize, Calibrator, CalibrationMethod from furiosa.runtime import session from furiosa.runtime.profiler import profile\\nonnx_model = onnx.load_model(\"model_nhwc.onnx\") onnx_model = optimize_model(onnx_model)\\ncalibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM) calibrator.collect_data([[torch.randn(1, 512, 512, 3).numpy()]]) ranges = calibrator.compute_range()\\ngraph = quantize(onnx_model, ranges)\\ncompiler_config = { \"permute_input\": [[0, 3, 1, 2]] }\\nwith open(\"trace.json\", \"w\") as trace:     with profile(file=trace) as profiler:         with session.create(graph, compiler_config=compiler_config) as session:             image = torch.randint(256, (1, 3, 512, 512), dtype=torch.uint8)             with profiler.record(\"pre\"):                 image = image.numpy()             with profiler.record(\"inf\"):                 outputs = session.run(image)             with profiler.record(\"post\"):                 prediction = outputs[0].numpy()\\n```\\nThis is another case to use `permute_input` option. In some cases, it’s necessary to change the order of the input tensor axes from `NxCxHxW` to `NxHxWxC` . Python OpenCV is a popular computer vision library. `cv2.imread()` of OpenCV returns a 3D NumPy array with `HxWxC` order. If the axes of the input tensors of a model are `NxCxHxW` , it requires to transpose the tensor. The transpose is a time-consuming operation running in CPU. In this case, we can remove the transpose operation if we change the order of the input tensor axes of the model to the same as OpenCV’s output; e.g., `NxHxWxC` . It will reduce the inference latency significantly.\\n### Optimization of Large Input and Output Tensors [\\uf0c1](#optimization-of-large-input-and-output-tensors \"Permalink to this heading\")\\nSome models have large images and as inputs and outputs. For example, Denoising and super resolution models basically take large images as inputs and outputs. Depending on your implementation, those models may be slow in Furiosa SDK and Warboy. Furiosa Compiler optimizes the models with various techniques while preserving the semantics of the original models. Basically, Furiosa Compiler handles large tensors as defined by the model. However, if the size of tensors is too large, it may exceed SRAM memory of Warboy, causing more I/O operations between DRAM and SRAM. It may result in poor performance.\\nWe can optimize this case by splitting a large tensor into a number of smaller tensors and then merging the results. Generally, we can apply this optimization to denosing and super resolution models because the small parts of images can be independently processed and merged to get the final results. The small parts of images are called patches, and the size of patches is called patch size.\\nTo understand the optimization mechanism, we need to understand how the Furiosa Compiler works. Furiosa Compiler tries to hide IO times between DRAM and SRAM by overlapping them with NPU executions. In other words, NPU can execute operators while I/O operations are working. If we split a large tensor into a number of smaller tensors, the number of I/O operations can be hidden by NPU executions.\\nOnce we decide to use this optimization, the next step is to determine the patch size. Here, one good metric to determine the patch size is the ratio of the time spent on NPU executions. The smaller the patch size, the more time is spent on NPU computation. In contrast, the larger the patch size, the more time is spent on I/O operations.\\nAlso, this optimization can be combined with using multiple NPU devices. The multiple patches can run across multiple NPU devices in parallel.\\n### More Batch, More NPU Utilization [\\uf0c1](#more-batch-more-npu-utilization \"Permalink to this heading\")\\nFor some models with small weights or few layers, the NPU utilization may be low. In this case, we can increase the batch size to make the NPU utilization higher. With this optimization, the inference may still have the same latency, but its throughput can be increased significantly.\\nA batch size can be specified when compiling a model with `--batch-size` option as follows:\\n`furiosa-compiler\\n--batch-size\\n32\\n--target-npu\\nwarboy\\nmnist.dfg\\n-o\\nmnist.enf`\\nA batch size also can be specified when creating a session with `batch_size` option. You can learn more about the `batch_size` option from [Runner API](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#furiosa.runtime.furiosa.runtime.Runtime.create_runner) .\\n### Single PE vs Fusion PE [\\uf0c1](#single-pe-vs-fusion-pe \"Permalink to this heading\")\\nA single Warboy chip consists of two processing elements (PEs). Each PE of Warboy has its own control unit, and the two PEs can work independently. In this mode, each PE works with spatially-partitioned memory and processing units. In contrast, two PEs can also be fused as a single PE. In this fused mode, two PEs work as a single PE with an unified memory and processing units.\\nThese two modes allow applications to have more flexibility to optimize the performance. For example, if a model has large weights, we can use a 2PE-fused mode to load the weights for a lower latency. If a model fits in a single PE, we can use two single PEs separately to run the two model instances for higher throughput.\\nIf a workload is latency-oriented, using a 2PE-fused mode is generally recommended. If a workload is throughput-oriented, using two single PEs is generally recommended. It still depends on models and workloads. You need to find the optimal NPU configuration through experiments.\\nThe followings are example commands to compile a model with a single PE or a fused-PE respectively.\\n* Single PE:   `furiosa-compiler      --target-npu      warboy      resnet50.dfg      -o      resnet50.enf` * Fusion PE:   `furiosa-compiler      --target-npu      warboy-2pe      resnet50.dfg      -o      resnet50_2pe.enf`\\nThis NPU configuration can also be specified when creating a [Runtime](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#furiosa.runtime.Runtime) with `device` option which are specified by [Device Configuration](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#device-specification)\\nRuntime Optimization [\\uf0c1](#runtime-optimization \"Permalink to this heading\") ---------------------------------------------------------------------------\\nSo far, we have discussed the model optimization techniques to reduce the inference latency. After we apply the model optimization, we can futher optimize the performance in Runtime level.\\nAs we mentioned above, an end-to-end inference consists of three operations: NPU execution, CPU computation, and IO operation. Three kinds of operations can run independently without blocking one another. They can be overlapped if we run multiple inferences simultaneously. Leveraging this characteristic is a key idea of the runtime optimization.\\n### More inference concurrency (the number of workers) [\\uf0c1](#more-inference-concurrency-the-number-of-workers \"Permalink to this heading\")\\nWhen we create a session through [Runner API](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#furiosa.runtime.furiosa.runtime.Runtime.create_runner) ,\\nwe can specify the number of workers as an option. A single worker is a unit that can run inferences independently sharing NPUs. This concept is similar to a thread and CPUs.\\nIf there is only one worker, multiple inference requests are processed sequentially through a single worker. When one inference is completed, the next inference is processed by the owrker. In this case, the NPU can be idle while the CPU is working, causing low NPU utilization.\\nHowever, if there are multiple workers, the workers consume requests from the request queue in Runtime. The multiple inferences can be processed simultaneously. In this case, NPU executions are overlapped with CPU executions, possibly leading to higher NPU utilization.\\nEach worker requires more memory resources to maintain context information for its execution. If the number of workers is too large, the memory resources may be exhausted. If the number of workers is too small, the NPU utilization may be low. Finding the optimal number of workers is important to maximize the performance of the model. Usually, we can find the optimal number of workers through experimentation.\\n### Sync API vs Async APIs [\\uf0c1](#sync-api-vs-async-apis \"Permalink to this heading\")\\nThere are two types of runtime APIs: Sync API and Async API. Sync API is a blocking API that waits for the completion of the inference. Async APIs are non-blocking APIs that don’t wait for the completion of the inference. Async APIs allow to request multiple inferences simultaneously and wait for the results asynchronously.  `furiosa.session.create()` a creates a syncronous session. As the below example, `session.run()` is blocked until the result is returned. It generally is enough for batch workloads with large batch sizes, but it’s not sufficient for serving workloads that handle multiple current requests simultaneously.\\n``` from furiosa.runtime import session\\nwith session.create(model) as sess:     input = ...     outputs = sess.run(input) # Wait for completion     ...\\n```\\nTo overcome this limitation, Furiosa SDK provides two types of Async APIs: Queue API and Async/Await API. They allow to request multiple inferences simultaneously and wait for the results asynchronously. They are also useful to hide I/O and CPU computation by overlapping them with NPU executions.\\n#### Queue API [\\uf0c1](#queue-api \"Permalink to this heading\")  `create_async()` creates a pair of a submitter and a queue. With both, we can submit inference requests without waiting for completion and wait for the inference results asynchronously.\\n``` import numpy as np import random\\nfrom furiosa.runtime import session\\nsubmitter, queue = session.create_async(\"mnist.onnx\",                                         worker_num=2,                                         # Determine how many asynchronous requests you can submit                                         # without blocking.                                         input_queue_size=100,                                         output_queue_size=100)\\nfor i in range(0, 5):     idx = random.randint(0, 59999)     input = np.random.rand(1, 1, 28, 28).astype(np.float32)     submitter.submit(input, context=idx) # non blocking call\\nfor i in range(0, 5):     context, outputs = queue.recv(100) # 100 ms for timeout. If None, queue.recv() will be blocking.     print(outputs[0].numpy())\\nif queue:     queue.close() if submitter:     submitter.close()\\n```\\n#### Using Async/Await syntax [\\uf0c1](#using-async-await-syntax \"Permalink to this heading\")\\nIn the the example below, `NPUModel` of furiosa-server provide an easier way to implement a serving application using async/await API.\\n``` import asyncio import numpy as np\\nfrom furiosa.server.model import NPUModel, NPUModelConfig\\nclass SimpleApplication:     def __init__(self):         self.model = NPUModel(             NPUModelConfig(                 name=\"MNIST\",                 model=\"mnist.onnx\",             )         )\\n    async def load(self):         await self.model.load()\\n    async def process(self, image):         input = self.preprocess(image)         tensor = await self.model.predict(input)         output = self.postprocess(tensor)         return output\\n    def preprocess(self, image):         # do preprocess         return image\\n    def postprocess(self, tensor):         # do postprocess         return tensor\\nAPP = SimpleApplication()\\nasync def startup():     await APP.load()\\nasync def run(image):     result = await APP.process(image)     return result\\nif __name__ == \"__main__\":     asyncio.run(startup())\\n    image = np.random.rand(1, 1, 28, 28).astype(np.float32)     asyncio.run(run(image))\\n```\\n[Previous](quantization.html \"Model Quantization\") [Next](profiler.html \"Performance Profiling\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Performance Optimization\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/performance.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"performance-optimization\">\\n     <span id=\"performanceoptimization\">\\n     </span>\\n     <h1>\\n      Performance Optimization\\n      <a class=\"headerlink\" href=\"#performance-optimization\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      To ensure efficient inference serving in production,\\nit’s essential to focus on throughput and latency as key metrics.\\nFuriosa SDK offers two optimization methods for both throughput and latency:\\n     </p>\\n     <ul class=\"simple\">\\n      <li>\\n       <p>\\n        <strong>\\n         Model Optimization\\n        </strong>\\n        : are ways to optimize models during the phases of model development,\\nquantization, and compilation. Some optimization techniques may modify the models, leading to\\nmore efficient compiled programs.\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        <strong>\\n         Runtime Optimization\\n        </strong>\\n        : are ways to optimize the runtime execution of compiled programs.\\nThey are about how to optimize inference codes through Runtime library depending\\non the characteristics of models workloads for higher throughput.\\n       </p>\\n      </li>\\n     </ul>\\n     <p>\\n      In this section, we will discuss the performance metrics and how to optimize them\\nin both above ways.\\n     </p>\\n     <section id=\"performance-metrics-latency-and-throughput\">\\n      <h2>\\n       Performance Metrics: Latency and Throughput\\n       <a class=\"headerlink\" href=\"#performance-metrics-latency-and-throughput\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       <em>\\n        Latency\\n       </em>\\n       is one of the major performance evaluation criteria for model inference.\\nit’s a measure of how long a single inference takes\\nfrom when the input data is passed to the model until the output value is received.\\nWith low latency, users can experience high responsiveness.\\n      </p>\\n      <p>\\n       Another performance evaluation criterion is throughput.\\nThroughput means the number of inferences that can be processed within a unit of time.\\nThroughput implies that how many requests a system handle simultaneously.\\n      </p>\\n      <p>\\n       A single end-to-end inference consists of three kinds of operations: NPU execution, CPU computation\\nand IO operation between host and NPU device. Three kinds of operations run independently without blocking one another.\\nSo, multiple inferences can run simultaneously while different operations run.\\nWhen we continue to run multiple requests simultaneously,\\nthe longer operation among NPU, CPU, and IO operations is likely to determine the inference time.\\nThis is because the shorter operations will be hidden by other longer operations.\\nThis is a key characteristic to understand how to optimize the performance of a model.\\nYou can find more details at\\n       <a class=\"reference internal\" href=\"#concurrencyoptimization\">\\n        <span class=\"std std-ref\">\\n         Concurrency Optimization\\n        </span>\\n       </a>\\n       .\\n      </p>\\n      <p>\\n       NPU utilization is not a performance metrics, but it’s one of the key metrics to indicate\\nhow much the model utilizes a NPU device for inferences.\\nNPU utilization can be defined as the proportion of time the NPU is used during inference.\\nWith NPU utilziation, we can evaluate how well the model is well-optimized for NPU acceleration.\\nSometimes, it may also imply the room for further optimization opportunities.\\nPlease refer to\\n       <a class=\"reference internal\" href=\"cli.html#toolkit\">\\n        <span class=\"std std-ref\">\\n         Toolkit\\n        </span>\\n       </a>\\n       for how to measure NPU utilization.\\n      </p>\\n      <section id=\"performance-profiling\">\\n       <h3>\\n        Performance Profiling\\n        <a class=\"headerlink\" href=\"#performance-profiling\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        To analyze the performance of a workload, we need to measure performance metrics as well as\\nwe need to have a closer look at the times of NPU executions, CPU computations, and I/O operations.\\n       </p>\\n       <p>\\n        For them, there are two useful tools in Furiosa SDK.\\n       </p>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          <a class=\"reference internal\" href=\"cli.html#furiosabench\">\\n           <span class=\"std std-ref\">\\n            furiosa-bench (Benchmark Tool)\\n           </span>\\n          </a>\\n          is a tool to measure the performance metrics such as latencies and\\nthroughput (i.e., QPS - queries per second).\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          <a class=\"reference internal\" href=\"profiler.html#profiling\">\\n           <span class=\"std std-ref\">\\n            Performance Profiling\\n           </span>\\n          </a>\\n          provides a way to measure the durations of NPU executions and other operations.\\n         </p>\\n        </li>\\n       </ul>\\n       <p>\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-bench\\n         </span>\\n        </code>\\n        also provides\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          --trace-output\\n         </span>\\n        </code>\\n        option to generate a trace file.\\nThis is another easy way to measure the durations of NPU executions and other operations\\nfrom a running workload.\\n       </p>\\n      </section>\\n     </section>\\n     <section id=\"model-optimization\">\\n      <h2>\\n       Model Optimization\\n       <a class=\"headerlink\" href=\"#model-optimization\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       In this seciton, we introduce some model optimization techniques to improve the performance\\nof models. They key idea of the model optimization is to identify the bottleneck parts\\n(usually operators) of the model and to reduce the times of the bottleneck parts or remove them.\\n      </p>\\n      <p>\\n       For example, if some operators of a model are not accelerated by NPU,\\nthey can be a major bottleneck. If you remove the operators or replace them with other equivalents,\\nthe inference latency can be reduced significantly.\\n      </p>\\n      <section id=\"optimizing-quantize-operator\">\\n       <h3>\\n        Optimizing\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Quantize\\n         </span>\\n        </code>\\n        Operator\\n        <a class=\"headerlink\" href=\"#optimizing-quantize-operator\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        FuriosaAI’s first-generation NPU, Warboy, supports only int8 type.\\nAs the majority of deep learning models are built upon floating point types like fp32 and fp16,\\nto execute these models on Warboy,\\na quantization step is necessary to convert the fp32 weights to int8 model weights.\\nIn addition, the quantization step adds\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          quantize\\n         </span>\\n        </code>\\n        ,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          dequantize\\n         </span>\\n        </code>\\n        operators to the\\ninput and output parts of the model respectively.\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          quantize\\n         </span>\\n        </code>\\n        and\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          dequantize\\n         </span>\\n        </code>\\n        operators convert fp32 input values to int8 values and vice versa.\\nThose operators are executed on the CPU and are time-consuming.\\n       </p>\\n       <p>\\n        Inputs of many CNN-based models are images. In particular, an image is represented as\\nRGB channels. In other words, a single image is composed of three images for each channel of RGB,\\nwhere each image is represented with 8-bit integer values, ranging from 0 to 255.\\n       </p>\\n       <p>\\n        To feed an image to a model, we need to convert the\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          int8\\n         </span>\\n        </code>\\n        values of each RGB channel\\nto\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          fp32\\n         </span>\\n        </code>\\n        values, and\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          quantize\\n         </span>\\n        </code>\\n        operator in the model converts\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          fp32\\n         </span>\\n        </code>\\n        values\\nto\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          int8\\n         </span>\\n        </code>\\n        values. It’s unnecessary if we can feed RGB images in\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          int8\\n         </span>\\n        </code>\\n        to a model directly.\\n       </p>\\n       <p>\\n        To support this optimization,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-quantizer\\n         </span>\\n        </code>\\n        provides the\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          ModelEditor\\n         </span>\\n        </code>\\n        API.\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          ModelEditor\\n         </span>\\n        </code>\\n        takes the model optimized by\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          optimize_model()\\n         </span>\\n        </code>\\n        .\\n       </p>\\n       <div class=\"highlight-python notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">onnx</span><span class=\"o\">.</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s2\">\"yolox_l.onnx\"</span><span class=\"p\">)</span>\\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">optimize_model</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\\n\\n<span class=\"n\">editor</span> <span class=\"o\">=</span> <span class=\"n\">ModelEditor</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          convert_input_type()\\n         </span>\\n        </code>\\n        method of\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          ModelEditor\\n         </span>\\n        </code>\\n        takes a tensor name and a data type as arguments.\\nIt modifies the data type of the input tensor in the model to be the given arguments.\\nThe target type can be either\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          INT8\\n         </span>\\n        </code>\\n        or\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          UINT8\\n         </span>\\n        </code>\\n        . You can find the tensor name\\nthrough\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          get_pure_input_names\\n         </span>\\n        </code>\\n        method.\\n       </p>\\n       <div class=\"highlight-python notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"n\">input_tensor_name</span> <span class=\"o\">=</span> <span class=\"n\">get_pure_input_names</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\\n\\n<span class=\"c1\"># Convert this input tensor to uint8</span>\\n<span class=\"n\">editor</span><span class=\"o\">.</span><span class=\"n\">convert_input_type</span><span class=\"p\">(</span><span class=\"n\">input_tensor_name</span><span class=\"p\">,</span> <span class=\"n\">TensorType</span><span class=\"o\">.</span><span class=\"n\">UINT8</span><span class=\"p\">)</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        As you can see in the above example,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          convert_input_type\\n         </span>\\n        </code>\\n        changes the data type of the input tensor to be\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          uint8\\n         </span>\\n        </code>\\n        .\\nThe reason why we use\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          uint8\\n         </span>\\n        </code>\\n        instead of\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          int8\\n         </span>\\n        </code>\\n        is that the pixel values are represented as positive values.\\n       </p>\\n       <p>\\n        Before this model modification,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          quantize\\n         </span>\\n        </code>\\n        operator converts\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          float32\\n         </span>\\n        </code>\\n        values to\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          int8\\n         </span>\\n        </code>\\n        values.\\nAfter this model modification, the quantize operator converts\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          uint8\\n         </span>\\n        </code>\\n        values to\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          int8\\n         </span>\\n        </code>\\n        values.\\nThis conversion from\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          uint8\\n         </span>\\n        </code>\\n        to\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          int8\\n         </span>\\n        </code>\\n        is much faster than the conversion from\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          float32\\n         </span>\\n        </code>\\n        to\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          int8\\n         </span>\\n        </code>\\n        .\\nThe followings are the the benchmark results of before and after the model modification.\\nAlso, the figure shows how the\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          quantize\\n         </span>\\n        </code>\\n        operator is changed.\\n       </p>\\n       <table class=\"docutils align-center\" id=\"id2\">\\n        <caption>\\n         <span class=\"caption-text\">\\n          Quantization in YOLOX_L\\n         </span>\\n         <a class=\"headerlink\" href=\"#id2\" title=\"Permalink to this table\">\\n          \\uf0c1\\n         </a>\\n        </caption>\\n        <thead>\\n         <tr class=\"row-odd\">\\n          <th class=\"head\">\\n           <p>\\n            Input type\\n           </p>\\n          </th>\\n          <th class=\"head\">\\n           <p>\\n            <code class=\"docutils literal notranslate\">\\n             <span class=\"pre\">\\n              Quantize\\n             </span>\\n            </code>\\n            execution time\\n           </p>\\n          </th>\\n         </tr>\\n        </thead>\\n        <tbody>\\n         <tr class=\"row-even\">\\n          <td>\\n           <p>\\n            float32\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            60.639 ms\\n           </p>\\n          </td>\\n         </tr>\\n         <tr class=\"row-odd\">\\n          <td>\\n           <p>\\n            uint8\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            0.277 ms\\n           </p>\\n          </td>\\n         </tr>\\n        </tbody>\\n       </table>\\n       <figure class=\"align-center\" id=\"id3\">\\n        <img alt=\"../_images/quantize_0.png\" class=\"with-border\" src=\"../_images/quantize_0.png\"/>\\n        <figcaption>\\n         <p>\\n          <span class=\"caption-text\">\\n           quantize without\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             ModelEditor\\n            </span>\\n           </code>\\n          </span>\\n          <a class=\"headerlink\" href=\"#id3\" title=\"Permalink to this image\">\\n           \\uf0c1\\n          </a>\\n         </p>\\n        </figcaption>\\n       </figure>\\n       <figure class=\"align-center\" id=\"id4\">\\n        <img alt=\"../_images/quantize_1.png\" class=\"with-border\" src=\"../_images/quantize_1.png\"/>\\n        <figcaption>\\n         <p>\\n          <span class=\"caption-text\">\\n           quantize with\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             convert_input_type\\n            </span>\\n           </code>\\n          </span>\\n          <a class=\"headerlink\" href=\"#id4\" title=\"Permalink to this image\">\\n           \\uf0c1\\n          </a>\\n         </p>\\n        </figcaption>\\n       </figure>\\n       <div class=\"admonition warning\">\\n        <p class=\"admonition-title\">\\n         Warning\\n        </p>\\n        <p>\\n         This optmization may affect the accurarcy of the model.\\nSince it depends on models and applications,\\nit is recommended to validate the accuracy of the model.\\n        </p>\\n       </div>\\n       <p>\\n        The following is a real example code to use\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          ModelEditor\\n         </span>\\n        </code>\\n        API with\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          convert_input_type()\\n         </span>\\n        </code>\\n        .\\n       </p>\\n       <div class=\"code highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"ch\">#!/usr/bin/env python</span>\\n\\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">onnx</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">torchvision</span> <span class=\"kn\">import</span> <span class=\"n\">transforms</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">tqdm</span>\\n\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.optimizer</span> <span class=\"kn\">import</span> <span class=\"n\">optimize_model</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.quantizer</span> <span class=\"kn\">import</span> <span class=\"n\">get_pure_input_names</span><span class=\"p\">,</span> <span class=\"n\">quantize</span><span class=\"p\">,</span> <span class=\"n\">Calibrator</span><span class=\"p\">,</span> <span class=\"n\">CalibrationMethod</span><span class=\"p\">,</span> <span class=\"n\">ModelEditor</span><span class=\"p\">,</span> <span class=\"n\">TensorType</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime</span> <span class=\"kn\">import</span> <span class=\"n\">session</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime.profiler</span> <span class=\"kn\">import</span> <span class=\"n\">profile</span>\\n\\n\\n<span class=\"n\">torch_model</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">resnet50</span><span class=\"p\">(</span><span class=\"n\">weights</span><span class=\"o\">=</span><span class=\"s1\">\\'DEFAULT\\'</span><span class=\"p\">)</span>\\n<span class=\"n\">torch_model</span> <span class=\"o\">=</span> <span class=\"n\">torch_model</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">()</span>\\n\\n<span class=\"n\">dummy_input</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">),)</span>\\n\\n<span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">onnx</span><span class=\"o\">.</span><span class=\"n\">export</span><span class=\"p\">(</span>\\n    <span class=\"n\">torch_model</span><span class=\"p\">,</span>  <span class=\"c1\"># PyTorch model to export</span>\\n    <span class=\"n\">dummy_input</span><span class=\"p\">,</span>  <span class=\"c1\"># model input</span>\\n    <span class=\"s2\">\"resnet50.onnx\"</span><span class=\"p\">,</span>  <span class=\"c1\"># where to save the exported ONNX model</span>\\n    <span class=\"n\">opset_version</span><span class=\"o\">=</span><span class=\"mi\">13</span><span class=\"p\">,</span>  <span class=\"c1\"># the ONNX OpSet version to export the model to</span>\\n    <span class=\"n\">do_constant_folding</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>  <span class=\"c1\"># whether to execute constant folding for optimization</span>\\n    <span class=\"n\">input_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"input\"</span><span class=\"p\">],</span>  <span class=\"c1\"># the ONNX model\\'s input names</span>\\n    <span class=\"n\">output_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"output\"</span><span class=\"p\">],</span>  <span class=\"c1\"># the ONNX model\\'s output names</span>\\n<span class=\"p\">)</span>\\n\\n<span class=\"n\">onnx_model</span> <span class=\"o\">=</span> <span class=\"n\">onnx</span><span class=\"o\">.</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s2\">\"resnet50.onnx\"</span><span class=\"p\">)</span>\\n<span class=\"n\">onnx_model</span> <span class=\"o\">=</span> <span class=\"n\">optimize_model</span><span class=\"p\">(</span><span class=\"n\">onnx_model</span><span class=\"p\">)</span>\\n\\n<span class=\"n\">calibrator</span> <span class=\"o\">=</span> <span class=\"n\">Calibrator</span><span class=\"p\">(</span><span class=\"n\">onnx_model</span><span class=\"p\">,</span> <span class=\"n\">CalibrationMethod</span><span class=\"o\">.</span><span class=\"n\">MIN_MAX_ASYM</span><span class=\"p\">)</span>\\n<span class=\"n\">calibrator</span><span class=\"o\">.</span><span class=\"n\">collect_data</span><span class=\"p\">([[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">()]])</span>\\n<span class=\"n\">ranges</span> <span class=\"o\">=</span> <span class=\"n\">calibrator</span><span class=\"o\">.</span><span class=\"n\">compute_range</span><span class=\"p\">()</span>\\n\\n<span class=\"n\">editor</span> <span class=\"o\">=</span> <span class=\"n\">ModelEditor</span><span class=\"p\">(</span><span class=\"n\">onnx_model</span><span class=\"p\">)</span>\\n<span class=\"n\">input_tensor_name</span> <span class=\"o\">=</span> <span class=\"n\">get_pure_input_names</span><span class=\"p\">(</span><span class=\"n\">onnx_model</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\\n\\n<span class=\"c1\"># Convert the input type to uint8</span>\\n<span class=\"n\">editor</span><span class=\"o\">.</span><span class=\"n\">convert_input_type</span><span class=\"p\">(</span><span class=\"n\">input_tensor_name</span><span class=\"p\">,</span> <span class=\"n\">TensorType</span><span class=\"o\">.</span><span class=\"n\">UINT8</span><span class=\"p\">)</span>\\n\\n<span class=\"n\">graph</span> <span class=\"o\">=</span> <span class=\"n\">quantize</span><span class=\"p\">(</span><span class=\"n\">onnx_model</span><span class=\"p\">,</span> <span class=\"n\">ranges</span><span class=\"p\">)</span>\\n\\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">\"trace.json\"</span><span class=\"p\">,</span> <span class=\"s2\">\"w\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">trace</span><span class=\"p\">:</span>\\n    <span class=\"k\">with</span> <span class=\"n\">profile</span><span class=\"p\">(</span><span class=\"n\">file</span><span class=\"o\">=</span><span class=\"n\">trace</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">profiler</span><span class=\"p\">:</span>\\n        <span class=\"k\">with</span> <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"n\">graph</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">session</span><span class=\"p\">:</span>\\n            <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">)</span>\\n            <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"pre\"</span><span class=\"p\">):</span>\\n                <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">()</span>\\n            <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"inf\"</span><span class=\"p\">):</span>\\n                <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>\\n            <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"post\"</span><span class=\"p\">):</span>\\n                <span class=\"n\">prediction</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">(),</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\\n</pre>\\n        </div>\\n       </div>\\n      </section>\\n      <section id=\"optimizing-dequantize-operator\">\\n       <h3>\\n        Optimizing\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Dequantize\\n         </span>\\n        </code>\\n        Operator\\n        <a class=\"headerlink\" href=\"#optimizing-dequantize-operator\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Similar to the above\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Quantize\\n         </span>\\n        </code>\\n        operator optimization,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Dequantize\\n         </span>\\n        </code>\\n        operator also can be optimized in the similar way.\\n       </p>\\n       <p>\\n        If the model output tensor is\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          fp32\\n         </span>\\n        </code>\\n        , the output of\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          int8\\n         </span>\\n        </code>\\n        values must be converted to f32 values.\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Dequantize\\n         </span>\\n        </code>\\n        operator converts int8 values to fp32 values, and it’s executed on CPU.\\nIf the model output is an RGB image or something else which can be represented as\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          int8\\n         </span>\\n        </code>\\n        or\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          uint8\\n         </span>\\n        </code>\\n        values,\\nwe can skip converting\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          int8\\n         </span>\\n        </code>\\n        or\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          uint8\\n         </span>\\n        </code>\\n        to\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          fp32\\n         </span>\\n        </code>\\n        . It will reduce the inference latency significantly.\\n       </p>\\n       <p>\\n        We can enable this optimization by using\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          convert_output_type()\\n         </span>\\n        </code>\\n        method of\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          ModelEditor\\n         </span>\\n        </code>\\n        .\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          convert_output_type()\\n         </span>\\n        </code>\\n        method can modifies a model output by a given tensor name and a target data type.\\nThe target type can be either\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          INT8\\n         </span>\\n        </code>\\n        or\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          UINT8\\n         </span>\\n        </code>\\n        .\\n       </p>\\n       <figure class=\"align-center\" id=\"id5\">\\n        <img alt=\"../_images/quantize_2.png\" class=\"with-border\" src=\"../_images/quantize_2.png\"/>\\n        <figcaption>\\n         <p>\\n          <span class=\"caption-text\">\\n           quantize with\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             convert_output_type\\n            </span>\\n           </code>\\n          </span>\\n          <a class=\"headerlink\" href=\"#id5\" title=\"Permalink to this image\">\\n           \\uf0c1\\n          </a>\\n         </p>\\n        </figcaption>\\n       </figure>\\n       <figure class=\"align-center\" id=\"id6\">\\n        <img alt=\"../_images/quantize_3.png\" class=\"with-border\" src=\"../_images/quantize_3.png\"/>\\n        <figcaption>\\n         <p>\\n          <span class=\"caption-text\">\\n           quantize with\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             convert_input_type\\n            </span>\\n           </code>\\n           and\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             convert_output_type\\n            </span>\\n           </code>\\n          </span>\\n          <a class=\"headerlink\" href=\"#id6\" title=\"Permalink to this image\">\\n           \\uf0c1\\n          </a>\\n         </p>\\n        </figcaption>\\n       </figure>\\n       <div class=\"admonition note\">\\n        <p class=\"admonition-title\">\\n         Note\\n        </p>\\n        <p>\\n         Furiosa Compiler may automatically apply this optimization\\nto the model even if this optmization is not explicitly applied.\\nIn that case, the optimization by Furiosa Compiler may result in lower latency\\nthan the one manually applied by\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           ModelEditor\\n          </span>\\n         </code>\\n         .\\nIt is recommended to do experiments to find the best option.\\n        </p>\\n       </div>\\n       <div class=\"admonition warning\">\\n        <p class=\"admonition-title\">\\n         Warning\\n        </p>\\n        <p>\\n         This optmization may affect the accurarcy of the model.\\nSince it depends on models and applications,\\nit is recommended to validate the accuracy of the model.\\n        </p>\\n       </div>\\n       <p>\\n        The following is an real example code to use\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          convert_output_type\\n         </span>\\n        </code>\\n        option.\\n       </p>\\n       <div class=\"code highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"ch\">#!/usr/bin/env python</span>\\n\\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">onnx</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">torchvision</span> <span class=\"kn\">import</span> <span class=\"n\">transforms</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">tqdm</span>\\n\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.optimizer</span> <span class=\"kn\">import</span> <span class=\"n\">optimize_model</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.quantizer</span> <span class=\"kn\">import</span> <span class=\"n\">get_output_names</span><span class=\"p\">,</span> <span class=\"n\">quantize</span><span class=\"p\">,</span> <span class=\"n\">Calibrator</span><span class=\"p\">,</span> <span class=\"n\">CalibrationMethod</span><span class=\"p\">,</span> <span class=\"n\">ModelEditor</span><span class=\"p\">,</span> <span class=\"n\">TensorType</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime</span> <span class=\"kn\">import</span> <span class=\"n\">session</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime.profiler</span> <span class=\"kn\">import</span> <span class=\"n\">profile</span>\\n\\n\\n<span class=\"n\">torch_model</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">resnet50</span><span class=\"p\">(</span><span class=\"n\">weights</span><span class=\"o\">=</span><span class=\"s1\">\\'DEFAULT\\'</span><span class=\"p\">)</span>\\n<span class=\"n\">torch_model</span> <span class=\"o\">=</span> <span class=\"n\">torch_model</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">()</span>\\n\\n<span class=\"n\">dummy_input</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">),)</span>\\n\\n<span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">onnx</span><span class=\"o\">.</span><span class=\"n\">export</span><span class=\"p\">(</span>\\n    <span class=\"n\">torch_model</span><span class=\"p\">,</span>  <span class=\"c1\"># PyTorch model to export</span>\\n    <span class=\"n\">dummy_input</span><span class=\"p\">,</span>  <span class=\"c1\"># model input</span>\\n    <span class=\"s2\">\"resnet50.onnx\"</span><span class=\"p\">,</span>  <span class=\"c1\"># where to save the exported ONNX model</span>\\n    <span class=\"n\">opset_version</span><span class=\"o\">=</span><span class=\"mi\">13</span><span class=\"p\">,</span>  <span class=\"c1\"># the ONNX OpSet version to export the model to</span>\\n    <span class=\"n\">do_constant_folding</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>  <span class=\"c1\"># whether to execute constant folding for optimization</span>\\n    <span class=\"n\">input_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"input\"</span><span class=\"p\">],</span>  <span class=\"c1\"># the ONNX model\\'s input names</span>\\n    <span class=\"n\">output_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"output\"</span><span class=\"p\">],</span>  <span class=\"c1\"># the ONNX model\\'s output names</span>\\n<span class=\"p\">)</span>\\n\\n<span class=\"n\">onnx_model</span> <span class=\"o\">=</span> <span class=\"n\">onnx</span><span class=\"o\">.</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s2\">\"resnet50.onnx\"</span><span class=\"p\">)</span>\\n<span class=\"n\">onnx_model</span> <span class=\"o\">=</span> <span class=\"n\">optimize_model</span><span class=\"p\">(</span><span class=\"n\">onnx_model</span><span class=\"p\">)</span>\\n\\n<span class=\"n\">calibrator</span> <span class=\"o\">=</span> <span class=\"n\">Calibrator</span><span class=\"p\">(</span><span class=\"n\">onnx_model</span><span class=\"p\">,</span> <span class=\"n\">CalibrationMethod</span><span class=\"o\">.</span><span class=\"n\">MIN_MAX_ASYM</span><span class=\"p\">)</span>\\n<span class=\"n\">calibrator</span><span class=\"o\">.</span><span class=\"n\">collect_data</span><span class=\"p\">([[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">()]])</span>\\n<span class=\"n\">ranges</span> <span class=\"o\">=</span> <span class=\"n\">calibrator</span><span class=\"o\">.</span><span class=\"n\">compute_range</span><span class=\"p\">()</span>\\n\\n<span class=\"n\">editor</span> <span class=\"o\">=</span> <span class=\"n\">ModelEditor</span><span class=\"p\">(</span><span class=\"n\">onnx_model</span><span class=\"p\">)</span>\\n<span class=\"n\">output_tensor_name</span> <span class=\"o\">=</span> <span class=\"n\">get_output_names</span><span class=\"p\">(</span><span class=\"n\">onnx_model</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\\n\\n<span class=\"c1\"># output 텐서의 자료형을 int8로 변환</span>\\n<span class=\"n\">editor</span><span class=\"o\">.</span><span class=\"n\">convert_output_type</span><span class=\"p\">(</span><span class=\"n\">output_tensor_name</span><span class=\"p\">,</span> <span class=\"n\">TensorType</span><span class=\"o\">.</span><span class=\"n\">INT8</span><span class=\"p\">)</span>\\n\\n<span class=\"n\">graph</span> <span class=\"o\">=</span> <span class=\"n\">quantize</span><span class=\"p\">(</span><span class=\"n\">onnx_model</span><span class=\"p\">,</span> <span class=\"n\">ranges</span><span class=\"p\">)</span>\\n\\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">\"trace.json\"</span><span class=\"p\">,</span> <span class=\"s2\">\"w\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">trace</span><span class=\"p\">:</span>\\n    <span class=\"k\">with</span> <span class=\"n\">profile</span><span class=\"p\">(</span><span class=\"n\">file</span><span class=\"o\">=</span><span class=\"n\">trace</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">profiler</span><span class=\"p\">:</span>\\n        <span class=\"k\">with</span> <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"n\">graph</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">session</span><span class=\"p\">:</span>\\n            <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">)</span>\\n            <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"pre\"</span><span class=\"p\">):</span>\\n                <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">()</span>\\n            <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"inf\"</span><span class=\"p\">):</span>\\n                <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>\\n            <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"post\"</span><span class=\"p\">):</span>\\n                <span class=\"n\">prediction</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">(),</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\\n</pre>\\n        </div>\\n       </div>\\n      </section>\\n      <section id=\"lower-unlower-acceleration\">\\n       <h3>\\n        Lower/Unlower Acceleration\\n        <a class=\"headerlink\" href=\"#lower-unlower-acceleration\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Warboy internally uses its inherent memory layout to accelerate the computation\\nby leveraging the NPU architecture.\\nFor the memory layout,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Lower\\n         </span>\\n        </code>\\n        operator reshapes the input tensor to the NPU’s memory layout and\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Unlower\\n         </span>\\n        </code>\\n        operator reshapes the output tensor from the NPU’s memory layout to the original shape.\\nFor them, Furiosa Compiler automatically adds\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Lower\\n         </span>\\n        </code>\\n        and\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Unlower\\n         </span>\\n        </code>\\n        operators to the model.\\n       </p>\\n       <p>\\n        In many cases,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Lower\\n         </span>\\n        </code>\\n        and\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Unlower\\n         </span>\\n        </code>\\n        are executed on CPU, causing some overhead\\nof the inference latency.\\nHowever, if the last axis of input or output tensor shape is\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          width\\n         </span>\\n        </code>\\n        and\\nthe size of the last axis is a multiple of 32,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Lower\\n         </span>\\n        </code>\\n        and\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Unlower\\n         </span>\\n        </code>\\n        operators can be accleerated on NPU.\\nThen, the inference latency can be reduced significantly.\\n       </p>\\n       <p>\\n        Therefore, if you are able to specify the shape of the input and output tensors,\\nit’s more optimal to use\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          NxCxHxW\\n         </span>\\n        </code>\\n        and specify the width as a multiple of 32.\\nAlso, this optimization can be applied independently to the input and output tensors respectively.\\n       </p>\\n      </section>\\n      <section id=\"removal-of-pad-slice\">\\n       <h3>\\n        Removal of Pad/Slice\\n        <a class=\"headerlink\" href=\"#removal-of-pad-slice\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        As described above, the\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Lower\\n         </span>\\n        </code>\\n        /\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Unlower\\n         </span>\\n        </code>\\n        operations can be accelerated\\nif the last axis of the tensor for either operator is\\n        <cite>\\n         width\\n        </cite>\\n        and\\nthe size of the last axis is a multiple of 32.\\n       </p>\\n       <p>\\n        If the last tensor axis of\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Lower\\n         </span>\\n        </code>\\n        is\\n        <cite>\\n         width\\n        </cite>\\n        but not a multiple of 32,\\nFuriosa Compiler may automatically add\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Pad\\n         </span>\\n        </code>\\n        operator before\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Lower\\n         </span>\\n        </code>\\n        operator\\nto adjust the size of the last axis to a multiple of 32.\\nIn the similar way, Furiosa Compiler may automatically add\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Slice\\n         </span>\\n        </code>\\n        operator after\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Unlower\\n         </span>\\n        </code>\\n        operator to\\nslice data contents from the tensor with the last axis of a multiple of 32 to the original tensor shape.\\n       </p>\\n       <p>\\n        This optimization gains some performance benefits by accelerating\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Lower\\n         </span>\\n        </code>\\n        /\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Unlower\\n         </span>\\n        </code>\\n        operations.\\nHowever,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Pad\\n         </span>\\n        </code>\\n        and\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Slice\\n         </span>\\n        </code>\\n        requires CPU computation.\\nThere’s futher optimization opportunity to remove even\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Pad\\n         </span>\\n        </code>\\n        and\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Slice\\n         </span>\\n        </code>\\n        operators too.\\nIf you can accept the constraints of the input and output tensor shapes,\\nit is strongly recommended using the shape of the tensors\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          NxCxHxW\\n         </span>\\n        </code>\\n        and\\na multiple of 32 of the width.\\n       </p>\\n      </section>\\n      <section id=\"change-the-order-of-input-tensor-axes-at-compiler-time\">\\n       <h3>\\n        Change the Order of Input Tensor Axes at Compiler Time\\n        <a class=\"headerlink\" href=\"#change-the-order-of-input-tensor-axes-at-compiler-time\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        As we discussed above, there are more optimization opportunities\\nif the last axis of the input tensor is\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          width\\n         </span>\\n        </code>\\n        .\\nHowever, changing the order of axes requires to modify the models.\\nIt may require some effort to modify the original models in some cases.\\n       </p>\\n       <p>\\n        So, Furiosa Compiler provides a way to change the order of the input tensor axes\\nat compile time. You can specify\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          permute_input\\n         </span>\\n        </code>\\n        option in compiler config\\nto specify the new order of the input tensor axes as follows:\\n       </p>\\n       <ul>\\n        <li>\\n         <p>\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            compiler_config\\n           </span>\\n           <span class=\"pre\">\\n            =\\n           </span>\\n           <span class=\"pre\">\\n            {\\n           </span>\\n           <span class=\"pre\">\\n            \"permute_input\":\\n           </span>\\n           <span class=\"pre\">\\n            [[0,\\n           </span>\\n           <span class=\"pre\">\\n            3,\\n           </span>\\n           <span class=\"pre\">\\n            1,\\n           </span>\\n           <span class=\"pre\">\\n            2]]\\n           </span>\\n           <span class=\"pre\">\\n            }\\n           </span>\\n          </code>\\n         </p>\\n         <blockquote>\\n          <div>\\n           <ul class=\"simple\">\\n            <li>\\n             <p>\\n              The parameter of\\n              <code class=\"docutils literal notranslate\">\\n               <span class=\"pre\">\\n                permute_input\\n               </span>\\n              </code>\\n              is the same as\\n              <a class=\"reference external\" href=\"https://pytorch.org/docs/stable/generated/torch.permute.html\">\\n               torch.permute\\n              </a>\\n              .\\n             </p>\\n            </li>\\n            <li>\\n             <p>\\n              For example, the above example code will change\\n              <code class=\"docutils literal notranslate\">\\n               <span class=\"pre\">\\n                NxHxWxC\\n               </span>\\n              </code>\\n              to\\n              <code class=\"docutils literal notranslate\">\\n               <span class=\"pre\">\\n                NxCxHxW\\n               </span>\\n              </code>\\n              .\\n             </p>\\n            </li>\\n           </ul>\\n          </div>\\n         </blockquote>\\n        </li>\\n       </ul>\\n       <p>\\n        The following is a real example code to use\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          permute_input\\n         </span>\\n        </code>\\n        option.\\n       </p>\\n       <div class=\"code highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"ch\">#!/usr/bin/env python</span>\\n\\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\\n\\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">onnx</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">tqdm</span>\\n\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.optimizer</span> <span class=\"kn\">import</span> <span class=\"n\">optimize_model</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.quantizer</span> <span class=\"kn\">import</span> <span class=\"n\">quantize</span><span class=\"p\">,</span> <span class=\"n\">Calibrator</span><span class=\"p\">,</span> <span class=\"n\">CalibrationMethod</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime</span> <span class=\"kn\">import</span> <span class=\"n\">session</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime.profiler</span> <span class=\"kn\">import</span> <span class=\"n\">profile</span>\\n\\n\\n<span class=\"n\">onnx_model</span> <span class=\"o\">=</span> <span class=\"n\">onnx</span><span class=\"o\">.</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s2\">\"model_nhwc.onnx\"</span><span class=\"p\">)</span>\\n<span class=\"n\">onnx_model</span> <span class=\"o\">=</span> <span class=\"n\">optimize_model</span><span class=\"p\">(</span><span class=\"n\">onnx_model</span><span class=\"p\">)</span>\\n\\n<span class=\"n\">calibrator</span> <span class=\"o\">=</span> <span class=\"n\">Calibrator</span><span class=\"p\">(</span><span class=\"n\">onnx_model</span><span class=\"p\">,</span> <span class=\"n\">CalibrationMethod</span><span class=\"o\">.</span><span class=\"n\">MIN_MAX_ASYM</span><span class=\"p\">)</span>\\n<span class=\"n\">calibrator</span><span class=\"o\">.</span><span class=\"n\">collect_data</span><span class=\"p\">([[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">()]])</span>\\n<span class=\"n\">ranges</span> <span class=\"o\">=</span> <span class=\"n\">calibrator</span><span class=\"o\">.</span><span class=\"n\">compute_range</span><span class=\"p\">()</span>\\n\\n<span class=\"n\">graph</span> <span class=\"o\">=</span> <span class=\"n\">quantize</span><span class=\"p\">(</span><span class=\"n\">onnx_model</span><span class=\"p\">,</span> <span class=\"n\">ranges</span><span class=\"p\">)</span>\\n\\n<span class=\"n\">compiler_config</span> <span class=\"o\">=</span> <span class=\"p\">{</span> <span class=\"s2\">\"permute_input\"</span><span class=\"p\">:</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">]]</span> <span class=\"p\">}</span>\\n\\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">\"trace.json\"</span><span class=\"p\">,</span> <span class=\"s2\">\"w\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">trace</span><span class=\"p\">:</span>\\n    <span class=\"k\">with</span> <span class=\"n\">profile</span><span class=\"p\">(</span><span class=\"n\">file</span><span class=\"o\">=</span><span class=\"n\">trace</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">profiler</span><span class=\"p\">:</span>\\n        <span class=\"k\">with</span> <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"n\">graph</span><span class=\"p\">,</span> <span class=\"n\">compiler_config</span><span class=\"o\">=</span><span class=\"n\">compiler_config</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">session</span><span class=\"p\">:</span>\\n            <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">)</span>\\n            <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"pre\"</span><span class=\"p\">):</span>\\n                <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">()</span>\\n            <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"inf\"</span><span class=\"p\">):</span>\\n                <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>\\n            <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"post\"</span><span class=\"p\">):</span>\\n                <span class=\"n\">prediction</span> <span class=\"o\">=</span> <span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">()</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        This is another case to use\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          permute_input\\n         </span>\\n        </code>\\n        option.\\nIn some cases, it’s necessary to change the order of the input tensor axes\\nfrom\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          NxCxHxW\\n         </span>\\n        </code>\\n        to\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          NxHxWxC\\n         </span>\\n        </code>\\n        .\\nPython OpenCV is a popular computer vision library.\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          cv2.imread()\\n         </span>\\n        </code>\\n        of OpenCV returns a 3D NumPy array with\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          HxWxC\\n         </span>\\n        </code>\\n        order.\\nIf the axes of the input tensors of a model are\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          NxCxHxW\\n         </span>\\n        </code>\\n        , it requires to transpose the tensor.\\nThe transpose is a time-consuming operation running in CPU.\\nIn this case, we can remove the transpose operation\\nif we change the order of the input tensor axes of the model to the same as\\nOpenCV’s output; e.g.,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          NxHxWxC\\n         </span>\\n        </code>\\n        . It will reduce the inference latency significantly.\\n       </p>\\n      </section>\\n      <section id=\"optimization-of-large-input-and-output-tensors\">\\n       <h3>\\n        Optimization of Large Input and Output Tensors\\n        <a class=\"headerlink\" href=\"#optimization-of-large-input-and-output-tensors\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Some models have large images and as inputs and outputs. For example,\\nDenoising and super resolution models basically take large images as inputs and outputs.\\nDepending on your implementation, those models may be slow in Furiosa SDK and Warboy.\\nFuriosa Compiler optimizes the models with various techniques\\nwhile preserving the semantics of the original models.\\nBasically, Furiosa Compiler handles large tensors as defined by the model.\\nHowever, if the size of tensors is too large, it may exceed SRAM memory of Warboy,\\ncausing more I/O operations between DRAM and SRAM. It may result in poor performance.\\n       </p>\\n       <p>\\n        We can optimize this case by splitting a large tensor\\ninto a number of smaller tensors and then merging the results.\\nGenerally, we can apply this optimization to denosing and super resolution models\\nbecause the small parts of images can be independently processed and merged to get the final results.\\nThe small parts of images are called patches, and the size of patches is called patch size.\\n       </p>\\n       <p>\\n        To understand the optimization mechanism, we need to understand how the Furiosa Compiler works.\\nFuriosa Compiler tries to hide IO times between DRAM and SRAM by overlapping\\nthem with NPU executions. In other words, NPU can execute operators while I/O operations are working.\\nIf we split a large tensor into a number of smaller tensors,\\nthe number of I/O operations can be hidden by NPU executions.\\n       </p>\\n       <p>\\n        Once we decide to use this optimization, the next step is to determine the patch size.\\nHere, one good metric to determine the patch size is the ratio of the time spent on NPU executions.\\nThe smaller the patch size, the more time is spent on NPU computation.\\nIn contrast, the larger the patch size, the more time is spent on I/O operations.\\n       </p>\\n       <p>\\n        Also, this optimization can be combined with using multiple NPU devices.\\nThe multiple patches can run across multiple NPU devices in parallel.\\n       </p>\\n      </section>\\n      <section id=\"more-batch-more-npu-utilization\">\\n       <h3>\\n        More Batch, More NPU Utilization\\n        <a class=\"headerlink\" href=\"#more-batch-more-npu-utilization\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        For some models with small weights or few layers, the NPU utilization may be low.\\nIn this case, we can increase the batch size to make the NPU utilization higher.\\nWith this optimization, the inference may still have the same latency,\\nbut its throughput can be increased significantly.\\n       </p>\\n       <p>\\n        A batch size can be specified when compiling a model with\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          --batch-size\\n         </span>\\n        </code>\\n        option as follows:\\n       </p>\\n       <p>\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-compiler\\n         </span>\\n         <span class=\"pre\">\\n          --batch-size\\n         </span>\\n         <span class=\"pre\">\\n          32\\n         </span>\\n         <span class=\"pre\">\\n          --target-npu\\n         </span>\\n         <span class=\"pre\">\\n          warboy\\n         </span>\\n         <span class=\"pre\">\\n          mnist.dfg\\n         </span>\\n         <span class=\"pre\">\\n          -o\\n         </span>\\n         <span class=\"pre\">\\n          mnist.enf\\n         </span>\\n        </code>\\n       </p>\\n       <p>\\n        A batch size also can be specified when creating a session with\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          batch_size\\n         </span>\\n        </code>\\n        option.\\nYou can learn more about the\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          batch_size\\n         </span>\\n        </code>\\n        option from\\n        <a class=\"reference external\" href=\"https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#furiosa.runtime.furiosa.runtime.Runtime.create_runner\">\\n         Runner API\\n        </a>\\n        .\\n       </p>\\n      </section>\\n      <section id=\"single-pe-vs-fusion-pe\">\\n       <h3>\\n        Single PE vs Fusion PE\\n        <a class=\"headerlink\" href=\"#single-pe-vs-fusion-pe\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        A single Warboy chip consists of two processing elements (PEs).\\nEach PE of Warboy has its own control unit, and the two PEs can work independently.\\nIn this mode, each PE works with spatially-partitioned memory and processing units.\\nIn contrast, two PEs can also be fused as a single PE.\\nIn this fused mode, two PEs work as a single PE with an unified memory and processing units.\\n       </p>\\n       <p>\\n        These two modes allow applications to have more flexibility to optimize the performance.\\nFor example, if a model has large weights, we can use a 2PE-fused mode to load the weights\\nfor a lower latency. If a model fits in a single PE, we can use two single PEs separately\\nto run the two model instances for higher throughput.\\n       </p>\\n       <p>\\n        If a workload is latency-oriented, using a 2PE-fused mode is generally recommended.\\nIf a workload is throughput-oriented, using two single PEs is generally recommended.\\nIt still depends on models and workloads.\\nYou need to find the optimal NPU configuration through experiments.\\n       </p>\\n       <p>\\n        The followings are example commands to compile a model with a single PE or a fused-PE respectively.\\n       </p>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          Single PE:\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            furiosa-compiler\\n           </span>\\n           <span class=\"pre\">\\n            --target-npu\\n           </span>\\n           <span class=\"pre\">\\n            warboy\\n           </span>\\n           <span class=\"pre\">\\n            resnet50.dfg\\n           </span>\\n           <span class=\"pre\">\\n            -o\\n           </span>\\n           <span class=\"pre\">\\n            resnet50.enf\\n           </span>\\n          </code>\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Fusion PE:\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            furiosa-compiler\\n           </span>\\n           <span class=\"pre\">\\n            --target-npu\\n           </span>\\n           <span class=\"pre\">\\n            warboy-2pe\\n           </span>\\n           <span class=\"pre\">\\n            resnet50.dfg\\n           </span>\\n           <span class=\"pre\">\\n            -o\\n           </span>\\n           <span class=\"pre\">\\n            resnet50_2pe.enf\\n           </span>\\n          </code>\\n         </p>\\n        </li>\\n       </ul>\\n       <p>\\n        This NPU configuration can also be specified\\nwhen creating a\\n        <a class=\"reference external\" href=\"https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#furiosa.runtime.Runtime\">\\n         Runtime\\n        </a>\\n        with\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          device\\n         </span>\\n        </code>\\n        option which are specified by\\n        <a class=\"reference external\" href=\"https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#device-specification\">\\n         Device Configuration\\n        </a>\\n       </p>\\n      </section>\\n     </section>\\n     <section id=\"runtime-optimization\">\\n      <h2>\\n       Runtime Optimization\\n       <a class=\"headerlink\" href=\"#runtime-optimization\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       So far, we have discussed the model optimization techniques to reduce the inference latency.\\nAfter we apply the model optimization, we can futher optimize the performance in Runtime level.\\n      </p>\\n      <p>\\n       As we mentioned above, an end-to-end inference consists of three operations:\\nNPU execution, CPU computation, and IO operation.\\nThree kinds of operations can run independently without blocking one another.\\nThey can be overlapped if we run multiple inferences simultaneously.\\nLeveraging this characteristic is a key idea of the runtime optimization.\\n      </p>\\n      <section id=\"more-inference-concurrency-the-number-of-workers\">\\n       <span id=\"concurrencyoptimization\">\\n       </span>\\n       <h3>\\n        More inference concurrency (the number of workers)\\n        <a class=\"headerlink\" href=\"#more-inference-concurrency-the-number-of-workers\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        When we create a session through\\n        <a class=\"reference external\" href=\"https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#furiosa.runtime.furiosa.runtime.Runtime.create_runner\">\\n         Runner API\\n        </a>\\n        ,\\nwe can specify the number of workers as an option.\\nA single worker is a unit that can run inferences independently sharing NPUs.\\nThis concept is similar to a thread and CPUs.\\n       </p>\\n       <p>\\n        If there is only one worker, multiple inference requests are processed sequentially through a single worker.\\nWhen one inference is completed, the next inference is processed by the owrker.\\nIn this case, the NPU can be idle while the CPU is working, causing low NPU utilization.\\n       </p>\\n       <figure class=\"align-center\">\\n        <img alt=\"Single Worker\" class=\"with-shadow\" src=\"../_images/worker_single.png\"/>\\n       </figure>\\n       <p>\\n       </p>\\n       <p>\\n        However, if there are multiple workers, the workers consume requests from the request queue\\nin Runtime. The multiple inferences can be processed simultaneously.\\nIn this case, NPU executions are overlapped with CPU executions, possibly leading to higher NPU utilization.\\n       </p>\\n       <figure class=\"align-center\">\\n        <img alt=\"Multiple Workers\" class=\"with-shadow\" src=\"../_images/worker_multiple.png\"/>\\n       </figure>\\n       <p>\\n       </p>\\n       <p>\\n        Each worker requires more memory resources to maintain context information for its execution.\\nIf the number of workers is too large, the memory resources may be exhausted.\\nIf the number of workers is too small, the NPU utilization may be low.\\nFinding the optimal number of workers is important to maximize the performance of the model.\\nUsually, we can find the optimal number of workers through experimentation.\\n       </p>\\n      </section>\\n      <section id=\"sync-api-vs-async-apis\">\\n       <h3>\\n        Sync API vs Async APIs\\n        <a class=\"headerlink\" href=\"#sync-api-vs-async-apis\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        There are two types of runtime APIs: Sync API and Async API.\\nSync API is a blocking API that waits for the completion of the inference.\\nAsync APIs are non-blocking APIs that don’t wait for the completion of the inference.\\nAsync APIs allow to request multiple inferences simultaneously and wait for the results asynchronously.\\n       </p>\\n       <p>\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa.session.create()\\n         </span>\\n        </code>\\n        a creates a syncronous session.\\nAs the below example,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          session.run()\\n         </span>\\n        </code>\\n        is blocked until the result is returned.\\nIt generally is enough for batch workloads with large batch sizes,\\nbut it’s not sufficient for serving workloads that handle multiple current requests simultaneously.\\n       </p>\\n       <div class=\"highlight-python notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime</span> <span class=\"kn\">import</span> <span class=\"n\">session</span>\\n\\n<span class=\"k\">with</span> <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">sess</span><span class=\"p\">:</span>\\n    <span class=\"nb\">input</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\\n    <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span> <span class=\"c1\"># Wait for completion</span>\\n    <span class=\"o\">...</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        To overcome this limitation, Furiosa SDK provides two types of Async APIs: Queue API and Async/Await API.\\nThey allow to request multiple inferences simultaneously and wait for the results asynchronously.\\nThey are also useful to hide I/O and CPU computation by overlapping them with NPU executions.\\n       </p>\\n       <section id=\"queue-api\">\\n        <h4>\\n         Queue API\\n         <a class=\"headerlink\" href=\"#queue-api\" title=\"Permalink to this heading\">\\n          \\uf0c1\\n         </a>\\n        </h4>\\n        <p>\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           create_async()\\n          </span>\\n         </code>\\n         creates a pair of a submitter and a queue.\\nWith both, we can submit inference requests without waiting for completion and\\nwait for the inference results asynchronously.\\n        </p>\\n        <div class=\"code highlight-default notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">random</span>\\n\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime</span> <span class=\"kn\">import</span> <span class=\"n\">session</span>\\n\\n<span class=\"n\">submitter</span><span class=\"p\">,</span> <span class=\"n\">queue</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">create_async</span><span class=\"p\">(</span><span class=\"s2\">\"mnist.onnx\"</span><span class=\"p\">,</span>\\n                                        <span class=\"n\">worker_num</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\\n                                        <span class=\"c1\"># Determine how many asynchronous requests you can submit</span>\\n                                        <span class=\"c1\"># without blocking.</span>\\n                                        <span class=\"n\">input_queue_size</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\\n                                        <span class=\"n\">output_queue_size</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\\n\\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">):</span>\\n    <span class=\"n\">idx</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">59999</span><span class=\"p\">)</span>\\n    <span class=\"nb\">input</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\\n    <span class=\"n\">submitter</span><span class=\"o\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"o\">=</span><span class=\"n\">idx</span><span class=\"p\">)</span> <span class=\"c1\"># non blocking call</span>\\n\\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">):</span>\\n    <span class=\"n\">context</span><span class=\"p\">,</span> <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">queue</span><span class=\"o\">.</span><span class=\"n\">recv</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)</span> <span class=\"c1\"># 100 ms for timeout. If None, queue.recv() will be blocking.</span>\\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">())</span>\\n\\n<span class=\"k\">if</span> <span class=\"n\">queue</span><span class=\"p\">:</span>\\n    <span class=\"n\">queue</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\\n<span class=\"k\">if</span> <span class=\"n\">submitter</span><span class=\"p\">:</span>\\n    <span class=\"n\">submitter</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\\n</pre>\\n         </div>\\n        </div>\\n       </section>\\n       <section id=\"using-async-await-syntax\">\\n        <h4>\\n         Using Async/Await syntax\\n         <a class=\"headerlink\" href=\"#using-async-await-syntax\" title=\"Permalink to this heading\">\\n          \\uf0c1\\n         </a>\\n        </h4>\\n        <p>\\n         In the the example below,\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           NPUModel\\n          </span>\\n         </code>\\n         of furiosa-server provide an easier way to implement\\na serving application using async/await API.\\n        </p>\\n        <div class=\"code highlight-default notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">asyncio</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\\n\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.server.model</span> <span class=\"kn\">import</span> <span class=\"n\">NPUModel</span><span class=\"p\">,</span> <span class=\"n\">NPUModelConfig</span>\\n\\n<span class=\"k\">class</span> <span class=\"nc\">SimpleApplication</span><span class=\"p\">:</span>\\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">NPUModel</span><span class=\"p\">(</span>\\n            <span class=\"n\">NPUModelConfig</span><span class=\"p\">(</span>\\n                <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"MNIST\"</span><span class=\"p\">,</span>\\n                <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">\"mnist.onnx\"</span><span class=\"p\">,</span>\\n            <span class=\"p\">)</span>\\n        <span class=\"p\">)</span>\\n\\n    <span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">load</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\\n        <span class=\"k\">await</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">()</span>\\n\\n    <span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">process</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"p\">):</span>\\n        <span class=\"nb\">input</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">preprocess</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>\\n        <span class=\"n\">tensor</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span>\\n        <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">postprocess</span><span class=\"p\">(</span><span class=\"n\">tensor</span><span class=\"p\">)</span>\\n        <span class=\"k\">return</span> <span class=\"n\">output</span>\\n\\n    <span class=\"k\">def</span> <span class=\"nf\">preprocess</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"p\">):</span>\\n        <span class=\"c1\"># do preprocess</span>\\n        <span class=\"k\">return</span> <span class=\"n\">image</span>\\n\\n    <span class=\"k\">def</span> <span class=\"nf\">postprocess</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">tensor</span><span class=\"p\">):</span>\\n        <span class=\"c1\"># do postprocess</span>\\n        <span class=\"k\">return</span> <span class=\"n\">tensor</span>\\n\\n\\n<span class=\"n\">APP</span> <span class=\"o\">=</span> <span class=\"n\">SimpleApplication</span><span class=\"p\">()</span>\\n\\n<span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">startup</span><span class=\"p\">():</span>\\n    <span class=\"k\">await</span> <span class=\"n\">APP</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">()</span>\\n\\n<span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">run</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">):</span>\\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">APP</span><span class=\"o\">.</span><span class=\"n\">process</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>\\n    <span class=\"k\">return</span> <span class=\"n\">result</span>\\n\\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">\"__main__\"</span><span class=\"p\">:</span>\\n    <span class=\"n\">asyncio</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">startup</span><span class=\"p\">())</span>\\n\\n    <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\\n    <span class=\"n\">asyncio</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">))</span>\\n</pre>\\n         </div>\\n        </div>\\n       </section>\\n      </section>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"quantization.html\" rel=\"prev\" title=\"Model Quantization\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"profiler.html\" rel=\"next\" title=\"Performance Profiling\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='13973d88-9f7d-49e4-8019-68c7f29141b3', link=Url('https://furiosa-ai.github.io/docs/latest/en/releases/0.9.0.html'), name='0', parent='', child=[], description='\\n\\n\\n* Release Notes - 0.9.0\\n* [View page source](../_sources/releases/0.9.0.rst.txt)\\n\\n---\\n\\n\\n\\nRelease Notes - 0.9.0\\n[\\uf0c1](#release-notes-0-9-0 \"Permalink to this heading\")\\n===========================================================================\\n\\nFuriosa SDK 0.9.0 is a major release, including many performance enhancements,\\nadditional functions, and bug fixes.\\nIn partcular, 0.9.0 release includes the significant improvements of the quantization tools.\\n\\n\\nComponent Version Information\\n\\n[\\uf0c1](#id1 \"Permalink to this table\")\\n\\n\\n| Package Name | Version |\\n| --- | --- |\\n| NPU Driver | 1.7.0 |\\n| NPU Firmware Tools | 1.4.0 |\\n| NPU Firmware Image | 1.7.0 |\\n| HAL (Hardware Abstraction Layer) | 0.11.0 |\\n| Furiosa Compiler | 0.9.0 |\\n| Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..) | 0.9.0 |\\n| NPU Management CLI (furiosactl) | 0.11.0 |\\n| NPU Device Plugin | 0.10.1 |\\n| NPU Feature Discovery | 0.2.0 |\\n\\nInstalling the latest SDK\\n[\\uf0c1](#installing-the-latest-sdk \"Permalink to this heading\")\\n-------------------------------------------------------------------------------------\\n\\nIf you are using APT repository, the upgrade process is simpler.\\n\\n```\\napt-get update && apt-get upgrade\\n\\n```\\n\\nIf you wish to designate a specific package for upgrade, execute as below:\\nYou can find more details about APT repository setup at\\n[Driver, Firmware, and Runtime Installation](../software/installation.html#requiredpackages)\\n.\\n\\n```\\napt-get update && \\\\\\napt-get install -y furiosa-driver-pdma furiosa-libhal-warboy furiosa-libnux furiosactl\\n\\n```\\n\\nYou can upgrade firmware as follows:\\n\\n```\\napt-get update && \\\\\\napt-get install -y furiosa-firmware-tools furiosa-firmware-image\\n\\n```\\n\\nYou can upgrade Python package as follows:\\n\\n```\\npip install --upgrade pip setuptools wheel\\npip install --upgrade furiosa-sdk\\n\\n```\\n\\n\\nWarning\\n\\nWhen installing or upgrading the furiosa-sdk without updating pip to the latest version, you may encounter the following errors.\\n\\n```\\nERROR: Could not find a version that satisfies the requirement furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk) (from versions: none)\\nERROR: No matching distribution found for furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk)\\n\\n```\\n\\n\\n\\n\\nMajor changes\\n[\\uf0c1](#major-changes \"Permalink to this heading\")\\n-------------------------------------------------------------\\n\\n### Quantization tool [\\uf0c1](#quantization-tool \"Permalink to this heading\")\\n\\nQuantization tool is a library that converts a pre-trained model to a quantized model.\\nYou can refer to more details at\\n[Model Quantization](../software/quantization.html#modelquantization)\\n0.9.0 release includes the API improvement and new calibration methods,\\npossibly leading to better accuracy.\\n\\n* Added new quantization-related APIs that are more flexible and solid. (\\n  `furiosa.quantizer`\\n  ,\\n  `furiosa.optimizer`\\n  )\\n\\n```\\noptimized_onnx_model = optimize_model(source_onnx_model)\\ncalibrator = Calibrator(optimized_onnx_model, CalibrationMethod.MIN_MAX_ASYM)\\nfor calibration_data, _ in tqdm.tqdm(calibration_dataloader, desc=\"Calibration\", unit=\"images\", mininterval=0.5):\\n  calibrator.collect_data([[calibration_data.numpy()]])\\nranges = calibrator.compute_range()\\nquantizated_graph = quantize(optimized_onnx_model, ranges)\\n\\n```\\n\\n* Added an option to decide whether to perform quantize at the beginning of the model.\\n  \\n  + Instead of\\n    `without_quantize`\\n    being removed from the compiler options, it can be specified via the argument\\n    `with_quantize`\\n    to the\\n    `quantize`\\n    function.\\n* The\\n  `normalized_pixel_outputs`\\n  argument to the\\n  `quantize`\\n  function can be set to convert the model output to uint8 instead of dequantizing to fp32.\\n  \\n  + A tensor with an element range of\\n    `(0.\\n    \\n    ,\\n    \\n    1.)`\\n    can be optimized to convert to pixel data in uint8.\\n* Provides more calibration methods.\\n\\n\\nSupported Calibration Methods\\n\\n[\\uf0c1](#id2 \"Permalink to this table\")\\n\\n\\n\\n| Calibration Method | Asymmetric | QuasiSymmetric |\\n| --- | --- | --- |\\n| Min-Max | MIN\\\\_MAX\\\\_ASYM | MIN\\\\_MAX\\\\_SYM |\\n| Entropy | ENTROPY\\\\_ASYM | ENTROPY\\\\_SYM |\\n| Percentile | PERCENTILE\\\\_ASYM | PERCENTILE\\\\_SYM |\\n| Mean squared error | MSE\\\\_ASYM | MSE\\\\_SYM |\\n| Signal-to-quantization-noise ratio | SQNR\\\\_ASYM | SQNR\\\\_SYM |\\n\\nTo ensure the effectiveness of new calibration methods,\\nwe measured the accuracy of 10 popular models with the new calibration methods.\\nAmong them, 8 models showed better accuracy than the existing calibration methods.\\nFor example, the accuracy of EfficientNet-B0 increased by 57.452%.\\nWith the min-max calibration method, EfficientNet-B0 had an accuracy of 16.104%.\\nIn contrast, with the percentile calibration method, the accuracy was 73.556%.\\nThe details of the experiment results can be found at\\n[Quantization Accuracy](../software/quantization.html#quantizationaccuracytable)\\n.\\n\\nFor more information on installing and using the new quantizer, you can refer to the following examples.\\n\\n* [Tutorial: How to use Furiosa SDK from Start to Finish](https://github.com/furiosa-ai/furiosa-sdk/blob/v0.9.0/examples/notebooks/HowToUseFuriosaSDKFromStartToFinish.ipynb)\\n\\n### Compiler [\\uf0c1](#compiler \"Permalink to this heading\")\\n\\n* Added acceleration support for operators Lower, Unlower\\n* Added acceleration support for operator Dequantize\\n* Support for executing binaries that are larger than the hardware’s instruction memory\\n* Improved scheduler and memory allocator to eliminate unnecessary I/O\\n* Various improvements optimize compilation for better execution performance\\n\\n### furiosa-toolkit [\\uf0c1](#furiosa-toolkit \"Permalink to this heading\")\\n\\nThe\\n`furiosactl`\\ncommand-line tool included in the furiosa-toolkit 0.11.0 release includes improvements to the\\nincludes the following major improvements\\n\\nThe newly added\\n`furiosactl\\n\\ntop`\\ncommand is used to view utilization by NPU device over time.\\n\\n```\\n$ furiosactl top --interval 200\\nNOTE: furiosa top is under development. Usage and output formats may change.\\nPlease enter Ctrl+C to stop.\\nDatetime                        PID       Device        NPU(%)   Comp(%)   I/O(%)   Command\\n2023-03-21T09:45:56.699483936Z  152616    npu1pe0-1      19.06    100.00     0.00   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n2023-03-21T09:45:56.906443888Z  152616    npu1pe0-1      51.09     93.05     6.95   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n2023-03-21T09:45:57.110489333Z  152616    npu1pe0-1      46.40     97.98     2.02   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n2023-03-21T09:45:57.316060982Z  152616    npu1pe0-1      51.43    100.00     0.00   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n2023-03-21T09:45:57.521140588Z  152616    npu1pe0-1      54.28     94.10     5.90   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n2023-03-21T09:45:57.725910558Z  152616    npu1pe0-1      48.93     98.93     1.07   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n2023-03-21T09:45:57.935041998Z  152616    npu1pe0-1      47.91    100.00     0.00   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n2023-03-21T09:45:58.13929122Z   152616    npu1pe0-1      49.06     94.94     5.06   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n\\n```\\n\\nThe\\n`furiosactl\\n\\ninfo`\\ncommand has been improved to display concise information about each device. As before, you can enter the\\n`--full`\\noption if you want to see more information about a device.\\n\\n```\\n$ furiosactl info\\n+------+--------+----------------+-------+--------+--------------+\\n| NPU  | Name   | Firmware       | Temp. | Power  | PCI-BDF      |\\n+------+--------+----------------+-------+--------+--------------+\\n| npu1 | warboy | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 |\\n+------+--------+----------------+-------+--------+--------------+\\n\\n$ furiosactl info --full\\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n| NPU  | Name   | UUID                                 | S/N               | Firmware       | Temp. | Power  | PCI-BDF      | PCI-DEV |\\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n| npu1 | warboy | 00000000-0000-0000-0000-000000000000 | WBYB0000000000000 | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 | 511:0   |\\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n\\n```\\n\\nMore information about installing and using\\n`furiosactl`\\ncan be found in\\n[furiosa-toolkit](../software/cli.html#toolkit)\\n.\\n\\n\\n\\n\\n\\n\\n[Previous](0.10.0.html \"Release Notes - 0.10.0\")\\n[Next](0.8.0.html \"Release Notes - 0.8.0\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Release Notes - 0.9.0 * [View page source](../_sources/releases/0.9.0.rst.txt)\\n---\\nRelease Notes - 0.9.0 [\\uf0c1](#release-notes-0-9-0 \"Permalink to this heading\") ===========================================================================\\nFuriosa SDK 0.9.0 is a major release, including many performance enhancements, additional functions, and bug fixes. In partcular, 0.9.0 release includes the significant improvements of the quantization tools.\\nComponent Version Information\\n[\\uf0c1](#id1 \"Permalink to this table\")\\n| Package Name | Version | | --- | --- | | NPU Driver | 1.7.0 | | NPU Firmware Tools | 1.4.0 | | NPU Firmware Image | 1.7.0 | | HAL (Hardware Abstraction Layer) | 0.11.0 | | Furiosa Compiler | 0.9.0 | | Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..) | 0.9.0 | | NPU Management CLI (furiosactl) | 0.11.0 | | NPU Device Plugin | 0.10.1 | | NPU Feature Discovery | 0.2.0 |\\nInstalling the latest SDK [\\uf0c1](#installing-the-latest-sdk \"Permalink to this heading\") -------------------------------------------------------------------------------------\\nIf you are using APT repository, the upgrade process is simpler.\\n``` apt-get update && apt-get upgrade\\n```\\nIf you wish to designate a specific package for upgrade, execute as below: You can find more details about APT repository setup at [Driver, Firmware, and Runtime Installation](../software/installation.html#requiredpackages) .\\n``` apt-get update && \\\\ apt-get install -y furiosa-driver-pdma furiosa-libhal-warboy furiosa-libnux furiosactl\\n```\\nYou can upgrade firmware as follows:\\n``` apt-get update && \\\\ apt-get install -y furiosa-firmware-tools furiosa-firmware-image\\n```\\nYou can upgrade Python package as follows:\\n``` pip install --upgrade pip setuptools wheel pip install --upgrade furiosa-sdk\\n```\\nWarning\\nWhen installing or upgrading the furiosa-sdk without updating pip to the latest version, you may encounter the following errors.\\n``` ERROR: Could not find a version that satisfies the requirement furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk) (from versions: none) ERROR: No matching distribution found for furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk)\\n```\\nMajor changes [\\uf0c1](#major-changes \"Permalink to this heading\") -------------------------------------------------------------\\n### Quantization tool [\\uf0c1](#quantization-tool \"Permalink to this heading\")\\nQuantization tool is a library that converts a pre-trained model to a quantized model. You can refer to more details at [Model Quantization](../software/quantization.html#modelquantization) 0.9.0 release includes the API improvement and new calibration methods, possibly leading to better accuracy.\\n* Added new quantization-related APIs that are more flexible and solid. (   `furiosa.quantizer`   ,   `furiosa.optimizer`   )\\n``` optimized_onnx_model = optimize_model(source_onnx_model) calibrator = Calibrator(optimized_onnx_model, CalibrationMethod.MIN_MAX_ASYM) for calibration_data, _ in tqdm.tqdm(calibration_dataloader, desc=\"Calibration\", unit=\"images\", mininterval=0.5):   calibrator.collect_data([[calibration_data.numpy()]]) ranges = calibrator.compute_range() quantizated_graph = quantize(optimized_onnx_model, ranges)\\n```\\n* Added an option to decide whether to perform quantize at the beginning of the model.      + Instead of     `without_quantize`     being removed from the compiler options, it can be specified via the argument     `with_quantize`     to the     `quantize`     function. * The   `normalized_pixel_outputs`   argument to the   `quantize`   function can be set to convert the model output to uint8 instead of dequantizing to fp32.      + A tensor with an element range of     `(0.          ,          1.)`     can be optimized to convert to pixel data in uint8. * Provides more calibration methods.\\nSupported Calibration Methods\\n[\\uf0c1](#id2 \"Permalink to this table\")\\n| Calibration Method | Asymmetric | QuasiSymmetric | | --- | --- | --- | | Min-Max | MIN\\\\_MAX\\\\_ASYM | MIN\\\\_MAX\\\\_SYM | | Entropy | ENTROPY\\\\_ASYM | ENTROPY\\\\_SYM | | Percentile | PERCENTILE\\\\_ASYM | PERCENTILE\\\\_SYM | | Mean squared error | MSE\\\\_ASYM | MSE\\\\_SYM | | Signal-to-quantization-noise ratio | SQNR\\\\_ASYM | SQNR\\\\_SYM |\\nTo ensure the effectiveness of new calibration methods, we measured the accuracy of 10 popular models with the new calibration methods. Among them, 8 models showed better accuracy than the existing calibration methods. For example, the accuracy of EfficientNet-B0 increased by 57.452%. With the min-max calibration method, EfficientNet-B0 had an accuracy of 16.104%. In contrast, with the percentile calibration method, the accuracy was 73.556%. The details of the experiment results can be found at [Quantization Accuracy](../software/quantization.html#quantizationaccuracytable) .\\nFor more information on installing and using the new quantizer, you can refer to the following examples.\\n* [Tutorial: How to use Furiosa SDK from Start to Finish](https://github.com/furiosa-ai/furiosa-sdk/blob/v0.9.0/examples/notebooks/HowToUseFuriosaSDKFromStartToFinish.ipynb)\\n### Compiler [\\uf0c1](#compiler \"Permalink to this heading\")\\n* Added acceleration support for operators Lower, Unlower * Added acceleration support for operator Dequantize * Support for executing binaries that are larger than the hardware’s instruction memory * Improved scheduler and memory allocator to eliminate unnecessary I/O * Various improvements optimize compilation for better execution performance\\n### furiosa-toolkit [\\uf0c1](#furiosa-toolkit \"Permalink to this heading\")\\nThe `furiosactl` command-line tool included in the furiosa-toolkit 0.11.0 release includes improvements to the includes the following major improvements\\nThe newly added `furiosactl\\ntop` command is used to view utilization by NPU device over time.\\n``` $ furiosactl top --interval 200 NOTE: furiosa top is under development. Usage and output formats may change. Please enter Ctrl+C to stop. Datetime                        PID       Device        NPU(%)   Comp(%)   I/O(%)   Command 2023-03-21T09:45:56.699483936Z  152616    npu1pe0-1      19.06    100.00     0.00   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:56.906443888Z  152616    npu1pe0-1      51.09     93.05     6.95   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.110489333Z  152616    npu1pe0-1      46.40     97.98     2.02   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.316060982Z  152616    npu1pe0-1      51.43    100.00     0.00   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.521140588Z  152616    npu1pe0-1      54.28     94.10     5.90   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.725910558Z  152616    npu1pe0-1      48.93     98.93     1.07   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.935041998Z  152616    npu1pe0-1      47.91    100.00     0.00   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:58.13929122Z   152616    npu1pe0-1      49.06     94.94     5.06   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n```\\nThe `furiosactl\\ninfo` command has been improved to display concise information about each device. As before, you can enter the `--full` option if you want to see more information about a device.\\n``` $ furiosactl info +------+--------+----------------+-------+--------+--------------+ | NPU  | Name   | Firmware       | Temp. | Power  | PCI-BDF      | +------+--------+----------------+-------+--------+--------------+ | npu1 | warboy | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 | +------+--------+----------------+-------+--------+--------------+\\n$ furiosactl info --full +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+ | NPU  | Name   | UUID                                 | S/N               | Firmware       | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+ | npu1 | warboy | 00000000-0000-0000-0000-000000000000 | WBYB0000000000000 | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 | 511:0   | +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n```\\nMore information about installing and using `furiosactl` can be found in [furiosa-toolkit](../software/cli.html#toolkit) .\\n[Previous](0.10.0.html \"Release Notes - 0.10.0\") [Next](0.8.0.html \"Release Notes - 0.8.0\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Release Notes - 0.9.0\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/releases/0.9.0.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"release-notes-0-9-0\">\\n     <h1>\\n      Release Notes - 0.9.0\\n      <a class=\"headerlink\" href=\"#release-notes-0-9-0\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      Furiosa SDK 0.9.0 is a major release, including many performance enhancements,\\nadditional functions, and bug fixes.\\nIn partcular, 0.9.0 release includes the significant improvements of the quantization tools.\\n     </p>\\n     <table class=\"docutils align-default\" id=\"id1\">\\n      <caption>\\n       <span class=\"caption-text\">\\n        Component Version Information\\n       </span>\\n       <a class=\"headerlink\" href=\"#id1\" title=\"Permalink to this table\">\\n        \\uf0c1\\n       </a>\\n      </caption>\\n      <colgroup>\\n       <col style=\"width: 80%\"/>\\n       <col style=\"width: 20%\"/>\\n      </colgroup>\\n      <thead>\\n       <tr class=\"row-odd\">\\n        <th class=\"head\">\\n         <p>\\n          Package Name\\n         </p>\\n        </th>\\n        <th class=\"head\">\\n         <p>\\n          Version\\n         </p>\\n        </th>\\n       </tr>\\n      </thead>\\n      <tbody>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          NPU Driver\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          1.7.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          NPU Firmware Tools\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          1.4.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          NPU Firmware Image\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          1.7.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          HAL (Hardware Abstraction Layer)\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.11.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          Furiosa Compiler\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.9.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..)\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.9.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          NPU Management CLI (furiosactl)\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.11.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          NPU Device Plugin\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.10.1\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          NPU Feature Discovery\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.2.0\\n         </p>\\n        </td>\\n       </tr>\\n      </tbody>\\n     </table>\\n     <section id=\"installing-the-latest-sdk\">\\n      <h2>\\n       Installing the latest SDK\\n       <a class=\"headerlink\" href=\"#installing-the-latest-sdk\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       If you are using APT repository, the upgrade process is simpler.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>apt-get<span class=\"w\"> </span>update<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span>apt-get<span class=\"w\"> </span>upgrade\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       If you wish to designate a specific package for upgrade, execute as below:\\nYou can find more details about APT repository setup at\\n       <a class=\"reference internal\" href=\"../software/installation.html#requiredpackages\">\\n        <span class=\"std std-ref\">\\n         Driver, Firmware, and Runtime Installation\\n        </span>\\n       </a>\\n       .\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>apt-get<span class=\"w\"> </span>update<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span><span class=\"se\">\\\\</span>\\napt-get<span class=\"w\"> </span>install<span class=\"w\"> </span>-y<span class=\"w\"> </span>furiosa-driver-pdma<span class=\"w\"> </span>furiosa-libhal-warboy<span class=\"w\"> </span>furiosa-libnux<span class=\"w\"> </span>furiosactl\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       You can upgrade firmware as follows:\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>apt-get<span class=\"w\"> </span>update<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span><span class=\"se\">\\\\</span>\\napt-get<span class=\"w\"> </span>install<span class=\"w\"> </span>-y<span class=\"w\"> </span>furiosa-firmware-tools<span class=\"w\"> </span>furiosa-firmware-image\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       You can upgrade Python package as follows:\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>--upgrade<span class=\"w\"> </span>pip<span class=\"w\"> </span>setuptools<span class=\"w\"> </span>wheel\\npip<span class=\"w\"> </span>install<span class=\"w\"> </span>--upgrade<span class=\"w\"> </span>furiosa-sdk\\n</pre>\\n       </div>\\n      </div>\\n      <div class=\"admonition warning\">\\n       <p class=\"admonition-title\">\\n        Warning\\n       </p>\\n       <p>\\n        When installing or upgrading the furiosa-sdk without updating pip to the latest version, you may encounter the following errors.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>ERROR:<span class=\"w\"> </span>Could<span class=\"w\"> </span>not<span class=\"w\"> </span>find<span class=\"w\"> </span>a<span class=\"w\"> </span>version<span class=\"w\"> </span>that<span class=\"w\"> </span>satisfies<span class=\"w\"> </span>the<span class=\"w\"> </span>requirement<span class=\"w\"> </span>furiosa-quantizer-impl<span class=\"o\">==</span><span class=\"m\">0</span>.9.*<span class=\"w\"> </span><span class=\"o\">(</span>from<span class=\"w\"> </span>furiosa-quantizer<span class=\"o\">==</span><span class=\"m\">0</span>.9.*-&gt;furiosa-sdk<span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span>from<span class=\"w\"> </span>versions:<span class=\"w\"> </span>none<span class=\"o\">)</span>\\nERROR:<span class=\"w\"> </span>No<span class=\"w\"> </span>matching<span class=\"w\"> </span>distribution<span class=\"w\"> </span>found<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>furiosa-quantizer-impl<span class=\"o\">==</span><span class=\"m\">0</span>.9.*<span class=\"w\"> </span><span class=\"o\">(</span>from<span class=\"w\"> </span>furiosa-quantizer<span class=\"o\">==</span><span class=\"m\">0</span>.9.*-&gt;furiosa-sdk<span class=\"o\">)</span>\\n</pre>\\n        </div>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"major-changes\">\\n      <h2>\\n       Major changes\\n       <a class=\"headerlink\" href=\"#major-changes\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <section id=\"quantization-tool\">\\n       <h3>\\n        Quantization tool\\n        <a class=\"headerlink\" href=\"#quantization-tool\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Quantization tool is a library that converts a pre-trained model to a quantized model.\\nYou can refer to more details at\\n        <a class=\"reference internal\" href=\"../software/quantization.html#modelquantization\">\\n         <span class=\"std std-ref\">\\n          Model Quantization\\n         </span>\\n        </a>\\n        0.9.0 release includes the API improvement and new calibration methods,\\npossibly leading to better accuracy.\\n       </p>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          Added new quantization-related APIs that are more flexible and solid. (\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            furiosa.quantizer\\n           </span>\\n          </code>\\n          ,\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            furiosa.optimizer\\n           </span>\\n          </code>\\n          )\\n         </p>\\n        </li>\\n       </ul>\\n       <div class=\"highlight-python notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"n\">optimized_onnx_model</span> <span class=\"o\">=</span> <span class=\"n\">optimize_model</span><span class=\"p\">(</span><span class=\"n\">source_onnx_model</span><span class=\"p\">)</span>\\n<span class=\"n\">calibrator</span> <span class=\"o\">=</span> <span class=\"n\">Calibrator</span><span class=\"p\">(</span><span class=\"n\">optimized_onnx_model</span><span class=\"p\">,</span> <span class=\"n\">CalibrationMethod</span><span class=\"o\">.</span><span class=\"n\">MIN_MAX_ASYM</span><span class=\"p\">)</span>\\n<span class=\"k\">for</span> <span class=\"n\">calibration_data</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"o\">.</span><span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"n\">calibration_dataloader</span><span class=\"p\">,</span> <span class=\"n\">desc</span><span class=\"o\">=</span><span class=\"s2\">\"Calibration\"</span><span class=\"p\">,</span> <span class=\"n\">unit</span><span class=\"o\">=</span><span class=\"s2\">\"images\"</span><span class=\"p\">,</span> <span class=\"n\">mininterval</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">):</span>\\n  <span class=\"n\">calibrator</span><span class=\"o\">.</span><span class=\"n\">collect_data</span><span class=\"p\">([[</span><span class=\"n\">calibration_data</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">()]])</span>\\n<span class=\"n\">ranges</span> <span class=\"o\">=</span> <span class=\"n\">calibrator</span><span class=\"o\">.</span><span class=\"n\">compute_range</span><span class=\"p\">()</span>\\n<span class=\"n\">quantizated_graph</span> <span class=\"o\">=</span> <span class=\"n\">quantize</span><span class=\"p\">(</span><span class=\"n\">optimized_onnx_model</span><span class=\"p\">,</span> <span class=\"n\">ranges</span><span class=\"p\">)</span>\\n</pre>\\n        </div>\\n       </div>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          Added an option to decide whether to perform quantize at the beginning of the model.\\n         </p>\\n         <ul>\\n          <li>\\n           <p>\\n            Instead of\\n            <code class=\"docutils literal notranslate\">\\n             <span class=\"pre\">\\n              without_quantize\\n             </span>\\n            </code>\\n            being removed from the compiler options, it can be specified via the argument\\n            <code class=\"docutils literal notranslate\">\\n             <span class=\"pre\">\\n              with_quantize\\n             </span>\\n            </code>\\n            to the\\n            <code class=\"docutils literal notranslate\">\\n             <span class=\"pre\">\\n              quantize\\n             </span>\\n            </code>\\n            function.\\n           </p>\\n          </li>\\n         </ul>\\n        </li>\\n        <li>\\n         <p>\\n          The\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            normalized_pixel_outputs\\n           </span>\\n          </code>\\n          argument to the\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            quantize\\n           </span>\\n          </code>\\n          function can be set to convert the model output to uint8 instead of dequantizing to fp32.\\n         </p>\\n         <ul>\\n          <li>\\n           <p>\\n            A tensor with an element range of\\n            <code class=\"docutils literal notranslate\">\\n             <span class=\"pre\">\\n              (0.\\n             </span>\\n             <span class=\"pre\">\\n              ,\\n             </span>\\n             <span class=\"pre\">\\n              1.)\\n             </span>\\n            </code>\\n            can be optimized to convert to pixel data in uint8.\\n           </p>\\n          </li>\\n         </ul>\\n        </li>\\n        <li>\\n         <p>\\n          Provides more calibration methods.\\n         </p>\\n        </li>\\n       </ul>\\n       <table class=\"docutils align-default\" id=\"id2\">\\n        <caption>\\n         <span class=\"caption-text\">\\n          Supported Calibration Methods\\n         </span>\\n         <a class=\"headerlink\" href=\"#id2\" title=\"Permalink to this table\">\\n          \\uf0c1\\n         </a>\\n        </caption>\\n        <colgroup>\\n         <col style=\"width: 75%\"/>\\n         <col style=\"width: 13%\"/>\\n         <col style=\"width: 13%\"/>\\n        </colgroup>\\n        <thead>\\n         <tr class=\"row-odd\">\\n          <th class=\"head\">\\n           <p>\\n            Calibration Method\\n           </p>\\n          </th>\\n          <th class=\"head\">\\n           <p>\\n            Asymmetric\\n           </p>\\n          </th>\\n          <th class=\"head\">\\n           <p>\\n            QuasiSymmetric\\n           </p>\\n          </th>\\n         </tr>\\n        </thead>\\n        <tbody>\\n         <tr class=\"row-even\">\\n          <td>\\n           <p>\\n            Min-Max\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            MIN_MAX_ASYM\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            MIN_MAX_SYM\\n           </p>\\n          </td>\\n         </tr>\\n         <tr class=\"row-odd\">\\n          <td>\\n           <p>\\n            Entropy\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            ENTROPY_ASYM\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            ENTROPY_SYM\\n           </p>\\n          </td>\\n         </tr>\\n         <tr class=\"row-even\">\\n          <td>\\n           <p>\\n            Percentile\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            PERCENTILE_ASYM\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            PERCENTILE_SYM\\n           </p>\\n          </td>\\n         </tr>\\n         <tr class=\"row-odd\">\\n          <td>\\n           <p>\\n            Mean squared error\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            MSE_ASYM\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            MSE_SYM\\n           </p>\\n          </td>\\n         </tr>\\n         <tr class=\"row-even\">\\n          <td>\\n           <p>\\n            Signal-to-quantization-noise ratio\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            SQNR_ASYM\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            SQNR_SYM\\n           </p>\\n          </td>\\n         </tr>\\n        </tbody>\\n       </table>\\n       <p>\\n        To ensure the effectiveness of new calibration methods,\\nwe measured the accuracy of 10 popular models with the new calibration methods.\\nAmong them, 8 models showed better accuracy than the existing calibration methods.\\nFor example, the accuracy of EfficientNet-B0 increased by 57.452%.\\nWith the min-max calibration method, EfficientNet-B0 had an accuracy of 16.104%.\\nIn contrast, with the percentile calibration method, the accuracy was 73.556%.\\nThe details of the experiment results can be found at\\n        <a class=\"reference internal\" href=\"../software/quantization.html#quantizationaccuracytable\">\\n         <span class=\"std std-ref\">\\n          Quantization Accuracy\\n         </span>\\n        </a>\\n        .\\n       </p>\\n       <p>\\n        For more information on installing and using the new quantizer, you can refer to the following examples.\\n       </p>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/blob/v0.9.0/examples/notebooks/HowToUseFuriosaSDKFromStartToFinish.ipynb\">\\n           Tutorial: How to use Furiosa SDK from Start to Finish\\n          </a>\\n         </p>\\n        </li>\\n       </ul>\\n      </section>\\n      <section id=\"compiler\">\\n       <h3>\\n        Compiler\\n        <a class=\"headerlink\" href=\"#compiler\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          Added acceleration support for operators Lower, Unlower\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Added acceleration support for operator Dequantize\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Support for executing binaries that are larger than the hardware’s instruction memory\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Improved scheduler and memory allocator to eliminate unnecessary I/O\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Various improvements optimize compilation for better execution performance\\n         </p>\\n        </li>\\n       </ul>\\n      </section>\\n      <section id=\"furiosa-toolkit\">\\n       <h3>\\n        furiosa-toolkit\\n        <a class=\"headerlink\" href=\"#furiosa-toolkit\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        The\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosactl\\n         </span>\\n        </code>\\n        command-line tool included in the furiosa-toolkit 0.11.0 release includes improvements to the\\nincludes the following major improvements\\n       </p>\\n       <p>\\n        The newly added\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosactl\\n         </span>\\n         <span class=\"pre\">\\n          top\\n         </span>\\n        </code>\\n        command is used to view utilization by NPU device over time.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>$<span class=\"w\"> </span>furiosactl<span class=\"w\"> </span>top<span class=\"w\"> </span>--interval<span class=\"w\"> </span><span class=\"m\">200</span>\\nNOTE:<span class=\"w\"> </span>furiosa<span class=\"w\"> </span>top<span class=\"w\"> </span>is<span class=\"w\"> </span>under<span class=\"w\"> </span>development.<span class=\"w\"> </span>Usage<span class=\"w\"> </span>and<span class=\"w\"> </span>output<span class=\"w\"> </span>formats<span class=\"w\"> </span>may<span class=\"w\"> </span>change.\\nPlease<span class=\"w\"> </span>enter<span class=\"w\"> </span>Ctrl+C<span class=\"w\"> </span>to<span class=\"w\"> </span>stop.\\nDatetime<span class=\"w\">                        </span>PID<span class=\"w\">       </span>Device<span class=\"w\">        </span>NPU<span class=\"o\">(</span>%<span class=\"o\">)</span><span class=\"w\">   </span>Comp<span class=\"o\">(</span>%<span class=\"o\">)</span><span class=\"w\">   </span>I/O<span class=\"o\">(</span>%<span class=\"o\">)</span><span class=\"w\">   </span>Command\\n<span class=\"m\">2023</span>-03-21T09:45:56.699483936Z<span class=\"w\">  </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">19</span>.06<span class=\"w\">    </span><span class=\"m\">100</span>.00<span class=\"w\">     </span><span class=\"m\">0</span>.00<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n<span class=\"m\">2023</span>-03-21T09:45:56.906443888Z<span class=\"w\">  </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">51</span>.09<span class=\"w\">     </span><span class=\"m\">93</span>.05<span class=\"w\">     </span><span class=\"m\">6</span>.95<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n<span class=\"m\">2023</span>-03-21T09:45:57.110489333Z<span class=\"w\">  </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">46</span>.40<span class=\"w\">     </span><span class=\"m\">97</span>.98<span class=\"w\">     </span><span class=\"m\">2</span>.02<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n<span class=\"m\">2023</span>-03-21T09:45:57.316060982Z<span class=\"w\">  </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">51</span>.43<span class=\"w\">    </span><span class=\"m\">100</span>.00<span class=\"w\">     </span><span class=\"m\">0</span>.00<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n<span class=\"m\">2023</span>-03-21T09:45:57.521140588Z<span class=\"w\">  </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">54</span>.28<span class=\"w\">     </span><span class=\"m\">94</span>.10<span class=\"w\">     </span><span class=\"m\">5</span>.90<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n<span class=\"m\">2023</span>-03-21T09:45:57.725910558Z<span class=\"w\">  </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">48</span>.93<span class=\"w\">     </span><span class=\"m\">98</span>.93<span class=\"w\">     </span><span class=\"m\">1</span>.07<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n<span class=\"m\">2023</span>-03-21T09:45:57.935041998Z<span class=\"w\">  </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">47</span>.91<span class=\"w\">    </span><span class=\"m\">100</span>.00<span class=\"w\">     </span><span class=\"m\">0</span>.00<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n<span class=\"m\">2023</span>-03-21T09:45:58.13929122Z<span class=\"w\">   </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">49</span>.06<span class=\"w\">     </span><span class=\"m\">94</span>.94<span class=\"w\">     </span><span class=\"m\">5</span>.06<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        The\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosactl\\n         </span>\\n         <span class=\"pre\">\\n          info\\n         </span>\\n        </code>\\n        command has been improved to display concise information about each device. As before, you can enter the\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          --full\\n         </span>\\n        </code>\\n        option if you want to see more information about a device.\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>$ furiosactl info\\n+------+--------+----------------+-------+--------+--------------+\\n| NPU  | Name   | Firmware       | Temp. | Power  | PCI-BDF      |\\n+------+--------+----------------+-------+--------+--------------+\\n| npu1 | warboy | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 |\\n+------+--------+----------------+-------+--------+--------------+\\n\\n$ furiosactl info --full\\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n| NPU  | Name   | UUID                                 | S/N               | Firmware       | Temp. | Power  | PCI-BDF      | PCI-DEV |\\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n| npu1 | warboy | 00000000-0000-0000-0000-000000000000 | WBYB0000000000000 | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 | 511:0   |\\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        More information about installing and using\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosactl\\n         </span>\\n        </code>\\n        can be found in\\n        <a class=\"reference internal\" href=\"../software/cli.html#toolkit\">\\n         <span class=\"std std-ref\">\\n          furiosa-toolkit\\n         </span>\\n        </a>\\n        .\\n       </p>\\n      </section>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"0.10.0.html\" rel=\"prev\" title=\"Release Notes - 0.10.0\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"0.8.0.html\" rel=\"next\" title=\"Release Notes - 0.8.0\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='64f0ffea-6087-4f27-8889-83479e61e89e', link=Url('https://furiosa-ai.github.io/docs/latest/en/software/profiler.html'), name='profiler', parent='', child=[], description='\\n\\n\\n* Performance Profiling\\n* [View page source](../_sources/software/profiler.rst.txt)\\n\\n---\\n\\n\\n\\nPerformance Profiling\\n[\\uf0c1](#performance-profiling \"Permalink to this heading\")\\n=============================================================================\\n\\nLow latency and high throughput performance are critical factors in many DNN applications.\\nFor performance optimization, model developers and ML engineers must understand the model performance and be able to analyze bottlenecks.\\nTo assist developers with this process, Furiosa SDK provides a profiling tool.\\n\\nTrace Analysis\\n[\\uf0c1](#trace-analysis \"Permalink to this heading\")\\n---------------------------------------------------------------\\n\\nTrace analysis provides structured data on execution time by step, by actually executing model inference task.\\nYou can also visualize the data using the\\n[Trace Event Profiling Tool](https://www.chromium.org/developers/how-tos/trace-event-profiling-tool/)\\nfunction of the Chrome web browser.\\n\\nThough small, trace generation generates temporal overheads as it measures time for each step and writes the results to a file.\\nIt is thus not enabled by default. You can create trace by using one of the following methods.\\n\\n\\nTracing via Environment Variable\\n[\\uf0c1](#tracing-via-environment-variable \"Permalink to this heading\")\\n---------------------------------------------------------------------------------------------------\\n\\nYou can enable trace generation by setting the path of the file to which the trace result will be written in\\n`FURIOSA_PROFILER_OUTPUT_PATH`\\n. The advantage of this method is that the code remains unchanged. The downside is that you cannot set a specific section or category for measurement.\\n\\n```\\ngit clone https://github.com/furiosa-ai/furiosa-sdk\\ncd furiosa-sdk/examples/inferences\\nexport FURIOSA_PROFILER_OUTPUT_PATH=`pwd`/tracing.json\\n./image_classify.py ../assets/images/car.jpg\\n\\nls -l ./tracing.json\\n-rw-r--r-- 1 furiosa furiosa 456493 Jul 27 17:56 ./tracing.json\\n\\n```\\n\\nIf you enable trace generation through environment variables as described above, a JSON file will be written to the path specified by the environment variable\\n`FURIOSA_PROFILER_OUTPUT_PATH`\\n.\\nIf you enter\\n`chrome://tracing`\\nin Chrome’s address bar, the trace viewer will start. Click the\\n`Load`\\nbutton in the upper left corner of the trace viewer, and select the saved file (\\n`tracing.json`\\nin the example above) to view the trace result.\\n\\n\\nTracing via Profiler Context\\n[\\uf0c1](#tracing-via-profiler-context \"Permalink to this heading\")\\n-------------------------------------------------------------------------------------------\\n\\nYou can also trace a model inference performance by using a Profiler Context in your Python code. The advantages of this method, in comparison to the tracing by environment variable, are as follows:\\n\\n* Allow to enable trace immediately even in interactive environments, such as Python Interpreter or Jupyter Notebook\\n* Allow to specify labels to certain inference runs\\n* Allow to measure specified operator categories selectively\\n\\n```\\n#!/usr/bin/env python\\n\\nimport numpy as np\\nfrom furiosa.runtime.profiler import profile\\nfrom furiosa.runtime.sync import create_runner\\n\\n# You can find \\'examples\\' directory of the root of furiosa-sdk source tree\\nmodel_path = \"examples/assets/quantized_models/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\"\\n\\nwith open(\"mobilenet_v1_trace.json\", \"w\") as output:\\n    with profile(file=output) as profiler:\\n        with create_runner(model_path) as runner:\\n            input_shape = runner.model.input(0).shape\\n\\n            with profiler.record(\"warm up\") as record:\\n                for _ in range(0, 2):\\n                    runner.run([np.uint8(np.random.rand(*input_shape))])\\n\\n            with profiler.record(\"trace\") as record:\\n                for _ in range(0, 2):\\n                    runner.run([np.uint8(np.random.rand(*input_shape))])\\n\\n```\\n\\nThe above is a code example using a profiling context. Once the above Python code is executed, the\\nmnist\\\\_trace.json\\n\\nfile is created. The trace results are labelled ‘warm up’ and ‘trace’ as shown below.\\n\\n### Pause/Resume of Profiler Context [\\uf0c1](#pause-resume-of-profiler-context \"Permalink to this heading\")\\n\\nTracing long-running jobs can cause following problems:\\n\\n* Produce large trace files which take huge disk space and are difficult to be shared.\\n* Make it hard to identify interesting section when the trace is visualized, without additional processing.\\n* Take much time to produce trace files.\\n\\nTo avoid the above issues, the profiler provides an additional API to temporarily pause or resume a profiler within the context.\\nUsers can exclude execution they do not want to profile, thereby reducing profiling overhead and trace file size.\\n\\nThe below is an example of pausing profiler not to trace\\n`warm\\n\\nup`\\nphase between\\n`profile.pause`\\nand\\n`profile.resume`\\n.\\n\\n```\\n#!/usr/bin/env python\\n\\nimport numpy as np\\nfrom furiosa.runtime.profiler import RecordFormat, profile\\nfrom furiosa.runtime.sync import create_runner\\n\\n# You can find \\'examples\\' directory of the root of furiosa-sdk source tree\\nmodel_path = \"examples/assets/quantized_models/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\"\\n\\nwith profile(format=RecordFormat.PandasDataFrame) as profiler:\\n    with create_runner(model_path) as runner:\\n        input_shape = runner.model.input(0).shape\\n\\n        # pause profiling during warmup\\n        profiler.pause()\\n\\n        for _ in range(0, 10):\\n            with profiler.record(\"warm up\") as record:\\n                runner.run([np.uint8(np.random.rand(*input_shape))])\\n\\n        # resume profiling\\n        profiler.resume()\\n\\n        with profiler.record(\"trace\") as record:\\n            runner.run([np.uint8(np.random.rand(*input_shape))])\\n\\ndf = profiler.get_pandas_dataframe()\\n\\nassert len(df[df[\"name\"] == \"trace\"]) == 1\\nassert len(df[df[\"name\"] == \"warm up\"]) == 0\\n\\n```\\n\\n\\n\\n### Trace analysis using Pandas DataFrame [\\uf0c1](#trace-analysis-using-pandas-dataframe \"Permalink to this heading\")\\n\\nWith the measured tracing data, in addition to visualizing it with Chrome Trace Format, it can also be expressed and used in Pandas DataFrame, commonly used for data analysis. These are the advantages in comparison to Chrome Trace Format.\\n\\n* Can be used directly in Python Interpreter or Jupyter Notebook interactive shell\\n* Users can directly access DataFrame for analysis, on top of the reporting function which is provided as default\\n\\n```\\n#!/usr/bin/env python\\n\\nimport numpy as np\\nfrom furiosa.runtime.profiler import RecordFormat, profile\\nfrom furiosa.runtime.sync import create_runner\\n\\n# You can find \\'examples\\' directory of the root of furiosa-sdk source tree\\nmodel_path = \"examples/assets/quantized_models/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\"\\n\\nwith profile(format=RecordFormat.PandasDataFrame) as profiler:\\n    with create_runner(model_path) as runner:\\n        input_shape = runner.model.input(0).shape\\n\\n        with profiler.record(\"warm up\") as record:\\n            for _ in range(0, 2):\\n                runner.run([np.uint8(np.random.rand(*input_shape))])\\n\\n        with profiler.record(\"trace\") as record:\\n            for _ in range(0, 2):\\n                runner.run([np.uint8(np.random.rand(*input_shape))])\\n\\nprofiler.print_summary()  # (1)\\n\\nprofiler.print_inferences()  # (2)\\n\\nprofiler.print_npu_executions()  # (3)\\n\\nprofiler.print_npu_operators()  # (4)\\n\\nprofiler.print_external_operators()  # (5)\\n\\ndf = profiler.get_pandas_dataframe()  # (6)\\nprint(df[df[\"name\"] == \"trace\"][[\"trace_id\", \"name\", \"thread.id\", \"dur\"]])\\n\\n```\\n\\nAbove is a code example that designates a profiling context format into PandasDataFrame.\\n\\nWhen\\n`(1)`\\nline is executed, the following summary of the results is produced.\\n\\n```\\n================================================\\n  Inference Results Summary\\n================================================\\nInference counts                : 4\\nMin latency (ns)                : 1584494\\nMax latency (ns)                : 3027309\\nMean latency (ns)               : 2136984\\nMedian latency (ns)             : 1968066\\n90.0 percentile Latency (ns)    : 2752525\\n95.0 percentile Latency (ns)    : 2889917\\n97.0 percentile Latency (ns)    : 2944874\\n99.0 percentile Latency (ns)    : 2999831\\n99.9 percentile Latency (ns)    : 3024561\\n\\n```\\n\\nWhen\\n`(2)`\\nline is executed, duration of one inference query is shown.\\n\\n```\\n┌──────────────────────────────────┬──────────────────┬───────────┬─────────┐\\n│ trace_id                         ┆ span_id          ┆ thread.id ┆ dur     │\\n╞══════════════════════════════════╪══════════════════╪═══════════╪═════════╡\\n│ 7cf3d3b7439cf4c3fac1a47998783102 ┆ 403ada67f1d8220e ┆ 1         ┆ 3027309 │\\n│ 16d65f6f8f1db256d0f39953855dea72 ┆ 78b065c19c3675ef ┆ 1         ┆ 2111363 │\\n│ d0534e3a9f19edadab81954ad28ab44f ┆ 9a7addaf0f28c9fe ┆ 1         ┆ 1824769 │\\n│ 70512188522f45b87cfe4f545de3cf2c ┆ c75f697f8e72d333 ┆ 1         ┆ 1584494 │\\n└──────────────────────────────────┴──────────────────┴───────────┴─────────┘\\n\\n```\\n\\nWhen\\n`(3)`\\nline is executed, elapsed times of NPU executions will be shown:\\n\\n```\\n┌──────────────────────────────────┬──────────────────┬──────────┬─────────────────┬───────────┬─────────┬──────────────────────┐\\n│ trace_id                         ┆ span_id          ┆ pe_index ┆ execution_index ┆ NPU Total ┆ NPU Run ┆ NPU IoWait           │\\n╞══════════════════════════════════╪══════════════════╪══════════╪═════════════════╪═══════════╪═════════╪══════════════════════╡\\n│ 8f6fce6c0e52b4735cae3379732a0943 ┆ 3e1e4a76523cbf89 ┆ 0        ┆ 0               ┆ 119145    ┆ 108134  ┆ 18446744073709540605 │\\n│ 195366613b1da9b0350c0a3c2a608f42 ┆ 07dff2e92172fabd ┆ 0        ┆ 0               ┆ 119363    ┆ 108134  ┆ 18446744073709540387 │\\n│ 3b65b8fa3eabfaf8f815ec9f41fcc7d9 ┆ 639a366a7f932a23 ┆ 0        ┆ 0               ┆ 119157    ┆ 108134  ┆ 18446744073709540593 │\\n│ e48825df32a07e5559f7f50048c08e1f ┆ ecaab4915bfda725 ┆ 0        ┆ 0               ┆ 119219    ┆ 108134  ┆ 18446744073709540531 │\\n└──────────────────────────────────┴──────────────────┴──────────┴─────────────────┴───────────┴─────────┴──────────────────────┘\\n\\n```\\n\\nWhen\\n`(4)`\\nline is executed, elapsed times of operators will be shown:\\n\\n```\\n┌─────────────────────────┬──────────────────────┬───────┐\\n│ name                    ┆ average_elapsed (ns) ┆ count │\\n╞═════════════════════════╪══════════════════════╪═══════╡\\n│ LowLevelConv2d          ┆ 5327.8               ┆ 60    │\\n│ LowLevelDepthwiseConv2d ┆ 1412.285714          ┆ 56    │\\n│ LowLevelPad             ┆ 575.785714           ┆ 56    │\\n│ LowLevelTranspose       ┆ 250.0                ┆ 4     │\\n│ LowLevelReshape         ┆ 2.0                  ┆ 240   │\\n│ LowLevelSlice           ┆ 2.0                  ┆ 12    │\\n│ LowLevelExpand          ┆ 2.0                  ┆ 16    │\\n└─────────────────────────┴──────────────────────┴───────┘\\n\\n```\\n\\nWhen\\n`(5)`\\nline is executed, the time data for operators in the CPU is shown as below.\\n\\n```\\n┌──────────────────────────────────┬──────────────────┬───────────┬────────────┬────────────────┬────────┐\\n│ trace_id                         ┆ span_id          ┆ thread.id ┆ name       ┆ operator_index ┆ dur    │\\n╞══════════════════════════════════╪══════════════════╪═══════════╪════════════╪════════════════╪════════╡\\n│ e7ab6656cc090a8d05992a9e4683b8b7 ┆ 206a1d6f351ca4b1 ┆ 40        ┆ Quantize   ┆ 0              ┆ 136285 │\\n│ 03636fd6c7dbc42f0a9dd29a7283d3fc ┆ f636740983e095a6 ┆ 40        ┆ Lower      ┆ 1              ┆ 133350 │\\n│ c9a0858f7e0885a976f51c6cb57d3e0f ┆ bb6c84f88e453055 ┆ 40        ┆ Unlower    ┆ 2              ┆ 44775  │\\n│ 8777c67ad9fe597139bbd6970362c2fc ┆ 63bac982c7b98aba ┆ 40        ┆ Dequantize ┆ 3              ┆ 14682  │\\n│ 98aeba2a25b0525166b6a4065ab01774 ┆ 34ccd560571d733f ┆ 40        ┆ Quantize   ┆ 0              ┆ 45465  │\\n│ 420525dc13ba9624083e0a276f7ee718 ┆ 9f6d342da5eb86bc ┆ 40        ┆ Lower      ┆ 1              ┆ 152748 │\\n│ cb67393f6949bbbb396053c1e00931ff ┆ 2d724fa6ab8ca024 ┆ 40        ┆ Unlower    ┆ 2              ┆ 67140  │\\n│ 00424b4f02039ae0ca98388a964062b0 ┆ a5fb9fbd5bffe6a6 ┆ 40        ┆ Dequantize ┆ 3              ┆ 32388  │\\n│ d7412c59d360067e8b7a2508a30d1079 ┆ 8e426d778fa95722 ┆ 40        ┆ Quantize   ┆ 0              ┆ 71736  │\\n│ 6820acf9345c5b373c512f6cd5edcbc7 ┆ 2d787c2df381f010 ┆ 40        ┆ Lower      ┆ 1              ┆ 311310 │\\n│ 84d24b02a95c63c3e40f7682384749e4 ┆ 1236a974a619ff1a ┆ 40        ┆ Unlower    ┆ 2              ┆ 51930  │\\n│ 8d25dff1cfd6624509cbf95503e93382 ┆ 673efb3bfb8deac6 ┆ 40        ┆ Dequantize ┆ 3              ┆ 12362  │\\n│ 4cc60ec1eee7d9f3cdd290d07b303a18 ┆ e7903b0a584d6388 ┆ 40        ┆ Quantize   ┆ 0              ┆ 56736  │\\n│ c5f04d9fea26e5b52c6ec5e5406775fc ┆ 701118dabd065e6f ┆ 40        ┆ Lower      ┆ 1              ┆ 265447 │\\n│ c5fdfb9cf454da130148e8e364eeee93 ┆ 5cf3750def19c6e8 ┆ 40        ┆ Unlower    ┆ 2              ┆ 35869  │\\n│ e1e650d23061140404915f1df36daf9c ┆ ddd76ff19b5cd713 ┆ 40        ┆ Dequantize ┆ 3              ┆ 14688  │\\n└──────────────────────────────────┴──────────────────┴───────────┴────────────┴────────────────┴────────┘\\n\\n```\\n\\nWith line\\n`(6)`\\n, you can access DataFrame from the code and perform direct analysis.\\n\\n```\\n                            trace_id   name  thread.id       dur\\n487  f3b158734e3684f2e043ed41309c4c2d  trace          1  11204385\\n\\n```\\n\\n\\n\\n\\n\\n\\n[Previous](performance.html \"Performance Optimization\")\\n[Next](serving.html \"Model Server (Serving Framework)\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Performance Profiling * [View page source](../_sources/software/profiler.rst.txt)\\n---\\nPerformance Profiling [\\uf0c1](#performance-profiling \"Permalink to this heading\") =============================================================================\\nLow latency and high throughput performance are critical factors in many DNN applications. For performance optimization, model developers and ML engineers must understand the model performance and be able to analyze bottlenecks. To assist developers with this process, Furiosa SDK provides a profiling tool.\\nTrace Analysis [\\uf0c1](#trace-analysis \"Permalink to this heading\") ---------------------------------------------------------------\\nTrace analysis provides structured data on execution time by step, by actually executing model inference task. You can also visualize the data using the [Trace Event Profiling Tool](https://www.chromium.org/developers/how-tos/trace-event-profiling-tool/) function of the Chrome web browser.\\nThough small, trace generation generates temporal overheads as it measures time for each step and writes the results to a file. It is thus not enabled by default. You can create trace by using one of the following methods.\\nTracing via Environment Variable [\\uf0c1](#tracing-via-environment-variable \"Permalink to this heading\") ---------------------------------------------------------------------------------------------------\\nYou can enable trace generation by setting the path of the file to which the trace result will be written in `FURIOSA_PROFILER_OUTPUT_PATH` . The advantage of this method is that the code remains unchanged. The downside is that you cannot set a specific section or category for measurement.\\n``` git clone https://github.com/furiosa-ai/furiosa-sdk cd furiosa-sdk/examples/inferences export FURIOSA_PROFILER_OUTPUT_PATH=`pwd`/tracing.json ./image_classify.py ../assets/images/car.jpg\\nls -l ./tracing.json -rw-r--r-- 1 furiosa furiosa 456493 Jul 27 17:56 ./tracing.json\\n```\\nIf you enable trace generation through environment variables as described above, a JSON file will be written to the path specified by the environment variable `FURIOSA_PROFILER_OUTPUT_PATH` . If you enter `chrome://tracing` in Chrome’s address bar, the trace viewer will start. Click the `Load` button in the upper left corner of the trace viewer, and select the saved file ( `tracing.json` in the example above) to view the trace result.\\nTracing via Profiler Context [\\uf0c1](#tracing-via-profiler-context \"Permalink to this heading\") -------------------------------------------------------------------------------------------\\nYou can also trace a model inference performance by using a Profiler Context in your Python code. The advantages of this method, in comparison to the tracing by environment variable, are as follows:\\n* Allow to enable trace immediately even in interactive environments, such as Python Interpreter or Jupyter Notebook * Allow to specify labels to certain inference runs * Allow to measure specified operator categories selectively\\n``` #!/usr/bin/env python\\nimport numpy as np from furiosa.runtime.profiler import profile from furiosa.runtime.sync import create_runner\\n# You can find \\'examples\\' directory of the root of furiosa-sdk source tree model_path = \"examples/assets/quantized_models/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\"\\nwith open(\"mobilenet_v1_trace.json\", \"w\") as output:     with profile(file=output) as profiler:         with create_runner(model_path) as runner:             input_shape = runner.model.input(0).shape\\n            with profiler.record(\"warm up\") as record:                 for _ in range(0, 2):                     runner.run([np.uint8(np.random.rand(*input_shape))])\\n            with profiler.record(\"trace\") as record:                 for _ in range(0, 2):                     runner.run([np.uint8(np.random.rand(*input_shape))])\\n```\\nThe above is a code example using a profiling context. Once the above Python code is executed, the mnist\\\\_trace.json\\nfile is created. The trace results are labelled ‘warm up’ and ‘trace’ as shown below.\\n### Pause/Resume of Profiler Context [\\uf0c1](#pause-resume-of-profiler-context \"Permalink to this heading\")\\nTracing long-running jobs can cause following problems:\\n* Produce large trace files which take huge disk space and are difficult to be shared. * Make it hard to identify interesting section when the trace is visualized, without additional processing. * Take much time to produce trace files.\\nTo avoid the above issues, the profiler provides an additional API to temporarily pause or resume a profiler within the context. Users can exclude execution they do not want to profile, thereby reducing profiling overhead and trace file size.\\nThe below is an example of pausing profiler not to trace `warm\\nup` phase between `profile.pause` and `profile.resume` .\\n``` #!/usr/bin/env python\\nimport numpy as np from furiosa.runtime.profiler import RecordFormat, profile from furiosa.runtime.sync import create_runner\\n# You can find \\'examples\\' directory of the root of furiosa-sdk source tree model_path = \"examples/assets/quantized_models/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\"\\nwith profile(format=RecordFormat.PandasDataFrame) as profiler:     with create_runner(model_path) as runner:         input_shape = runner.model.input(0).shape\\n        # pause profiling during warmup         profiler.pause()\\n        for _ in range(0, 10):             with profiler.record(\"warm up\") as record:                 runner.run([np.uint8(np.random.rand(*input_shape))])\\n        # resume profiling         profiler.resume()\\n        with profiler.record(\"trace\") as record:             runner.run([np.uint8(np.random.rand(*input_shape))])\\ndf = profiler.get_pandas_dataframe()\\nassert len(df[df[\"name\"] == \"trace\"]) == 1 assert len(df[df[\"name\"] == \"warm up\"]) == 0\\n```\\n### Trace analysis using Pandas DataFrame [\\uf0c1](#trace-analysis-using-pandas-dataframe \"Permalink to this heading\")\\nWith the measured tracing data, in addition to visualizing it with Chrome Trace Format, it can also be expressed and used in Pandas DataFrame, commonly used for data analysis. These are the advantages in comparison to Chrome Trace Format.\\n* Can be used directly in Python Interpreter or Jupyter Notebook interactive shell * Users can directly access DataFrame for analysis, on top of the reporting function which is provided as default\\n``` #!/usr/bin/env python\\nimport numpy as np from furiosa.runtime.profiler import RecordFormat, profile from furiosa.runtime.sync import create_runner\\n# You can find \\'examples\\' directory of the root of furiosa-sdk source tree model_path = \"examples/assets/quantized_models/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\"\\nwith profile(format=RecordFormat.PandasDataFrame) as profiler:     with create_runner(model_path) as runner:         input_shape = runner.model.input(0).shape\\n        with profiler.record(\"warm up\") as record:             for _ in range(0, 2):                 runner.run([np.uint8(np.random.rand(*input_shape))])\\n        with profiler.record(\"trace\") as record:             for _ in range(0, 2):                 runner.run([np.uint8(np.random.rand(*input_shape))])\\nprofiler.print_summary()  # (1)\\nprofiler.print_inferences()  # (2)\\nprofiler.print_npu_executions()  # (3)\\nprofiler.print_npu_operators()  # (4)\\nprofiler.print_external_operators()  # (5)\\ndf = profiler.get_pandas_dataframe()  # (6) print(df[df[\"name\"] == \"trace\"][[\"trace_id\", \"name\", \"thread.id\", \"dur\"]])\\n```\\nAbove is a code example that designates a profiling context format into PandasDataFrame.\\nWhen `(1)` line is executed, the following summary of the results is produced.\\n``` ================================================   Inference Results Summary ================================================ Inference counts                : 4 Min latency (ns)                : 1584494 Max latency (ns)                : 3027309 Mean latency (ns)               : 2136984 Median latency (ns)             : 1968066 90.0 percentile Latency (ns)    : 2752525 95.0 percentile Latency (ns)    : 2889917 97.0 percentile Latency (ns)    : 2944874 99.0 percentile Latency (ns)    : 2999831 99.9 percentile Latency (ns)    : 3024561\\n```\\nWhen `(2)` line is executed, duration of one inference query is shown.\\n``` ┌──────────────────────────────────┬──────────────────┬───────────┬─────────┐ │ trace_id                         ┆ span_id          ┆ thread.id ┆ dur     │ ╞══════════════════════════════════╪══════════════════╪═══════════╪═════════╡ │ 7cf3d3b7439cf4c3fac1a47998783102 ┆ 403ada67f1d8220e ┆ 1         ┆ 3027309 │ │ 16d65f6f8f1db256d0f39953855dea72 ┆ 78b065c19c3675ef ┆ 1         ┆ 2111363 │ │ d0534e3a9f19edadab81954ad28ab44f ┆ 9a7addaf0f28c9fe ┆ 1         ┆ 1824769 │ │ 70512188522f45b87cfe4f545de3cf2c ┆ c75f697f8e72d333 ┆ 1         ┆ 1584494 │ └──────────────────────────────────┴──────────────────┴───────────┴─────────┘\\n```\\nWhen `(3)` line is executed, elapsed times of NPU executions will be shown:\\n``` ┌──────────────────────────────────┬──────────────────┬──────────┬─────────────────┬───────────┬─────────┬──────────────────────┐ │ trace_id                         ┆ span_id          ┆ pe_index ┆ execution_index ┆ NPU Total ┆ NPU Run ┆ NPU IoWait           │ ╞══════════════════════════════════╪══════════════════╪══════════╪═════════════════╪═══════════╪═════════╪══════════════════════╡ │ 8f6fce6c0e52b4735cae3379732a0943 ┆ 3e1e4a76523cbf89 ┆ 0        ┆ 0               ┆ 119145    ┆ 108134  ┆ 18446744073709540605 │ │ 195366613b1da9b0350c0a3c2a608f42 ┆ 07dff2e92172fabd ┆ 0        ┆ 0               ┆ 119363    ┆ 108134  ┆ 18446744073709540387 │ │ 3b65b8fa3eabfaf8f815ec9f41fcc7d9 ┆ 639a366a7f932a23 ┆ 0        ┆ 0               ┆ 119157    ┆ 108134  ┆ 18446744073709540593 │ │ e48825df32a07e5559f7f50048c08e1f ┆ ecaab4915bfda725 ┆ 0        ┆ 0               ┆ 119219    ┆ 108134  ┆ 18446744073709540531 │ └──────────────────────────────────┴──────────────────┴──────────┴─────────────────┴───────────┴─────────┴──────────────────────┘\\n```\\nWhen `(4)` line is executed, elapsed times of operators will be shown:\\n``` ┌─────────────────────────┬──────────────────────┬───────┐ │ name                    ┆ average_elapsed (ns) ┆ count │ ╞═════════════════════════╪══════════════════════╪═══════╡ │ LowLevelConv2d          ┆ 5327.8               ┆ 60    │ │ LowLevelDepthwiseConv2d ┆ 1412.285714          ┆ 56    │ │ LowLevelPad             ┆ 575.785714           ┆ 56    │ │ LowLevelTranspose       ┆ 250.0                ┆ 4     │ │ LowLevelReshape         ┆ 2.0                  ┆ 240   │ │ LowLevelSlice           ┆ 2.0                  ┆ 12    │ │ LowLevelExpand          ┆ 2.0                  ┆ 16    │ └─────────────────────────┴──────────────────────┴───────┘\\n```\\nWhen `(5)` line is executed, the time data for operators in the CPU is shown as below.\\n``` ┌──────────────────────────────────┬──────────────────┬───────────┬────────────┬────────────────┬────────┐ │ trace_id                         ┆ span_id          ┆ thread.id ┆ name       ┆ operator_index ┆ dur    │ ╞══════════════════════════════════╪══════════════════╪═══════════╪════════════╪════════════════╪════════╡ │ e7ab6656cc090a8d05992a9e4683b8b7 ┆ 206a1d6f351ca4b1 ┆ 40        ┆ Quantize   ┆ 0              ┆ 136285 │ │ 03636fd6c7dbc42f0a9dd29a7283d3fc ┆ f636740983e095a6 ┆ 40        ┆ Lower      ┆ 1              ┆ 133350 │ │ c9a0858f7e0885a976f51c6cb57d3e0f ┆ bb6c84f88e453055 ┆ 40        ┆ Unlower    ┆ 2              ┆ 44775  │ │ 8777c67ad9fe597139bbd6970362c2fc ┆ 63bac982c7b98aba ┆ 40        ┆ Dequantize ┆ 3              ┆ 14682  │ │ 98aeba2a25b0525166b6a4065ab01774 ┆ 34ccd560571d733f ┆ 40        ┆ Quantize   ┆ 0              ┆ 45465  │ │ 420525dc13ba9624083e0a276f7ee718 ┆ 9f6d342da5eb86bc ┆ 40        ┆ Lower      ┆ 1              ┆ 152748 │ │ cb67393f6949bbbb396053c1e00931ff ┆ 2d724fa6ab8ca024 ┆ 40        ┆ Unlower    ┆ 2              ┆ 67140  │ │ 00424b4f02039ae0ca98388a964062b0 ┆ a5fb9fbd5bffe6a6 ┆ 40        ┆ Dequantize ┆ 3              ┆ 32388  │ │ d7412c59d360067e8b7a2508a30d1079 ┆ 8e426d778fa95722 ┆ 40        ┆ Quantize   ┆ 0              ┆ 71736  │ │ 6820acf9345c5b373c512f6cd5edcbc7 ┆ 2d787c2df381f010 ┆ 40        ┆ Lower      ┆ 1              ┆ 311310 │ │ 84d24b02a95c63c3e40f7682384749e4 ┆ 1236a974a619ff1a ┆ 40        ┆ Unlower    ┆ 2              ┆ 51930  │ │ 8d25dff1cfd6624509cbf95503e93382 ┆ 673efb3bfb8deac6 ┆ 40        ┆ Dequantize ┆ 3              ┆ 12362  │ │ 4cc60ec1eee7d9f3cdd290d07b303a18 ┆ e7903b0a584d6388 ┆ 40        ┆ Quantize   ┆ 0              ┆ 56736  │ │ c5f04d9fea26e5b52c6ec5e5406775fc ┆ 701118dabd065e6f ┆ 40        ┆ Lower      ┆ 1              ┆ 265447 │ │ c5fdfb9cf454da130148e8e364eeee93 ┆ 5cf3750def19c6e8 ┆ 40        ┆ Unlower    ┆ 2              ┆ 35869  │ │ e1e650d23061140404915f1df36daf9c ┆ ddd76ff19b5cd713 ┆ 40        ┆ Dequantize ┆ 3              ┆ 14688  │ └──────────────────────────────────┴──────────────────┴───────────┴────────────┴────────────────┴────────┘\\n```\\nWith line `(6)` , you can access DataFrame from the code and perform direct analysis.\\n```                             trace_id   name  thread.id       dur 487  f3b158734e3684f2e043ed41309c4c2d  trace          1  11204385\\n```\\n[Previous](performance.html \"Performance Optimization\") [Next](serving.html \"Model Server (Serving Framework)\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Performance Profiling\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/profiler.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"performance-profiling\">\\n     <span id=\"profiling\">\\n     </span>\\n     <h1>\\n      Performance Profiling\\n      <a class=\"headerlink\" href=\"#performance-profiling\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      Low latency and high throughput performance are critical factors in many DNN applications.\\nFor performance optimization, model developers and ML engineers must understand the model performance and be able to analyze bottlenecks.\\nTo assist developers with this process, Furiosa SDK provides a profiling tool.\\n     </p>\\n     <section id=\"trace-analysis\">\\n      <h2>\\n       Trace Analysis\\n       <a class=\"headerlink\" href=\"#trace-analysis\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       Trace analysis provides structured data on execution time by step, by actually executing model inference task.\\nYou can also visualize the data using the\\n       <a class=\"reference external\" href=\"https://www.chromium.org/developers/how-tos/trace-event-profiling-tool/\">\\n        Trace Event Profiling Tool\\n       </a>\\n       function of the Chrome web browser.\\n      </p>\\n      <p>\\n       Though small, trace generation generates temporal overheads as it measures time for each step and writes the results to a file.\\nIt is thus not enabled by default. You can create trace by using one of the following methods.\\n      </p>\\n     </section>\\n     <section id=\"tracing-via-environment-variable\">\\n      <span id=\"profilerenabledbyenv\">\\n      </span>\\n      <h2>\\n       Tracing via Environment Variable\\n       <a class=\"headerlink\" href=\"#tracing-via-environment-variable\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       You can enable trace generation by setting the path of the file to which the trace result will be written in\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         FURIOSA_PROFILER_OUTPUT_PATH\\n        </span>\\n       </code>\\n       . The advantage of this method is that the code remains unchanged. The downside is that you cannot set a specific section or category for measurement.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>git<span class=\"w\"> </span>clone<span class=\"w\"> </span>https://github.com/furiosa-ai/furiosa-sdk\\n<span class=\"nb\">cd</span><span class=\"w\"> </span>furiosa-sdk/examples/inferences\\n<span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">FURIOSA_PROFILER_OUTPUT_PATH</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">pwd</span><span class=\"sb\">`</span>/tracing.json\\n./image_classify.py<span class=\"w\"> </span>../assets/images/car.jpg\\n\\nls<span class=\"w\"> </span>-l<span class=\"w\"> </span>./tracing.json\\n-rw-r--r--<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>furiosa<span class=\"w\"> </span>furiosa<span class=\"w\"> </span><span class=\"m\">456493</span><span class=\"w\"> </span>Jul<span class=\"w\"> </span><span class=\"m\">27</span><span class=\"w\"> </span><span class=\"m\">17</span>:56<span class=\"w\"> </span>./tracing.json\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       If you enable trace generation through environment variables as described above, a JSON file will be written to the path specified by the environment variable\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         FURIOSA_PROFILER_OUTPUT_PATH\\n        </span>\\n       </code>\\n       .\\nIf you enter\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         chrome://tracing\\n        </span>\\n       </code>\\n       in Chrome’s address bar, the trace viewer will start. Click the\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         Load\\n        </span>\\n       </code>\\n       button in the upper left corner of the trace viewer, and select the saved file (\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         tracing.json\\n        </span>\\n       </code>\\n       in the example above) to view the trace result.\\n      </p>\\n      <a class=\"with-shadow reference internal image-reference\" href=\"../_images/tracing.png\">\\n       <img alt=\"Tracing\" class=\"with-shadow align-center\" src=\"../_images/tracing.png\" style=\"width: 600px;\"/>\\n      </a>\\n      <p>\\n      </p>\\n     </section>\\n     <section id=\"tracing-via-profiler-context\">\\n      <span id=\"profilerenabledbycontext\">\\n      </span>\\n      <h2>\\n       Tracing via Profiler Context\\n       <a class=\"headerlink\" href=\"#tracing-via-profiler-context\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       You can also trace a model inference performance by using a Profiler Context in your Python code. The advantages of this method, in comparison to the tracing by environment variable, are as follows:\\n      </p>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         Allow to enable trace immediately even in interactive environments, such as Python Interpreter or Jupyter Notebook\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Allow to specify labels to certain inference runs\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Allow to measure specified operator categories selectively\\n        </p>\\n       </li>\\n      </ul>\\n      <div class=\"code highlight-default notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"ch\">#!/usr/bin/env python</span>\\n\\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime.profiler</span> <span class=\"kn\">import</span> <span class=\"n\">profile</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime.sync</span> <span class=\"kn\">import</span> <span class=\"n\">create_runner</span>\\n\\n<span class=\"c1\"># You can find \\'examples\\' directory of the root of furiosa-sdk source tree</span>\\n<span class=\"n\">model_path</span> <span class=\"o\">=</span> <span class=\"s2\">\"examples/assets/quantized_models/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\"</span>\\n\\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">\"mobilenet_v1_trace.json\"</span><span class=\"p\">,</span> <span class=\"s2\">\"w\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">output</span><span class=\"p\">:</span>\\n    <span class=\"k\">with</span> <span class=\"n\">profile</span><span class=\"p\">(</span><span class=\"n\">file</span><span class=\"o\">=</span><span class=\"n\">output</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">profiler</span><span class=\"p\">:</span>\\n        <span class=\"k\">with</span> <span class=\"n\">create_runner</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">runner</span><span class=\"p\">:</span>\\n            <span class=\"n\">input_shape</span> <span class=\"o\">=</span> <span class=\"n\">runner</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">input</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">shape</span>\\n\\n            <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"warm up\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">record</span><span class=\"p\">:</span>\\n                <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">):</span>\\n                    <span class=\"n\">runner</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">input_shape</span><span class=\"p\">))])</span>\\n\\n            <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"trace\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">record</span><span class=\"p\">:</span>\\n                <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">):</span>\\n                    <span class=\"n\">runner</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">input_shape</span><span class=\"p\">))])</span>\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       The above is a code example using a profiling context. Once the above Python code is executed, the\\n       <cite>\\n        mnist_trace.json\\n       </cite>\\n       file is created. The trace results are labelled ‘warm up’ and ‘trace’ as shown below.\\n      </p>\\n      <a class=\"with-shadow reference internal image-reference\" href=\"../_images/tracing_with_record.png\">\\n       <img alt=\"Tracing with Profiler Context\" class=\"with-shadow align-center\" src=\"../_images/tracing_with_record.png\" style=\"width: 600px;\"/>\\n      </a>\\n      <p>\\n      </p>\\n      <section id=\"pause-resume-of-profiler-context\">\\n       <span id=\"temporarilydisablingprofiler\">\\n       </span>\\n       <h3>\\n        Pause/Resume of Profiler Context\\n        <a class=\"headerlink\" href=\"#pause-resume-of-profiler-context\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Tracing long-running jobs can cause following problems:\\n       </p>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          Produce large trace files which take huge disk space and are difficult to be shared.\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Make it hard to identify interesting section when the trace is visualized, without additional processing.\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Take much time to produce trace files.\\n         </p>\\n        </li>\\n       </ul>\\n       <p>\\n        To avoid the above issues, the profiler provides an additional API to temporarily pause or resume a profiler within the context.\\nUsers can exclude execution they do not want to profile, thereby reducing profiling overhead and trace file size.\\n       </p>\\n       <p>\\n        The below is an example of pausing profiler not to trace\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          warm\\n         </span>\\n         <span class=\"pre\">\\n          up\\n         </span>\\n        </code>\\n        phase between\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          profile.pause\\n         </span>\\n        </code>\\n        and\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          profile.resume\\n         </span>\\n        </code>\\n        .\\n       </p>\\n       <div class=\"code highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"ch\">#!/usr/bin/env python</span>\\n\\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime.profiler</span> <span class=\"kn\">import</span> <span class=\"n\">RecordFormat</span><span class=\"p\">,</span> <span class=\"n\">profile</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime.sync</span> <span class=\"kn\">import</span> <span class=\"n\">create_runner</span>\\n\\n<span class=\"c1\"># You can find \\'examples\\' directory of the root of furiosa-sdk source tree</span>\\n<span class=\"n\">model_path</span> <span class=\"o\">=</span> <span class=\"s2\">\"examples/assets/quantized_models/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\"</span>\\n\\n<span class=\"k\">with</span> <span class=\"n\">profile</span><span class=\"p\">(</span><span class=\"nb\">format</span><span class=\"o\">=</span><span class=\"n\">RecordFormat</span><span class=\"o\">.</span><span class=\"n\">PandasDataFrame</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">profiler</span><span class=\"p\">:</span>\\n    <span class=\"k\">with</span> <span class=\"n\">create_runner</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">runner</span><span class=\"p\">:</span>\\n        <span class=\"n\">input_shape</span> <span class=\"o\">=</span> <span class=\"n\">runner</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">input</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">shape</span>\\n\\n        <span class=\"c1\"># pause profiling during warmup</span>\\n        <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">pause</span><span class=\"p\">()</span>\\n\\n        <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">):</span>\\n            <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"warm up\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">record</span><span class=\"p\">:</span>\\n                <span class=\"n\">runner</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">input_shape</span><span class=\"p\">))])</span>\\n\\n        <span class=\"c1\"># resume profiling</span>\\n        <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">resume</span><span class=\"p\">()</span>\\n\\n        <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"trace\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">record</span><span class=\"p\">:</span>\\n            <span class=\"n\">runner</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">input_shape</span><span class=\"p\">))])</span>\\n\\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">get_pandas_dataframe</span><span class=\"p\">()</span>\\n\\n<span class=\"k\">assert</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s2\">\"name\"</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"s2\">\"trace\"</span><span class=\"p\">])</span> <span class=\"o\">==</span> <span class=\"mi\">1</span>\\n<span class=\"k\">assert</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s2\">\"name\"</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"s2\">\"warm up\"</span><span class=\"p\">])</span> <span class=\"o\">==</span> <span class=\"mi\">0</span>\\n</pre>\\n        </div>\\n       </div>\\n      </section>\\n      <section id=\"trace-analysis-using-pandas-dataframe\">\\n       <span id=\"pandasprofilinganalysis\">\\n       </span>\\n       <h3>\\n        Trace analysis using Pandas DataFrame\\n        <a class=\"headerlink\" href=\"#trace-analysis-using-pandas-dataframe\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        With the measured tracing data, in addition to visualizing it with Chrome Trace Format, it can also be expressed and used in Pandas DataFrame, commonly used for data analysis. These are the advantages in comparison to Chrome Trace Format.\\n       </p>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          Can be used directly in Python Interpreter or Jupyter Notebook interactive shell\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Users can directly access DataFrame for analysis, on top of the reporting function which is provided as default\\n         </p>\\n        </li>\\n       </ul>\\n       <div class=\"code highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"ch\">#!/usr/bin/env python</span>\\n\\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime.profiler</span> <span class=\"kn\">import</span> <span class=\"n\">RecordFormat</span><span class=\"p\">,</span> <span class=\"n\">profile</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime.sync</span> <span class=\"kn\">import</span> <span class=\"n\">create_runner</span>\\n\\n<span class=\"c1\"># You can find \\'examples\\' directory of the root of furiosa-sdk source tree</span>\\n<span class=\"n\">model_path</span> <span class=\"o\">=</span> <span class=\"s2\">\"examples/assets/quantized_models/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\"</span>\\n\\n<span class=\"k\">with</span> <span class=\"n\">profile</span><span class=\"p\">(</span><span class=\"nb\">format</span><span class=\"o\">=</span><span class=\"n\">RecordFormat</span><span class=\"o\">.</span><span class=\"n\">PandasDataFrame</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">profiler</span><span class=\"p\">:</span>\\n    <span class=\"k\">with</span> <span class=\"n\">create_runner</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">runner</span><span class=\"p\">:</span>\\n        <span class=\"n\">input_shape</span> <span class=\"o\">=</span> <span class=\"n\">runner</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">input</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">shape</span>\\n\\n        <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"warm up\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">record</span><span class=\"p\">:</span>\\n            <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">):</span>\\n                <span class=\"n\">runner</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">input_shape</span><span class=\"p\">))])</span>\\n\\n        <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"trace\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">record</span><span class=\"p\">:</span>\\n            <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">):</span>\\n                <span class=\"n\">runner</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">input_shape</span><span class=\"p\">))])</span>\\n\\n<span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">print_summary</span><span class=\"p\">()</span>  <span class=\"c1\"># (1)</span>\\n\\n<span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">print_inferences</span><span class=\"p\">()</span>  <span class=\"c1\"># (2)</span>\\n\\n<span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">print_npu_executions</span><span class=\"p\">()</span>  <span class=\"c1\"># (3)</span>\\n\\n<span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">print_npu_operators</span><span class=\"p\">()</span>  <span class=\"c1\"># (4)</span>\\n\\n<span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">print_external_operators</span><span class=\"p\">()</span>  <span class=\"c1\"># (5)</span>\\n\\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">get_pandas_dataframe</span><span class=\"p\">()</span>  <span class=\"c1\"># (6)</span>\\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s2\">\"name\"</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"s2\">\"trace\"</span><span class=\"p\">][[</span><span class=\"s2\">\"trace_id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"name\"</span><span class=\"p\">,</span> <span class=\"s2\">\"thread.id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"dur\"</span><span class=\"p\">]])</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        Above is a code example that designates a profiling context format into PandasDataFrame.\\n       </p>\\n       <p>\\n        When\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          (1)\\n         </span>\\n        </code>\\n        line is executed, the following summary of the results is produced.\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"o\">================================================</span>\\n  <span class=\"n\">Inference</span> <span class=\"n\">Results</span> <span class=\"n\">Summary</span>\\n<span class=\"o\">================================================</span>\\n<span class=\"n\">Inference</span> <span class=\"n\">counts</span>                <span class=\"p\">:</span> <span class=\"mi\">4</span>\\n<span class=\"n\">Min</span> <span class=\"n\">latency</span> <span class=\"p\">(</span><span class=\"n\">ns</span><span class=\"p\">)</span>                <span class=\"p\">:</span> <span class=\"mi\">1584494</span>\\n<span class=\"n\">Max</span> <span class=\"n\">latency</span> <span class=\"p\">(</span><span class=\"n\">ns</span><span class=\"p\">)</span>                <span class=\"p\">:</span> <span class=\"mi\">3027309</span>\\n<span class=\"n\">Mean</span> <span class=\"n\">latency</span> <span class=\"p\">(</span><span class=\"n\">ns</span><span class=\"p\">)</span>               <span class=\"p\">:</span> <span class=\"mi\">2136984</span>\\n<span class=\"n\">Median</span> <span class=\"n\">latency</span> <span class=\"p\">(</span><span class=\"n\">ns</span><span class=\"p\">)</span>             <span class=\"p\">:</span> <span class=\"mi\">1968066</span>\\n<span class=\"mf\">90.0</span> <span class=\"n\">percentile</span> <span class=\"n\">Latency</span> <span class=\"p\">(</span><span class=\"n\">ns</span><span class=\"p\">)</span>    <span class=\"p\">:</span> <span class=\"mi\">2752525</span>\\n<span class=\"mf\">95.0</span> <span class=\"n\">percentile</span> <span class=\"n\">Latency</span> <span class=\"p\">(</span><span class=\"n\">ns</span><span class=\"p\">)</span>    <span class=\"p\">:</span> <span class=\"mi\">2889917</span>\\n<span class=\"mf\">97.0</span> <span class=\"n\">percentile</span> <span class=\"n\">Latency</span> <span class=\"p\">(</span><span class=\"n\">ns</span><span class=\"p\">)</span>    <span class=\"p\">:</span> <span class=\"mi\">2944874</span>\\n<span class=\"mf\">99.0</span> <span class=\"n\">percentile</span> <span class=\"n\">Latency</span> <span class=\"p\">(</span><span class=\"n\">ns</span><span class=\"p\">)</span>    <span class=\"p\">:</span> <span class=\"mi\">2999831</span>\\n<span class=\"mf\">99.9</span> <span class=\"n\">percentile</span> <span class=\"n\">Latency</span> <span class=\"p\">(</span><span class=\"n\">ns</span><span class=\"p\">)</span>    <span class=\"p\">:</span> <span class=\"mi\">3024561</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        When\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          (2)\\n         </span>\\n        </code>\\n        line is executed, duration of one inference query is shown.\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>┌──────────────────────────────────┬──────────────────┬───────────┬─────────┐\\n│ trace_id                         ┆ span_id          ┆ thread.id ┆ dur     │\\n╞══════════════════════════════════╪══════════════════╪═══════════╪═════════╡\\n│ 7cf3d3b7439cf4c3fac1a47998783102 ┆ 403ada67f1d8220e ┆ 1         ┆ 3027309 │\\n│ 16d65f6f8f1db256d0f39953855dea72 ┆ 78b065c19c3675ef ┆ 1         ┆ 2111363 │\\n│ d0534e3a9f19edadab81954ad28ab44f ┆ 9a7addaf0f28c9fe ┆ 1         ┆ 1824769 │\\n│ 70512188522f45b87cfe4f545de3cf2c ┆ c75f697f8e72d333 ┆ 1         ┆ 1584494 │\\n└──────────────────────────────────┴──────────────────┴───────────┴─────────┘\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        When\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          (3)\\n         </span>\\n        </code>\\n        line is executed, elapsed times of NPU executions will be shown:\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>┌──────────────────────────────────┬──────────────────┬──────────┬─────────────────┬───────────┬─────────┬──────────────────────┐\\n│ trace_id                         ┆ span_id          ┆ pe_index ┆ execution_index ┆ NPU Total ┆ NPU Run ┆ NPU IoWait           │\\n╞══════════════════════════════════╪══════════════════╪══════════╪═════════════════╪═══════════╪═════════╪══════════════════════╡\\n│ 8f6fce6c0e52b4735cae3379732a0943 ┆ 3e1e4a76523cbf89 ┆ 0        ┆ 0               ┆ 119145    ┆ 108134  ┆ 18446744073709540605 │\\n│ 195366613b1da9b0350c0a3c2a608f42 ┆ 07dff2e92172fabd ┆ 0        ┆ 0               ┆ 119363    ┆ 108134  ┆ 18446744073709540387 │\\n│ 3b65b8fa3eabfaf8f815ec9f41fcc7d9 ┆ 639a366a7f932a23 ┆ 0        ┆ 0               ┆ 119157    ┆ 108134  ┆ 18446744073709540593 │\\n│ e48825df32a07e5559f7f50048c08e1f ┆ ecaab4915bfda725 ┆ 0        ┆ 0               ┆ 119219    ┆ 108134  ┆ 18446744073709540531 │\\n└──────────────────────────────────┴──────────────────┴──────────┴─────────────────┴───────────┴─────────┴──────────────────────┘\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        When\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          (4)\\n         </span>\\n        </code>\\n        line is executed, elapsed times of operators will be shown:\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>┌─────────────────────────┬──────────────────────┬───────┐\\n│ name                    ┆ average_elapsed (ns) ┆ count │\\n╞═════════════════════════╪══════════════════════╪═══════╡\\n│ LowLevelConv2d          ┆ 5327.8               ┆ 60    │\\n│ LowLevelDepthwiseConv2d ┆ 1412.285714          ┆ 56    │\\n│ LowLevelPad             ┆ 575.785714           ┆ 56    │\\n│ LowLevelTranspose       ┆ 250.0                ┆ 4     │\\n│ LowLevelReshape         ┆ 2.0                  ┆ 240   │\\n│ LowLevelSlice           ┆ 2.0                  ┆ 12    │\\n│ LowLevelExpand          ┆ 2.0                  ┆ 16    │\\n└─────────────────────────┴──────────────────────┴───────┘\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        When\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          (5)\\n         </span>\\n        </code>\\n        line is executed, the time data for operators in the CPU is shown as below.\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>┌──────────────────────────────────┬──────────────────┬───────────┬────────────┬────────────────┬────────┐\\n│ trace_id                         ┆ span_id          ┆ thread.id ┆ name       ┆ operator_index ┆ dur    │\\n╞══════════════════════════════════╪══════════════════╪═══════════╪════════════╪════════════════╪════════╡\\n│ e7ab6656cc090a8d05992a9e4683b8b7 ┆ 206a1d6f351ca4b1 ┆ 40        ┆ Quantize   ┆ 0              ┆ 136285 │\\n│ 03636fd6c7dbc42f0a9dd29a7283d3fc ┆ f636740983e095a6 ┆ 40        ┆ Lower      ┆ 1              ┆ 133350 │\\n│ c9a0858f7e0885a976f51c6cb57d3e0f ┆ bb6c84f88e453055 ┆ 40        ┆ Unlower    ┆ 2              ┆ 44775  │\\n│ 8777c67ad9fe597139bbd6970362c2fc ┆ 63bac982c7b98aba ┆ 40        ┆ Dequantize ┆ 3              ┆ 14682  │\\n│ 98aeba2a25b0525166b6a4065ab01774 ┆ 34ccd560571d733f ┆ 40        ┆ Quantize   ┆ 0              ┆ 45465  │\\n│ 420525dc13ba9624083e0a276f7ee718 ┆ 9f6d342da5eb86bc ┆ 40        ┆ Lower      ┆ 1              ┆ 152748 │\\n│ cb67393f6949bbbb396053c1e00931ff ┆ 2d724fa6ab8ca024 ┆ 40        ┆ Unlower    ┆ 2              ┆ 67140  │\\n│ 00424b4f02039ae0ca98388a964062b0 ┆ a5fb9fbd5bffe6a6 ┆ 40        ┆ Dequantize ┆ 3              ┆ 32388  │\\n│ d7412c59d360067e8b7a2508a30d1079 ┆ 8e426d778fa95722 ┆ 40        ┆ Quantize   ┆ 0              ┆ 71736  │\\n│ 6820acf9345c5b373c512f6cd5edcbc7 ┆ 2d787c2df381f010 ┆ 40        ┆ Lower      ┆ 1              ┆ 311310 │\\n│ 84d24b02a95c63c3e40f7682384749e4 ┆ 1236a974a619ff1a ┆ 40        ┆ Unlower    ┆ 2              ┆ 51930  │\\n│ 8d25dff1cfd6624509cbf95503e93382 ┆ 673efb3bfb8deac6 ┆ 40        ┆ Dequantize ┆ 3              ┆ 12362  │\\n│ 4cc60ec1eee7d9f3cdd290d07b303a18 ┆ e7903b0a584d6388 ┆ 40        ┆ Quantize   ┆ 0              ┆ 56736  │\\n│ c5f04d9fea26e5b52c6ec5e5406775fc ┆ 701118dabd065e6f ┆ 40        ┆ Lower      ┆ 1              ┆ 265447 │\\n│ c5fdfb9cf454da130148e8e364eeee93 ┆ 5cf3750def19c6e8 ┆ 40        ┆ Unlower    ┆ 2              ┆ 35869  │\\n│ e1e650d23061140404915f1df36daf9c ┆ ddd76ff19b5cd713 ┆ 40        ┆ Dequantize ┆ 3              ┆ 14688  │\\n└──────────────────────────────────┴──────────────────┴───────────┴────────────┴────────────────┴────────┘\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        With line\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          (6)\\n         </span>\\n        </code>\\n        , you can access DataFrame from the code and perform direct analysis.\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>                            <span class=\"n\">trace_id</span>   <span class=\"n\">name</span>  <span class=\"n\">thread</span><span class=\"o\">.</span><span class=\"n\">id</span>       <span class=\"n\">dur</span>\\n<span class=\"mi\">487</span>  <span class=\"n\">f3b158734e3684f2e043ed41309c4c2d</span>  <span class=\"n\">trace</span>          <span class=\"mi\">1</span>  <span class=\"mi\">11204385</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n       </p>\\n      </section>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"performance.html\" rel=\"prev\" title=\"Performance Optimization\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"serving.html\" rel=\"next\" title=\"Model Server (Serving Framework)\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='65a65779-8c0e-48f0-890a-f6ecc20a9f41', link=Url('https://furiosa-ai.github.io/docs/latest/en/releases/0.5.0.html'), name='0', parent='', child=[], description='\\n\\n\\n* Release Notes - 0.5.0\\n* [View page source](../_sources/releases/0.5.0.rst.txt)\\n\\n---\\n\\n\\n\\nRelease Notes - 0.5.0\\n[\\uf0c1](#release-notes-0-5-0 \"Permalink to this heading\")\\n===========================================================================\\n\\nFuriosaAI SDK 0.5.0 release includes approximately 87 bug fixes, added functionalities, and improvements.\\nThe following are some of the key changes:\\n\\nCompiler Improvement\\n[\\uf0c1](#compiler-improvement \"Permalink to this heading\")\\n---------------------------------------------------------------------------\\n\\n0.5.0 release adds NPU acceleration support for the following operators.\\nYou can find the entire list of acceleration-supported operators at\\n[List of Supported Operators for Warboy Acceleration](../npu/warboy.html#supportedoperators)\\n.\\n\\n* BatchNormalization\\n* ConvTranspose\\n* LeakyRelu\\n* Pow\\n* Sqrt\\n* Sub\\n\\nAdditionally, we have improved the compiler with this release by adding support for\\n[Opset 13](https://github.com/onnx/onnx/releases/tag/v1.8.0)\\noperator of Onnx.\\n\\n\\nSession API Improvement\\n[\\uf0c1](#session-api-improvement \"Permalink to this heading\")\\n---------------------------------------------------------------------------------\\n\\nAPI Improvement 1 (\\n[commit b1d2b74](https://github.com/furiosa-ai/furiosa-sdk/commit/b1d2b742f9f1ed43dbe7aa5f2ed822ae38d636e4)\\n): you can now designate NPU device\\nin Session API.\\nWith 0.5.0, you can designate NPU device when generating a session, whereas previously you could only designate\\ndevice to be used through the environment variable\\n`NPU_DEVNAME`\\n.\\nIf not explicitly stated, designated device in the environment variable\\n`NPU_DEVNAME`\\nwill be used, as was done before.\\n\\n```\\nfrom furiosa.runtime import session\\n\\nsess1 = session.create(\"model1.onnx\", device=\"npu0pe0\")\\nsess2 = session.create(\"model2.onnx\", device=\"npu0pe1\")\\n\\n# Asynchronous API\\nasync_sess, queue = session.create_async(\"model2.onnx\", device=\"npu1pe2\")\\n\\n```\\n\\nAPI Improvement 2 (\\n[commit 4f1f114](https://github.com/furiosa-ai/furiosa-sdk/commit/4f1f1149d137a58ada31df57de6e1234881ccf5b)\\n) is support for tensor name.\\nThe existing API was limited in that it could only identify the order of the tensor built into the model, and pass it as an input parameter to\\nsession.run()\\n\\n.\\nFrom 0.5.0, you can explicitly designate name of input and output tensors as shown below.\\n\\n```\\nnp1 = np.random.randint(0, 255, session_input.shape, dtype=np.uint8)\\nnp2 = np.random.randint(0, 255, session_input.shape, dtype=np.uint8)\\nsess1.run_with(outputs=[\"output1\"], inputs={\\n  \"input2\": np2,\\n  \"input1\": np1,\\n}\\n\\n```\\n\\n\\n\\nError Diagnosis Message & Error Handling Improvements\\n[\\uf0c1](#error-diagnosis-message-error-handling-improvements \"Permalink to this heading\")\\n-------------------------------------------------------------------------------------------------------------------------------------------\\n\\nIn case of an error, version information for debugging and compiler log are output independently. This enables easier bug reporting.\\nYou can find more information about reporting issues and customer service at\\n[Bug Report](../customer-support/bugs.html#bugreport)\\n.\\n\\n```\\n>>> from furiosa.runtime import session\\n>>> session.create(\"mnist-8.onnx\")\\nSaving the compilation log into /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log\\n...\\n2021-11-22T06:30:28.423371Z ERROR fail to compile the model: the static shape of tensor \\'input\\' contains an unsupported dimension value: Some(DimParam(\"batch_size\"))\\n================================================================================\\nInformation Dump\\n================================================================================\\n- Python version: 3.8.10 (default, Sep 28 2021, 16:10:42)  [GCC 9.3.0]\\n- furiosa-libnux path: libnux.so.0.5.0\\n- furiosa-libnux version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34)\\n- furiosa-compiler version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34)\\n- furiosa-sdk-runtime version: Furiosa SDK Runtime  (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\\n\\nPlease check the compiler log at /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log.\\nIf you have a problem, please report the log file to https://furiosa-ai.atlassian.net/servicedesk/customer/portals with the information dumped above.\\n================================================================================\\n\\n```\\n\\nImprovements such as error handling for the following issues are also included.\\n\\n* Error fix when using duplicate devices (\\n  [commit 01aaa40](https://github.com/furiosa-ai/furiosa-sdk/commit/01aaa40fd31573dc578fa1c805e1ed36decc9088)\\n  )\\n* Added timeout of CompletionQueue & error handling fix for session connection termination (\\n  [commit 21cba85](https://github.com/furiosa-ai/furiosa-sdk/commit/21cba85737840546357f2dd709d33d9bc2b00390)\\n  )\\n* Hanging issue fix for interruption during compiling\\n  [(commit a0f4bd7](https://github.com/furiosa-ai/furiosa-sdk/commit/a0f4bd7ab8d199e3e46f9820fe4dc75bfa845e61)\\n  )\\n\\nIntroducing Furiosa Server (serving framework)\\n[\\uf0c1](#introducing-furiosa-server-serving-framework \"Permalink to this heading\")\\n-----------------------------------------------------------------------------------------------------------------------------\\n\\n0.5.0 includes\\nFuriosa Server\\n\\n, a serving framework that supports GRPC and REST API.\\nYou can easily install it by running\\n`pip\\n\\ninstall\\n\\nfuriosa-sdk[server]`\\n. By running it with the\\ncommand below, the model can be served immediately with the NPU.\\nYou can find more detailed instructions and functions at\\n[Model Server (Serving Framework)](../software/serving.html#modelserving)\\n.\\n\\n```\\nfuriosa server \\\\\\n--model-path MNISTnet_uint8_quant_without_softmax.tflite \\\\\\n--model-name mnist\\n\\n```\\n\\n\\n\\nIntroducing Furiosa Model package\\n[\\uf0c1](#introducing-furiosa-model-package \"Permalink to this heading\")\\n-----------------------------------------------------------------------------------------------------\\n\\nFrom 0.5.0, the optimized model for the FuriosaAI NPU can be used directly as a Python package.\\nYou can easily install it with the command\\n`pip\\n\\ninstall\\n\\nfuriosa-sdk[models]`\\n,\\nand can immediately be used in Session API as shown in the following example.\\n\\n```\\nimport asyncio\\n\\nfrom furiosa.registry import Model\\nfrom furiosa.models.vision import MLCommonsResNet50\\nfrom furiosa.runtime import session\\n\\nresnet50: Model = asyncio.run(MLCommonsResNet50())\\nsess = session.create(resnet50.model, device=\\'npu0pe0\\')\\n\\n```\\n\\n\\n\\nCommand line NPU management tool: furiosactl\\n[\\uf0c1](#command-line-npu-management-tool-furiosactl \"Permalink to this heading\")\\n--------------------------------------------------------------------------------------------------------------------------\\n\\n0.5.0 includes furiosactl, a command line NPU management tool.\\nYou can install it with\\n`apt\\n\\ninstall\\n\\nfuriosa-toolkit`\\n. You can use this tool to check\\nNPU device status, as well as identify idle NPUs.\\nYou can find\\n`apt`\\nserver configuration instructions at\\n[APT server configuration](../software/installation.html#setupaptrepository)\\n.\\n\\n```\\n$ furiosactl info\\n\\n+------+------------------+-------+---------+--------------+---------+\\n| NPU  | Name             | Temp. | Power   | PCI-BDF      | PCI-DEV |\\n+------+------------------+-------+---------+--------------+---------+\\n| npu0 | FuriosaAI Warboy |  34°C | 12.92 W | 0000:01:00.0 | 510:0   |\\n+------+------------------+-------+---------+--------------+---------+\\n\\n$ furiosactl list\\n+------+-----------+-----------+--------+\\n| NPU  | DEVNAME   | Type      | Status |\\n+------+-----------+-----------+--------+\\n| npu0 | npu0      | All PE(s) | Ready  |\\n|      | npu0pe0   | Single PE | Ready  |\\n|      | npu0pe1   | Single PE | Ready  |\\n|      | npu0pe0-1 | PE Fusion | Ready  |\\n+------+-----------+-----------+--------+\\n\\n```\\n\\n\\n\\nKubernetes support\\n[\\uf0c1](#kubernetes-support \"Permalink to this heading\")\\n-----------------------------------------------------------------------\\n\\n0.5.0 includes NPU support for Kubernetes.\\nYou can install the NPU device plugin and node labeller with the command below,\\nand have the NPU be scheduled together when deploying pods.\\nMore details can be found at\\n[Kubernetes Support](../software/kubernetes_support.html#kubernetesintegration)\\n.\\n\\n```\\nkubectl apply -f https://raw.githubusercontent.com/furiosa-ai/furiosa-sdk/main/kubernetes/deployments/device-plugin.yaml\\nkubectl apply -f https://raw.githubusercontent.com/furiosa-ai/furiosa-sdk/main/kubernetes/deployments/node-labeller.yaml\\n\\n```\\n\\n\\n\\n\\n\\n\\n[Previous](0.6.0.html \"Release Notes - 0.6.0\")\\n[Next](../customer-support/bugs.html \"Bug Report\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Release Notes - 0.5.0 * [View page source](../_sources/releases/0.5.0.rst.txt)\\n---\\nRelease Notes - 0.5.0 [\\uf0c1](#release-notes-0-5-0 \"Permalink to this heading\") ===========================================================================\\nFuriosaAI SDK 0.5.0 release includes approximately 87 bug fixes, added functionalities, and improvements. The following are some of the key changes:\\nCompiler Improvement [\\uf0c1](#compiler-improvement \"Permalink to this heading\") ---------------------------------------------------------------------------\\n0.5.0 release adds NPU acceleration support for the following operators. You can find the entire list of acceleration-supported operators at [List of Supported Operators for Warboy Acceleration](../npu/warboy.html#supportedoperators) .\\n* BatchNormalization * ConvTranspose * LeakyRelu * Pow * Sqrt * Sub\\nAdditionally, we have improved the compiler with this release by adding support for [Opset 13](https://github.com/onnx/onnx/releases/tag/v1.8.0) operator of Onnx.\\nSession API Improvement [\\uf0c1](#session-api-improvement \"Permalink to this heading\") ---------------------------------------------------------------------------------\\nAPI Improvement 1 ( [commit b1d2b74](https://github.com/furiosa-ai/furiosa-sdk/commit/b1d2b742f9f1ed43dbe7aa5f2ed822ae38d636e4) ): you can now designate NPU device in Session API. With 0.5.0, you can designate NPU device when generating a session, whereas previously you could only designate device to be used through the environment variable `NPU_DEVNAME` . If not explicitly stated, designated device in the environment variable `NPU_DEVNAME` will be used, as was done before.\\n``` from furiosa.runtime import session\\nsess1 = session.create(\"model1.onnx\", device=\"npu0pe0\") sess2 = session.create(\"model2.onnx\", device=\"npu0pe1\")\\n# Asynchronous API async_sess, queue = session.create_async(\"model2.onnx\", device=\"npu1pe2\")\\n```\\nAPI Improvement 2 ( [commit 4f1f114](https://github.com/furiosa-ai/furiosa-sdk/commit/4f1f1149d137a58ada31df57de6e1234881ccf5b) ) is support for tensor name. The existing API was limited in that it could only identify the order of the tensor built into the model, and pass it as an input parameter to session.run()\\n. From 0.5.0, you can explicitly designate name of input and output tensors as shown below.\\n``` np1 = np.random.randint(0, 255, session_input.shape, dtype=np.uint8) np2 = np.random.randint(0, 255, session_input.shape, dtype=np.uint8) sess1.run_with(outputs=[\"output1\"], inputs={   \"input2\": np2,   \"input1\": np1, }\\n```\\nError Diagnosis Message & Error Handling Improvements [\\uf0c1](#error-diagnosis-message-error-handling-improvements \"Permalink to this heading\") -------------------------------------------------------------------------------------------------------------------------------------------\\nIn case of an error, version information for debugging and compiler log are output independently. This enables easier bug reporting. You can find more information about reporting issues and customer service at [Bug Report](../customer-support/bugs.html#bugreport) .\\n``` >>> from furiosa.runtime import session >>> session.create(\"mnist-8.onnx\") Saving the compilation log into /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log ... 2021-11-22T06:30:28.423371Z ERROR fail to compile the model: the static shape of tensor \\'input\\' contains an unsupported dimension value: Some(DimParam(\"batch_size\")) ================================================================================ Information Dump ================================================================================ - Python version: 3.8.10 (default, Sep 28 2021, 16:10:42)  [GCC 9.3.0] - furiosa-libnux path: libnux.so.0.5.0 - furiosa-libnux version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34) - furiosa-compiler version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34) - furiosa-sdk-runtime version: Furiosa SDK Runtime  (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\\nPlease check the compiler log at /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log. If you have a problem, please report the log file to https://furiosa-ai.atlassian.net/servicedesk/customer/portals with the information dumped above. ================================================================================\\n```\\nImprovements such as error handling for the following issues are also included.\\n* Error fix when using duplicate devices (   [commit 01aaa40](https://github.com/furiosa-ai/furiosa-sdk/commit/01aaa40fd31573dc578fa1c805e1ed36decc9088)   ) * Added timeout of CompletionQueue & error handling fix for session connection termination (   [commit 21cba85](https://github.com/furiosa-ai/furiosa-sdk/commit/21cba85737840546357f2dd709d33d9bc2b00390)   ) * Hanging issue fix for interruption during compiling   [(commit a0f4bd7](https://github.com/furiosa-ai/furiosa-sdk/commit/a0f4bd7ab8d199e3e46f9820fe4dc75bfa845e61)   )\\nIntroducing Furiosa Server (serving framework) [\\uf0c1](#introducing-furiosa-server-serving-framework \"Permalink to this heading\") -----------------------------------------------------------------------------------------------------------------------------\\n0.5.0 includes Furiosa Server\\n, a serving framework that supports GRPC and REST API. You can easily install it by running `pip\\ninstall\\nfuriosa-sdk[server]` . By running it with the command below, the model can be served immediately with the NPU. You can find more detailed instructions and functions at [Model Server (Serving Framework)](../software/serving.html#modelserving) .\\n``` furiosa server \\\\ --model-path MNISTnet_uint8_quant_without_softmax.tflite \\\\ --model-name mnist\\n```\\nIntroducing Furiosa Model package [\\uf0c1](#introducing-furiosa-model-package \"Permalink to this heading\") -----------------------------------------------------------------------------------------------------\\nFrom 0.5.0, the optimized model for the FuriosaAI NPU can be used directly as a Python package. You can easily install it with the command `pip\\ninstall\\nfuriosa-sdk[models]` ,\\nand can immediately be used in Session API as shown in the following example.\\n``` import asyncio\\nfrom furiosa.registry import Model from furiosa.models.vision import MLCommonsResNet50 from furiosa.runtime import session\\nresnet50: Model = asyncio.run(MLCommonsResNet50()) sess = session.create(resnet50.model, device=\\'npu0pe0\\')\\n```\\nCommand line NPU management tool: furiosactl [\\uf0c1](#command-line-npu-management-tool-furiosactl \"Permalink to this heading\") --------------------------------------------------------------------------------------------------------------------------\\n0.5.0 includes furiosactl, a command line NPU management tool. You can install it with `apt\\ninstall\\nfuriosa-toolkit` . You can use this tool to check NPU device status, as well as identify idle NPUs. You can find `apt` server configuration instructions at [APT server configuration](../software/installation.html#setupaptrepository) .\\n``` $ furiosactl info\\n+------+------------------+-------+---------+--------------+---------+ | NPU  | Name             | Temp. | Power   | PCI-BDF      | PCI-DEV | +------+------------------+-------+---------+--------------+---------+ | npu0 | FuriosaAI Warboy |  34°C | 12.92 W | 0000:01:00.0 | 510:0   | +------+------------------+-------+---------+--------------+---------+\\n$ furiosactl list +------+-----------+-----------+--------+ | NPU  | DEVNAME   | Type      | Status | +------+-----------+-----------+--------+ | npu0 | npu0      | All PE(s) | Ready  | |      | npu0pe0   | Single PE | Ready  | |      | npu0pe1   | Single PE | Ready  | |      | npu0pe0-1 | PE Fusion | Ready  | +------+-----------+-----------+--------+\\n```\\nKubernetes support [\\uf0c1](#kubernetes-support \"Permalink to this heading\") -----------------------------------------------------------------------\\n0.5.0 includes NPU support for Kubernetes. You can install the NPU device plugin and node labeller with the command below, and have the NPU be scheduled together when deploying pods. More details can be found at [Kubernetes Support](../software/kubernetes_support.html#kubernetesintegration) .\\n``` kubectl apply -f https://raw.githubusercontent.com/furiosa-ai/furiosa-sdk/main/kubernetes/deployments/device-plugin.yaml kubectl apply -f https://raw.githubusercontent.com/furiosa-ai/furiosa-sdk/main/kubernetes/deployments/node-labeller.yaml\\n```\\n[Previous](0.6.0.html \"Release Notes - 0.6.0\") [Next](../customer-support/bugs.html \"Bug Report\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Release Notes - 0.5.0\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/releases/0.5.0.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"release-notes-0-5-0\">\\n     <h1>\\n      Release Notes - 0.5.0\\n      <a class=\"headerlink\" href=\"#release-notes-0-5-0\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      FuriosaAI SDK 0.5.0 release includes approximately 87 bug fixes, added functionalities, and improvements.\\nThe following are some of the key changes:\\n     </p>\\n     <section id=\"compiler-improvement\">\\n      <span id=\"compilerimprovement\">\\n      </span>\\n      <h2>\\n       Compiler Improvement\\n       <a class=\"headerlink\" href=\"#compiler-improvement\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       0.5.0 release adds NPU acceleration support for the following operators.\\nYou can find the entire list of acceleration-supported operators at\\n       <a class=\"reference internal\" href=\"../npu/warboy.html#supportedoperators\">\\n        <span class=\"std std-ref\">\\n         List of Supported Operators for Warboy Acceleration\\n        </span>\\n       </a>\\n       .\\n      </p>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         BatchNormalization\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         ConvTranspose\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         LeakyRelu\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Pow\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Sqrt\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Sub\\n        </p>\\n       </li>\\n      </ul>\\n      <p>\\n       Additionally, we have improved the compiler with this release by adding support for\\n       <a class=\"reference external\" href=\"https://github.com/onnx/onnx/releases/tag/v1.8.0\">\\n        Opset 13\\n       </a>\\n       operator of Onnx.\\n      </p>\\n     </section>\\n     <section id=\"session-api-improvement\">\\n      <h2>\\n       Session API Improvement\\n       <a class=\"headerlink\" href=\"#session-api-improvement\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       API Improvement 1 (\\n       <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/commit/b1d2b742f9f1ed43dbe7aa5f2ed822ae38d636e4\">\\n        commit b1d2b74\\n       </a>\\n       ): you can now designate NPU device\\nin Session API.\\nWith 0.5.0, you can designate NPU device when generating a session, whereas previously you could only designate\\ndevice to be used through the environment variable\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         NPU_DEVNAME\\n        </span>\\n       </code>\\n       .\\nIf not explicitly stated, designated device in the environment variable\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         NPU_DEVNAME\\n        </span>\\n       </code>\\n       will be used, as was done before.\\n      </p>\\n      <div class=\"highlight-python notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime</span> <span class=\"kn\">import</span> <span class=\"n\">session</span>\\n\\n<span class=\"n\">sess1</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"s2\">\"model1.onnx\"</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">\"npu0pe0\"</span><span class=\"p\">)</span>\\n<span class=\"n\">sess2</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"s2\">\"model2.onnx\"</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">\"npu0pe1\"</span><span class=\"p\">)</span>\\n\\n<span class=\"c1\"># Asynchronous API</span>\\n<span class=\"n\">async_sess</span><span class=\"p\">,</span> <span class=\"n\">queue</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">create_async</span><span class=\"p\">(</span><span class=\"s2\">\"model2.onnx\"</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">\"npu1pe2\"</span><span class=\"p\">)</span>\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       API Improvement 2 (\\n       <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/commit/4f1f1149d137a58ada31df57de6e1234881ccf5b\">\\n        commit 4f1f114\\n       </a>\\n       ) is support for tensor name.\\nThe existing API was limited in that it could only identify the order of the tensor built into the model, and pass it as an input parameter to\\n       <cite>\\n        session.run()\\n       </cite>\\n       .\\nFrom 0.5.0, you can explicitly designate name of input and output tensors as shown below.\\n      </p>\\n      <div class=\"highlight-python notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"n\">np1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">255</span><span class=\"p\">,</span> <span class=\"n\">session_input</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">)</span>\\n<span class=\"n\">np2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">255</span><span class=\"p\">,</span> <span class=\"n\">session_input</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">)</span>\\n<span class=\"n\">sess1</span><span class=\"o\">.</span><span class=\"n\">run_with</span><span class=\"p\">(</span><span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"output1\"</span><span class=\"p\">],</span> <span class=\"n\">inputs</span><span class=\"o\">=</span><span class=\"p\">{</span>\\n  <span class=\"s2\">\"input2\"</span><span class=\"p\">:</span> <span class=\"n\">np2</span><span class=\"p\">,</span>\\n  <span class=\"s2\">\"input1\"</span><span class=\"p\">:</span> <span class=\"n\">np1</span><span class=\"p\">,</span>\\n<span class=\"p\">}</span>\\n</pre>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"error-diagnosis-message-error-handling-improvements\">\\n      <h2>\\n       Error Diagnosis Message &amp; Error Handling Improvements\\n       <a class=\"headerlink\" href=\"#error-diagnosis-message-error-handling-improvements\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       In case of an error, version information for debugging and compiler log are output independently. This enables easier bug reporting.\\nYou can find more information about reporting issues and customer service at\\n       <a class=\"reference internal\" href=\"../customer-support/bugs.html#bugreport\">\\n        <span class=\"std std-ref\">\\n         Bug Report\\n        </span>\\n       </a>\\n       .\\n      </p>\\n      <div class=\"highlight-python notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime</span> <span class=\"kn\">import</span> <span class=\"n\">session</span>\\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"s2\">\"mnist-8.onnx\"</span><span class=\"p\">)</span>\\n<span class=\"go\">Saving the compilation log into /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log</span>\\n<span class=\"go\">...</span>\\n<span class=\"go\">2021-11-22T06:30:28.423371Z ERROR fail to compile the model: the static shape of tensor \\'input\\' contains an unsupported dimension value: Some(DimParam(\"batch_size\"))</span>\\n<span class=\"go\">================================================================================</span>\\n<span class=\"go\">Information Dump</span>\\n<span class=\"go\">================================================================================</span>\\n<span class=\"go\">- Python version: 3.8.10 (default, Sep 28 2021, 16:10:42)  [GCC 9.3.0]</span>\\n<span class=\"go\">- furiosa-libnux path: libnux.so.0.5.0</span>\\n<span class=\"go\">- furiosa-libnux version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34)</span>\\n<span class=\"go\">- furiosa-compiler version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34)</span>\\n<span class=\"go\">- furiosa-sdk-runtime version: Furiosa SDK Runtime  (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)</span>\\n\\n<span class=\"go\">Please check the compiler log at /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log.</span>\\n<span class=\"go\">If you have a problem, please report the log file to https://furiosa-ai.atlassian.net/servicedesk/customer/portals with the information dumped above.</span>\\n<span class=\"go\">================================================================================</span>\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       Improvements such as error handling for the following issues are also included.\\n      </p>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         Error fix when using duplicate devices (\\n         <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/commit/01aaa40fd31573dc578fa1c805e1ed36decc9088\">\\n          commit 01aaa40\\n         </a>\\n         )\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Added timeout of CompletionQueue &amp; error handling fix for session connection termination (\\n         <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/commit/21cba85737840546357f2dd709d33d9bc2b00390\">\\n          commit 21cba85\\n         </a>\\n         )\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Hanging issue fix for interruption during compiling\\n         <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/commit/a0f4bd7ab8d199e3e46f9820fe4dc75bfa845e61\">\\n          (commit a0f4bd7\\n         </a>\\n         )\\n        </p>\\n       </li>\\n      </ul>\\n     </section>\\n     <section id=\"introducing-furiosa-server-serving-framework\">\\n      <h2>\\n       Introducing Furiosa Server (serving framework)\\n       <a class=\"headerlink\" href=\"#introducing-furiosa-server-serving-framework\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       0.5.0 includes\\n       <cite>\\n        Furiosa Server\\n       </cite>\\n       , a serving framework that supports GRPC and REST API.\\nYou can easily install it by running\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         pip\\n        </span>\\n        <span class=\"pre\">\\n         install\\n        </span>\\n        <span class=\"pre\">\\n         furiosa-sdk[server]\\n        </span>\\n       </code>\\n       . By running it with the\\ncommand below, the model can be served immediately with the NPU.\\nYou can find more detailed instructions and functions at\\n       <a class=\"reference internal\" href=\"../software/serving.html#modelserving\">\\n        <span class=\"std std-ref\">\\n         Model Server (Serving Framework)\\n        </span>\\n       </a>\\n       .\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>furiosa<span class=\"w\"> </span>server<span class=\"w\"> </span><span class=\"se\">\\\\</span>\\n--model-path<span class=\"w\"> </span>MNISTnet_uint8_quant_without_softmax.tflite<span class=\"w\"> </span><span class=\"se\">\\\\</span>\\n--model-name<span class=\"w\"> </span>mnist\\n</pre>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"introducing-furiosa-model-package\">\\n      <h2>\\n       Introducing Furiosa Model package\\n       <a class=\"headerlink\" href=\"#introducing-furiosa-model-package\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       From 0.5.0, the optimized model for the FuriosaAI NPU can be used directly as a Python package.\\nYou can easily install it with the command\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         pip\\n        </span>\\n        <span class=\"pre\">\\n         install\\n        </span>\\n        <span class=\"pre\">\\n         furiosa-sdk[models]\\n        </span>\\n       </code>\\n       ,\\nand can immediately be used in Session API as shown in the following example.\\n      </p>\\n      <div class=\"highlight-python notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">asyncio</span>\\n\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.registry</span> <span class=\"kn\">import</span> <span class=\"n\">Model</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.models.vision</span> <span class=\"kn\">import</span> <span class=\"n\">MLCommonsResNet50</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime</span> <span class=\"kn\">import</span> <span class=\"n\">session</span>\\n\\n<span class=\"n\">resnet50</span><span class=\"p\">:</span> <span class=\"n\">Model</span> <span class=\"o\">=</span> <span class=\"n\">asyncio</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">MLCommonsResNet50</span><span class=\"p\">())</span>\\n<span class=\"n\">sess</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"n\">resnet50</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">\\'npu0pe0\\'</span><span class=\"p\">)</span>\\n</pre>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"command-line-npu-management-tool-furiosactl\">\\n      <h2>\\n       Command line NPU management tool: furiosactl\\n       <a class=\"headerlink\" href=\"#command-line-npu-management-tool-furiosactl\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       0.5.0 includes furiosactl, a command line NPU management tool.\\nYou can install it with\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         apt\\n        </span>\\n        <span class=\"pre\">\\n         install\\n        </span>\\n        <span class=\"pre\">\\n         furiosa-toolkit\\n        </span>\\n       </code>\\n       . You can use this tool to check\\nNPU device status, as well as identify idle NPUs.\\nYou can find\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         apt\\n        </span>\\n       </code>\\n       server configuration instructions at\\n       <a class=\"reference internal\" href=\"../software/installation.html#setupaptrepository\">\\n        <span class=\"std std-ref\">\\n         APT server configuration\\n        </span>\\n       </a>\\n       .\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span>furiosactl<span class=\"w\"> </span>info\\n\\n+------+------------------+-------+---------+--------------+---------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>NPU<span class=\"w\">  </span><span class=\"p\">|</span><span class=\"w\"> </span>Name<span class=\"w\">             </span><span class=\"p\">|</span><span class=\"w\"> </span>Temp.<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>Power<span class=\"w\">   </span><span class=\"p\">|</span><span class=\"w\"> </span>PCI-BDF<span class=\"w\">      </span><span class=\"p\">|</span><span class=\"w\"> </span>PCI-DEV<span class=\"w\"> </span><span class=\"p\">|</span>\\n+------+------------------+-------+---------+--------------+---------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>npu0<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>FuriosaAI<span class=\"w\"> </span>Warboy<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\">  </span><span class=\"m\">34</span>°C<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">12</span>.92<span class=\"w\"> </span>W<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0000</span>:01:00.0<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">510</span>:0<span class=\"w\">   </span><span class=\"p\">|</span>\\n+------+------------------+-------+---------+--------------+---------+\\n\\n$<span class=\"w\"> </span>furiosactl<span class=\"w\"> </span>list\\n+------+-----------+-----------+--------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>NPU<span class=\"w\">  </span><span class=\"p\">|</span><span class=\"w\"> </span>DEVNAME<span class=\"w\">   </span><span class=\"p\">|</span><span class=\"w\"> </span>Type<span class=\"w\">      </span><span class=\"p\">|</span><span class=\"w\"> </span>Status<span class=\"w\"> </span><span class=\"p\">|</span>\\n+------+-----------+-----------+--------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>npu0<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>npu0<span class=\"w\">      </span><span class=\"p\">|</span><span class=\"w\"> </span>All<span class=\"w\"> </span>PE<span class=\"o\">(</span>s<span class=\"o\">)</span><span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>Ready<span class=\"w\">  </span><span class=\"p\">|</span>\\n<span class=\"p\">|</span><span class=\"w\">      </span><span class=\"p\">|</span><span class=\"w\"> </span>npu0pe0<span class=\"w\">   </span><span class=\"p\">|</span><span class=\"w\"> </span>Single<span class=\"w\"> </span>PE<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>Ready<span class=\"w\">  </span><span class=\"p\">|</span>\\n<span class=\"p\">|</span><span class=\"w\">      </span><span class=\"p\">|</span><span class=\"w\"> </span>npu0pe1<span class=\"w\">   </span><span class=\"p\">|</span><span class=\"w\"> </span>Single<span class=\"w\"> </span>PE<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>Ready<span class=\"w\">  </span><span class=\"p\">|</span>\\n<span class=\"p\">|</span><span class=\"w\">      </span><span class=\"p\">|</span><span class=\"w\"> </span>npu0pe0-1<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>PE<span class=\"w\"> </span>Fusion<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>Ready<span class=\"w\">  </span><span class=\"p\">|</span>\\n+------+-----------+-----------+--------+\\n</pre>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"kubernetes-support\">\\n      <h2>\\n       Kubernetes support\\n       <a class=\"headerlink\" href=\"#kubernetes-support\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       0.5.0 includes NPU support for Kubernetes.\\nYou can install the NPU device plugin and node labeller with the command below,\\nand have the NPU be scheduled together when deploying pods.\\nMore details can be found at\\n       <a class=\"reference internal\" href=\"../software/kubernetes_support.html#kubernetesintegration\">\\n        <span class=\"std std-ref\">\\n         Kubernetes Support\\n        </span>\\n       </a>\\n       .\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>kubectl<span class=\"w\"> </span>apply<span class=\"w\"> </span>-f<span class=\"w\"> </span>https://raw.githubusercontent.com/furiosa-ai/furiosa-sdk/main/kubernetes/deployments/device-plugin.yaml\\nkubectl<span class=\"w\"> </span>apply<span class=\"w\"> </span>-f<span class=\"w\"> </span>https://raw.githubusercontent.com/furiosa-ai/furiosa-sdk/main/kubernetes/deployments/node-labeller.yaml\\n</pre>\\n       </div>\\n      </div>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"0.6.0.html\" rel=\"prev\" title=\"Release Notes - 0.6.0\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"../customer-support/bugs.html\" rel=\"next\" title=\"Bug Report\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='b6c45005-84e1-46e6-a185-42be1be00b6e', link=Url('https://furiosa-ai.github.io/docs/latest/en/software/compiler.html'), name='compiler', parent='', child=[], description='\\n\\n\\n* Compiler\\n* [View page source](../_sources/software/compiler.rst.txt)\\n\\n---\\n\\n\\n\\nCompiler\\n[\\uf0c1](#compiler \"Permalink to this heading\")\\n===================================================\\n\\nThe FuriosaAI compiler compiles models of formats\\n[TFLite](https://www.tensorflow.org/lite)\\nand\\n[Onnx](https://onnx.ai/)\\nmodel ((\\n[OpSet 13](https://github.com/onnx/onnx/blob/main/docs/Changelog.md#version-13-of-the-default-onnx-operator-set)\\nor lower version), thereby generating programs that execute inference using FuriosaAI NPU and resources (CPU, memory, etc) of the host machine.\\nIn this process, the compiler analyses the model at the operator level, optimizes it, and generates a program so as to maximize NPU acceleration and host resources utilization. Even for models that are not well known,\\nso long as supported operators are utilized well, you can design models that are optimized for the NPU .\\n\\nYou can find the list of NPU acceleration supported operators at\\n[List of Supported Operators for Warboy Acceleration](../npu/warboy.html#supportedoperators)\\n.\\n\\n`furiosa-compiler`\\n[\\uf0c1](#furiosa-compiler \"Permalink to this heading\")\\n---------------------------------------------------------------------\\n\\nThe most common ways to use a compiler would be to automatically call it\\nduring the process of resetting the inference API or the NPU.\\n\\nBut you can directly compile a model and generate a program by using the command line tool\\n`furiosa-compiler`\\nin shell. You can install\\n`furiosa-compiler`\\ncommand via APT package manager.\\n\\n```\\n$ apt install furiosa-compiler\\n\\n```\\n\\nThe usage of\\n`furiosa-compiler`\\nis as follows:\\n\\n```\\n$ furiosa-compiler --help\\n\\nFuriosa SDK Compiler v0.10.0 (f8f05c8ea 2023-07-31T19:30:30Z)\\n\\nUsage: furiosa-compiler [OPTIONS] <SOURCE>\\n\\nArguments:\\n  <SOURCE>\\n          Path to source file (tflite, onnx, and other IR formats, such as dfg, cdfg, gir, lir)\\n\\nOptions:\\n  -o, --output <OUTPUT>\\n          Writes output to <OUTPUT>\\n\\n          [default: output.<TARGET_IR>]\\n\\n  -b, --batch-size <BATCH_SIZE>\\n          Specifies the batch size which is effective when SOURCE is TFLite, ONNX, or DFG\\n\\n      --target-ir <TARGET_IR>\\n          (experimental) Target IR - possible values: [enf]\\n\\n          [default: enf]\\n\\n      --target-npu <TARGET_NPU>\\n          Target NPU family - possible values: [warboy, warboy-2pe]\\n\\n          [default: warboy-2pe]\\n\\n      --dot-graph <DOT_GRAPH>\\n          Filename to write DOT-formatted graph to\\n\\n      --analyze-memory <ANALYZE_MEMORY>\\n          Analyzes the memory allocation and save the report to <ANALYZE_MEMORY>\\n\\n  -v, --verbose\\n          Shows details about the compilation process\\n\\n      --no-cache\\n          Disables the compiler result cache\\n\\n  -h, --help\\n          Print help (see a summary with \\'-h\\')\\n\\n  -V, --version\\n          Print version\\n\\n```\\n\\n`SOURCE`\\nis the file path of\\n[TFLite](https://www.tensorflow.org/lite)\\nor\\n[ONNX](https://onnx.ai/)\\n.\\nYou have to use quantized models through\\n[Model Quantization](quantization.html#modelquantization)\\nfor NPU accleration.\\n\\nYou can omit the option\\n-o OUTPUT\\n\\n, and you can also choose to designate the output file name.\\nWhen omitted, the default output file name is\\n`output.enf`\\n. Here, enf stands for Executable NPU Format.\\nSo if you run as shown below, it will generate a\\n`output.enf`\\nfile.\\n\\n```\\nfuriosa-compiler foo.onnx\\n\\n```\\n\\nIf you designate the output file name as below, it will generate a\\n`foo.enf`\\nfile.\\n\\n```\\nfuriosa-compiler foo.onnx -o foo.enf\\n\\n```\\n\\n`--target-npu`\\nlets the generated binary to designate target NPU는.\\n\\n\\nTarget NPUs\\n\\n[\\uf0c1](#id4 \"Permalink to this table\")\\n\\n\\n\\n| NPU Family | Number of PEs | Value |\\n| --- | --- | --- |\\n| Warboy | 1 | warboy |\\n| Warboy | 2 | warboy-2pe |\\n\\nIf generated program’s target NPU is Warboy that uses one PE independently, you can run the following command.\\n\\n```\\nfuriosa-compiler foo.onnx --target-npu warboy\\n\\n```\\n\\nWhen 2 PEs are fused, execute as follows.\\n\\n```\\nfuriosa-compiler foo.onnx --target-npu warboy-2pe\\n\\n```\\n\\nThe\\n`--batch-size`\\noption lets you specify\\nbatch size\\n\\n, the number of samples\\nto be passed as input when executing inference through the inference API.\\nThe larger the batch size, the higher the NPU utilization, since more data is given as input and executed\\nat once. This allows the inference process to be shared across the batch, increasing efficiency.\\nHowever, if the larger batch size results in the necessary memory size exceeding NPU DRAM size,\\nthe memory I/O cost between the host and the NPU may increase and lead to significant performance degradation.\\nThe default value of batch size is one. Appropriate value can usually be found through trial and error.\\nFor reference, the optimal batch sizes for some models included in the\\n[MLPerf™ Inference Edge v2.0](https://mlcommons.org/en/inference-edge-20/)\\nbenchmark are as follows.\\n\\n\\nOptimal Batch Size for Well-known Models\\n\\n[\\uf0c1](#id5 \"Permalink to this table\")\\n\\n\\n| Model | Optimal Batch |\\n| --- | --- |\\n| SSD-MobileNets-v1 | 2 |\\n| Resnet50-v1.5 | 1 |\\n| SSD-ResNet34 | 1 |\\n\\nIf your desired batch size is two, you can run the following command.\\n\\n```\\nfuriosa-compiler foo.onnx --batch-size 2\\n\\n```\\n\\n\\n\\nUsing ENF files\\n[\\uf0c1](#using-enf-files \"Permalink to this heading\")\\n-----------------------------------------------------------------\\n\\nAfter the compilation process, the final output of the FuriosaAI compiler is ENF (Executable NPU Format) type data.\\nIn general, the compilation process takes from a few seconds to several minutes depending on the model.\\nOnce you have the ENF file, you can reuse it to omit this compilation process.\\n\\nThis may be useful if you need to frequently create sessions or\\nserve one model across several machines in an actual operation environment.\\n\\nFor example, you can first create an ENF file by referring to\\n[furiosa-compiler](#compilercli)\\n.\\nThen, with\\n[PythonSDK](python-sdk.html#pythonsdk)\\nas shown below,\\nyou can instantly create a runner without the compilation process by\\npassing the ENF file as an argument to the\\n`create_runner()`\\nfunction as follows:\\n\\n```\\nfrom furiosa.runtime import sync\\n\\nwith sync.create_runner(\"path/to/model.enf\") as runner:\\n  outputs = runner.run(inputs)\\n\\n```\\n\\n\\n\\nCompiler Cache\\n[\\uf0c1](#compiler-cache \"Permalink to this heading\")\\n---------------------------------------------------------------\\n\\nCompiler cache allows to user applications to reuse once-compiled results.\\nIt’s very helpful especially when you are developing applications because the compilation\\nusually takes at least a couple of minutes.\\n\\nBy default, the compiler cache uses a local file system (\\n`$HOME/.cache/furiosa/compiler`\\n) as a cache storage.\\nIf you specify a configuration, you can also use Redis as a remote and distributed cache storage.\\n\\nThe compiler cache is enabled by default, but you can explicitly enable or disable the cache by setting\\n`FC_CACHE_ENABLED`\\n.\\nThis setting is effective in CLI tools, Python SDK, and serving frameworks.\\n\\n```\\n# Enable Compiler Cache\\nexport FC_CACHE_ENABLED=1\\n# Disable Compiler Cache\\nexport FC_CACHE_ENABLED=0\\n\\n```\\n\\nThe default cache location is\\n`$HOME/.cache/furiosa/compiler`\\n, but you can explicitly specify the cache storage\\nby setting the shell environment variable\\n`FC_CACHE_STORE_URL`\\n. If you want to Redis as a cache storage,\\nyou can specify some URLs starting with\\n`redis://`\\nor\\n`rediss://`\\n(over SSL).\\n\\n```\\n# When you want to specify a cache directory\\nexport FC_CACHE_STORE_URL=/tmp/cache\\n\\n# When you want to specify a Redis cluster as the cache storage\\nexport FC_CACHE_STORE_URL=redis://:<PASSWORD>@127.0.0.1:6379\\n# When you want to specify a Redis cluster over SSL as the cache storage\\nexport FC_CACHE_STORE_URL=rediss://:<PASSWORD>@127.0.0.1:25945\\n\\n```\\n\\nThe cache will be valid for 30 days by default, but you can explicitly specify the cache lifetime by setting\\nseconds to the environment variable\\n`FC_CACHE_LIFETIME`\\n.\\n\\n```\\n# 2 hours cache lifetime\\nexport FC_CACHE_LIFETIME=7200\\n\\n```\\n\\nAlso, you can control more the cache behavior according to your purpose as follows:\\n\\n\\nCache behaviors according to\\n`FC_CACHE_LIFETIME`\\n\\n[\\uf0c1](#id6 \"Permalink to this table\")\\n\\n\\n\\n| Value (secs) | Description | Example |\\n| --- | --- | --- |\\n| *N* > 0 | Cache will be alive for N secs | 7200 (2 hours) |\\n| 0 | All previous cache will be invalidated. (When you want to compile the model without cache) | 0 |\\n| *N* < 0 | Cache will be alive forever without expiration. (it can be useful when you want read-only cache) | -1 |\\n\\n\\n\\n\\n\\n[Previous](cli.html \"Command Line Tools\")\\n[Next](quantization.html \"Model Quantization\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Compiler * [View page source](../_sources/software/compiler.rst.txt)\\n---\\nCompiler [\\uf0c1](#compiler \"Permalink to this heading\") ===================================================\\nThe FuriosaAI compiler compiles models of formats [TFLite](https://www.tensorflow.org/lite) and [Onnx](https://onnx.ai/) model (( [OpSet 13](https://github.com/onnx/onnx/blob/main/docs/Changelog.md#version-13-of-the-default-onnx-operator-set) or lower version), thereby generating programs that execute inference using FuriosaAI NPU and resources (CPU, memory, etc) of the host machine. In this process, the compiler analyses the model at the operator level, optimizes it, and generates a program so as to maximize NPU acceleration and host resources utilization. Even for models that are not well known, so long as supported operators are utilized well, you can design models that are optimized for the NPU .\\nYou can find the list of NPU acceleration supported operators at [List of Supported Operators for Warboy Acceleration](../npu/warboy.html#supportedoperators) .\\n `furiosa-compiler` [\\uf0c1](#furiosa-compiler \"Permalink to this heading\") ---------------------------------------------------------------------\\nThe most common ways to use a compiler would be to automatically call it during the process of resetting the inference API or the NPU.\\nBut you can directly compile a model and generate a program by using the command line tool `furiosa-compiler` in shell. You can install `furiosa-compiler` command via APT package manager.\\n``` $ apt install furiosa-compiler\\n```\\nThe usage of `furiosa-compiler` is as follows:\\n``` $ furiosa-compiler --help\\nFuriosa SDK Compiler v0.10.0 (f8f05c8ea 2023-07-31T19:30:30Z)\\nUsage: furiosa-compiler [OPTIONS] <SOURCE>\\nArguments:   <SOURCE>           Path to source file (tflite, onnx, and other IR formats, such as dfg, cdfg, gir, lir)\\nOptions:   -o, --output <OUTPUT>           Writes output to <OUTPUT>\\n          [default: output.<TARGET_IR>]\\n  -b, --batch-size <BATCH_SIZE>           Specifies the batch size which is effective when SOURCE is TFLite, ONNX, or DFG\\n      --target-ir <TARGET_IR>           (experimental) Target IR - possible values: [enf]\\n          [default: enf]\\n      --target-npu <TARGET_NPU>           Target NPU family - possible values: [warboy, warboy-2pe]\\n          [default: warboy-2pe]\\n      --dot-graph <DOT_GRAPH>           Filename to write DOT-formatted graph to\\n      --analyze-memory <ANALYZE_MEMORY>           Analyzes the memory allocation and save the report to <ANALYZE_MEMORY>\\n  -v, --verbose           Shows details about the compilation process\\n      --no-cache           Disables the compiler result cache\\n  -h, --help           Print help (see a summary with \\'-h\\')\\n  -V, --version           Print version\\n```  `SOURCE` is the file path of [TFLite](https://www.tensorflow.org/lite) or [ONNX](https://onnx.ai/) .\\nYou have to use quantized models through [Model Quantization](quantization.html#modelquantization) for NPU accleration.\\nYou can omit the option -o OUTPUT\\n, and you can also choose to designate the output file name. When omitted, the default output file name is `output.enf` . Here, enf stands for Executable NPU Format. So if you run as shown below, it will generate a `output.enf` file.\\n``` furiosa-compiler foo.onnx\\n```\\nIf you designate the output file name as below, it will generate a `foo.enf` file.\\n``` furiosa-compiler foo.onnx -o foo.enf\\n```  `--target-npu` lets the generated binary to designate target NPU는.\\nTarget NPUs\\n[\\uf0c1](#id4 \"Permalink to this table\")\\n| NPU Family | Number of PEs | Value | | --- | --- | --- | | Warboy | 1 | warboy | | Warboy | 2 | warboy-2pe |\\nIf generated program’s target NPU is Warboy that uses one PE independently, you can run the following command.\\n``` furiosa-compiler foo.onnx --target-npu warboy\\n```\\nWhen 2 PEs are fused, execute as follows.\\n``` furiosa-compiler foo.onnx --target-npu warboy-2pe\\n```\\nThe `--batch-size` option lets you specify batch size\\n, the number of samples to be passed as input when executing inference through the inference API. The larger the batch size, the higher the NPU utilization, since more data is given as input and executed at once. This allows the inference process to be shared across the batch, increasing efficiency. However, if the larger batch size results in the necessary memory size exceeding NPU DRAM size, the memory I/O cost between the host and the NPU may increase and lead to significant performance degradation. The default value of batch size is one. Appropriate value can usually be found through trial and error. For reference, the optimal batch sizes for some models included in the [MLPerf™ Inference Edge v2.0](https://mlcommons.org/en/inference-edge-20/) benchmark are as follows.\\nOptimal Batch Size for Well-known Models\\n[\\uf0c1](#id5 \"Permalink to this table\")\\n| Model | Optimal Batch | | --- | --- | | SSD-MobileNets-v1 | 2 | | Resnet50-v1.5 | 1 | | SSD-ResNet34 | 1 |\\nIf your desired batch size is two, you can run the following command.\\n``` furiosa-compiler foo.onnx --batch-size 2\\n```\\nUsing ENF files [\\uf0c1](#using-enf-files \"Permalink to this heading\") -----------------------------------------------------------------\\nAfter the compilation process, the final output of the FuriosaAI compiler is ENF (Executable NPU Format) type data. In general, the compilation process takes from a few seconds to several minutes depending on the model. Once you have the ENF file, you can reuse it to omit this compilation process.\\nThis may be useful if you need to frequently create sessions or serve one model across several machines in an actual operation environment.\\nFor example, you can first create an ENF file by referring to [furiosa-compiler](#compilercli) .\\nThen, with [PythonSDK](python-sdk.html#pythonsdk) as shown below, you can instantly create a runner without the compilation process by passing the ENF file as an argument to the `create_runner()` function as follows:\\n``` from furiosa.runtime import sync\\nwith sync.create_runner(\"path/to/model.enf\") as runner:   outputs = runner.run(inputs)\\n```\\nCompiler Cache [\\uf0c1](#compiler-cache \"Permalink to this heading\") ---------------------------------------------------------------\\nCompiler cache allows to user applications to reuse once-compiled results. It’s very helpful especially when you are developing applications because the compilation usually takes at least a couple of minutes.\\nBy default, the compiler cache uses a local file system ( `$HOME/.cache/furiosa/compiler` ) as a cache storage. If you specify a configuration, you can also use Redis as a remote and distributed cache storage.\\nThe compiler cache is enabled by default, but you can explicitly enable or disable the cache by setting `FC_CACHE_ENABLED` . This setting is effective in CLI tools, Python SDK, and serving frameworks.\\n``` # Enable Compiler Cache export FC_CACHE_ENABLED=1 # Disable Compiler Cache export FC_CACHE_ENABLED=0\\n```\\nThe default cache location is `$HOME/.cache/furiosa/compiler` , but you can explicitly specify the cache storage by setting the shell environment variable `FC_CACHE_STORE_URL` . If you want to Redis as a cache storage, you can specify some URLs starting with `redis://` or `rediss://` (over SSL).\\n``` # When you want to specify a cache directory export FC_CACHE_STORE_URL=/tmp/cache\\n# When you want to specify a Redis cluster as the cache storage export FC_CACHE_STORE_URL=redis://:<PASSWORD>@127.0.0.1:6379 # When you want to specify a Redis cluster over SSL as the cache storage export FC_CACHE_STORE_URL=rediss://:<PASSWORD>@127.0.0.1:25945\\n```\\nThe cache will be valid for 30 days by default, but you can explicitly specify the cache lifetime by setting seconds to the environment variable `FC_CACHE_LIFETIME` .\\n``` # 2 hours cache lifetime export FC_CACHE_LIFETIME=7200\\n```\\nAlso, you can control more the cache behavior according to your purpose as follows:\\nCache behaviors according to `FC_CACHE_LIFETIME`  [\\uf0c1](#id6 \"Permalink to this table\")\\n| Value (secs) | Description | Example | | --- | --- | --- | | *N* > 0 | Cache will be alive for N secs | 7200 (2 hours) | | 0 | All previous cache will be invalidated. (When you want to compile the model without cache) | 0 | | *N* < 0 | Cache will be alive forever without expiration. (it can be useful when you want read-only cache) | -1 |\\n[Previous](cli.html \"Command Line Tools\") [Next](quantization.html \"Model Quantization\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Compiler\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/compiler.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"compiler\">\\n     <span id=\"id1\">\\n     </span>\\n     <h1>\\n      Compiler\\n      <a class=\"headerlink\" href=\"#compiler\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      The FuriosaAI compiler compiles models of formats\\n      <a class=\"reference external\" href=\"https://www.tensorflow.org/lite\">\\n       TFLite\\n      </a>\\n      and\\n      <a class=\"reference external\" href=\"https://onnx.ai/\">\\n       Onnx\\n      </a>\\n      model ((\\n      <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/main/docs/Changelog.md#version-13-of-the-default-onnx-operator-set\">\\n       OpSet 13\\n      </a>\\n      or lower version), thereby generating programs that execute inference using FuriosaAI NPU and resources (CPU, memory, etc) of the host machine.\\nIn this process, the compiler analyses the model at the operator level, optimizes it, and generates a program so as to maximize NPU acceleration and host resources utilization. Even for models that are not well known,\\nso long as supported operators are utilized well, you can design models that are optimized for the NPU .\\n     </p>\\n     <p>\\n      You can find the list of NPU acceleration supported operators at\\n      <a class=\"reference internal\" href=\"../npu/warboy.html#supportedoperators\">\\n       <span class=\"std std-ref\">\\n        List of Supported Operators for Warboy Acceleration\\n       </span>\\n      </a>\\n      .\\n     </p>\\n     <section id=\"furiosa-compiler\">\\n      <span id=\"compilercli\">\\n      </span>\\n      <h2>\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         furiosa-compiler\\n        </span>\\n       </code>\\n       <a class=\"headerlink\" href=\"#furiosa-compiler\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       The most common ways to use a compiler would be to automatically call it\\nduring the process of resetting the inference API or the NPU.\\n      </p>\\n      <p>\\n       But you can directly compile a model and generate a program by using the command line tool\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         furiosa-compiler\\n        </span>\\n       </code>\\n       in shell. You can install\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         furiosa-compiler\\n        </span>\\n       </code>\\n       command via APT package manager.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span>apt<span class=\"w\"> </span>install<span class=\"w\"> </span>furiosa-compiler\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       The usage of\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         furiosa-compiler\\n        </span>\\n       </code>\\n       is as follows:\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span>furiosa-compiler<span class=\"w\"> </span>--help\\n\\nFuriosa<span class=\"w\"> </span>SDK<span class=\"w\"> </span>Compiler<span class=\"w\"> </span>v0.10.0<span class=\"w\"> </span><span class=\"o\">(</span>f8f05c8ea<span class=\"w\"> </span><span class=\"m\">2023</span>-07-31T19:30:30Z<span class=\"o\">)</span>\\n\\nUsage:<span class=\"w\"> </span>furiosa-compiler<span class=\"w\"> </span><span class=\"o\">[</span>OPTIONS<span class=\"o\">]</span><span class=\"w\"> </span>&lt;SOURCE&gt;\\n\\nArguments:\\n<span class=\"w\">  </span>&lt;SOURCE&gt;\\n<span class=\"w\">          </span>Path<span class=\"w\"> </span>to<span class=\"w\"> </span><span class=\"nb\">source</span><span class=\"w\"> </span>file<span class=\"w\"> </span><span class=\"o\">(</span>tflite,<span class=\"w\"> </span>onnx,<span class=\"w\"> </span>and<span class=\"w\"> </span>other<span class=\"w\"> </span>IR<span class=\"w\"> </span>formats,<span class=\"w\"> </span>such<span class=\"w\"> </span>as<span class=\"w\"> </span>dfg,<span class=\"w\"> </span>cdfg,<span class=\"w\"> </span>gir,<span class=\"w\"> </span>lir<span class=\"o\">)</span>\\n\\nOptions:\\n<span class=\"w\">  </span>-o,<span class=\"w\"> </span>--output<span class=\"w\"> </span>&lt;OUTPUT&gt;\\n<span class=\"w\">          </span>Writes<span class=\"w\"> </span>output<span class=\"w\"> </span>to<span class=\"w\"> </span>&lt;OUTPUT&gt;\\n\\n<span class=\"w\">          </span><span class=\"o\">[</span>default:<span class=\"w\"> </span>output.&lt;TARGET_IR&gt;<span class=\"o\">]</span>\\n\\n<span class=\"w\">  </span>-b,<span class=\"w\"> </span>--batch-size<span class=\"w\"> </span>&lt;BATCH_SIZE&gt;\\n<span class=\"w\">          </span>Specifies<span class=\"w\"> </span>the<span class=\"w\"> </span>batch<span class=\"w\"> </span>size<span class=\"w\"> </span>which<span class=\"w\"> </span>is<span class=\"w\"> </span>effective<span class=\"w\"> </span>when<span class=\"w\"> </span>SOURCE<span class=\"w\"> </span>is<span class=\"w\"> </span>TFLite,<span class=\"w\"> </span>ONNX,<span class=\"w\"> </span>or<span class=\"w\"> </span>DFG\\n\\n<span class=\"w\">      </span>--target-ir<span class=\"w\"> </span>&lt;TARGET_IR&gt;\\n<span class=\"w\">          </span><span class=\"o\">(</span>experimental<span class=\"o\">)</span><span class=\"w\"> </span>Target<span class=\"w\"> </span>IR<span class=\"w\"> </span>-<span class=\"w\"> </span>possible<span class=\"w\"> </span>values:<span class=\"w\"> </span><span class=\"o\">[</span>enf<span class=\"o\">]</span>\\n\\n<span class=\"w\">          </span><span class=\"o\">[</span>default:<span class=\"w\"> </span>enf<span class=\"o\">]</span>\\n\\n<span class=\"w\">      </span>--target-npu<span class=\"w\"> </span>&lt;TARGET_NPU&gt;\\n<span class=\"w\">          </span>Target<span class=\"w\"> </span>NPU<span class=\"w\"> </span>family<span class=\"w\"> </span>-<span class=\"w\"> </span>possible<span class=\"w\"> </span>values:<span class=\"w\"> </span><span class=\"o\">[</span>warboy,<span class=\"w\"> </span>warboy-2pe<span class=\"o\">]</span>\\n\\n<span class=\"w\">          </span><span class=\"o\">[</span>default:<span class=\"w\"> </span>warboy-2pe<span class=\"o\">]</span>\\n\\n<span class=\"w\">      </span>--dot-graph<span class=\"w\"> </span>&lt;DOT_GRAPH&gt;\\n<span class=\"w\">          </span>Filename<span class=\"w\"> </span>to<span class=\"w\"> </span>write<span class=\"w\"> </span>DOT-formatted<span class=\"w\"> </span>graph<span class=\"w\"> </span>to\\n\\n<span class=\"w\">      </span>--analyze-memory<span class=\"w\"> </span>&lt;ANALYZE_MEMORY&gt;\\n<span class=\"w\">          </span>Analyzes<span class=\"w\"> </span>the<span class=\"w\"> </span>memory<span class=\"w\"> </span>allocation<span class=\"w\"> </span>and<span class=\"w\"> </span>save<span class=\"w\"> </span>the<span class=\"w\"> </span>report<span class=\"w\"> </span>to<span class=\"w\"> </span>&lt;ANALYZE_MEMORY&gt;\\n\\n<span class=\"w\">  </span>-v,<span class=\"w\"> </span>--verbose\\n<span class=\"w\">          </span>Shows<span class=\"w\"> </span>details<span class=\"w\"> </span>about<span class=\"w\"> </span>the<span class=\"w\"> </span>compilation<span class=\"w\"> </span>process\\n\\n<span class=\"w\">      </span>--no-cache\\n<span class=\"w\">          </span>Disables<span class=\"w\"> </span>the<span class=\"w\"> </span>compiler<span class=\"w\"> </span>result<span class=\"w\"> </span>cache\\n\\n<span class=\"w\">  </span>-h,<span class=\"w\"> </span>--help\\n<span class=\"w\">          </span>Print<span class=\"w\"> </span><span class=\"nb\">help</span><span class=\"w\"> </span><span class=\"o\">(</span>see<span class=\"w\"> </span>a<span class=\"w\"> </span>summary<span class=\"w\"> </span>with<span class=\"w\"> </span><span class=\"s1\">\\'-h\\'</span><span class=\"o\">)</span>\\n\\n<span class=\"w\">  </span>-V,<span class=\"w\"> </span>--version\\n<span class=\"w\">          </span>Print<span class=\"w\"> </span>version\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         SOURCE\\n        </span>\\n       </code>\\n       is the file path of\\n       <a class=\"reference external\" href=\"https://www.tensorflow.org/lite\">\\n        TFLite\\n       </a>\\n       or\\n       <a class=\"reference external\" href=\"https://onnx.ai/\">\\n        ONNX\\n       </a>\\n       .\\nYou have to use quantized models through\\n       <a class=\"reference internal\" href=\"quantization.html#modelquantization\">\\n        <span class=\"std std-ref\">\\n         Model Quantization\\n        </span>\\n       </a>\\n       for NPU accleration.\\n      </p>\\n      <p>\\n       You can omit the option\\n       <cite>\\n        -o OUTPUT\\n       </cite>\\n       , and you can also choose to designate the output file name.\\nWhen omitted, the default output file name is\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         output.enf\\n        </span>\\n       </code>\\n       . Here, enf stands for Executable NPU Format.\\nSo if you run as shown below, it will generate a\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         output.enf\\n        </span>\\n       </code>\\n       file.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>furiosa-compiler<span class=\"w\"> </span>foo.onnx\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       If you designate the output file name as below, it will generate a\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         foo.enf\\n        </span>\\n       </code>\\n       file.\\n      </p>\\n      <div class=\"highlight-default notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"n\">furiosa</span><span class=\"o\">-</span><span class=\"n\">compiler</span> <span class=\"n\">foo</span><span class=\"o\">.</span><span class=\"n\">onnx</span> <span class=\"o\">-</span><span class=\"n\">o</span> <span class=\"n\">foo</span><span class=\"o\">.</span><span class=\"n\">enf</span>\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         --target-npu\\n        </span>\\n       </code>\\n       lets the generated binary to designate target NPU는.\\n      </p>\\n      <table class=\"docutils align-default\" id=\"id4\">\\n       <caption>\\n        <span class=\"caption-text\">\\n         Target NPUs\\n        </span>\\n        <a class=\"headerlink\" href=\"#id4\" title=\"Permalink to this table\">\\n         \\uf0c1\\n        </a>\\n       </caption>\\n       <colgroup>\\n        <col style=\"width: 33%\"/>\\n        <col style=\"width: 33%\"/>\\n        <col style=\"width: 33%\"/>\\n       </colgroup>\\n       <thead>\\n        <tr class=\"row-odd\">\\n         <th class=\"head\">\\n          <p>\\n           NPU Family\\n          </p>\\n         </th>\\n         <th class=\"head\">\\n          <p>\\n           Number of PEs\\n          </p>\\n         </th>\\n         <th class=\"head\">\\n          <p>\\n           Value\\n          </p>\\n         </th>\\n        </tr>\\n       </thead>\\n       <tbody>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           Warboy\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           1\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           warboy\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           Warboy\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           2\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           warboy-2pe\\n          </p>\\n         </td>\\n        </tr>\\n       </tbody>\\n      </table>\\n      <p>\\n       If generated program’s target NPU is Warboy that uses one PE independently, you can run the following command.\\n      </p>\\n      <div class=\"highlight-default notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"n\">furiosa</span><span class=\"o\">-</span><span class=\"n\">compiler</span> <span class=\"n\">foo</span><span class=\"o\">.</span><span class=\"n\">onnx</span> <span class=\"o\">--</span><span class=\"n\">target</span><span class=\"o\">-</span><span class=\"n\">npu</span> <span class=\"n\">warboy</span>\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       When 2 PEs are fused, execute as follows.\\n      </p>\\n      <div class=\"highlight-default notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"n\">furiosa</span><span class=\"o\">-</span><span class=\"n\">compiler</span> <span class=\"n\">foo</span><span class=\"o\">.</span><span class=\"n\">onnx</span> <span class=\"o\">--</span><span class=\"n\">target</span><span class=\"o\">-</span><span class=\"n\">npu</span> <span class=\"n\">warboy</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"n\">pe</span>\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       The\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         --batch-size\\n        </span>\\n       </code>\\n       option lets you specify\\n       <cite>\\n        batch size\\n       </cite>\\n       , the number of samples\\nto be passed as input when executing inference through the inference API.\\nThe larger the batch size, the higher the NPU utilization, since more data is given as input and executed\\nat once. This allows the inference process to be shared across the batch, increasing efficiency.\\nHowever, if the larger batch size results in the necessary memory size exceeding NPU DRAM size,\\nthe memory I/O cost between the host and the NPU may increase and lead to significant performance degradation.\\nThe default value of batch size is one. Appropriate value can usually be found through trial and error.\\nFor reference, the optimal batch sizes for some models included in the\\n       <a class=\"reference external\" href=\"https://mlcommons.org/en/inference-edge-20/\">\\n        MLPerf™ Inference Edge v2.0\\n       </a>\\n       benchmark are as follows.\\n      </p>\\n      <table class=\"docutils align-default\" id=\"id5\">\\n       <caption>\\n        <span class=\"caption-text\">\\n         Optimal Batch Size for Well-known Models\\n        </span>\\n        <a class=\"headerlink\" href=\"#id5\" title=\"Permalink to this table\">\\n         \\uf0c1\\n        </a>\\n       </caption>\\n       <colgroup>\\n        <col style=\"width: 50%\"/>\\n        <col style=\"width: 50%\"/>\\n       </colgroup>\\n       <thead>\\n        <tr class=\"row-odd\">\\n         <th class=\"head\">\\n          <p>\\n           Model\\n          </p>\\n         </th>\\n         <th class=\"head\">\\n          <p>\\n           Optimal Batch\\n          </p>\\n         </th>\\n        </tr>\\n       </thead>\\n       <tbody>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           SSD-MobileNets-v1\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           2\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           Resnet50-v1.5\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           1\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           SSD-ResNet34\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           1\\n          </p>\\n         </td>\\n        </tr>\\n       </tbody>\\n      </table>\\n      <p>\\n       If your desired batch size is two, you can run the following command.\\n      </p>\\n      <div class=\"highlight-default notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"n\">furiosa</span><span class=\"o\">-</span><span class=\"n\">compiler</span> <span class=\"n\">foo</span><span class=\"o\">.</span><span class=\"n\">onnx</span> <span class=\"o\">--</span><span class=\"n\">batch</span><span class=\"o\">-</span><span class=\"n\">size</span> <span class=\"mi\">2</span>\\n</pre>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"using-enf-files\">\\n      <h2>\\n       Using ENF files\\n       <a class=\"headerlink\" href=\"#using-enf-files\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       After the compilation process, the final output of the FuriosaAI compiler is ENF (Executable NPU Format) type data.\\nIn general, the compilation process takes from a few seconds to several minutes depending on the model.\\nOnce you have the ENF file, you can reuse it to omit this compilation process.\\n      </p>\\n      <p>\\n       This may be useful if you need to frequently create sessions or\\nserve one model across several machines in an actual operation environment.\\n      </p>\\n      <p>\\n       For example, you can first create an ENF file by referring to\\n       <a class=\"reference internal\" href=\"#compilercli\">\\n        <span class=\"std std-ref\">\\n         furiosa-compiler\\n        </span>\\n       </a>\\n       .\\nThen, with\\n       <a class=\"reference internal\" href=\"python-sdk.html#pythonsdk\">\\n        <span class=\"std std-ref\">\\n         PythonSDK\\n        </span>\\n       </a>\\n       as shown below,\\nyou can instantly create a runner without the compilation process by\\npassing the ENF file as an argument to the\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         create_runner()\\n        </span>\\n       </code>\\n       function as follows:\\n      </p>\\n      <div class=\"highlight-python notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime</span> <span class=\"kn\">import</span> <span class=\"n\">sync</span>\\n\\n<span class=\"k\">with</span> <span class=\"n\">sync</span><span class=\"o\">.</span><span class=\"n\">create_runner</span><span class=\"p\">(</span><span class=\"s2\">\"path/to/model.enf\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">runner</span><span class=\"p\">:</span>\\n  <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">runner</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\\n</pre>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"compiler-cache\">\\n      <span id=\"compilercache\">\\n      </span>\\n      <h2>\\n       Compiler Cache\\n       <a class=\"headerlink\" href=\"#compiler-cache\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       Compiler cache allows to user applications to reuse once-compiled results.\\nIt’s very helpful especially when you are developing applications because the compilation\\nusually takes at least a couple of minutes.\\n      </p>\\n      <p>\\n       By default, the compiler cache uses a local file system (\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         $HOME/.cache/furiosa/compiler\\n        </span>\\n       </code>\\n       ) as a cache storage.\\nIf you specify a configuration, you can also use Redis as a remote and distributed cache storage.\\n      </p>\\n      <p>\\n       The compiler cache is enabled by default, but you can explicitly enable or disable the cache by setting\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         FC_CACHE_ENABLED\\n        </span>\\n       </code>\\n       .\\nThis setting is effective in CLI tools, Python SDK, and serving frameworks.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"c1\"># Enable Compiler Cache</span>\\n<span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">FC_CACHE_ENABLED</span><span class=\"o\">=</span><span class=\"m\">1</span>\\n<span class=\"c1\"># Disable Compiler Cache</span>\\n<span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">FC_CACHE_ENABLED</span><span class=\"o\">=</span><span class=\"m\">0</span>\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       The default cache location is\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         $HOME/.cache/furiosa/compiler\\n        </span>\\n       </code>\\n       , but you can explicitly specify the cache storage\\nby setting the shell environment variable\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         FC_CACHE_STORE_URL\\n        </span>\\n       </code>\\n       . If you want to Redis as a cache storage,\\nyou can specify some URLs starting with\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         redis://\\n        </span>\\n       </code>\\n       or\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         rediss://\\n        </span>\\n       </code>\\n       (over SSL).\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"c1\"># When you want to specify a cache directory</span>\\n<span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">FC_CACHE_STORE_URL</span><span class=\"o\">=</span>/tmp/cache\\n\\n<span class=\"c1\"># When you want to specify a Redis cluster as the cache storage</span>\\n<span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">FC_CACHE_STORE_URL</span><span class=\"o\">=</span>redis://:&lt;PASSWORD&gt;@127.0.0.1:6379\\n<span class=\"c1\"># When you want to specify a Redis cluster over SSL as the cache storage</span>\\n<span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">FC_CACHE_STORE_URL</span><span class=\"o\">=</span>rediss://:&lt;PASSWORD&gt;@127.0.0.1:25945\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       The cache will be valid for 30 days by default, but you can explicitly specify the cache lifetime by setting\\nseconds to the environment variable\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         FC_CACHE_LIFETIME\\n        </span>\\n       </code>\\n       .\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"c1\"># 2 hours cache lifetime</span>\\n<span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">FC_CACHE_LIFETIME</span><span class=\"o\">=</span><span class=\"m\">7200</span>\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       Also, you can control more the cache behavior according to your purpose as follows:\\n      </p>\\n      <table class=\"docutils align-default\" id=\"id6\">\\n       <caption>\\n        <span class=\"caption-text\">\\n         Cache behaviors according to\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           FC_CACHE_LIFETIME\\n          </span>\\n         </code>\\n        </span>\\n        <a class=\"headerlink\" href=\"#id6\" title=\"Permalink to this table\">\\n         \\uf0c1\\n        </a>\\n       </caption>\\n       <colgroup>\\n        <col style=\"width: 17%\"/>\\n        <col style=\"width: 67%\"/>\\n        <col style=\"width: 17%\"/>\\n       </colgroup>\\n       <thead>\\n        <tr class=\"row-odd\">\\n         <th class=\"head\">\\n          <p>\\n           Value (secs)\\n          </p>\\n         </th>\\n         <th class=\"head\">\\n          <p>\\n           Description\\n          </p>\\n         </th>\\n         <th class=\"head\">\\n          <p>\\n           Example\\n          </p>\\n         </th>\\n        </tr>\\n       </thead>\\n       <tbody>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <em>\\n            N\\n           </em>\\n           &gt; 0\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Cache will be alive for N secs\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           7200 (2 hours)\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           0\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           All previous cache will be invalidated. (When you want to compile the model without cache)\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           0\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <em>\\n            N\\n           </em>\\n           &lt; 0\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Cache will be alive forever without expiration. (it can be useful when you want read-only cache)\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           -1\\n          </p>\\n         </td>\\n        </tr>\\n       </tbody>\\n      </table>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"cli.html\" rel=\"prev\" title=\"Command Line Tools\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"quantization.html\" rel=\"next\" title=\"Model Quantization\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='a50ce0f1-25f2-4882-a3e5-8da4cb7215b7', link=Url('https://furiosa-ai.github.io/docs/latest/en/software/references.html'), name='references', parent='', child=[], description='\\n\\n\\n* References\\n* [View page source](../_sources/software/references.rst.txt)\\n\\n---\\n\\n\\n\\nReferences\\n[\\uf0c1](#references \"Permalink to this heading\")\\n=======================================================\\n\\n* [C Language SDK Reference](https://furiosa-ai.github.io/docs/v0.10.0/en/api/c/index.html)\\n* [Python SDK Reference](../api/python/modules.html)\\n* [Model Zoo Reference](https://furiosa-ai.github.io/furiosa-models/v0.10.0/)\\n\\n\\n\\n\\n\\n[Previous](tutorials.html \"Tutorial and Code Examples\")\\n[Next](../api/python/modules.html \"Furiosa SDK 0.10.1 API Documentation\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* References * [View page source](../_sources/software/references.rst.txt)\\n---\\nReferences [\\uf0c1](#references \"Permalink to this heading\") =======================================================\\n* [C Language SDK Reference](https://furiosa-ai.github.io/docs/v0.10.0/en/api/c/index.html) * [Python SDK Reference](../api/python/modules.html) * [Model Zoo Reference](https://furiosa-ai.github.io/furiosa-models/v0.10.0/)\\n[Previous](tutorials.html \"Tutorial and Code Examples\") [Next](../api/python/modules.html \"Furiosa SDK 0.10.1 API Documentation\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     References\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/references.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"references\">\\n     <h1>\\n      References\\n      <a class=\"headerlink\" href=\"#references\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <ul class=\"simple\">\\n      <li>\\n       <p>\\n        <a class=\"reference external\" href=\"https://furiosa-ai.github.io/docs/v0.10.0/en/api/c/index.html\">\\n         C Language SDK Reference\\n        </a>\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        <a class=\"reference internal\" href=\"../api/python/modules.html\">\\n         <span class=\"doc\">\\n          Python SDK Reference\\n         </span>\\n        </a>\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        <a class=\"reference external\" href=\"https://furiosa-ai.github.io/furiosa-models/v0.10.0/\">\\n         Model Zoo Reference\\n        </a>\\n       </p>\\n      </li>\\n     </ul>\\n     <div class=\"toctree-wrapper compound\">\\n     </div>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"tutorials.html\" rel=\"prev\" title=\"Tutorial and Code Examples\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"../api/python/modules.html\" rel=\"next\" title=\"Furiosa SDK 0.10.1 API Documentation\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='6c328d98-54c8-4c8e-bea7-5927c3921609', link=Url('https://furiosa-ai.github.io/docs/latest/en/software/serving.html'), name='serving', parent='', child=[], description='\\n\\n\\n* Model Server (Serving Framework)\\n* [View page source](../_sources/software/serving.rst.txt)\\n\\n---\\n\\n\\n\\nModel Server (Serving Framework)\\n[\\uf0c1](#model-server-serving-framework \"Permalink to this heading\")\\n=================================================================================================\\n\\nTo serve DNN models through GRPC and REST API, you can use\\n[Furiosa Model Server](https://github.com/furiosa-ai/furiosa-sdk/tree/main/python/furiosa-server)\\n.\\nModel Server provides the endpoints compatible with\\n[KServe Predict Protocol Version 2](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md)\\n.\\n\\nIts major features are:\\n\\n> * REST/GRPC endpoints support\\n> * Multiple model serving using multiple NPU devices\\n\\nInstallation\\n[\\uf0c1](#installation \"Permalink to this heading\")\\n-----------------------------------------------------------\\n\\nIts requirements are:\\n\\n* Ubuntu 20.04 LTS (Debian bullseye) or higher\\n* [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages)\\n* Python 3.8 or higher version\\n\\nIf you need Python environment, please refer to\\n[Python execution environment setup](python-sdk.html#setuppython)\\nfirst.\\n\\nInstallation using PIP\\n\\nInstallation from source code\\n\\n\\nRun the following command\\n\\n```\\n$ pip install \\'furiosa-sdk[server]\\'\\n\\n```\\n\\n\\n\\nCheck out the source code and run the following command\\n\\n```\\n$ git clone https://github.com/furiosa-ai/furiosa-sdk.git\\n$ cd furiosa-sdk/python/furiosa-server\\n$ pip install .\\n\\n```\\n\\n\\n\\n\\n\\nRunning a Model Server\\n[\\uf0c1](#running-a-model-server \"Permalink to this heading\")\\n-------------------------------------------------------------------------------\\n\\nYou can run model sever command by running\\n`furiosa\\n\\nserver`\\nin your shell.\\n\\nTo run simply a model server with\\n`tflite`\\nor\\n`onnx`\\n, you need to specify\\njust the model path and its name as follows:\\n\\n```\\n$ cd furiosa-sdk\\n$ furiosa server \\\\\\n--model-path examples/assets/quantized_models/MNISTnet_uint8_quant_without_softmax.tflite \\\\\\n--model-name mnist\\n\\n```\\n\\n`--model-path`\\noption allows to specify a path of a model file.\\nIf you want to use a specific binding address and port, you can use additionally\\n`--host`\\n,\\n`--host-port`\\n.\\n\\nPlease run\\n`furiosa\\n\\nserver\\n\\n--help`\\nif you want to learn more\\nabout the command with various options.\\n\\n```\\n$ furiosa server --help\\nUsage: furiosa server [OPTIONS]\\n\\n    Start serving models from FuriosaAI model server\\n\\nOptions:\\n    --log-level [ERROR|INFO|WARN|DEBUG|TRACE]\\n                                    [default: LogLevel.INFO]\\n    --model-path TEXT               Path to Model file (tflite, onnx are\\n                                    supported)\\n    --model-name TEXT               Model name used in URL path\\n    --model-version TEXT            Model version used in URL path  [default:\\n                                    default]\\n    --host TEXT                     IP address to bind  [default: 0.0.0.0]\\n    --http-port INTEGER             HTTP port to listen to requests  [default:\\n                                    8080]\\n    --model-config FILENAME         Path to a config file about models with\\n                                    specific configurations\\n    --server-config FILENAME        Path to Model file (tflite, onnx are\\n                                    supported)\\n    --install-completion [bash|zsh|fish|powershell|pwsh]\\n                                    Install completion for the specified shell.\\n    --show-completion [bash|zsh|fish|powershell|pwsh]\\n                                    Show completion for the specified shell, to\\n                                    copy it or customize the installation.\\n    --help                          Show this message and exit.\\n\\n```\\n\\n\\n\\nRunning a Model Server with a Configuration File\\n[\\uf0c1](#running-a-model-server-with-a-configuration-file \"Permalink to this heading\")\\n-----------------------------------------------------------------------------------------------------------------------------------\\n\\nIf you need more advanced configurations like compilation options and device options,\\nyou can use a configuration file based on Yaml.\\n\\n```\\nmodel_config_list:\\n  - name: mnist\\n    model: \"samples/data/MNISTnet_uint8_quant.tflite\"\\n    version: \"1\"\\n    platform: npu\\n    npu_device: warboy(1)*1\\n    compiler_config:\\n      keep_unsignedness: true\\n      split_unit: 0\\n  - name: ssd\\n    model: \"samples/data/SSD512_MOBILENET_V2_BDD_int_without_reshape.tflite\"\\n    version: \"1\"\\n    platform: npu\\n    npu_device: warboy(1)*1\\n\\n```\\n\\nWhen you run a model sever with a configuration file,\\nyou need to specify\\n`--model-config`\\nas follows.\\nYou can find the model files described in the above example from\\n[furiosa-models/samples](https://github.com/furiosa-ai/furiosa-sdk/tree/main/python/furiosa-server/samples)\\n.\\n\\n```\\n$ cd furiosa-sdk/python/furiosa-server\\n$ furiosa server --model-config samples/model_config_example.yaml\\nlibfuriosa_hal.so --- v0.11.0, built @ 43c901f\\n2023-08-02T07:42:42.263133Z  INFO furiosa_rt_core::driver::event_driven::device: DeviceManager has detected 1 NPUs\\n2023-08-02T07:42:42.267247Z  INFO furiosa_rt_core::driver::event_driven::device: [1] npu:6:1 (warboy-b0, 64dpes)\\n2023-08-02T07:42:42.267264Z  INFO furiosa_rt_core::driver::event_driven::coord: furiosa-rt (v0.10.0-rc6, rev: d021ff71d, built_at: 2023-07-31T19:05:26Z) is being initialized\\n2023-08-02T07:42:42.267269Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:1-io-thread-0 thread has started\\n2023-08-02T07:42:42.267398Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:1-commit-thread thread has started\\n2023-08-02T07:42:42.267405Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:1-io-thread-1 thread has started\\n2023-08-02T07:42:42.270837Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libcompiler 0.10.0 (rev: f8f05c built: 2023-07-26T09:49:17Z)\\n2023-08-02T07:42:42.270851Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libhal-warboy 0.11.0 (rev: 43c901f built: 2023-04-19T14:04:55Z)\\n2023-08-02T07:42:42.271144Z  INFO furiosa_rt_core::driver::event_driven::coord: [NONAME] Runtime has started\\n2023-08-02T07:42:42.273772Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0001 is being loaded to npu:6:1\\n2023-08-02T07:42:42.283260Z  INFO furiosa_rt_core::driver::event_driven::coord: Compiling Model#0001 (target: warboy-b0, 64dpes, file: MNISTnet_uint8_quant.tflite, size: 18.2 kiB)\\n2023-08-02T07:42:42.299091Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0001 has been compiled successfully (took 0 secs)\\n2023-08-02T07:42:42.299293Z  INFO furiosa_rt_core::dag: Task Statistics: TaskStats { cpu: 5, npu: 1, alias: 0, coalesce: 0 }\\n2023-08-02T07:42:42.300701Z  INFO furiosa_rt_core::driver::event_driven::coord: NpuApi (AsyncNpuApiImpl) has started..\\n2023-08-02T07:42:42.300721Z  INFO furiosa_rt_core::driver::event_driven::coord: Creating 1 Contexts on npu:6:1 (DRAM usage: 6.0 kiB / 16.0 GiB, SRAM usage: 124.0 kiB / 64.0 MiB)\\n2023-08-02T07:42:42.300789Z  INFO furiosa_rt_core::driver::event_driven::coord: npu:6:1 has scheduled to Model#0001\\n2023-08-02T07:42:42.304216Z  WARN furiosa_rt_core::consts::envs: NPU_DEVNAME will be deprecated. Use FURIOSA_DEVICES instead.\\n2023-08-02T07:42:42.313084Z  INFO furiosa_rt_core::driver::event_driven::device: DeviceManager has detected 1 NPUs\\n2023-08-02T07:42:42.315470Z  INFO furiosa_rt_core::driver::event_driven::device: [1] npu:6:0 (warboy-b0, 64dpes)\\n2023-08-02T07:42:42.315483Z  INFO furiosa_rt_core::driver::event_driven::coord: furiosa-rt (v0.10.0-rc6, rev: d021ff71d, built_at: 2023-07-31T19:05:26Z) is being initialized\\n2023-08-02T07:42:42.315560Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:0-io-thread-1 thread has started\\n2023-08-02T07:42:42.315610Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:0-io-thread-0 thread has started\\n2023-08-02T07:42:42.315657Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:0-commit-thread thread has started\\n2023-08-02T07:42:42.319127Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libcompiler 0.10.0 (rev: f8f05c built: 2023-07-26T09:49:17Z)\\n2023-08-02T07:42:42.319141Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libhal-warboy 0.11.0 (rev: 43c901f built: 2023-04-19T14:04:55Z)\\n2023-08-02T07:42:42.319364Z  INFO furiosa_rt_core::driver::event_driven::coord: [NONAME] Runtime has started\\n2023-08-02T07:42:42.324283Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0002 is being loaded to npu:6:0\\n2023-08-02T07:42:42.333521Z  INFO furiosa_rt_core::driver::event_driven::coord: Compiling Model#0002 (target: warboy-b0, 64dpes, file: SSD512_MOBILENET_V2_BDD_int_without_reshape.tflite, size: 5.2 MiB)\\n2023-08-02T07:42:42.814260Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0002 has been compiled successfully (took 0 secs)\\n2023-08-02T07:42:42.815406Z  INFO furiosa_rt_core::dag: Task Statistics: TaskStats { cpu: 26, npu: 1, alias: 0, coalesce: 0 }\\n2023-08-02T07:42:42.893745Z  INFO furiosa_rt_core::driver::event_driven::coord: NpuApi (AsyncNpuApiImpl) has started..\\n2023-08-02T07:42:42.893772Z  INFO furiosa_rt_core::driver::event_driven::coord: Creating 1 Contexts on npu:6:0 (DRAM usage: 1.0 MiB / 16.0 GiB, SRAM usage: 14.8 MiB / 64.0 MiB)\\n2023-08-02T07:42:42.894265Z  INFO furiosa_rt_core::driver::event_driven::coord: npu:6:0 has scheduled to Model#0002\\nINFO:     Started server process [2448540]\\nINFO:     Waiting for application startup.\\nINFO:     Application startup complete.\\nINFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\\n\\n```\\n\\nOnce a model server starts up, you can call the inference request through HTTP protocol.\\nIf the model name is\\n`mnist`\\nand its version\\n`1`\\n, the endpoint of the model will be\\n`http://<host>:<port>/v2/models/mnist/version/1/infer`\\n, accepting\\n`POST`\\nhttp request.\\nThe following is an example using\\n`curl`\\nto send the inference request and return the response.\\n\\nThe following is a Python example, doing same as\\n`curl`\\ndoes in the above example.\\n\\n```\\nimport requests\\nimport mnist\\nimport numpy as np\\n\\nmnist_images = mnist.train_images().reshape((60000, 1, 28, 28)).astype(np.uint8)\\nurl = \\'http://localhost:8080/v2/models/mnist/versions/1/infer\\'\\n\\ndata = mnist_images[0:1].flatten().tolist()\\nrequest = {\\n    \"inputs\": [{\\n        \"name\":\\n        \"mnist\",\\n        \"datatype\": \"UINT8\",\\n        \"shape\": (1, 1, 28, 28),\\n        \"data\": data\\n    }]\\n}\\n\\nresponse = requests.post(url, json=request)\\nprint(response.json())\\n\\n```\\n\\n\\n\\nEndpoints\\n[\\uf0c1](#endpoints \"Permalink to this heading\")\\n-----------------------------------------------------\\n\\nThe following table shows REST API endpoints and its descriptions.\\nThe model server is following KServe Predict Protocol Version 2.\\nSo, you can find more details from\\n[KServe Predict Protocol Version 2 - HTTP/REST](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#httprest)\\n.\\n\\n\\nEndpoints of KServe Predict Protocol Version 2\\n\\n[\\uf0c1](#id1 \"Permalink to this table\")\\n\\n\\n| Method and Endpoint | Description |\\n| --- | --- |\\n| GET /v2/health/live | Returns HTTP Ok (200) if the inference server is able to receive and respond to metadata and inference requests. This API can be directly used for the Kubernetes livenessProbe. |\\n| GET /v2/health/ready | Returns HTTP Ok (200) if all the models are ready for inferencing. This API can be directly used for the Kubernetes readinessProbe. |\\n| GET /v2/models/${MODEL\\\\_NAME}/versions/${MODEL\\\\_VERSION} | Returns a model metadata |\\n| GET /v2/models/${MODEL\\\\_NAME}/versions/${MODEL\\\\_VERSION}/ready | Returns HTTP Ok (200) if a specific model is ready for inferencing. |\\n| POST /v2/models/${MODEL\\\\_NAME}[/versions/${MODEL\\\\_VERSION}]/infer | Inference request |\\n\\n\\n\\n\\n\\n[Previous](profiler.html \"Performance Profiling\")\\n[Next](kubernetes_support.html \"Kubernetes Support\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Model Server (Serving Framework) * [View page source](../_sources/software/serving.rst.txt)\\n---\\nModel Server (Serving Framework) [\\uf0c1](#model-server-serving-framework \"Permalink to this heading\") =================================================================================================\\nTo serve DNN models through GRPC and REST API, you can use [Furiosa Model Server](https://github.com/furiosa-ai/furiosa-sdk/tree/main/python/furiosa-server) .\\nModel Server provides the endpoints compatible with [KServe Predict Protocol Version 2](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md) .\\nIts major features are:\\n> * REST/GRPC endpoints support > * Multiple model serving using multiple NPU devices\\nInstallation [\\uf0c1](#installation \"Permalink to this heading\") -----------------------------------------------------------\\nIts requirements are:\\n* Ubuntu 20.04 LTS (Debian bullseye) or higher * [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) * Python 3.8 or higher version\\nIf you need Python environment, please refer to [Python execution environment setup](python-sdk.html#setuppython) first.\\nInstallation using PIP\\nInstallation from source code\\nRun the following command\\n``` $ pip install \\'furiosa-sdk[server]\\'\\n```\\nCheck out the source code and run the following command\\n``` $ git clone https://github.com/furiosa-ai/furiosa-sdk.git $ cd furiosa-sdk/python/furiosa-server $ pip install .\\n```\\nRunning a Model Server [\\uf0c1](#running-a-model-server \"Permalink to this heading\") -------------------------------------------------------------------------------\\nYou can run model sever command by running `furiosa\\nserver` in your shell.\\nTo run simply a model server with `tflite` or `onnx` , you need to specify just the model path and its name as follows:\\n``` $ cd furiosa-sdk $ furiosa server \\\\ --model-path examples/assets/quantized_models/MNISTnet_uint8_quant_without_softmax.tflite \\\\ --model-name mnist\\n```  `--model-path` option allows to specify a path of a model file. If you want to use a specific binding address and port, you can use additionally `--host` , `--host-port` .\\nPlease run `furiosa\\nserver\\n--help` if you want to learn more about the command with various options.\\n``` $ furiosa server --help Usage: furiosa server [OPTIONS]\\n    Start serving models from FuriosaAI model server\\nOptions:     --log-level [ERROR|INFO|WARN|DEBUG|TRACE]                                     [default: LogLevel.INFO]     --model-path TEXT               Path to Model file (tflite, onnx are                                     supported)     --model-name TEXT               Model name used in URL path     --model-version TEXT            Model version used in URL path  [default:                                     default]     --host TEXT                     IP address to bind  [default: 0.0.0.0]     --http-port INTEGER             HTTP port to listen to requests  [default:                                     8080]     --model-config FILENAME         Path to a config file about models with                                     specific configurations     --server-config FILENAME        Path to Model file (tflite, onnx are                                     supported)     --install-completion [bash|zsh|fish|powershell|pwsh]                                     Install completion for the specified shell.     --show-completion [bash|zsh|fish|powershell|pwsh]                                     Show completion for the specified shell, to                                     copy it or customize the installation.     --help                          Show this message and exit.\\n```\\nRunning a Model Server with a Configuration File [\\uf0c1](#running-a-model-server-with-a-configuration-file \"Permalink to this heading\") -----------------------------------------------------------------------------------------------------------------------------------\\nIf you need more advanced configurations like compilation options and device options, you can use a configuration file based on Yaml.\\n``` model_config_list:   - name: mnist     model: \"samples/data/MNISTnet_uint8_quant.tflite\"     version: \"1\"     platform: npu     npu_device: warboy(1)*1     compiler_config:       keep_unsignedness: true       split_unit: 0   - name: ssd     model: \"samples/data/SSD512_MOBILENET_V2_BDD_int_without_reshape.tflite\"     version: \"1\"     platform: npu     npu_device: warboy(1)*1\\n```\\nWhen you run a model sever with a configuration file, you need to specify `--model-config` as follows. You can find the model files described in the above example from [furiosa-models/samples](https://github.com/furiosa-ai/furiosa-sdk/tree/main/python/furiosa-server/samples) .\\n``` $ cd furiosa-sdk/python/furiosa-server $ furiosa server --model-config samples/model_config_example.yaml libfuriosa_hal.so --- v0.11.0, built @ 43c901f 2023-08-02T07:42:42.263133Z  INFO furiosa_rt_core::driver::event_driven::device: DeviceManager has detected 1 NPUs 2023-08-02T07:42:42.267247Z  INFO furiosa_rt_core::driver::event_driven::device: [1] npu:6:1 (warboy-b0, 64dpes) 2023-08-02T07:42:42.267264Z  INFO furiosa_rt_core::driver::event_driven::coord: furiosa-rt (v0.10.0-rc6, rev: d021ff71d, built_at: 2023-07-31T19:05:26Z) is being initialized 2023-08-02T07:42:42.267269Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:1-io-thread-0 thread has started 2023-08-02T07:42:42.267398Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:1-commit-thread thread has started 2023-08-02T07:42:42.267405Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:1-io-thread-1 thread has started 2023-08-02T07:42:42.270837Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libcompiler 0.10.0 (rev: f8f05c built: 2023-07-26T09:49:17Z) 2023-08-02T07:42:42.270851Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libhal-warboy 0.11.0 (rev: 43c901f built: 2023-04-19T14:04:55Z) 2023-08-02T07:42:42.271144Z  INFO furiosa_rt_core::driver::event_driven::coord: [NONAME] Runtime has started 2023-08-02T07:42:42.273772Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0001 is being loaded to npu:6:1 2023-08-02T07:42:42.283260Z  INFO furiosa_rt_core::driver::event_driven::coord: Compiling Model#0001 (target: warboy-b0, 64dpes, file: MNISTnet_uint8_quant.tflite, size: 18.2 kiB) 2023-08-02T07:42:42.299091Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0001 has been compiled successfully (took 0 secs) 2023-08-02T07:42:42.299293Z  INFO furiosa_rt_core::dag: Task Statistics: TaskStats { cpu: 5, npu: 1, alias: 0, coalesce: 0 } 2023-08-02T07:42:42.300701Z  INFO furiosa_rt_core::driver::event_driven::coord: NpuApi (AsyncNpuApiImpl) has started.. 2023-08-02T07:42:42.300721Z  INFO furiosa_rt_core::driver::event_driven::coord: Creating 1 Contexts on npu:6:1 (DRAM usage: 6.0 kiB / 16.0 GiB, SRAM usage: 124.0 kiB / 64.0 MiB) 2023-08-02T07:42:42.300789Z  INFO furiosa_rt_core::driver::event_driven::coord: npu:6:1 has scheduled to Model#0001 2023-08-02T07:42:42.304216Z  WARN furiosa_rt_core::consts::envs: NPU_DEVNAME will be deprecated. Use FURIOSA_DEVICES instead. 2023-08-02T07:42:42.313084Z  INFO furiosa_rt_core::driver::event_driven::device: DeviceManager has detected 1 NPUs 2023-08-02T07:42:42.315470Z  INFO furiosa_rt_core::driver::event_driven::device: [1] npu:6:0 (warboy-b0, 64dpes) 2023-08-02T07:42:42.315483Z  INFO furiosa_rt_core::driver::event_driven::coord: furiosa-rt (v0.10.0-rc6, rev: d021ff71d, built_at: 2023-07-31T19:05:26Z) is being initialized 2023-08-02T07:42:42.315560Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:0-io-thread-1 thread has started 2023-08-02T07:42:42.315610Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:0-io-thread-0 thread has started 2023-08-02T07:42:42.315657Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:0-commit-thread thread has started 2023-08-02T07:42:42.319127Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libcompiler 0.10.0 (rev: f8f05c built: 2023-07-26T09:49:17Z) 2023-08-02T07:42:42.319141Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libhal-warboy 0.11.0 (rev: 43c901f built: 2023-04-19T14:04:55Z) 2023-08-02T07:42:42.319364Z  INFO furiosa_rt_core::driver::event_driven::coord: [NONAME] Runtime has started 2023-08-02T07:42:42.324283Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0002 is being loaded to npu:6:0 2023-08-02T07:42:42.333521Z  INFO furiosa_rt_core::driver::event_driven::coord: Compiling Model#0002 (target: warboy-b0, 64dpes, file: SSD512_MOBILENET_V2_BDD_int_without_reshape.tflite, size: 5.2 MiB) 2023-08-02T07:42:42.814260Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0002 has been compiled successfully (took 0 secs) 2023-08-02T07:42:42.815406Z  INFO furiosa_rt_core::dag: Task Statistics: TaskStats { cpu: 26, npu: 1, alias: 0, coalesce: 0 } 2023-08-02T07:42:42.893745Z  INFO furiosa_rt_core::driver::event_driven::coord: NpuApi (AsyncNpuApiImpl) has started.. 2023-08-02T07:42:42.893772Z  INFO furiosa_rt_core::driver::event_driven::coord: Creating 1 Contexts on npu:6:0 (DRAM usage: 1.0 MiB / 16.0 GiB, SRAM usage: 14.8 MiB / 64.0 MiB) 2023-08-02T07:42:42.894265Z  INFO furiosa_rt_core::driver::event_driven::coord: npu:6:0 has scheduled to Model#0002 INFO:     Started server process [2448540] INFO:     Waiting for application startup. INFO:     Application startup complete. INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\\n```\\nOnce a model server starts up, you can call the inference request through HTTP protocol. If the model name is `mnist` and its version `1` , the endpoint of the model will be `http://<host>:<port>/v2/models/mnist/version/1/infer` , accepting `POST` http request. The following is an example using `curl` to send the inference request and return the response.\\nThe following is a Python example, doing same as `curl` does in the above example.\\n``` import requests import mnist import numpy as np\\nmnist_images = mnist.train_images().reshape((60000, 1, 28, 28)).astype(np.uint8) url = \\'http://localhost:8080/v2/models/mnist/versions/1/infer\\'\\ndata = mnist_images[0:1].flatten().tolist() request = {     \"inputs\": [{         \"name\":         \"mnist\",         \"datatype\": \"UINT8\",         \"shape\": (1, 1, 28, 28),         \"data\": data     }] }\\nresponse = requests.post(url, json=request) print(response.json())\\n```\\nEndpoints [\\uf0c1](#endpoints \"Permalink to this heading\") -----------------------------------------------------\\nThe following table shows REST API endpoints and its descriptions. The model server is following KServe Predict Protocol Version 2. So, you can find more details from [KServe Predict Protocol Version 2 - HTTP/REST](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#httprest) .\\nEndpoints of KServe Predict Protocol Version 2\\n[\\uf0c1](#id1 \"Permalink to this table\")\\n| Method and Endpoint | Description | | --- | --- | | GET /v2/health/live | Returns HTTP Ok (200) if the inference server is able to receive and respond to metadata and inference requests. This API can be directly used for the Kubernetes livenessProbe. | | GET /v2/health/ready | Returns HTTP Ok (200) if all the models are ready for inferencing. This API can be directly used for the Kubernetes readinessProbe. | | GET /v2/models/${MODEL\\\\_NAME}/versions/${MODEL\\\\_VERSION} | Returns a model metadata | | GET /v2/models/${MODEL\\\\_NAME}/versions/${MODEL\\\\_VERSION}/ready | Returns HTTP Ok (200) if a specific model is ready for inferencing. | | POST /v2/models/${MODEL\\\\_NAME}[/versions/${MODEL\\\\_VERSION}]/infer | Inference request |\\n[Previous](profiler.html \"Performance Profiling\") [Next](kubernetes_support.html \"Kubernetes Support\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Model Server (Serving Framework)\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/serving.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"model-server-serving-framework\">\\n     <span id=\"modelserving\">\\n     </span>\\n     <h1>\\n      Model Server (Serving Framework)\\n      <a class=\"headerlink\" href=\"#model-server-serving-framework\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      To serve DNN models through GRPC and REST API, you can use\\n      <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/tree/main/python/furiosa-server\">\\n       Furiosa Model Server\\n      </a>\\n      .\\nModel Server provides the endpoints compatible with\\n      <a class=\"reference external\" href=\"https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md\">\\n       KServe Predict Protocol Version 2\\n      </a>\\n      .\\n     </p>\\n     <p>\\n      Its major features are:\\n     </p>\\n     <blockquote>\\n      <div>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          REST/GRPC endpoints support\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Multiple model serving using multiple NPU devices\\n         </p>\\n        </li>\\n       </ul>\\n      </div>\\n     </blockquote>\\n     <section id=\"installation\">\\n      <h2>\\n       Installation\\n       <a class=\"headerlink\" href=\"#installation\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       Its requirements are:\\n      </p>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         Ubuntu 20.04 LTS (Debian bullseye) or higher\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"installation.html#requiredpackages\">\\n          <span class=\"std std-ref\">\\n           Driver, Firmware, and Runtime Installation\\n          </span>\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Python 3.8 or higher version\\n        </p>\\n       </li>\\n      </ul>\\n      <p>\\n       If you need Python environment, please refer to\\n       <a class=\"reference internal\" href=\"python-sdk.html#setuppython\">\\n        <span class=\"std std-ref\">\\n         Python execution environment setup\\n        </span>\\n       </a>\\n       first.\\n      </p>\\n      <div class=\"sphinx-tabs docutils container\">\\n       <div aria-label=\"Tabbed content\" role=\"tablist\">\\n        <button aria-controls=\"panel-0-0-0\" aria-selected=\"true\" class=\"sphinx-tabs-tab\" id=\"tab-0-0-0\" name=\"0-0\" role=\"tab\" tabindex=\"0\">\\n         Installation using PIP\\n        </button>\\n        <button aria-controls=\"panel-0-0-1\" aria-selected=\"false\" class=\"sphinx-tabs-tab\" id=\"tab-0-0-1\" name=\"0-1\" role=\"tab\" tabindex=\"-1\">\\n         Installation from source code\\n        </button>\\n       </div>\\n       <div aria-labelledby=\"tab-0-0-0\" class=\"sphinx-tabs-panel\" id=\"panel-0-0-0\" name=\"0-0\" role=\"tabpanel\" tabindex=\"0\">\\n        <p>\\n         Run the following command\\n        </p>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>$<span class=\"w\"> </span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span><span class=\"s1\">\\'furiosa-sdk[server]\\'</span>\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n       <div aria-labelledby=\"tab-0-0-1\" class=\"sphinx-tabs-panel\" hidden=\"true\" id=\"panel-0-0-1\" name=\"0-1\" role=\"tabpanel\" tabindex=\"0\">\\n        <p>\\n         Check out the source code and run the following command\\n        </p>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>$<span class=\"w\"> </span>git<span class=\"w\"> </span>clone<span class=\"w\"> </span>https://github.com/furiosa-ai/furiosa-sdk.git\\n$<span class=\"w\"> </span><span class=\"nb\">cd</span><span class=\"w\"> </span>furiosa-sdk/python/furiosa-server\\n$<span class=\"w\"> </span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>.\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"running-a-model-server\">\\n      <h2>\\n       Running a Model Server\\n       <a class=\"headerlink\" href=\"#running-a-model-server\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       You can run model sever command by running\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         furiosa\\n        </span>\\n        <span class=\"pre\">\\n         server\\n        </span>\\n       </code>\\n       in your shell.\\n      </p>\\n      <p>\\n       To run simply a model server with\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         tflite\\n        </span>\\n       </code>\\n       or\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         onnx\\n        </span>\\n       </code>\\n       , you need to specify\\njust the model path and its name as follows:\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span><span class=\"nb\">cd</span><span class=\"w\"> </span>furiosa-sdk\\n$<span class=\"w\"> </span>furiosa<span class=\"w\"> </span>server<span class=\"w\"> </span><span class=\"se\">\\\\</span>\\n--model-path<span class=\"w\"> </span>examples/assets/quantized_models/MNISTnet_uint8_quant_without_softmax.tflite<span class=\"w\"> </span><span class=\"se\">\\\\</span>\\n--model-name<span class=\"w\"> </span>mnist\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         --model-path\\n        </span>\\n       </code>\\n       option allows to specify a path of a model file.\\nIf you want to use a specific binding address and port, you can use additionally\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         --host\\n        </span>\\n       </code>\\n       ,\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         --host-port\\n        </span>\\n       </code>\\n       .\\n      </p>\\n      <p>\\n       Please run\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         furiosa\\n        </span>\\n        <span class=\"pre\">\\n         server\\n        </span>\\n        <span class=\"pre\">\\n         --help\\n        </span>\\n       </code>\\n       if you want to learn more\\nabout the command with various options.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span>furiosa<span class=\"w\"> </span>server<span class=\"w\"> </span>--help\\nUsage:<span class=\"w\"> </span>furiosa<span class=\"w\"> </span>server<span class=\"w\"> </span><span class=\"o\">[</span>OPTIONS<span class=\"o\">]</span>\\n\\n<span class=\"w\">    </span>Start<span class=\"w\"> </span>serving<span class=\"w\"> </span>models<span class=\"w\"> </span>from<span class=\"w\"> </span>FuriosaAI<span class=\"w\"> </span>model<span class=\"w\"> </span>server\\n\\nOptions:\\n<span class=\"w\">    </span>--log-level<span class=\"w\"> </span><span class=\"o\">[</span>ERROR<span class=\"p\">|</span>INFO<span class=\"p\">|</span>WARN<span class=\"p\">|</span>DEBUG<span class=\"p\">|</span>TRACE<span class=\"o\">]</span>\\n<span class=\"w\">                                    </span><span class=\"o\">[</span>default:<span class=\"w\"> </span>LogLevel.INFO<span class=\"o\">]</span>\\n<span class=\"w\">    </span>--model-path<span class=\"w\"> </span>TEXT<span class=\"w\">               </span>Path<span class=\"w\"> </span>to<span class=\"w\"> </span>Model<span class=\"w\"> </span>file<span class=\"w\"> </span><span class=\"o\">(</span>tflite,<span class=\"w\"> </span>onnx<span class=\"w\"> </span>are\\n<span class=\"w\">                                    </span>supported<span class=\"o\">)</span>\\n<span class=\"w\">    </span>--model-name<span class=\"w\"> </span>TEXT<span class=\"w\">               </span>Model<span class=\"w\"> </span>name<span class=\"w\"> </span>used<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>URL<span class=\"w\"> </span>path\\n<span class=\"w\">    </span>--model-version<span class=\"w\"> </span>TEXT<span class=\"w\">            </span>Model<span class=\"w\"> </span>version<span class=\"w\"> </span>used<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>URL<span class=\"w\"> </span>path<span class=\"w\">  </span><span class=\"o\">[</span>default:\\n<span class=\"w\">                                    </span>default<span class=\"o\">]</span>\\n<span class=\"w\">    </span>--host<span class=\"w\"> </span>TEXT<span class=\"w\">                     </span>IP<span class=\"w\"> </span>address<span class=\"w\"> </span>to<span class=\"w\"> </span><span class=\"nb\">bind</span><span class=\"w\">  </span><span class=\"o\">[</span>default:<span class=\"w\"> </span><span class=\"m\">0</span>.0.0.0<span class=\"o\">]</span>\\n<span class=\"w\">    </span>--http-port<span class=\"w\"> </span>INTEGER<span class=\"w\">             </span>HTTP<span class=\"w\"> </span>port<span class=\"w\"> </span>to<span class=\"w\"> </span>listen<span class=\"w\"> </span>to<span class=\"w\"> </span>requests<span class=\"w\">  </span><span class=\"o\">[</span>default:\\n<span class=\"w\">                                    </span><span class=\"m\">8080</span><span class=\"o\">]</span>\\n<span class=\"w\">    </span>--model-config<span class=\"w\"> </span>FILENAME<span class=\"w\">         </span>Path<span class=\"w\"> </span>to<span class=\"w\"> </span>a<span class=\"w\"> </span>config<span class=\"w\"> </span>file<span class=\"w\"> </span>about<span class=\"w\"> </span>models<span class=\"w\"> </span>with\\n<span class=\"w\">                                    </span>specific<span class=\"w\"> </span>configurations\\n<span class=\"w\">    </span>--server-config<span class=\"w\"> </span>FILENAME<span class=\"w\">        </span>Path<span class=\"w\"> </span>to<span class=\"w\"> </span>Model<span class=\"w\"> </span>file<span class=\"w\"> </span><span class=\"o\">(</span>tflite,<span class=\"w\"> </span>onnx<span class=\"w\"> </span>are\\n<span class=\"w\">                                    </span>supported<span class=\"o\">)</span>\\n<span class=\"w\">    </span>--install-completion<span class=\"w\"> </span><span class=\"o\">[</span>bash<span class=\"p\">|</span>zsh<span class=\"p\">|</span>fish<span class=\"p\">|</span>powershell<span class=\"p\">|</span>pwsh<span class=\"o\">]</span>\\n<span class=\"w\">                                    </span>Install<span class=\"w\"> </span>completion<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>the<span class=\"w\"> </span>specified<span class=\"w\"> </span>shell.\\n<span class=\"w\">    </span>--show-completion<span class=\"w\"> </span><span class=\"o\">[</span>bash<span class=\"p\">|</span>zsh<span class=\"p\">|</span>fish<span class=\"p\">|</span>powershell<span class=\"p\">|</span>pwsh<span class=\"o\">]</span>\\n<span class=\"w\">                                    </span>Show<span class=\"w\"> </span>completion<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>the<span class=\"w\"> </span>specified<span class=\"w\"> </span>shell,<span class=\"w\"> </span>to\\n<span class=\"w\">                                    </span>copy<span class=\"w\"> </span>it<span class=\"w\"> </span>or<span class=\"w\"> </span>customize<span class=\"w\"> </span>the<span class=\"w\"> </span>installation.\\n<span class=\"w\">    </span>--help<span class=\"w\">                          </span>Show<span class=\"w\"> </span>this<span class=\"w\"> </span>message<span class=\"w\"> </span>and<span class=\"w\"> </span>exit.\\n</pre>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"running-a-model-server-with-a-configuration-file\">\\n      <h2>\\n       Running a Model Server with a Configuration File\\n       <a class=\"headerlink\" href=\"#running-a-model-server-with-a-configuration-file\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       If you need more advanced configurations like compilation options and device options,\\nyou can use a configuration file based on Yaml.\\n      </p>\\n      <div class=\"highlight-yaml notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"nt\">model_config_list</span><span class=\"p\">:</span>\\n<span class=\"w\">  </span><span class=\"p p-Indicator\">-</span><span class=\"w\"> </span><span class=\"nt\">name</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">mnist</span>\\n<span class=\"w\">    </span><span class=\"nt\">model</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s\">\"samples/data/MNISTnet_uint8_quant.tflite\"</span>\\n<span class=\"w\">    </span><span class=\"nt\">version</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s\">\"1\"</span>\\n<span class=\"w\">    </span><span class=\"nt\">platform</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">npu</span>\\n<span class=\"w\">    </span><span class=\"nt\">npu_device</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">warboy(1)*1</span>\\n<span class=\"w\">    </span><span class=\"nt\">compiler_config</span><span class=\"p\">:</span>\\n<span class=\"w\">      </span><span class=\"nt\">keep_unsignedness</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\\n<span class=\"w\">      </span><span class=\"nt\">split_unit</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">0</span>\\n<span class=\"w\">  </span><span class=\"p p-Indicator\">-</span><span class=\"w\"> </span><span class=\"nt\">name</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">ssd</span>\\n<span class=\"w\">    </span><span class=\"nt\">model</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s\">\"samples/data/SSD512_MOBILENET_V2_BDD_int_without_reshape.tflite\"</span>\\n<span class=\"w\">    </span><span class=\"nt\">version</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s\">\"1\"</span>\\n<span class=\"w\">    </span><span class=\"nt\">platform</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">npu</span>\\n<span class=\"w\">    </span><span class=\"nt\">npu_device</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">warboy(1)*1</span>\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       When you run a model sever with a configuration file,\\nyou need to specify\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         --model-config\\n        </span>\\n       </code>\\n       as follows.\\nYou can find the model files described in the above example from\\n       <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/tree/main/python/furiosa-server/samples\">\\n        furiosa-models/samples\\n       </a>\\n       .\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span><span class=\"nb\">cd</span><span class=\"w\"> </span>furiosa-sdk/python/furiosa-server\\n$<span class=\"w\"> </span>furiosa<span class=\"w\"> </span>server<span class=\"w\"> </span>--model-config<span class=\"w\"> </span>samples/model_config_example.yaml\\nlibfuriosa_hal.so<span class=\"w\"> </span>---<span class=\"w\"> </span>v0.11.0,<span class=\"w\"> </span>built<span class=\"w\"> </span>@<span class=\"w\"> </span>43c901f\\n<span class=\"m\">2023</span>-08-02T07:42:42.263133Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::device:<span class=\"w\"> </span>DeviceManager<span class=\"w\"> </span>has<span class=\"w\"> </span>detected<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>NPUs\\n<span class=\"m\">2023</span>-08-02T07:42:42.267247Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::device:<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">1</span><span class=\"o\">]</span><span class=\"w\"> </span>npu:6:1<span class=\"w\"> </span><span class=\"o\">(</span>warboy-b0,<span class=\"w\"> </span>64dpes<span class=\"o\">)</span>\\n<span class=\"m\">2023</span>-08-02T07:42:42.267264Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>furiosa-rt<span class=\"w\"> </span><span class=\"o\">(</span>v0.10.0-rc6,<span class=\"w\"> </span>rev:<span class=\"w\"> </span>d021ff71d,<span class=\"w\"> </span>built_at:<span class=\"w\"> </span><span class=\"m\">2023</span>-07-31T19:05:26Z<span class=\"o\">)</span><span class=\"w\"> </span>is<span class=\"w\"> </span>being<span class=\"w\"> </span>initialized\\n<span class=\"m\">2023</span>-08-02T07:42:42.267269Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::npu::async_impl::threaded:<span class=\"w\"> </span>npu:6:1-io-thread-0<span class=\"w\"> </span>thread<span class=\"w\"> </span>has<span class=\"w\"> </span>started\\n<span class=\"m\">2023</span>-08-02T07:42:42.267398Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::npu::async_impl::threaded:<span class=\"w\"> </span>npu:6:1-commit-thread<span class=\"w\"> </span>thread<span class=\"w\"> </span>has<span class=\"w\"> </span>started\\n<span class=\"m\">2023</span>-08-02T07:42:42.267405Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::npu::async_impl::threaded:<span class=\"w\"> </span>npu:6:1-io-thread-1<span class=\"w\"> </span>thread<span class=\"w\"> </span>has<span class=\"w\"> </span>started\\n<span class=\"m\">2023</span>-08-02T07:42:42.270837Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>Loaded<span class=\"w\"> </span>libcompiler<span class=\"w\"> </span><span class=\"m\">0</span>.10.0<span class=\"w\"> </span><span class=\"o\">(</span>rev:<span class=\"w\"> </span>f8f05c<span class=\"w\"> </span>built:<span class=\"w\"> </span><span class=\"m\">2023</span>-07-26T09:49:17Z<span class=\"o\">)</span>\\n<span class=\"m\">2023</span>-08-02T07:42:42.270851Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>Loaded<span class=\"w\"> </span>libhal-warboy<span class=\"w\"> </span><span class=\"m\">0</span>.11.0<span class=\"w\"> </span><span class=\"o\">(</span>rev:<span class=\"w\"> </span>43c901f<span class=\"w\"> </span>built:<span class=\"w\"> </span><span class=\"m\">2023</span>-04-19T14:04:55Z<span class=\"o\">)</span>\\n<span class=\"m\">2023</span>-08-02T07:42:42.271144Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span><span class=\"o\">[</span>NONAME<span class=\"o\">]</span><span class=\"w\"> </span>Runtime<span class=\"w\"> </span>has<span class=\"w\"> </span>started\\n<span class=\"m\">2023</span>-08-02T07:42:42.273772Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>Model#0001<span class=\"w\"> </span>is<span class=\"w\"> </span>being<span class=\"w\"> </span>loaded<span class=\"w\"> </span>to<span class=\"w\"> </span>npu:6:1\\n<span class=\"m\">2023</span>-08-02T07:42:42.283260Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>Compiling<span class=\"w\"> </span>Model#0001<span class=\"w\"> </span><span class=\"o\">(</span>target:<span class=\"w\"> </span>warboy-b0,<span class=\"w\"> </span>64dpes,<span class=\"w\"> </span>file:<span class=\"w\"> </span>MNISTnet_uint8_quant.tflite,<span class=\"w\"> </span>size:<span class=\"w\"> </span><span class=\"m\">18</span>.2<span class=\"w\"> </span>kiB<span class=\"o\">)</span>\\n<span class=\"m\">2023</span>-08-02T07:42:42.299091Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>Model#0001<span class=\"w\"> </span>has<span class=\"w\"> </span>been<span class=\"w\"> </span>compiled<span class=\"w\"> </span>successfully<span class=\"w\"> </span><span class=\"o\">(</span>took<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span>secs<span class=\"o\">)</span>\\n<span class=\"m\">2023</span>-08-02T07:42:42.299293Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::dag:<span class=\"w\"> </span>Task<span class=\"w\"> </span>Statistics:<span class=\"w\"> </span>TaskStats<span class=\"w\"> </span><span class=\"o\">{</span><span class=\"w\"> </span>cpu:<span class=\"w\"> </span><span class=\"m\">5</span>,<span class=\"w\"> </span>npu:<span class=\"w\"> </span><span class=\"m\">1</span>,<span class=\"w\"> </span>alias:<span class=\"w\"> </span><span class=\"m\">0</span>,<span class=\"w\"> </span>coalesce:<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span><span class=\"o\">}</span>\\n<span class=\"m\">2023</span>-08-02T07:42:42.300701Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>NpuApi<span class=\"w\"> </span><span class=\"o\">(</span>AsyncNpuApiImpl<span class=\"o\">)</span><span class=\"w\"> </span>has<span class=\"w\"> </span>started..\\n<span class=\"m\">2023</span>-08-02T07:42:42.300721Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>Creating<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>Contexts<span class=\"w\"> </span>on<span class=\"w\"> </span>npu:6:1<span class=\"w\"> </span><span class=\"o\">(</span>DRAM<span class=\"w\"> </span>usage:<span class=\"w\"> </span><span class=\"m\">6</span>.0<span class=\"w\"> </span>kiB<span class=\"w\"> </span>/<span class=\"w\"> </span><span class=\"m\">16</span>.0<span class=\"w\"> </span>GiB,<span class=\"w\"> </span>SRAM<span class=\"w\"> </span>usage:<span class=\"w\"> </span><span class=\"m\">124</span>.0<span class=\"w\"> </span>kiB<span class=\"w\"> </span>/<span class=\"w\"> </span><span class=\"m\">64</span>.0<span class=\"w\"> </span>MiB<span class=\"o\">)</span>\\n<span class=\"m\">2023</span>-08-02T07:42:42.300789Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>npu:6:1<span class=\"w\"> </span>has<span class=\"w\"> </span>scheduled<span class=\"w\"> </span>to<span class=\"w\"> </span>Model#0001\\n<span class=\"m\">2023</span>-08-02T07:42:42.304216Z<span class=\"w\">  </span>WARN<span class=\"w\"> </span>furiosa_rt_core::consts::envs:<span class=\"w\"> </span>NPU_DEVNAME<span class=\"w\"> </span>will<span class=\"w\"> </span>be<span class=\"w\"> </span>deprecated.<span class=\"w\"> </span>Use<span class=\"w\"> </span>FURIOSA_DEVICES<span class=\"w\"> </span>instead.\\n<span class=\"m\">2023</span>-08-02T07:42:42.313084Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::device:<span class=\"w\"> </span>DeviceManager<span class=\"w\"> </span>has<span class=\"w\"> </span>detected<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>NPUs\\n<span class=\"m\">2023</span>-08-02T07:42:42.315470Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::device:<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">1</span><span class=\"o\">]</span><span class=\"w\"> </span>npu:6:0<span class=\"w\"> </span><span class=\"o\">(</span>warboy-b0,<span class=\"w\"> </span>64dpes<span class=\"o\">)</span>\\n<span class=\"m\">2023</span>-08-02T07:42:42.315483Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>furiosa-rt<span class=\"w\"> </span><span class=\"o\">(</span>v0.10.0-rc6,<span class=\"w\"> </span>rev:<span class=\"w\"> </span>d021ff71d,<span class=\"w\"> </span>built_at:<span class=\"w\"> </span><span class=\"m\">2023</span>-07-31T19:05:26Z<span class=\"o\">)</span><span class=\"w\"> </span>is<span class=\"w\"> </span>being<span class=\"w\"> </span>initialized\\n<span class=\"m\">2023</span>-08-02T07:42:42.315560Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::npu::async_impl::threaded:<span class=\"w\"> </span>npu:6:0-io-thread-1<span class=\"w\"> </span>thread<span class=\"w\"> </span>has<span class=\"w\"> </span>started\\n<span class=\"m\">2023</span>-08-02T07:42:42.315610Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::npu::async_impl::threaded:<span class=\"w\"> </span>npu:6:0-io-thread-0<span class=\"w\"> </span>thread<span class=\"w\"> </span>has<span class=\"w\"> </span>started\\n<span class=\"m\">2023</span>-08-02T07:42:42.315657Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::npu::async_impl::threaded:<span class=\"w\"> </span>npu:6:0-commit-thread<span class=\"w\"> </span>thread<span class=\"w\"> </span>has<span class=\"w\"> </span>started\\n<span class=\"m\">2023</span>-08-02T07:42:42.319127Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>Loaded<span class=\"w\"> </span>libcompiler<span class=\"w\"> </span><span class=\"m\">0</span>.10.0<span class=\"w\"> </span><span class=\"o\">(</span>rev:<span class=\"w\"> </span>f8f05c<span class=\"w\"> </span>built:<span class=\"w\"> </span><span class=\"m\">2023</span>-07-26T09:49:17Z<span class=\"o\">)</span>\\n<span class=\"m\">2023</span>-08-02T07:42:42.319141Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>Loaded<span class=\"w\"> </span>libhal-warboy<span class=\"w\"> </span><span class=\"m\">0</span>.11.0<span class=\"w\"> </span><span class=\"o\">(</span>rev:<span class=\"w\"> </span>43c901f<span class=\"w\"> </span>built:<span class=\"w\"> </span><span class=\"m\">2023</span>-04-19T14:04:55Z<span class=\"o\">)</span>\\n<span class=\"m\">2023</span>-08-02T07:42:42.319364Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span><span class=\"o\">[</span>NONAME<span class=\"o\">]</span><span class=\"w\"> </span>Runtime<span class=\"w\"> </span>has<span class=\"w\"> </span>started\\n<span class=\"m\">2023</span>-08-02T07:42:42.324283Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>Model#0002<span class=\"w\"> </span>is<span class=\"w\"> </span>being<span class=\"w\"> </span>loaded<span class=\"w\"> </span>to<span class=\"w\"> </span>npu:6:0\\n<span class=\"m\">2023</span>-08-02T07:42:42.333521Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>Compiling<span class=\"w\"> </span>Model#0002<span class=\"w\"> </span><span class=\"o\">(</span>target:<span class=\"w\"> </span>warboy-b0,<span class=\"w\"> </span>64dpes,<span class=\"w\"> </span>file:<span class=\"w\"> </span>SSD512_MOBILENET_V2_BDD_int_without_reshape.tflite,<span class=\"w\"> </span>size:<span class=\"w\"> </span><span class=\"m\">5</span>.2<span class=\"w\"> </span>MiB<span class=\"o\">)</span>\\n<span class=\"m\">2023</span>-08-02T07:42:42.814260Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>Model#0002<span class=\"w\"> </span>has<span class=\"w\"> </span>been<span class=\"w\"> </span>compiled<span class=\"w\"> </span>successfully<span class=\"w\"> </span><span class=\"o\">(</span>took<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span>secs<span class=\"o\">)</span>\\n<span class=\"m\">2023</span>-08-02T07:42:42.815406Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::dag:<span class=\"w\"> </span>Task<span class=\"w\"> </span>Statistics:<span class=\"w\"> </span>TaskStats<span class=\"w\"> </span><span class=\"o\">{</span><span class=\"w\"> </span>cpu:<span class=\"w\"> </span><span class=\"m\">26</span>,<span class=\"w\"> </span>npu:<span class=\"w\"> </span><span class=\"m\">1</span>,<span class=\"w\"> </span>alias:<span class=\"w\"> </span><span class=\"m\">0</span>,<span class=\"w\"> </span>coalesce:<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span><span class=\"o\">}</span>\\n<span class=\"m\">2023</span>-08-02T07:42:42.893745Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>NpuApi<span class=\"w\"> </span><span class=\"o\">(</span>AsyncNpuApiImpl<span class=\"o\">)</span><span class=\"w\"> </span>has<span class=\"w\"> </span>started..\\n<span class=\"m\">2023</span>-08-02T07:42:42.893772Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>Creating<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span>Contexts<span class=\"w\"> </span>on<span class=\"w\"> </span>npu:6:0<span class=\"w\"> </span><span class=\"o\">(</span>DRAM<span class=\"w\"> </span>usage:<span class=\"w\"> </span><span class=\"m\">1</span>.0<span class=\"w\"> </span>MiB<span class=\"w\"> </span>/<span class=\"w\"> </span><span class=\"m\">16</span>.0<span class=\"w\"> </span>GiB,<span class=\"w\"> </span>SRAM<span class=\"w\"> </span>usage:<span class=\"w\"> </span><span class=\"m\">14</span>.8<span class=\"w\"> </span>MiB<span class=\"w\"> </span>/<span class=\"w\"> </span><span class=\"m\">64</span>.0<span class=\"w\"> </span>MiB<span class=\"o\">)</span>\\n<span class=\"m\">2023</span>-08-02T07:42:42.894265Z<span class=\"w\">  </span>INFO<span class=\"w\"> </span>furiosa_rt_core::driver::event_driven::coord:<span class=\"w\"> </span>npu:6:0<span class=\"w\"> </span>has<span class=\"w\"> </span>scheduled<span class=\"w\"> </span>to<span class=\"w\"> </span>Model#0002\\nINFO:<span class=\"w\">     </span>Started<span class=\"w\"> </span>server<span class=\"w\"> </span>process<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">2448540</span><span class=\"o\">]</span>\\nINFO:<span class=\"w\">     </span>Waiting<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>application<span class=\"w\"> </span>startup.\\nINFO:<span class=\"w\">     </span>Application<span class=\"w\"> </span>startup<span class=\"w\"> </span>complete.\\nINFO:<span class=\"w\">     </span>Uvicorn<span class=\"w\"> </span>running<span class=\"w\"> </span>on<span class=\"w\"> </span>http://0.0.0.0:8080<span class=\"w\"> </span><span class=\"o\">(</span>Press<span class=\"w\"> </span>CTRL+C<span class=\"w\"> </span>to<span class=\"w\"> </span>quit<span class=\"o\">)</span>\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       Once a model server starts up, you can call the inference request through HTTP protocol.\\nIf the model name is\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         mnist\\n        </span>\\n       </code>\\n       and its version\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         1\\n        </span>\\n       </code>\\n       , the endpoint of the model will be\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         http://&lt;host&gt;:&lt;port&gt;/v2/models/mnist/version/1/infer\\n        </span>\\n       </code>\\n       , accepting\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         POST\\n        </span>\\n       </code>\\n       http request.\\nThe following is an example using\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         curl\\n        </span>\\n       </code>\\n       to send the inference request and return the response.\\n      </p>\\n      <p>\\n       The following is a Python example, doing same as\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         curl\\n        </span>\\n       </code>\\n       does in the above example.\\n      </p>\\n      <div class=\"highlight-python notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">requests</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">mnist</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\\n\\n<span class=\"n\">mnist_images</span> <span class=\"o\">=</span> <span class=\"n\">mnist</span><span class=\"o\">.</span><span class=\"n\">train_images</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">((</span><span class=\"mi\">60000</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">)</span>\\n<span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">\\'http://localhost:8080/v2/models/mnist/versions/1/infer\\'</span>\\n\\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">mnist_images</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">flatten</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">()</span>\\n<span class=\"n\">request</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\\n    <span class=\"s2\">\"inputs\"</span><span class=\"p\">:</span> <span class=\"p\">[{</span>\\n        <span class=\"s2\">\"name\"</span><span class=\"p\">:</span>\\n        <span class=\"s2\">\"mnist\"</span><span class=\"p\">,</span>\\n        <span class=\"s2\">\"datatype\"</span><span class=\"p\">:</span> <span class=\"s2\">\"UINT8\"</span><span class=\"p\">,</span>\\n        <span class=\"s2\">\"shape\"</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">),</span>\\n        <span class=\"s2\">\"data\"</span><span class=\"p\">:</span> <span class=\"n\">data</span>\\n    <span class=\"p\">}]</span>\\n<span class=\"p\">}</span>\\n\\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">json</span><span class=\"o\">=</span><span class=\"n\">request</span><span class=\"p\">)</span>\\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">json</span><span class=\"p\">())</span>\\n</pre>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"endpoints\">\\n      <h2>\\n       Endpoints\\n       <a class=\"headerlink\" href=\"#endpoints\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       The following table shows REST API endpoints and its descriptions.\\nThe model server is following KServe Predict Protocol Version 2.\\nSo, you can find more details from\\n       <a class=\"reference external\" href=\"https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#httprest\">\\n        KServe Predict Protocol Version 2 - HTTP/REST\\n       </a>\\n       .\\n      </p>\\n      <table class=\"docutils align-default\" id=\"id1\">\\n       <caption>\\n        <span class=\"caption-text\">\\n         Endpoints of KServe Predict Protocol Version 2\\n        </span>\\n        <a class=\"headerlink\" href=\"#id1\" title=\"Permalink to this table\">\\n         \\uf0c1\\n        </a>\\n       </caption>\\n       <colgroup>\\n        <col style=\"width: 50%\"/>\\n        <col style=\"width: 50%\"/>\\n       </colgroup>\\n       <thead>\\n        <tr class=\"row-odd\">\\n         <th class=\"head\">\\n          <p>\\n           Method and Endpoint\\n          </p>\\n         </th>\\n         <th class=\"head\">\\n          <p>\\n           Description\\n          </p>\\n         </th>\\n        </tr>\\n       </thead>\\n       <tbody>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           GET /v2/health/live\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Returns HTTP Ok (200) if the inference server is able to receive and respond to metadata and inference requests.\\nThis API can be directly used for the Kubernetes livenessProbe.\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           GET /v2/health/ready\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Returns HTTP Ok (200) if all the models are ready for inferencing.\\nThis API can be directly used for the Kubernetes readinessProbe.\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           GET /v2/models/${MODEL_NAME}/versions/${MODEL_VERSION}\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Returns a model metadata\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           GET /v2/models/${MODEL_NAME}/versions/${MODEL_VERSION}/ready\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Returns HTTP Ok (200) if a specific model is ready for inferencing.\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           POST /v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/infer\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Inference request\\n          </p>\\n         </td>\\n        </tr>\\n       </tbody>\\n      </table>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"profiler.html\" rel=\"prev\" title=\"Performance Profiling\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"kubernetes_support.html\" rel=\"next\" title=\"Kubernetes Support\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='4336ecbc-28cf-41e6-863f-25212b860e38', link=Url('https://furiosa-ai.github.io/docs/latest/en/software/tutorials.html'), name='tutorials', parent='', child=[], description='\\n\\n\\n* Tutorial and Code Examples\\n* [View page source](../_sources/software/tutorials.rst.txt)\\n\\n---\\n\\n\\n\\nTutorial and Code Examples\\n[\\uf0c1](#tutorial-and-code-examples \"Permalink to this heading\")\\n=======================================================================================\\n\\nTutorial\\n[\\uf0c1](#id1 \"Permalink to this heading\")\\n----------------------------------------------\\n\\n* [How to use Furiosa SDK from Start to Finish](https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/HowToUseFuriosaSDKFromStartToFinish.ipynb)\\n* [Basic Inference API](https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/GettingStartedWithPythonSDK.ipynb)\\n* [Advanced Inference API](https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/AdvancedTopicsInInferenceAPIs.ipynb)\\n\\nCode Examples\\n[\\uf0c1](#code-examples \"Permalink to this heading\")\\n-------------------------------------------------------------\\n\\n* [Comparing Accuracy with CPU-based inference](https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/InferenceAccuracyCheck.ipynb)\\n* [Image Classification Model Inference](https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/Image_Classification.ipynb)\\n* [SSD Object Detection Model Inference](https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/SSD_Object_Detection.ipynb)\\n* [Other Python Code Examples](https://github.com/furiosa-ai/furiosa-sdk/tree/branch-0.10.0/examples/inferences)\\n\\n\\n\\n\\n[Previous](vm_support.html \"Configuring Warboy Pass-through for Virtual Machine\")\\n[Next](references.html \"References\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Tutorial and Code Examples * [View page source](../_sources/software/tutorials.rst.txt)\\n---\\nTutorial and Code Examples [\\uf0c1](#tutorial-and-code-examples \"Permalink to this heading\") =======================================================================================\\nTutorial [\\uf0c1](#id1 \"Permalink to this heading\") ----------------------------------------------\\n* [How to use Furiosa SDK from Start to Finish](https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/HowToUseFuriosaSDKFromStartToFinish.ipynb) * [Basic Inference API](https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/GettingStartedWithPythonSDK.ipynb) * [Advanced Inference API](https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/AdvancedTopicsInInferenceAPIs.ipynb)\\nCode Examples [\\uf0c1](#code-examples \"Permalink to this heading\") -------------------------------------------------------------\\n* [Comparing Accuracy with CPU-based inference](https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/InferenceAccuracyCheck.ipynb) * [Image Classification Model Inference](https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/Image_Classification.ipynb) * [SSD Object Detection Model Inference](https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/SSD_Object_Detection.ipynb) * [Other Python Code Examples](https://github.com/furiosa-ai/furiosa-sdk/tree/branch-0.10.0/examples/inferences)\\n[Previous](vm_support.html \"Configuring Warboy Pass-through for Virtual Machine\") [Next](references.html \"References\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Tutorial and Code Examples\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/tutorials.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"tutorial-and-code-examples\">\\n     <span id=\"tutorial\">\\n     </span>\\n     <h1>\\n      Tutorial and Code Examples\\n      <a class=\"headerlink\" href=\"#tutorial-and-code-examples\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <section id=\"id1\">\\n      <h2>\\n       Tutorial\\n       <a class=\"headerlink\" href=\"#id1\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/HowToUseFuriosaSDKFromStartToFinish.ipynb\">\\n          How to use Furiosa SDK from Start to Finish\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/GettingStartedWithPythonSDK.ipynb\">\\n          Basic Inference API\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/AdvancedTopicsInInferenceAPIs.ipynb\">\\n          Advanced Inference API\\n         </a>\\n        </p>\\n       </li>\\n      </ul>\\n     </section>\\n     <section id=\"code-examples\">\\n      <h2>\\n       Code Examples\\n       <a class=\"headerlink\" href=\"#code-examples\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/InferenceAccuracyCheck.ipynb\">\\n          Comparing Accuracy with CPU-based inference\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/Image_Classification.ipynb\">\\n          Image Classification Model Inference\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/SSD_Object_Detection.ipynb\">\\n          SSD Object Detection Model Inference\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/tree/branch-0.10.0/examples/inferences\">\\n          Other Python Code Examples\\n         </a>\\n        </p>\\n       </li>\\n      </ul>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"vm_support.html\" rel=\"prev\" title=\"Configuring Warboy Pass-through for Virtual Machine\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"references.html\" rel=\"next\" title=\"References\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='e527d72f-b0b3-4132-b935-b89558bc7add', link=Url('https://furiosa-ai.github.io/docs/latest/en/releases/0.8.0.html'), name='0', parent='', child=[], description='\\n\\n\\n* Release Notes - 0.8.0\\n* [View page source](../_sources/releases/0.8.0.rst.txt)\\n\\n---\\n\\n\\n\\nRelease Notes - 0.8.0\\n[\\uf0c1](#release-notes-0-8-0 \"Permalink to this heading\")\\n===========================================================================\\n\\nFuriosa SDK 0.8.0 is a major release, including many performance enhancements,\\nadditional functions, and bug fixes.\\n0.8.0 also includes the serving framework, a core tool of user application development,\\nas well as major improvements to the Model Zoo.\\n\\n\\nComponent Version Information\\n\\n[\\uf0c1](#id3 \"Permalink to this table\")\\n\\n\\n| Package Name | Version |\\n| --- | --- |\\n| NPU Driver | 1.4.0 |\\n| NPU Firmware Tools | 1.2.0 |\\n| NPU Firmware Image | 1.2.0 |\\n| HAL (Hardware Abstraction Layer) | 0.9.0 |\\n| Furiosa Compiler | 0.8.0 |\\n| Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..) | 0.8.0 |\\n| NPU Device Plugin | 0.10.1 |\\n| NPU Feature Discovery | 0.2.0 |\\n| NPU Management CLI (furiosactl) | 0.10.0 |\\n\\nInstalling the latest SDK\\n[\\uf0c1](#installing-the-latest-sdk \"Permalink to this heading\")\\n-------------------------------------------------------------------------------------\\n\\nIf you are using APT repository, the upgrade process is simpler.\\n\\n> ```\\n> apt-get update && apt-get upgrade\\n> \\n> ```\\n\\nIf you wish to designate a specific package for upgrade, execute as below:\\nYou can find more details about APT repository setup at\\n[Driver, Firmware, and Runtime Installation](../software/installation.html#requiredpackages)\\n.\\n\\n> ```\\n> apt-get update && \\\\\\n> apt-get install -y furiosa-driver-pdma furiosa-libhal-warboy furiosa-libnux furiosactl\\n> \\n> ```\\n\\nYou can upgrade firmware as follows:\\n\\n> ```\\n> apt-get update && \\\\\\n> apt-get install -y furiosa-firmware-tools furiosa-firmware-image\\n> \\n> ```\\n\\nYou can upgrade Python package as follows:\\n\\n> ```\\n> pip install --upgrade furiosa-sdk\\n> \\n> ```\\n\\n\\nMajor changes\\n[\\uf0c1](#major-changes \"Permalink to this heading\")\\n-------------------------------------------------------------\\n\\n### Improvements to serving framework API [\\uf0c1](#improvements-to-serving-framework-api \"Permalink to this heading\")\\n\\nThe\\n[furiosa-serving](https://github.com/furiosa-ai/furiosa-sdk/tree/branch-0.8.0/python/furiosa-serving)\\nis a FastAPI-based serving framework.\\nWith the\\n[furiosa-serving](https://github.com/furiosa-ai/furiosa-sdk/tree/branch-0.8.0/python/furiosa-serving)\\n,\\nusers can quickly develop Python-based high performance web service applications that utilize NPUs.\\nThe 0.8.0 release includes the following major updates.\\n\\n**Session pool that enables model serving with multiple NPUs**\\n\\nSession pools improve significantly throughput of model APIs by using multiple NPUs. If large inputs can be divided\\ninto a number of small inputs, this improvement can be used to reduce the latency of serving applications.\\n\\n```\\nmodel: NPUServeModel = synchronous(serve.model(\"nux\"))(\\n    \"MNIST\",\\n    location=\"./assets/models/MNISTnet_uint8_quant_without_softmax.tflite\",\\n    # Specify multiple devices\\n    npu_device=\"npu0pe0,npu0pe1,npu1pe0\"\\n    worker_num=4,\\n)\\n\\n```\\n\\n**Shift from thread-based to asyncio-based NPU query processing**\\n\\nSmall and frequent NPU inference queries may now be processed with lower latency.\\nAs shown in the example below, applications requiring multiple NPU inferences in a\\nsingle API query can be processed with better performance.\\n\\n```\\nasync def inference(self, tensors: List[np.ndarray]) -> List[np.ndarray]:\\n    # The following code runs multiple inferences at the same time and wait until all requests are completed.\\n    return await asyncio.gather(*(self.model.predict(tensor) for tensor in tensors))\\n\\n```\\n\\n**Added expanded support for external device & runtime**\\n\\nIn complex serving scenarios, additional/external device and runtime programs may be\\nrequired, in addition to NPU-based Furiosa Runtime. In this release, the framework\\nhas been expanded such that external device and runtime may be used. The first\\nexternal runtime added is OpenVINO.\\n\\n```\\nimagenet: ServeModel = synchronous(serve.model(\"openvino\"))(\\n    \\'imagenet\\',\\n    location=\\'./examples/assets/models/image_classification.onnx\\'\\n)\\n\\n```\\n\\n**Support for S3 cloud storage repository**\\n\\nSet model\\n`location`\\nas S3 URL.\\n\\n```\\n# Load model from S3 (Auth environment variable for aioboto library required)\\ndensenet: ServeModel = synchronous(serve.model(\"nux\"))(\\n    \\'imagenet\\',\\n location=\\'s3://furiosa/models/93d63f654f0f192cc4ff5691be60fb9379e9d7fd\\'\\n)\\n\\n```\\n\\n**Support for OpenTelemetry compatible tracing**\\n\\nWith the\\n[OpenTelemetry Collector](https://opentelemetry.io/docs/collector/)\\nfunction, you can now track the execution time of specific code sections of the\\nserving applications.\\n\\nTo use this function, you can activate\\n`trace.get_tracer()`\\n, reset the tracer,\\nactivate the\\n`tracer.start_as_current_span()`\\nfunction, and designate the section.\\n\\n```\\nfrom opentelemetry import trace\\n\\ntracer = trace.get_tracer(__name__)\\n\\nclass Application:\\n\\n        async def process(self, image: Image.Image) -> int:\\n            with tracer.start_as_current_span(\"preprocess\"):\\n                input_tensors = self.preprocess(image)\\n            with tracer.start_as_current_span(\"inference\"):\\n                output_tensors = await self.inference(input_tensors)\\n            with tracer.start_as_current_span(\"postprocess\"):\\n                return self.postprocess(output_tensors)\\n\\n```\\n\\nThe specification of\\n[OpenTelemetry Collector](https://opentelemetry.io/docs/collector/)\\ncan be done through the configuration of\\n`FURIOSA_SERVING_OTLP_ENDPOINT`\\n, as shown below.\\nThe following diagram is an example that visualizes the tracing result with Grafana.\\n\\nOther major improvements are as follows:\\n\\n* Several inference requests can be executed at once, with serving API now supporting compiler setting\\n  `batch_size`\\n* More threads can share the NPU, with serving API now supporting session option\\n  `worker_num`\\n\\n### Profiler [\\uf0c1](#profiler \"Permalink to this heading\")\\n\\nYou can now analyze the profiler tracing results with\\n[Pandas](https://pandas.pydata.org/)\\n,\\na data analysis framework. With this function, you can analyze the tracing result data,\\nallowing you to quickly identify bottlenecks and reasons for model performance changes.\\nMore detailed instructions can be found at\\n[Trace analysis using Pandas DataFrame](../software/profiler.html#pandasprofilinganalysis)\\n.\\n\\n```\\nfrom furiosa.runtime import session, tensor\\nfrom furiosa.runtime.profiler import RecordFormat, profile\\n\\nwith profile(format=RecordFormat.PandasDataFrame) as profiler:\\n    with session.create(\"MNISTnet_uint8_quant_without_softmax.tflite\") as sess:\\n        input_shape = sess.input(0)\\n\\n        with profiler.record(\"record\") as record:\\n            for _ in range(0, 2):\\n                sess.run(tensor.rand(input_shape))\\n\\ndf = profiler.get_pandas_dataframe()\\nprint(df[df[\"name\"] == \"trace\"][[\"trace_id\", \"name\", \"thread.id\", \"dur\"]])\\n\\n```\\n\\n\\n\\n### Quantization tool [\\uf0c1](#quantization-tool \"Permalink to this heading\")\\n\\n[Model Quantization](../software/quantization.html#modelquantization)\\nis a tool that converts pre-trained models to quantized models.\\nThis release includes the following major updates.\\n\\n* Accuracy improvement when processing SiLU operator\\n* Improved usability of compiler setting\\n  `without_quantize`\\n* Accuracy improvement when processing MatMul/Gemm operators\\n* Accuracy improvement when processing Add/Sub/Mul/Div operators\\n* NPU acceleration now added for more auto\\\\_pad properties, when processing Conv/ConvTranspose/MaxPool operators\\n* NPU acceleration support for PRelu operator\\n\\n### furiosa-toolkit [\\uf0c1](#furiosa-toolkit \"Permalink to this heading\")\\n\\nThe\\n`furiosactl`\\ncommand line tool, which has been added to the\\nfuriosa-toolkit 0.10.0 release, includes the following improvements.\\n\\nThe newly added\\nfuriosactl ps\\n\\ncommand allows you to print\\nthe OS processes which are occupying the NPU device.\\n\\n```\\n# furiosactl ps\\n+-----------+--------+------------------------------------------------------------+\\n| NPU       | PID    | CMD                                                        |\\n+-----------+--------+------------------------------------------------------------+\\n| npu0pe0-1 | 132529 | /usr/bin/python3 /usr/local/bin/uvicorn image_classify:app |\\n+-----------+--------+------------------------------------------------------------+\\n\\n```\\n\\nThe\\nfuriosactl info\\n\\ncommand now prints the unique UUID for each device.\\n\\n```\\n$ furiosactl info\\n+------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+\\n| NPU  | Name   | UUID                                 | Firmware        | Temp. | Power  | PCI-BDF      | PCI-DEV |\\n+------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+\\n| npu0 | warboy | 72212674-61BE-4FCA-A2C9-555E4EE67AB5 | v1.1.0, 12180b0 |  49°C | 3.12 W | 0000:24:00.0 | 235:0   |\\n+------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+\\n| npu1 | warboy | DF80FB54-8190-44BC-B9FB-664FA36C754A | v1.1.0, 12180b0 |  54°C | 2.53 W | 0000:6d:00.0 | 511:0   |\\n+------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+\\n\\n```\\n\\nDetailed instructions on installation and usage for\\nfuriosactl\\n\\ncan be found in\\n[furiosa-toolkit](../software/cli.html#toolkit)\\n.\\n\\n\\n### Model Zoo API improvements, added models, and added native post-processing code [\\uf0c1](#model-zoo-api-improvements-added-models-and-added-native-post-processing-code \"Permalink to this heading\")\\n\\n[furioa-models](https://furiosa-ai.github.io/furiosa-models)\\nis a public Model Zoo project,\\nproviding FuriosaAI NPU-optimized models.\\nThe 0.8.0 release includes the following major updates.\\n\\n**YOLOv5 Large/Medium models added**\\n\\nSupport for\\n`YOLOv5l`\\n,\\n`YOLOv5m`\\n, which are SOTA object detection models, have been added.\\nThe total list of available models can be found in\\n[Model List](https://furiosa-ai.github.io/furiosa-models/v0.8.0/#model_list)\\n.\\n\\n**Improvements to model class and loading API**\\n\\nThe model class has been improved to include pre and post-processing code, while the\\nmodel loading API has been improved as shown below.\\n\\nMore explanation on model class and the API can be found at\\n[Model Object](https://furiosa-ai.github.io/furiosa-models/latest/model_object/)\\n.\\n\\nBlocking API\\n\\nNonblocking API\\n\\n\\nBefore update\\n\\n```\\nfrom furiosa.models.vision import MLCommonsResNet50\\n\\nresnet50 = MLCommonsResNet50()\\n\\n```\\n\\nUpdated code\\n\\n```\\nfrom furiosa.models.vision import ResNet50\\n\\nresnet50 = ResNet50.load()\\n\\n```\\n\\n\\n\\nBefore update\\n\\n```\\nimport asyncio\\n\\nfrom furiosa.models.nonblocking.vision import MLCommonsResNet50\\n\\nresnet50: Model = asyncio.run(MLCommonsResNet50())\\n\\n```\\n\\n0.8.0 improvements\\n\\n```\\nimport asyncio\\n\\nfrom furiosa.models.vision import ResNet50\\n\\nresnet50: Model = asyncio.run(ResNet50.load_async())\\n\\n```\\n\\n\\n\\nThe model post-processing process converts the inference ouput tensor into structural\\ndata, which is more accessible for the application. Depending on the model, this\\nmay require a longer execution time.\\nThe 0.8.0 release includes native post-processing code for ResNet50, SSD-MobileNet,\\nand SSD-ResNet34. Based on internal benchmarks, native post-processing code can reduce\\nlatency by up to 70%, depending on the model.\\n\\nThe following is a complete example of ResNet50, utilizing native post-processing code.\\nMore information can be found at\\n[Pre/Postprocessing](https://furiosa-ai.github.io/furiosa-models/v0.8.0/model_object/#prepostprocessing)\\n.\\n\\n> ```\\n> from furiosa.models.vision import ResNet50\\n> from furiosa.models.vision.resnet50 import NativePostProcessor, preprocess\\n> from furiosa.runtime import session\\n> \\n> model = ResNet50.load()\\n> \\n> postprocessor = NativePostProcessor(model)\\n> with session.create(model) as sess:\\n>     image = preprocess(\"tests/assets/cat.jpg\")\\n>     output = sess.run(image).numpy()\\n>     postprocessor.eval(output)\\n> \\n> ```\\n\\nOther changes and updates can be found at\\n[Furiosa Model - 0.8.0 Changelogs](https://furiosa-ai.github.io/furiosa-models/v0.8.0/changelog/)\\n.\\n\\n\\n\\n\\n\\n\\n[Previous](0.9.0.html \"Release Notes - 0.9.0\")\\n[Next](0.7.0.html \"Release Notes - 0.7.0\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Release Notes - 0.8.0 * [View page source](../_sources/releases/0.8.0.rst.txt)\\n---\\nRelease Notes - 0.8.0 [\\uf0c1](#release-notes-0-8-0 \"Permalink to this heading\") ===========================================================================\\nFuriosa SDK 0.8.0 is a major release, including many performance enhancements, additional functions, and bug fixes. 0.8.0 also includes the serving framework, a core tool of user application development, as well as major improvements to the Model Zoo.\\nComponent Version Information\\n[\\uf0c1](#id3 \"Permalink to this table\")\\n| Package Name | Version | | --- | --- | | NPU Driver | 1.4.0 | | NPU Firmware Tools | 1.2.0 | | NPU Firmware Image | 1.2.0 | | HAL (Hardware Abstraction Layer) | 0.9.0 | | Furiosa Compiler | 0.8.0 | | Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..) | 0.8.0 | | NPU Device Plugin | 0.10.1 | | NPU Feature Discovery | 0.2.0 | | NPU Management CLI (furiosactl) | 0.10.0 |\\nInstalling the latest SDK [\\uf0c1](#installing-the-latest-sdk \"Permalink to this heading\") -------------------------------------------------------------------------------------\\nIf you are using APT repository, the upgrade process is simpler.\\n> ``` > apt-get update && apt-get upgrade >  > ```\\nIf you wish to designate a specific package for upgrade, execute as below: You can find more details about APT repository setup at [Driver, Firmware, and Runtime Installation](../software/installation.html#requiredpackages) .\\n> ``` > apt-get update && \\\\ > apt-get install -y furiosa-driver-pdma furiosa-libhal-warboy furiosa-libnux furiosactl >  > ```\\nYou can upgrade firmware as follows:\\n> ``` > apt-get update && \\\\ > apt-get install -y furiosa-firmware-tools furiosa-firmware-image >  > ```\\nYou can upgrade Python package as follows:\\n> ``` > pip install --upgrade furiosa-sdk >  > ```\\nMajor changes [\\uf0c1](#major-changes \"Permalink to this heading\") -------------------------------------------------------------\\n### Improvements to serving framework API [\\uf0c1](#improvements-to-serving-framework-api \"Permalink to this heading\")\\nThe [furiosa-serving](https://github.com/furiosa-ai/furiosa-sdk/tree/branch-0.8.0/python/furiosa-serving) is a FastAPI-based serving framework. With the [furiosa-serving](https://github.com/furiosa-ai/furiosa-sdk/tree/branch-0.8.0/python/furiosa-serving) ,\\nusers can quickly develop Python-based high performance web service applications that utilize NPUs. The 0.8.0 release includes the following major updates.\\n**Session pool that enables model serving with multiple NPUs**\\nSession pools improve significantly throughput of model APIs by using multiple NPUs. If large inputs can be divided into a number of small inputs, this improvement can be used to reduce the latency of serving applications.\\n``` model: NPUServeModel = synchronous(serve.model(\"nux\"))(     \"MNIST\",     location=\"./assets/models/MNISTnet_uint8_quant_without_softmax.tflite\",     # Specify multiple devices     npu_device=\"npu0pe0,npu0pe1,npu1pe0\"     worker_num=4, )\\n```\\n**Shift from thread-based to asyncio-based NPU query processing**\\nSmall and frequent NPU inference queries may now be processed with lower latency. As shown in the example below, applications requiring multiple NPU inferences in a single API query can be processed with better performance.\\n``` async def inference(self, tensors: List[np.ndarray]) -> List[np.ndarray]:     # The following code runs multiple inferences at the same time and wait until all requests are completed.     return await asyncio.gather(*(self.model.predict(tensor) for tensor in tensors))\\n```\\n**Added expanded support for external device & runtime**\\nIn complex serving scenarios, additional/external device and runtime programs may be required, in addition to NPU-based Furiosa Runtime. In this release, the framework has been expanded such that external device and runtime may be used. The first external runtime added is OpenVINO.\\n``` imagenet: ServeModel = synchronous(serve.model(\"openvino\"))(     \\'imagenet\\',     location=\\'./examples/assets/models/image_classification.onnx\\' )\\n```\\n**Support for S3 cloud storage repository**\\nSet model `location` as S3 URL.\\n``` # Load model from S3 (Auth environment variable for aioboto library required) densenet: ServeModel = synchronous(serve.model(\"nux\"))(     \\'imagenet\\',  location=\\'s3://furiosa/models/93d63f654f0f192cc4ff5691be60fb9379e9d7fd\\' )\\n```\\n**Support for OpenTelemetry compatible tracing**\\nWith the [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/) function, you can now track the execution time of specific code sections of the serving applications.\\nTo use this function, you can activate `trace.get_tracer()` , reset the tracer, activate the `tracer.start_as_current_span()` function, and designate the section.\\n``` from opentelemetry import trace\\ntracer = trace.get_tracer(__name__)\\nclass Application:\\n        async def process(self, image: Image.Image) -> int:             with tracer.start_as_current_span(\"preprocess\"):                 input_tensors = self.preprocess(image)             with tracer.start_as_current_span(\"inference\"):                 output_tensors = await self.inference(input_tensors)             with tracer.start_as_current_span(\"postprocess\"):                 return self.postprocess(output_tensors)\\n```\\nThe specification of [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/) can be done through the configuration of `FURIOSA_SERVING_OTLP_ENDPOINT` , as shown below. The following diagram is an example that visualizes the tracing result with Grafana.\\nOther major improvements are as follows:\\n* Several inference requests can be executed at once, with serving API now supporting compiler setting   `batch_size` * More threads can share the NPU, with serving API now supporting session option   `worker_num`  ### Profiler [\\uf0c1](#profiler \"Permalink to this heading\")\\nYou can now analyze the profiler tracing results with [Pandas](https://pandas.pydata.org/) ,\\na data analysis framework. With this function, you can analyze the tracing result data, allowing you to quickly identify bottlenecks and reasons for model performance changes. More detailed instructions can be found at [Trace analysis using Pandas DataFrame](../software/profiler.html#pandasprofilinganalysis) .\\n``` from furiosa.runtime import session, tensor from furiosa.runtime.profiler import RecordFormat, profile\\nwith profile(format=RecordFormat.PandasDataFrame) as profiler:     with session.create(\"MNISTnet_uint8_quant_without_softmax.tflite\") as sess:         input_shape = sess.input(0)\\n        with profiler.record(\"record\") as record:             for _ in range(0, 2):                 sess.run(tensor.rand(input_shape))\\ndf = profiler.get_pandas_dataframe() print(df[df[\"name\"] == \"trace\"][[\"trace_id\", \"name\", \"thread.id\", \"dur\"]])\\n```\\n### Quantization tool [\\uf0c1](#quantization-tool \"Permalink to this heading\")\\n[Model Quantization](../software/quantization.html#modelquantization) is a tool that converts pre-trained models to quantized models. This release includes the following major updates.\\n* Accuracy improvement when processing SiLU operator * Improved usability of compiler setting   `without_quantize` * Accuracy improvement when processing MatMul/Gemm operators * Accuracy improvement when processing Add/Sub/Mul/Div operators * NPU acceleration now added for more auto\\\\_pad properties, when processing Conv/ConvTranspose/MaxPool operators * NPU acceleration support for PRelu operator\\n### furiosa-toolkit [\\uf0c1](#furiosa-toolkit \"Permalink to this heading\")\\nThe `furiosactl` command line tool, which has been added to the furiosa-toolkit 0.10.0 release, includes the following improvements.\\nThe newly added furiosactl ps\\ncommand allows you to print the OS processes which are occupying the NPU device.\\n``` # furiosactl ps +-----------+--------+------------------------------------------------------------+ | NPU       | PID    | CMD                                                        | +-----------+--------+------------------------------------------------------------+ | npu0pe0-1 | 132529 | /usr/bin/python3 /usr/local/bin/uvicorn image_classify:app | +-----------+--------+------------------------------------------------------------+\\n```\\nThe furiosactl info\\ncommand now prints the unique UUID for each device.\\n``` $ furiosactl info +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+ | NPU  | Name   | UUID                                 | Firmware        | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+ | npu0 | warboy | 72212674-61BE-4FCA-A2C9-555E4EE67AB5 | v1.1.0, 12180b0 |  49°C | 3.12 W | 0000:24:00.0 | 235:0   | +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+ | npu1 | warboy | DF80FB54-8190-44BC-B9FB-664FA36C754A | v1.1.0, 12180b0 |  54°C | 2.53 W | 0000:6d:00.0 | 511:0   | +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+\\n```\\nDetailed instructions on installation and usage for furiosactl\\ncan be found in [furiosa-toolkit](../software/cli.html#toolkit) .\\n### Model Zoo API improvements, added models, and added native post-processing code [\\uf0c1](#model-zoo-api-improvements-added-models-and-added-native-post-processing-code \"Permalink to this heading\")\\n[furioa-models](https://furiosa-ai.github.io/furiosa-models) is a public Model Zoo project, providing FuriosaAI NPU-optimized models. The 0.8.0 release includes the following major updates.\\n**YOLOv5 Large/Medium models added**\\nSupport for `YOLOv5l` , `YOLOv5m` , which are SOTA object detection models, have been added. The total list of available models can be found in [Model List](https://furiosa-ai.github.io/furiosa-models/v0.8.0/#model_list) .\\n**Improvements to model class and loading API**\\nThe model class has been improved to include pre and post-processing code, while the model loading API has been improved as shown below.\\nMore explanation on model class and the API can be found at [Model Object](https://furiosa-ai.github.io/furiosa-models/latest/model_object/) .\\nBlocking API\\nNonblocking API\\nBefore update\\n``` from furiosa.models.vision import MLCommonsResNet50\\nresnet50 = MLCommonsResNet50()\\n```\\nUpdated code\\n``` from furiosa.models.vision import ResNet50\\nresnet50 = ResNet50.load()\\n```\\nBefore update\\n``` import asyncio\\nfrom furiosa.models.nonblocking.vision import MLCommonsResNet50\\nresnet50: Model = asyncio.run(MLCommonsResNet50())\\n```\\n0.8.0 improvements\\n``` import asyncio\\nfrom furiosa.models.vision import ResNet50\\nresnet50: Model = asyncio.run(ResNet50.load_async())\\n```\\nThe model post-processing process converts the inference ouput tensor into structural data, which is more accessible for the application. Depending on the model, this may require a longer execution time. The 0.8.0 release includes native post-processing code for ResNet50, SSD-MobileNet, and SSD-ResNet34. Based on internal benchmarks, native post-processing code can reduce latency by up to 70%, depending on the model.\\nThe following is a complete example of ResNet50, utilizing native post-processing code. More information can be found at [Pre/Postprocessing](https://furiosa-ai.github.io/furiosa-models/v0.8.0/model_object/#prepostprocessing) .\\n> ``` > from furiosa.models.vision import ResNet50 > from furiosa.models.vision.resnet50 import NativePostProcessor, preprocess > from furiosa.runtime import session >  > model = ResNet50.load() >  > postprocessor = NativePostProcessor(model) > with session.create(model) as sess: >     image = preprocess(\"tests/assets/cat.jpg\") >     output = sess.run(image).numpy() >     postprocessor.eval(output) >  > ```\\nOther changes and updates can be found at [Furiosa Model - 0.8.0 Changelogs](https://furiosa-ai.github.io/furiosa-models/v0.8.0/changelog/) .\\n[Previous](0.9.0.html \"Release Notes - 0.9.0\") [Next](0.7.0.html \"Release Notes - 0.7.0\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Release Notes - 0.8.0\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/releases/0.8.0.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"release-notes-0-8-0\">\\n     <h1>\\n      Release Notes - 0.8.0\\n      <a class=\"headerlink\" href=\"#release-notes-0-8-0\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      Furiosa SDK 0.8.0 is a major release, including many performance enhancements,\\nadditional functions, and bug fixes.\\n0.8.0 also includes the serving framework, a core tool of user application development,\\nas well as major improvements to the Model Zoo.\\n     </p>\\n     <table class=\"docutils align-default\" id=\"id3\">\\n      <caption>\\n       <span class=\"caption-text\">\\n        Component Version Information\\n       </span>\\n       <a class=\"headerlink\" href=\"#id3\" title=\"Permalink to this table\">\\n        \\uf0c1\\n       </a>\\n      </caption>\\n      <colgroup>\\n       <col style=\"width: 80%\"/>\\n       <col style=\"width: 20%\"/>\\n      </colgroup>\\n      <thead>\\n       <tr class=\"row-odd\">\\n        <th class=\"head\">\\n         <p>\\n          Package Name\\n         </p>\\n        </th>\\n        <th class=\"head\">\\n         <p>\\n          Version\\n         </p>\\n        </th>\\n       </tr>\\n      </thead>\\n      <tbody>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          NPU Driver\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          1.4.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          NPU Firmware Tools\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          1.2.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          NPU Firmware Image\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          1.2.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          HAL (Hardware Abstraction Layer)\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.9.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          Furiosa Compiler\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.8.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..)\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.8.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          NPU Device Plugin\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.10.1\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          NPU Feature Discovery\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.2.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          NPU Management CLI (furiosactl)\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.10.0\\n         </p>\\n        </td>\\n       </tr>\\n      </tbody>\\n     </table>\\n     <section id=\"installing-the-latest-sdk\">\\n      <h2>\\n       Installing the latest SDK\\n       <a class=\"headerlink\" href=\"#installing-the-latest-sdk\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       If you are using APT repository, the upgrade process is simpler.\\n      </p>\\n      <blockquote>\\n       <div>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>apt-get<span class=\"w\"> </span>update<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span>apt-get<span class=\"w\"> </span>upgrade\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n      </blockquote>\\n      <p>\\n       If you wish to designate a specific package for upgrade, execute as below:\\nYou can find more details about APT repository setup at\\n       <a class=\"reference internal\" href=\"../software/installation.html#requiredpackages\">\\n        <span class=\"std std-ref\">\\n         Driver, Firmware, and Runtime Installation\\n        </span>\\n       </a>\\n       .\\n      </p>\\n      <blockquote>\\n       <div>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>apt-get<span class=\"w\"> </span>update<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span><span class=\"se\">\\\\</span>\\napt-get<span class=\"w\"> </span>install<span class=\"w\"> </span>-y<span class=\"w\"> </span>furiosa-driver-pdma<span class=\"w\"> </span>furiosa-libhal-warboy<span class=\"w\"> </span>furiosa-libnux<span class=\"w\"> </span>furiosactl\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n      </blockquote>\\n      <p>\\n       You can upgrade firmware as follows:\\n      </p>\\n      <blockquote>\\n       <div>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>apt-get<span class=\"w\"> </span>update<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span><span class=\"se\">\\\\</span>\\napt-get<span class=\"w\"> </span>install<span class=\"w\"> </span>-y<span class=\"w\"> </span>furiosa-firmware-tools<span class=\"w\"> </span>furiosa-firmware-image\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n      </blockquote>\\n      <p>\\n       You can upgrade Python package as follows:\\n      </p>\\n      <blockquote>\\n       <div>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>--upgrade<span class=\"w\"> </span>furiosa-sdk\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n      </blockquote>\\n     </section>\\n     <section id=\"major-changes\">\\n      <h2>\\n       Major changes\\n       <a class=\"headerlink\" href=\"#major-changes\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <section id=\"improvements-to-serving-framework-api\">\\n       <h3>\\n        Improvements to serving framework API\\n        <a class=\"headerlink\" href=\"#improvements-to-serving-framework-api\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        The\\n        <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/tree/branch-0.8.0/python/furiosa-serving\">\\n         furiosa-serving\\n        </a>\\n        is a FastAPI-based serving framework.\\nWith the\\n        <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/tree/branch-0.8.0/python/furiosa-serving\">\\n         furiosa-serving\\n        </a>\\n        ,\\nusers can quickly develop Python-based high performance web service applications that utilize NPUs.\\nThe 0.8.0 release includes the following major updates.\\n       </p>\\n       <p>\\n        <strong>\\n         Session pool that enables model serving with multiple NPUs\\n        </strong>\\n       </p>\\n       <p>\\n        Session pools improve significantly throughput of model APIs by using multiple NPUs. If large inputs can be divided\\ninto a number of small inputs, this improvement can be used to reduce the latency of serving applications.\\n       </p>\\n       <div class=\"highlight-python notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">NPUServeModel</span> <span class=\"o\">=</span> <span class=\"n\">synchronous</span><span class=\"p\">(</span><span class=\"n\">serve</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">(</span><span class=\"s2\">\"nux\"</span><span class=\"p\">))(</span>\\n    <span class=\"s2\">\"MNIST\"</span><span class=\"p\">,</span>\\n    <span class=\"n\">location</span><span class=\"o\">=</span><span class=\"s2\">\"./assets/models/MNISTnet_uint8_quant_without_softmax.tflite\"</span><span class=\"p\">,</span>\\n    <span class=\"c1\"># Specify multiple devices</span>\\n    <span class=\"n\">npu_device</span><span class=\"o\">=</span><span class=\"s2\">\"npu0pe0,npu0pe1,npu1pe0\"</span>\\n    <span class=\"n\">worker_num</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span>\\n<span class=\"p\">)</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        <strong>\\n         Shift from thread-based to asyncio-based NPU query processing\\n        </strong>\\n       </p>\\n       <p>\\n        Small and frequent NPU inference queries may now be processed with lower latency.\\nAs shown in the example below, applications requiring multiple NPU inferences in a\\nsingle API query can be processed with better performance.\\n       </p>\\n       <div class=\"highlight-python notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">inference</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">tensors</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">])</span> <span class=\"o\">-&gt;</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]:</span>\\n    <span class=\"c1\"># The following code runs multiple inferences at the same time and wait until all requests are completed.</span>\\n    <span class=\"k\">return</span> <span class=\"k\">await</span> <span class=\"n\">asyncio</span><span class=\"o\">.</span><span class=\"n\">gather</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">tensor</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">tensor</span> <span class=\"ow\">in</span> <span class=\"n\">tensors</span><span class=\"p\">))</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        <strong>\\n         Added expanded support for external device &amp; runtime\\n        </strong>\\n       </p>\\n       <p>\\n        In complex serving scenarios, additional/external device and runtime programs may be\\nrequired, in addition to NPU-based Furiosa Runtime. In this release, the framework\\nhas been expanded such that external device and runtime may be used. The first\\nexternal runtime added is OpenVINO.\\n       </p>\\n       <div class=\"highlight-python notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"n\">imagenet</span><span class=\"p\">:</span> <span class=\"n\">ServeModel</span> <span class=\"o\">=</span> <span class=\"n\">synchronous</span><span class=\"p\">(</span><span class=\"n\">serve</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">(</span><span class=\"s2\">\"openvino\"</span><span class=\"p\">))(</span>\\n    <span class=\"s1\">\\'imagenet\\'</span><span class=\"p\">,</span>\\n    <span class=\"n\">location</span><span class=\"o\">=</span><span class=\"s1\">\\'./examples/assets/models/image_classification.onnx\\'</span>\\n<span class=\"p\">)</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        <strong>\\n         Support for S3 cloud storage repository\\n        </strong>\\n       </p>\\n       <p>\\n        Set model\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          location\\n         </span>\\n        </code>\\n        as S3 URL.\\n       </p>\\n       <div class=\"highlight-python notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"c1\"># Load model from S3 (Auth environment variable for aioboto library required)</span>\\n<span class=\"n\">densenet</span><span class=\"p\">:</span> <span class=\"n\">ServeModel</span> <span class=\"o\">=</span> <span class=\"n\">synchronous</span><span class=\"p\">(</span><span class=\"n\">serve</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">(</span><span class=\"s2\">\"nux\"</span><span class=\"p\">))(</span>\\n    <span class=\"s1\">\\'imagenet\\'</span><span class=\"p\">,</span>\\n <span class=\"n\">location</span><span class=\"o\">=</span><span class=\"s1\">\\'s3://furiosa/models/93d63f654f0f192cc4ff5691be60fb9379e9d7fd\\'</span>\\n<span class=\"p\">)</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        <strong>\\n         Support for OpenTelemetry compatible tracing\\n        </strong>\\n       </p>\\n       <p>\\n        With the\\n        <a class=\"reference external\" href=\"https://opentelemetry.io/docs/collector/\">\\n         OpenTelemetry Collector\\n        </a>\\n        function, you can now track the execution time of specific code sections of the\\nserving applications.\\n       </p>\\n       <p>\\n        To use this function, you can activate\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          trace.get_tracer()\\n         </span>\\n        </code>\\n        , reset the tracer,\\nactivate the\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          tracer.start_as_current_span()\\n         </span>\\n        </code>\\n        function, and designate the section.\\n       </p>\\n       <div class=\"highlight-python notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">opentelemetry</span> <span class=\"kn\">import</span> <span class=\"n\">trace</span>\\n\\n<span class=\"n\">tracer</span> <span class=\"o\">=</span> <span class=\"n\">trace</span><span class=\"o\">.</span><span class=\"n\">get_tracer</span><span class=\"p\">(</span><span class=\"vm\">__name__</span><span class=\"p\">)</span>\\n\\n<span class=\"k\">class</span> <span class=\"nc\">Application</span><span class=\"p\">:</span>\\n\\n        <span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">process</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">Image</span><span class=\"o\">.</span><span class=\"n\">Image</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">int</span><span class=\"p\">:</span>\\n            <span class=\"k\">with</span> <span class=\"n\">tracer</span><span class=\"o\">.</span><span class=\"n\">start_as_current_span</span><span class=\"p\">(</span><span class=\"s2\">\"preprocess\"</span><span class=\"p\">):</span>\\n                <span class=\"n\">input_tensors</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">preprocess</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>\\n            <span class=\"k\">with</span> <span class=\"n\">tracer</span><span class=\"o\">.</span><span class=\"n\">start_as_current_span</span><span class=\"p\">(</span><span class=\"s2\">\"inference\"</span><span class=\"p\">):</span>\\n                <span class=\"n\">output_tensors</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">inference</span><span class=\"p\">(</span><span class=\"n\">input_tensors</span><span class=\"p\">)</span>\\n            <span class=\"k\">with</span> <span class=\"n\">tracer</span><span class=\"o\">.</span><span class=\"n\">start_as_current_span</span><span class=\"p\">(</span><span class=\"s2\">\"postprocess\"</span><span class=\"p\">):</span>\\n                <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">postprocess</span><span class=\"p\">(</span><span class=\"n\">output_tensors</span><span class=\"p\">)</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        The specification of\\n        <a class=\"reference external\" href=\"https://opentelemetry.io/docs/collector/\">\\n         OpenTelemetry Collector\\n        </a>\\n        can be done through the configuration of\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          FURIOSA_SERVING_OTLP_ENDPOINT\\n         </span>\\n        </code>\\n        , as shown below.\\nThe following diagram is an example that visualizes the tracing result with Grafana.\\n       </p>\\n       <a class=\"with-shadow reference internal image-reference\" href=\"../_images/jaeger_grafana.png\">\\n        <img alt=\"An example of visualization with Grafana\" class=\"with-shadow align-center\" src=\"../_images/jaeger_grafana.png\" style=\"width: 600px;\"/>\\n       </a>\\n       <p>\\n        Other major improvements are as follows:\\n       </p>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          Several inference requests can be executed at once, with serving API now supporting compiler setting\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            batch_size\\n           </span>\\n          </code>\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          More threads can share the NPU, with serving API now supporting session option\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            worker_num\\n           </span>\\n          </code>\\n         </p>\\n        </li>\\n       </ul>\\n      </section>\\n      <section id=\"profiler\">\\n       <h3>\\n        Profiler\\n        <a class=\"headerlink\" href=\"#profiler\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        You can now analyze the profiler tracing results with\\n        <a class=\"reference external\" href=\"https://pandas.pydata.org/\">\\n         Pandas\\n        </a>\\n        ,\\na data analysis framework. With this function, you can analyze the tracing result data,\\nallowing you to quickly identify bottlenecks and reasons for model performance changes.\\nMore detailed instructions can be found at\\n        <a class=\"reference internal\" href=\"../software/profiler.html#pandasprofilinganalysis\">\\n         <span class=\"std std-ref\">\\n          Trace analysis using Pandas DataFrame\\n         </span>\\n        </a>\\n        .\\n       </p>\\n       <div class=\"highlight-python notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime</span> <span class=\"kn\">import</span> <span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"n\">tensor</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime.profiler</span> <span class=\"kn\">import</span> <span class=\"n\">RecordFormat</span><span class=\"p\">,</span> <span class=\"n\">profile</span>\\n\\n<span class=\"k\">with</span> <span class=\"n\">profile</span><span class=\"p\">(</span><span class=\"nb\">format</span><span class=\"o\">=</span><span class=\"n\">RecordFormat</span><span class=\"o\">.</span><span class=\"n\">PandasDataFrame</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">profiler</span><span class=\"p\">:</span>\\n    <span class=\"k\">with</span> <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"s2\">\"MNISTnet_uint8_quant_without_softmax.tflite\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">sess</span><span class=\"p\">:</span>\\n        <span class=\"n\">input_shape</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">input</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\\n\\n        <span class=\"k\">with</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">record</span><span class=\"p\">(</span><span class=\"s2\">\"record\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">record</span><span class=\"p\">:</span>\\n            <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">):</span>\\n                <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">tensor</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"p\">))</span>\\n\\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">profiler</span><span class=\"o\">.</span><span class=\"n\">get_pandas_dataframe</span><span class=\"p\">()</span>\\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s2\">\"name\"</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"s2\">\"trace\"</span><span class=\"p\">][[</span><span class=\"s2\">\"trace_id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"name\"</span><span class=\"p\">,</span> <span class=\"s2\">\"thread.id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"dur\"</span><span class=\"p\">]])</span>\\n</pre>\\n        </div>\\n       </div>\\n      </section>\\n      <section id=\"quantization-tool\">\\n       <h3>\\n        Quantization tool\\n        <a class=\"headerlink\" href=\"#quantization-tool\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        <a class=\"reference internal\" href=\"../software/quantization.html#modelquantization\">\\n         <span class=\"std std-ref\">\\n          Model Quantization\\n         </span>\\n        </a>\\n        is a tool that converts pre-trained models to quantized models.\\nThis release includes the following major updates.\\n       </p>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          Accuracy improvement when processing SiLU operator\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Improved usability of compiler setting\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            without_quantize\\n           </span>\\n          </code>\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Accuracy improvement when processing MatMul/Gemm operators\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Accuracy improvement when processing Add/Sub/Mul/Div operators\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          NPU acceleration now added for more auto_pad properties, when processing Conv/ConvTranspose/MaxPool operators\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          NPU acceleration support for PRelu operator\\n         </p>\\n        </li>\\n       </ul>\\n      </section>\\n      <section id=\"furiosa-toolkit\">\\n       <h3>\\n        furiosa-toolkit\\n        <a class=\"headerlink\" href=\"#furiosa-toolkit\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        The\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosactl\\n         </span>\\n        </code>\\n        command line tool, which has been added to the\\nfuriosa-toolkit 0.10.0 release, includes the following improvements.\\n       </p>\\n       <p>\\n        The newly added\\n        <cite>\\n         furiosactl ps\\n        </cite>\\n        command allows you to print\\nthe OS processes which are occupying the NPU device.\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"c1\"># furiosactl ps</span>\\n<span class=\"o\">+-----------+--------+------------------------------------------------------------+</span>\\n<span class=\"o\">|</span> <span class=\"n\">NPU</span>       <span class=\"o\">|</span> <span class=\"n\">PID</span>    <span class=\"o\">|</span> <span class=\"n\">CMD</span>                                                        <span class=\"o\">|</span>\\n<span class=\"o\">+-----------+--------+------------------------------------------------------------+</span>\\n<span class=\"o\">|</span> <span class=\"n\">npu0pe0</span><span class=\"o\">-</span><span class=\"mi\">1</span> <span class=\"o\">|</span> <span class=\"mi\">132529</span> <span class=\"o\">|</span> <span class=\"o\">/</span><span class=\"n\">usr</span><span class=\"o\">/</span><span class=\"nb\">bin</span><span class=\"o\">/</span><span class=\"n\">python3</span> <span class=\"o\">/</span><span class=\"n\">usr</span><span class=\"o\">/</span><span class=\"n\">local</span><span class=\"o\">/</span><span class=\"nb\">bin</span><span class=\"o\">/</span><span class=\"n\">uvicorn</span> <span class=\"n\">image_classify</span><span class=\"p\">:</span><span class=\"n\">app</span> <span class=\"o\">|</span>\\n<span class=\"o\">+-----------+--------+------------------------------------------------------------+</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        The\\n        <cite>\\n         furiosactl info\\n        </cite>\\n        command now prints the unique UUID for each device.\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>$ furiosactl info\\n+------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+\\n| NPU  | Name   | UUID                                 | Firmware        | Temp. | Power  | PCI-BDF      | PCI-DEV |\\n+------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+\\n| npu0 | warboy | 72212674-61BE-4FCA-A2C9-555E4EE67AB5 | v1.1.0, 12180b0 |  49°C | 3.12 W | 0000:24:00.0 | 235:0   |\\n+------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+\\n| npu1 | warboy | DF80FB54-8190-44BC-B9FB-664FA36C754A | v1.1.0, 12180b0 |  54°C | 2.53 W | 0000:6d:00.0 | 511:0   |\\n+------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        Detailed instructions on installation and usage for\\n        <cite>\\n         furiosactl\\n        </cite>\\n        can be found in\\n        <a class=\"reference internal\" href=\"../software/cli.html#toolkit\">\\n         <span class=\"std std-ref\">\\n          furiosa-toolkit\\n         </span>\\n        </a>\\n        .\\n       </p>\\n      </section>\\n      <section id=\"model-zoo-api-improvements-added-models-and-added-native-post-processing-code\">\\n       <h3>\\n        Model Zoo API improvements, added models, and added native post-processing code\\n        <a class=\"headerlink\" href=\"#model-zoo-api-improvements-added-models-and-added-native-post-processing-code\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        <a class=\"reference external\" href=\"https://furiosa-ai.github.io/furiosa-models\">\\n         furioa-models\\n        </a>\\n        is a public Model Zoo project,\\nproviding FuriosaAI NPU-optimized models.\\nThe 0.8.0 release includes the following major updates.\\n       </p>\\n       <p>\\n        <strong>\\n         YOLOv5 Large/Medium models added\\n        </strong>\\n       </p>\\n       <p>\\n        Support for\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          YOLOv5l\\n         </span>\\n        </code>\\n        ,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          YOLOv5m\\n         </span>\\n        </code>\\n        , which are SOTA object detection models, have been added.\\nThe total list of available models can be found in\\n        <a class=\"reference external\" href=\"https://furiosa-ai.github.io/furiosa-models/v0.8.0/#model_list\">\\n         Model List\\n        </a>\\n        .\\n       </p>\\n       <p>\\n        <strong>\\n         Improvements to model class and loading API\\n        </strong>\\n       </p>\\n       <p>\\n        The model class has been improved to include pre and post-processing code, while the\\nmodel loading API has been improved as shown below.\\n       </p>\\n       <p>\\n        More explanation on model class and the API can be found at\\n        <a class=\"reference external\" href=\"https://furiosa-ai.github.io/furiosa-models/latest/model_object/\">\\n         Model Object\\n        </a>\\n        .\\n       </p>\\n       <div class=\"sphinx-tabs docutils container\">\\n        <div aria-label=\"Tabbed content\" role=\"tablist\">\\n         <button aria-controls=\"panel-0-0-0\" aria-selected=\"true\" class=\"sphinx-tabs-tab\" id=\"tab-0-0-0\" name=\"0-0\" role=\"tab\" tabindex=\"0\">\\n          Blocking API\\n         </button>\\n         <button aria-controls=\"panel-0-0-1\" aria-selected=\"false\" class=\"sphinx-tabs-tab\" id=\"tab-0-0-1\" name=\"0-1\" role=\"tab\" tabindex=\"-1\">\\n          Nonblocking API\\n         </button>\\n        </div>\\n        <div aria-labelledby=\"tab-0-0-0\" class=\"sphinx-tabs-panel\" id=\"panel-0-0-0\" name=\"0-0\" role=\"tabpanel\" tabindex=\"0\">\\n         <p>\\n          Before update\\n         </p>\\n         <div class=\"highlight-python notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">furiosa.models.vision</span> <span class=\"kn\">import</span> <span class=\"n\">MLCommonsResNet50</span>\\n\\n<span class=\"n\">resnet50</span> <span class=\"o\">=</span> <span class=\"n\">MLCommonsResNet50</span><span class=\"p\">()</span>\\n</pre>\\n          </div>\\n         </div>\\n         <p>\\n          Updated code\\n         </p>\\n         <div class=\"highlight-python notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">furiosa.models.vision</span> <span class=\"kn\">import</span> <span class=\"n\">ResNet50</span>\\n\\n<span class=\"n\">resnet50</span> <span class=\"o\">=</span> <span class=\"n\">ResNet50</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">()</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n        <div aria-labelledby=\"tab-0-0-1\" class=\"sphinx-tabs-panel\" hidden=\"true\" id=\"panel-0-0-1\" name=\"0-1\" role=\"tabpanel\" tabindex=\"0\">\\n         <p>\\n          Before update\\n         </p>\\n         <div class=\"highlight-python notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">asyncio</span>\\n\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.models.nonblocking.vision</span> <span class=\"kn\">import</span> <span class=\"n\">MLCommonsResNet50</span>\\n\\n<span class=\"n\">resnet50</span><span class=\"p\">:</span> <span class=\"n\">Model</span> <span class=\"o\">=</span> <span class=\"n\">asyncio</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">MLCommonsResNet50</span><span class=\"p\">())</span>\\n</pre>\\n          </div>\\n         </div>\\n         <p>\\n          0.8.0 improvements\\n         </p>\\n         <div class=\"highlight-python notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">asyncio</span>\\n\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.models.vision</span> <span class=\"kn\">import</span> <span class=\"n\">ResNet50</span>\\n\\n<span class=\"n\">resnet50</span><span class=\"p\">:</span> <span class=\"n\">Model</span> <span class=\"o\">=</span> <span class=\"n\">asyncio</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">ResNet50</span><span class=\"o\">.</span><span class=\"n\">load_async</span><span class=\"p\">())</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </div>\\n       <p>\\n        The model post-processing process converts the inference ouput tensor into structural\\ndata, which is more accessible for the application. Depending on the model, this\\nmay require a longer execution time.\\nThe 0.8.0 release includes native post-processing code for ResNet50, SSD-MobileNet,\\nand SSD-ResNet34. Based on internal benchmarks, native post-processing code can reduce\\nlatency by up to 70%, depending on the model.\\n       </p>\\n       <p>\\n        The following is a complete example of ResNet50, utilizing native post-processing code.\\nMore information can be found at\\n        <a class=\"reference external\" href=\"https://furiosa-ai.github.io/furiosa-models/v0.8.0/model_object/#prepostprocessing\">\\n         Pre/Postprocessing\\n        </a>\\n        .\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-python notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">furiosa.models.vision</span> <span class=\"kn\">import</span> <span class=\"n\">ResNet50</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.models.vision.resnet50</span> <span class=\"kn\">import</span> <span class=\"n\">NativePostProcessor</span><span class=\"p\">,</span> <span class=\"n\">preprocess</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.runtime</span> <span class=\"kn\">import</span> <span class=\"n\">session</span>\\n\\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ResNet50</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">()</span>\\n\\n<span class=\"n\">postprocessor</span> <span class=\"o\">=</span> <span class=\"n\">NativePostProcessor</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\\n<span class=\"k\">with</span> <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">sess</span><span class=\"p\">:</span>\\n    <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">preprocess</span><span class=\"p\">(</span><span class=\"s2\">\"tests/assets/cat.jpg\"</span><span class=\"p\">)</span>\\n    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">()</span>\\n    <span class=\"n\">postprocessor</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">)</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        Other changes and updates can be found at\\n        <a class=\"reference external\" href=\"https://furiosa-ai.github.io/furiosa-models/v0.8.0/changelog/\">\\n         Furiosa Model - 0.8.0 Changelogs\\n        </a>\\n        .\\n       </p>\\n      </section>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"0.9.0.html\" rel=\"prev\" title=\"Release Notes - 0.9.0\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"0.7.0.html\" rel=\"next\" title=\"Release Notes - 0.7.0\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='a643c5bc-d22e-493d-bee8-90e6b58fc9dd', link=Url('https://furiosa-ai.github.io/docs/latest/en/software/kubernetes_support.html'), name='kubernetes_support', parent='', child=[], description='\\n\\n\\n* Kubernetes Support\\n* [View page source](../_sources/software/kubernetes_support.rst.txt)\\n\\n---\\n\\n\\n\\nKubernetes Support\\n[\\uf0c1](#kubernetes-support \"Permalink to this heading\")\\n=======================================================================\\n\\n[Kuberentes](https://kubernetes.io/)\\nis an open source platform for managing containerized workloads and services.\\nFuriosa SDK provides the following components to support the Kubernetes environment.\\n\\n* FuriosaAI NPU Device Plugin (\\n  [Introduction to Kubernetes Device Plugin](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)\\n  )\\n* FuriosaAI NPU Feature Discovery (\\n  [Introduction to Node Feature Discovery](https://kubernetes-sigs.github.io/node-feature-discovery/stable/get-started/index.html)\\n  )\\n\\nThe two components above provide the following functions.\\n\\n* Make the Kubernetes cluster aware of the NPUs available to the node.\\n* Through Kubernetes\\n  `spec.containers[].resources.limits`\\n  , schedule the NPU simultaneously when distributing Pod workload.\\n* Identify NPU information of NPU-equipped machine, and register it as node label (you can selectively schedule Pods with this information and\\n  nodeSelector\\n  \\n  )\\n  \\n  + The node-feature-discovery needs to be installed to the cluster, and the\\n    `nfd-worker`\\n    Pod must be running in the nodes equipped with NPUs.\\n\\nThe setup process for Kubernetes support is as follows.\\n\\n1. Preparing NPU nodes\\n[\\uf0c1](#preparing-npu-nodes \"Permalink to this heading\")\\n----------------------------------------------------------------------------\\n\\nRequirements for Kubernetes nodes are as follows.\\n\\n* Ubuntu 20.04 or higher\\n* Intel compatible CPU\\n\\nYou also need to install NPU driver and toolkit on each node of NPU-equipped Kubernetes.\\nIf the APT server is set up (see\\n[APT server configuration](installation.html#setupaptrepository)\\n), you can easily install as follows.\\n\\n```\\napt-get update && apt install -y furiosa-driver-warboy furiosa-toolkit\\n\\n```\\n\\nOnce the required package is installed as above, you can check for NPU recognition as follows, with the\\n`furiosactl`\\ncommand included in furiosa-toolkit.\\nIf the NPU is not recognized with the command below, try again after rebooting - depending on the environment.\\n\\n```\\n$ furiosactl info\\n+------+------------------+-------+--------+--------------+---------+\\n| NPU  | Name             | Temp. | Power  | PCI-BDF      | PCI-DEV |\\n+------+------------------+-------+--------+--------------+---------+\\n| npu0 | FuriosaAI Warboy |  40°C | 1.37 W | 0000:01:00.0 | 509:0   |\\n+------+------------------+-------+--------+--------------+---------+\\n\\n```\\n\\n\\n\\n2. Installing Node Feature Discovery\\n[\\uf0c1](#installing-node-feature-discovery \"Permalink to this heading\")\\n--------------------------------------------------------------------------------------------------------\\n\\nIn order to make Kubernetes to recognize NPUs, you need to install Node Feature Discovery.\\nBy running the command as shown in the example below, if there is a node label that begins with\\n`feature.node.kubernetes.io/...`\\n, Node Feature Discovery’s DaemonSet has already been installed\\n\\n```\\n$ kubectl get no -o json | jq \\'.items[].metadata.labels\\'\\n{\\n  \"beta.kubernetes.io/arch\": \"amd64\",\\n  \"beta.kubernetes.io/os\": \"linux\",\\n  \"feature.node.kubernetes.io/cpu-cpuid.ADX\": \"true\",\\n  \"feature.node.kubernetes.io/cpu-cpuid.AESNI\": \"true\",\\n  ...\\n\\n```\\n\\n* If you do not have the Node Feature Discovery in your cluster, refer to the following document.\\n  \\n  > + [Quick start / Installation](https://kubernetes-sigs.github.io/node-feature-discovery/v0.11/get-started/quick-start.html#installation)\\n* The following options must be applied when executing Node Feature Discovery.\\n  \\n  + `beta.furiosa.ai`\\n    needs to be included in the\\n    `--extra-label-ns`\\n    option of\\n    `nfd-master`\\n  + In the config file of\\n    `nfd-worker`\\n    ,\\n    \\\\* Only\\n    `vendor`\\n    in the\\n    `sources.pci.deviceLabelFields`\\n    value\\n    \\\\*\\n    `\"12\"`\\n    must be included as a value in\\n    `sources.pci.deviceClassWhitelist`\\n\\nNote\\n\\nInstalling Node Feature Discovery is not mandatory, but is recommended. The next step\\nwill explain the additional tasks that must be performed if you are not using\\nNode Feature Discovery.\\n\\n\\n\\n3. Installing Device Plugin and NPU Feature Discovery\\n[\\uf0c1](#installing-device-plugin-and-npu-feature-discovery \"Permalink to this heading\")\\n------------------------------------------------------------------------------------------------------------------------------------------\\n\\nWhen the NPU node is ready, install Device Plugin and NPU Feature Discovery’s DaemonSet as follows.\\n\\n```\\nkubectl apply -f https://raw.githubusercontent.com/furiosa-ai/furiosa-sdk/v0.7.0/kubernetes/deployments/device-plugin.yaml\\nkubectl apply -f https://raw.githubusercontent.com/furiosa-ai/furiosa-sdk/v0.7.0/kubernetes/deployments/npu-feature-discovery.yaml\\n\\n```\\n\\nAfter executing the above command, you can check whether the installed daemonset is functioning normally with the\\n`kubectl\\n\\nget\\n\\ndaemonset\\n\\n-n\\n\\nkube-system`\\ncommand.\\nFor reference, the DaemonSet is distributed only to nodes equipped with NPUs, and uses\\n`alpha.furiosa.ai/npu.family=warboy`\\ninformation that the Node Feature Discovery (\\n`feature.node.kubernetes.io/pci-1ed2.present=true`\\n) attaches to each node.\\n\\n```\\n $ kubectl get daemonset -n kube-system\\nNAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                      AGE\\n furiosa-device-plugin          3         3         3       3            3           feature.node.kubernetes.io/pci-1ed2.present=true   128m\\n furiosa-npu-feature-discovery  3         3         3       3            3           feature.node.kubernetes.io/pci-1ed2.present=true   162m\\n\\n```\\n\\nThe metadata attached by the Node Feature Discovery is shown in the following table.\\n\\n\\nNPU Node Labels\\n\\n[\\uf0c1](#id1 \"Permalink to this table\")\\n\\n\\n\\n| Label | Value | Description |\\n| --- | --- | --- |\\n| beta.furiosa.ai/npu.count | 1 | The number of NPUs e x b number of NPUs attached to node |\\n| beta.furiosa.ai/npu.product | warboy, warboyB0 | NPU Product Name (Code) |\\n| beta.furiosa.ai/npu.family | warboy, renegade | NPU Architecture (Family) |\\n| beta.furiosa.ai/machine.vendor | (depends on machine) | Machine Manufacturer |\\n| beta.furiosa.ai/machine.name | (depends on machine) | The Nmae of Machine (Code) |\\n| beta.furiosa.ai/driver.version | 1.3.0 | NPU Device Driver Version |\\n| beta.furiosa.ai/driver.version.major | 1 | Major Version Number of NPU Device Driver Version |\\n| beta.furiosa.ai/driver.version.minor | 3 | Minor Version Number of NPU Device Driver |\\n| beta.furiosa.ai/driver.version.patch | 0 | Patch Version Number of NPU Device Driver |\\n| beta.furiosa.ai/driver.reference | 57ac7b0 | Build Commit Hash of NPU Device Driver |\\n\\nIf you want to check node labels, then execute the\\n`kubectl\\n\\nget\\n\\nnodes\\n\\n--show-labels`\\ncommand. If you see labels which start with\\n`beta.furiosa.ai`\\nNode Feature Discovery is successfully installed.\\n\\n```\\nkubectl get nodes --show-labels\\n\\nwarboy-node01     Ready   <none>  65d   v1.20.10   beta.furiosa.ai/npu.count=1,beta.furiosa.ai/npu.product=warboy...,kubernetes.io/os=linux\\nwarboy-node02     Ready   <none>  12d   v1.20.10   beta.furiosa.ai/npu.count=1,beta.furiosa.ai/npu.product=warboy...,kubernetes.io/os=linux\\n\\n```\\n\\n\\n### Device Plugin Configuration [\\uf0c1](#device-plugin-configuration \"Permalink to this heading\")\\n\\nExecution options for Device Plugin can be set by the argument of command line or configuration file.\\n\\n1. Command Line Arguments\\n\\nThe option can be set by the\\n`k8s-device-plugin`\\ncommand as follows.\\n\\n```\\n$ k8s-device-plugin --interval 10\\n\\n```\\n\\nFor the Pod or DaemonSet specification command line arguments can be set as follows.\\n\\n```\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: furiosa-device-plugin\\n  namespace: kube-system\\nspec:\\n  containers:\\n    - name: device-plugin\\n      image: ghcr.io/furiosa-ai/k8s-device-plugin:latest\\n      command: [\"/usr/bin/k8s-device-plugin\"]\\n      args: [\"--interval\", \"10\"]\\n# (the reset is omitted)\\n\\n```\\n\\n\\narguments of k8s-device-plugin\\n\\n[\\uf0c1](#id2 \"Permalink to this table\")\\n\\n\\n\\n| Item | Explanation | Default Value |\\n| --- | --- | --- |\\n| default-pe | default core type when pod is allocated (Fusion/Single) | Fusion |\\n| interval | interval for searching device (seconds) | 10 |\\n| disabled-devices | devices not for allocations (several devices can be designated using comma) |  |\\n| plugin-dir | directory path of kubelet device-plugin | /var/lib/kubelet/device-plugins |\\n| socket-name | file name of socket created under <plugin-dir> | furiosa-npu |\\n| resource-name | name of NPU resource registered for k8s node | beta.furiosa.ai/npu |\\n\\n2. Setting Configuration File\\n\\nYou may set configuration file by executing\\n`k8s-device-plugin`\\ncommand with argument\\n`config-file`\\n.\\nIf\\n`config-file`\\nis set then the other arguments are not permitted.\\n\\n```\\n$ k8s-device-plugin --config-file /etc/furiosa/device-plugin.conf\\n\\n```\\n\\n\\n/etc/furiosa/device-plugin.conf\\n\\n[\\uf0c1](#id3 \"Permalink to this code\")\\n\\n```\\ninterval: 10\\ndefaultPe: Fusion\\ndisabledDevices:             # device npu1 equipped in warboy-node01 will not be used\\n  - devName: npu1\\n    nodeName: warboy-node01\\npluginDir: /var/lib/kubelet/device-plugins\\nsocketName: furiosa-npu\\nresourceName: beta.furiosa.ai/npu\\n\\n```\\n\\n\\nConfiguration file is a text file with Yaml format. The modification of file contents is applied to Device Plugin immediately. Updated configuration is recorded on log of Device Plugin.\\n(but, modifications on\\n`pluginDir`\\n,\\n`socketName`\\n, or\\n`resourceName`\\nrequire reboot.)\\n\\n[3. Installing Device Plugin and NPU Feature Discovery](#installingdevicepluginandnfd)\\nprovides\\n`device-plugin.yaml`\\nwhich is default configuration file based on ConfigMap.\\nIf you want to modify execution options of Device Plugin, modify ConfigMap. Once modified ConfigMap is applied to Pod, Device Plugin reads the ConfigMap and then reflects modification.\\n\\n```\\n$ kubectl edit configmap npu-device-plugin -n kube-system\\n\\n```\\n\\n\\nconfigmap/npu-device-plugin\\n\\n[\\uf0c1](#id4 \"Permalink to this code\")\\n\\n```\\napiVersion: v1\\ndata:\\n  config.yaml: |\\n    defaultPe: Fusion\\n    interval: 15\\n    disabledDevices:\\n      - devName: npu2\\n        nodeName: npu-001\\nkind: ConfigMap\\n\\n```\\n\\n\\n\\n\\n\\n4. Creating a Pod with NPUs\\n[\\uf0c1](#creating-a-pod-with-npus \"Permalink to this heading\")\\n--------------------------------------------------------------------------------------\\n\\nTo allocate NPU to a Pod, add as shown below to\\n`spec.containers[].resources.limits`\\n.\\n\\n```\\nresources:\\n    limits:\\n        beta.furiosa.ai/npu: \"1\" # requesting 1 NPU\\n\\n```\\n\\n[Full example](https://github.com/furiosa-ai/furiosa-sdk/blob/v0.7.0/kubernetes/deployments/pod-example.yaml)\\nfor Pod creation is as follows.\\n\\n```\\n$ cat > npu-pod.yaml <<EOL\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: npu-pod\\nspec:\\n  containers:\\n    - name: npu-pod\\n      image: ubuntu:focal\\n      resources:\\n        limits:\\n          cpu: \"4\"\\n          memory: \"8Gi\"\\n          beta.furiosa.ai/npu: \"1\"\\n        requests:\\n          cpu: \"4\"\\n          memory: \"8Gi\"\\n          beta.furiosa.ai/npu: \"1\"\\nEOL\\n\\n$ kubectl apply -f npu-pod.yaml\\n\\n```\\n\\nAfter Pod creation, you can check NPU allocation as follows.\\n\\n```\\n$ kubectl get pods npu-pod -o yaml | grep alpha.furiosa.ai/npu\\n    beta.furiosa.ai/npu: \"1\"\\n    beta.furiosa.ai/npu: \"1\"\\n\\n```\\n\\nThe SDK application automatically recognizes the allocated NPU device.\\nIf there are multiple NPU devices on a node, you can check which device is allocated as follows:\\n\\n```\\n$ kubectl exec npu-pod -it -- /bin/bash\\nroot@npu-pod:/# echo $NPU_DEVNAME\\nnpu0pe0-1\\n\\n```\\n\\nIf furiosa-toolkit is installed in the Pod, you can check for more detailed device information using the\\nfuriosactl command as shown below.\\n\\nSee\\n[APT server configuration](installation.html#setupaptrepository)\\nfor installation guide using APT.\\n\\n```\\nroot@npu-pod:/# furiosactl\\nfuriosactl controls the FURIOSA NPU.\\n\\nFind more information at: https://furiosa.ai/\\n\\nBasic Commands:\\n    version    Print the furiosactl version information\\n    info       Show information one or many NPU(s)\\n    config     Get/Set configuration for NPU environment\\n\\nUsage:\\n    furiosactl COMMAND\\n\\nroot@npu-pod:/# furiosactl info\\n+------+------------------+-------+--------+--------------+---------+\\n| NPU  | Name             | Temp. | Power  | PCI-BDF      | PCI-DEV |\\n+------+------------------+-------+--------+--------------+---------+\\n| npu0 | FuriosaAI Warboy |  40°C | 1.37 W | 0000:01:00.0 | 509:0   |\\n+------+------------------+-------+--------+--------------+---------+\\n\\n```\\n\\n\\n\\n5. NPU monitoring\\n[\\uf0c1](#npu-monitoring \"Permalink to this heading\")\\n------------------------------------------------------------------\\n\\nIf you install\\n`npu-metrics-exporter`\\n, its daemon set and service will be created in your kubernetes cluster.\\nThe Pod that is executed through DaemonSet outputs various NPU status information that may be\\nuseful for monitoring. The data is expressed in Prometheus format. If Prometheus\\nis installed, and service discovery is active, Prometheus will automatically collect\\ndata through the Exporter.\\n\\nThe collected data may be reviewed with visualization tools such as Grafana.\\n\\n\\nnpu-metrics-exporter collection category list\\n\\n[\\uf0c1](#id5 \"Permalink to this table\")\\n\\n\\n| Name | Details |\\n| --- | --- |\\n| furiosa\\\\_npu\\\\_alive | NPU operation status (1:normal) |\\n| furiosa\\\\_npu\\\\_uptime | NPU operation time (s) |\\n| furiosa\\\\_npu\\\\_error | Number of detected NPU errors |\\n| furiosa\\\\_npu\\\\_hw\\\\_temperature | Temperature of each NPU components (°mC) |\\n| furiosa\\\\_npu\\\\_hw\\\\_power | NPU instantaneous power usage (µW) |\\n| furiosa\\\\_npu\\\\_hw\\\\_voltage | NPU instantaenous voltage (mV) |\\n| furiosa\\\\_npu\\\\_hw\\\\_current | NPU instantaneous current (mA) |\\n\\n\\n\\n\\n\\n[Previous](serving.html \"Model Server (Serving Framework)\")\\n[Next](vm_support.html \"Configuring Warboy Pass-through for Virtual Machine\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Kubernetes Support * [View page source](../_sources/software/kubernetes_support.rst.txt)\\n---\\nKubernetes Support [\\uf0c1](#kubernetes-support \"Permalink to this heading\") =======================================================================\\n[Kuberentes](https://kubernetes.io/) is an open source platform for managing containerized workloads and services. Furiosa SDK provides the following components to support the Kubernetes environment.\\n* FuriosaAI NPU Device Plugin (   [Introduction to Kubernetes Device Plugin](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)   ) * FuriosaAI NPU Feature Discovery (   [Introduction to Node Feature Discovery](https://kubernetes-sigs.github.io/node-feature-discovery/stable/get-started/index.html)   )\\nThe two components above provide the following functions.\\n* Make the Kubernetes cluster aware of the NPUs available to the node. * Through Kubernetes   `spec.containers[].resources.limits`   , schedule the NPU simultaneously when distributing Pod workload. * Identify NPU information of NPU-equipped machine, and register it as node label (you can selectively schedule Pods with this information and   nodeSelector      )      + The node-feature-discovery needs to be installed to the cluster, and the     `nfd-worker`     Pod must be running in the nodes equipped with NPUs.\\nThe setup process for Kubernetes support is as follows.\\n1. Preparing NPU nodes [\\uf0c1](#preparing-npu-nodes \"Permalink to this heading\") ----------------------------------------------------------------------------\\nRequirements for Kubernetes nodes are as follows.\\n* Ubuntu 20.04 or higher * Intel compatible CPU\\nYou also need to install NPU driver and toolkit on each node of NPU-equipped Kubernetes. If the APT server is set up (see [APT server configuration](installation.html#setupaptrepository) ), you can easily install as follows.\\n``` apt-get update && apt install -y furiosa-driver-warboy furiosa-toolkit\\n```\\nOnce the required package is installed as above, you can check for NPU recognition as follows, with the `furiosactl` command included in furiosa-toolkit. If the NPU is not recognized with the command below, try again after rebooting - depending on the environment.\\n``` $ furiosactl info +------+------------------+-------+--------+--------------+---------+ | NPU  | Name             | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+------------------+-------+--------+--------------+---------+ | npu0 | FuriosaAI Warboy |  40°C | 1.37 W | 0000:01:00.0 | 509:0   | +------+------------------+-------+--------+--------------+---------+\\n```\\n2. Installing Node Feature Discovery [\\uf0c1](#installing-node-feature-discovery \"Permalink to this heading\") --------------------------------------------------------------------------------------------------------\\nIn order to make Kubernetes to recognize NPUs, you need to install Node Feature Discovery. By running the command as shown in the example below, if there is a node label that begins with `feature.node.kubernetes.io/...` , Node Feature Discovery’s DaemonSet has already been installed\\n``` $ kubectl get no -o json | jq \\'.items[].metadata.labels\\' {\\n  \"beta.kubernetes.io/arch\": \"amd64\",   \"beta.kubernetes.io/os\": \"linux\",   \"feature.node.kubernetes.io/cpu-cpuid.ADX\": \"true\",   \"feature.node.kubernetes.io/cpu-cpuid.AESNI\": \"true\",   ...\\n```\\n* If you do not have the Node Feature Discovery in your cluster, refer to the following document.      > + [Quick start / Installation](https://kubernetes-sigs.github.io/node-feature-discovery/v0.11/get-started/quick-start.html#installation) * The following options must be applied when executing Node Feature Discovery.      + `beta.furiosa.ai`     needs to be included in the     `--extra-label-ns`     option of     `nfd-master`   + In the config file of     `nfd-worker`     ,     \\\\* Only     `vendor`     in the     `sources.pci.deviceLabelFields`     value     \\\\*     `\"12\"`     must be included as a value in     `sources.pci.deviceClassWhitelist`  Note\\nInstalling Node Feature Discovery is not mandatory, but is recommended. The next step will explain the additional tasks that must be performed if you are not using Node Feature Discovery.\\n3. Installing Device Plugin and NPU Feature Discovery [\\uf0c1](#installing-device-plugin-and-npu-feature-discovery \"Permalink to this heading\") ------------------------------------------------------------------------------------------------------------------------------------------\\nWhen the NPU node is ready, install Device Plugin and NPU Feature Discovery’s DaemonSet as follows.\\n``` kubectl apply -f https://raw.githubusercontent.com/furiosa-ai/furiosa-sdk/v0.7.0/kubernetes/deployments/device-plugin.yaml kubectl apply -f https://raw.githubusercontent.com/furiosa-ai/furiosa-sdk/v0.7.0/kubernetes/deployments/npu-feature-discovery.yaml\\n```\\nAfter executing the above command, you can check whether the installed daemonset is functioning normally with the `kubectl\\nget\\ndaemonset\\n-n\\nkube-system` command. For reference, the DaemonSet is distributed only to nodes equipped with NPUs, and uses `alpha.furiosa.ai/npu.family=warboy` information that the Node Feature Discovery ( `feature.node.kubernetes.io/pci-1ed2.present=true` ) attaches to each node.\\n```  $ kubectl get daemonset -n kube-system NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                      AGE  furiosa-device-plugin          3         3         3       3            3           feature.node.kubernetes.io/pci-1ed2.present=true   128m  furiosa-npu-feature-discovery  3         3         3       3            3           feature.node.kubernetes.io/pci-1ed2.present=true   162m\\n```\\nThe metadata attached by the Node Feature Discovery is shown in the following table.\\nNPU Node Labels\\n[\\uf0c1](#id1 \"Permalink to this table\")\\n| Label | Value | Description | | --- | --- | --- | | beta.furiosa.ai/npu.count | 1 | The number of NPUs e x b number of NPUs attached to node | | beta.furiosa.ai/npu.product | warboy, warboyB0 | NPU Product Name (Code) | | beta.furiosa.ai/npu.family | warboy, renegade | NPU Architecture (Family) | | beta.furiosa.ai/machine.vendor | (depends on machine) | Machine Manufacturer | | beta.furiosa.ai/machine.name | (depends on machine) | The Nmae of Machine (Code) | | beta.furiosa.ai/driver.version | 1.3.0 | NPU Device Driver Version | | beta.furiosa.ai/driver.version.major | 1 | Major Version Number of NPU Device Driver Version | | beta.furiosa.ai/driver.version.minor | 3 | Minor Version Number of NPU Device Driver | | beta.furiosa.ai/driver.version.patch | 0 | Patch Version Number of NPU Device Driver | | beta.furiosa.ai/driver.reference | 57ac7b0 | Build Commit Hash of NPU Device Driver |\\nIf you want to check node labels, then execute the `kubectl\\nget\\nnodes\\n--show-labels` command. If you see labels which start with `beta.furiosa.ai` Node Feature Discovery is successfully installed.\\n``` kubectl get nodes --show-labels\\nwarboy-node01     Ready   <none>  65d   v1.20.10   beta.furiosa.ai/npu.count=1,beta.furiosa.ai/npu.product=warboy...,kubernetes.io/os=linux warboy-node02     Ready   <none>  12d   v1.20.10   beta.furiosa.ai/npu.count=1,beta.furiosa.ai/npu.product=warboy...,kubernetes.io/os=linux\\n```\\n### Device Plugin Configuration [\\uf0c1](#device-plugin-configuration \"Permalink to this heading\")\\nExecution options for Device Plugin can be set by the argument of command line or configuration file.\\n1. Command Line Arguments\\nThe option can be set by the `k8s-device-plugin` command as follows.\\n``` $ k8s-device-plugin --interval 10\\n```\\nFor the Pod or DaemonSet specification command line arguments can be set as follows.\\n``` apiVersion: v1 kind: Pod metadata:   name: furiosa-device-plugin   namespace: kube-system spec:   containers:     - name: device-plugin       image: ghcr.io/furiosa-ai/k8s-device-plugin:latest       command: [\"/usr/bin/k8s-device-plugin\"]       args: [\"--interval\", \"10\"] # (the reset is omitted)\\n```\\narguments of k8s-device-plugin\\n[\\uf0c1](#id2 \"Permalink to this table\")\\n| Item | Explanation | Default Value | | --- | --- | --- | | default-pe | default core type when pod is allocated (Fusion/Single) | Fusion | | interval | interval for searching device (seconds) | 10 | | disabled-devices | devices not for allocations (several devices can be designated using comma) |  | | plugin-dir | directory path of kubelet device-plugin | /var/lib/kubelet/device-plugins | | socket-name | file name of socket created under <plugin-dir> | furiosa-npu | | resource-name | name of NPU resource registered for k8s node | beta.furiosa.ai/npu |\\n2. Setting Configuration File\\nYou may set configuration file by executing `k8s-device-plugin` command with argument `config-file` . If `config-file` is set then the other arguments are not permitted.\\n``` $ k8s-device-plugin --config-file /etc/furiosa/device-plugin.conf\\n```\\n/etc/furiosa/device-plugin.conf\\n[\\uf0c1](#id3 \"Permalink to this code\")\\n``` interval: 10 defaultPe: Fusion disabledDevices:             # device npu1 equipped in warboy-node01 will not be used   - devName: npu1     nodeName: warboy-node01 pluginDir: /var/lib/kubelet/device-plugins socketName: furiosa-npu resourceName: beta.furiosa.ai/npu\\n```\\nConfiguration file is a text file with Yaml format. The modification of file contents is applied to Device Plugin immediately. Updated configuration is recorded on log of Device Plugin. (but, modifications on `pluginDir` , `socketName` , or `resourceName` require reboot.)\\n[3. Installing Device Plugin and NPU Feature Discovery](#installingdevicepluginandnfd) provides `device-plugin.yaml` which is default configuration file based on ConfigMap. If you want to modify execution options of Device Plugin, modify ConfigMap. Once modified ConfigMap is applied to Pod, Device Plugin reads the ConfigMap and then reflects modification.\\n``` $ kubectl edit configmap npu-device-plugin -n kube-system\\n```\\nconfigmap/npu-device-plugin\\n[\\uf0c1](#id4 \"Permalink to this code\")\\n``` apiVersion: v1 data:   config.yaml: |     defaultPe: Fusion     interval: 15     disabledDevices:       - devName: npu2         nodeName: npu-001 kind: ConfigMap\\n```\\n4. Creating a Pod with NPUs [\\uf0c1](#creating-a-pod-with-npus \"Permalink to this heading\") --------------------------------------------------------------------------------------\\nTo allocate NPU to a Pod, add as shown below to `spec.containers[].resources.limits` .\\n``` resources:     limits:         beta.furiosa.ai/npu: \"1\" # requesting 1 NPU\\n```\\n[Full example](https://github.com/furiosa-ai/furiosa-sdk/blob/v0.7.0/kubernetes/deployments/pod-example.yaml) for Pod creation is as follows.\\n``` $ cat > npu-pod.yaml <<EOL apiVersion: v1 kind: Pod metadata:   name: npu-pod spec:   containers:     - name: npu-pod       image: ubuntu:focal       resources:         limits:           cpu: \"4\"           memory: \"8Gi\"           beta.furiosa.ai/npu: \"1\"         requests:           cpu: \"4\"           memory: \"8Gi\"           beta.furiosa.ai/npu: \"1\" EOL\\n$ kubectl apply -f npu-pod.yaml\\n```\\nAfter Pod creation, you can check NPU allocation as follows.\\n``` $ kubectl get pods npu-pod -o yaml | grep alpha.furiosa.ai/npu     beta.furiosa.ai/npu: \"1\"     beta.furiosa.ai/npu: \"1\"\\n```\\nThe SDK application automatically recognizes the allocated NPU device. If there are multiple NPU devices on a node, you can check which device is allocated as follows:\\n``` $ kubectl exec npu-pod -it -- /bin/bash root@npu-pod:/# echo $NPU_DEVNAME npu0pe0-1\\n```\\nIf furiosa-toolkit is installed in the Pod, you can check for more detailed device information using the furiosactl command as shown below.\\nSee [APT server configuration](installation.html#setupaptrepository) for installation guide using APT.\\n``` root@npu-pod:/# furiosactl furiosactl controls the FURIOSA NPU.\\nFind more information at: https://furiosa.ai/\\nBasic Commands:     version    Print the furiosactl version information     info       Show information one or many NPU(s)     config     Get/Set configuration for NPU environment\\nUsage:     furiosactl COMMAND\\nroot@npu-pod:/# furiosactl info +------+------------------+-------+--------+--------------+---------+ | NPU  | Name             | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+------------------+-------+--------+--------------+---------+ | npu0 | FuriosaAI Warboy |  40°C | 1.37 W | 0000:01:00.0 | 509:0   | +------+------------------+-------+--------+--------------+---------+\\n```\\n5. NPU monitoring [\\uf0c1](#npu-monitoring \"Permalink to this heading\") ------------------------------------------------------------------\\nIf you install `npu-metrics-exporter` , its daemon set and service will be created in your kubernetes cluster. The Pod that is executed through DaemonSet outputs various NPU status information that may be useful for monitoring. The data is expressed in Prometheus format. If Prometheus is installed, and service discovery is active, Prometheus will automatically collect data through the Exporter.\\nThe collected data may be reviewed with visualization tools such as Grafana.\\nnpu-metrics-exporter collection category list\\n[\\uf0c1](#id5 \"Permalink to this table\")\\n| Name | Details | | --- | --- | | furiosa\\\\_npu\\\\_alive | NPU operation status (1:normal) | | furiosa\\\\_npu\\\\_uptime | NPU operation time (s) | | furiosa\\\\_npu\\\\_error | Number of detected NPU errors | | furiosa\\\\_npu\\\\_hw\\\\_temperature | Temperature of each NPU components (°mC) | | furiosa\\\\_npu\\\\_hw\\\\_power | NPU instantaneous power usage (µW) | | furiosa\\\\_npu\\\\_hw\\\\_voltage | NPU instantaenous voltage (mV) | | furiosa\\\\_npu\\\\_hw\\\\_current | NPU instantaneous current (mA) |\\n[Previous](serving.html \"Model Server (Serving Framework)\") [Next](vm_support.html \"Configuring Warboy Pass-through for Virtual Machine\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Kubernetes Support\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/kubernetes_support.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"kubernetes-support\">\\n     <span id=\"kubernetesintegration\">\\n     </span>\\n     <h1>\\n      Kubernetes Support\\n      <a class=\"headerlink\" href=\"#kubernetes-support\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      <a class=\"reference external\" href=\"https://kubernetes.io/\">\\n       Kuberentes\\n      </a>\\n      is an open source platform for managing containerized workloads and services.\\nFuriosa SDK provides the following components to support the Kubernetes environment.\\n     </p>\\n     <ul class=\"simple\">\\n      <li>\\n       <p>\\n        FuriosaAI NPU Device Plugin (\\n        <a class=\"reference external\" href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/\">\\n         Introduction to Kubernetes Device Plugin\\n        </a>\\n        )\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        FuriosaAI NPU Feature Discovery (\\n        <a class=\"reference external\" href=\"https://kubernetes-sigs.github.io/node-feature-discovery/stable/get-started/index.html\">\\n         Introduction to Node Feature Discovery\\n        </a>\\n        )\\n       </p>\\n      </li>\\n     </ul>\\n     <p>\\n      The two components above provide the following functions.\\n     </p>\\n     <ul class=\"simple\">\\n      <li>\\n       <p>\\n        Make the Kubernetes cluster aware of the NPUs available to the node.\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Through Kubernetes\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          spec.containers[].resources.limits\\n         </span>\\n        </code>\\n        , schedule the NPU simultaneously when distributing Pod workload.\\n       </p>\\n      </li>\\n      <li>\\n       <dl class=\"simple\">\\n        <dt>\\n         Identify NPU information of NPU-equipped machine, and register it as node label (you can selectively schedule Pods with this information and\\n         <cite>\\n          nodeSelector\\n         </cite>\\n         )\\n        </dt>\\n        <dd>\\n         <ul>\\n          <li>\\n           <p>\\n            The node-feature-discovery needs to be installed to the cluster, and the\\n            <code class=\"docutils literal notranslate\">\\n             <span class=\"pre\">\\n              nfd-worker\\n             </span>\\n            </code>\\n            Pod must be running in the nodes equipped with NPUs.\\n           </p>\\n          </li>\\n         </ul>\\n        </dd>\\n       </dl>\\n      </li>\\n     </ul>\\n     <p>\\n      The setup process for Kubernetes support is as follows.\\n     </p>\\n     <section id=\"preparing-npu-nodes\">\\n      <h2>\\n       1. Preparing NPU nodes\\n       <a class=\"headerlink\" href=\"#preparing-npu-nodes\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       Requirements for Kubernetes nodes are as follows.\\n      </p>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         Ubuntu 20.04 or higher\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Intel compatible CPU\\n        </p>\\n       </li>\\n      </ul>\\n      <p>\\n       You also need to install NPU driver and toolkit on each node of NPU-equipped Kubernetes.\\nIf the APT server is set up (see\\n       <a class=\"reference internal\" href=\"installation.html#setupaptrepository\">\\n        <span class=\"std std-ref\">\\n         APT server configuration\\n        </span>\\n       </a>\\n       ), you can easily install as follows.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>apt-get<span class=\"w\"> </span>update<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span>apt<span class=\"w\"> </span>install<span class=\"w\"> </span>-y<span class=\"w\"> </span>furiosa-driver-warboy<span class=\"w\"> </span>furiosa-toolkit\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       Once the required package is installed as above, you can check for NPU recognition as follows, with the\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         furiosactl\\n        </span>\\n       </code>\\n       command included in furiosa-toolkit.\\nIf the NPU is not recognized with the command below, try again after rebooting - depending on the environment.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span>furiosactl<span class=\"w\"> </span>info\\n+------+------------------+-------+--------+--------------+---------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>NPU<span class=\"w\">  </span><span class=\"p\">|</span><span class=\"w\"> </span>Name<span class=\"w\">             </span><span class=\"p\">|</span><span class=\"w\"> </span>Temp.<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>Power<span class=\"w\">  </span><span class=\"p\">|</span><span class=\"w\"> </span>PCI-BDF<span class=\"w\">      </span><span class=\"p\">|</span><span class=\"w\"> </span>PCI-DEV<span class=\"w\"> </span><span class=\"p\">|</span>\\n+------+------------------+-------+--------+--------------+---------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>npu0<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>FuriosaAI<span class=\"w\"> </span>Warboy<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\">  </span><span class=\"m\">40</span>°C<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">1</span>.37<span class=\"w\"> </span>W<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0000</span>:01:00.0<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">509</span>:0<span class=\"w\">   </span><span class=\"p\">|</span>\\n+------+------------------+-------+--------+--------------+---------+\\n</pre>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"installing-node-feature-discovery\">\\n      <h2>\\n       2. Installing Node Feature Discovery\\n       <a class=\"headerlink\" href=\"#installing-node-feature-discovery\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       In order to make Kubernetes to recognize NPUs, you need to install Node Feature Discovery.\\nBy running the command as shown in the example below, if there is a node label that begins with\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         feature.node.kubernetes.io/...\\n        </span>\\n       </code>\\n       , Node Feature Discovery’s DaemonSet has already been installed\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span>kubectl<span class=\"w\"> </span>get<span class=\"w\"> </span>no<span class=\"w\"> </span>-o<span class=\"w\"> </span>json<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>jq<span class=\"w\"> </span><span class=\"s1\">\\'.items[].metadata.labels\\'</span>\\n<span class=\"o\">{</span>\\n<span class=\"w\">  </span><span class=\"s2\">\"beta.kubernetes.io/arch\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"amd64\"</span>,\\n<span class=\"w\">  </span><span class=\"s2\">\"beta.kubernetes.io/os\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"linux\"</span>,\\n<span class=\"w\">  </span><span class=\"s2\">\"feature.node.kubernetes.io/cpu-cpuid.ADX\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"true\"</span>,\\n<span class=\"w\">  </span><span class=\"s2\">\"feature.node.kubernetes.io/cpu-cpuid.AESNI\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"true\"</span>,\\n<span class=\"w\">  </span>...\\n</pre>\\n       </div>\\n      </div>\\n      <ul>\\n       <li>\\n        <p>\\n         If you do not have the Node Feature Discovery in your cluster, refer to the following document.\\n        </p>\\n        <blockquote>\\n         <div>\\n          <ul class=\"simple\">\\n           <li>\\n            <p>\\n             <a class=\"reference external\" href=\"https://kubernetes-sigs.github.io/node-feature-discovery/v0.11/get-started/quick-start.html#installation\">\\n              Quick start / Installation\\n             </a>\\n            </p>\\n           </li>\\n          </ul>\\n         </div>\\n        </blockquote>\\n       </li>\\n       <li>\\n        <p>\\n         The following options must be applied when executing Node Feature Discovery.\\n        </p>\\n        <ul class=\"simple\">\\n         <li>\\n          <p>\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             beta.furiosa.ai\\n            </span>\\n           </code>\\n           needs to be included in the\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             --extra-label-ns\\n            </span>\\n           </code>\\n           option of\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             nfd-master\\n            </span>\\n           </code>\\n          </p>\\n         </li>\\n         <li>\\n          <p>\\n           In the config file of\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             nfd-worker\\n            </span>\\n           </code>\\n           ,\\n* Only\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             vendor\\n            </span>\\n           </code>\\n           in the\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             sources.pci.deviceLabelFields\\n            </span>\\n           </code>\\n           value\\n*\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             \"12\"\\n            </span>\\n           </code>\\n           must be included as a value in\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             sources.pci.deviceClassWhitelist\\n            </span>\\n           </code>\\n          </p>\\n         </li>\\n        </ul>\\n       </li>\\n      </ul>\\n      <div class=\"admonition note\">\\n       <p class=\"admonition-title\">\\n        Note\\n       </p>\\n       <p>\\n        Installing Node Feature Discovery is not mandatory, but is recommended. The next step\\nwill explain the additional tasks that must be performed if you are not using\\nNode Feature Discovery.\\n       </p>\\n      </div>\\n     </section>\\n     <section id=\"installing-device-plugin-and-npu-feature-discovery\">\\n      <span id=\"installingdevicepluginandnfd\">\\n      </span>\\n      <h2>\\n       3. Installing Device Plugin and NPU Feature Discovery\\n       <a class=\"headerlink\" href=\"#installing-device-plugin-and-npu-feature-discovery\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       When the NPU node is ready, install Device Plugin and NPU Feature Discovery’s DaemonSet as follows.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>kubectl<span class=\"w\"> </span>apply<span class=\"w\"> </span>-f<span class=\"w\"> </span>https://raw.githubusercontent.com/furiosa-ai/furiosa-sdk/v0.7.0/kubernetes/deployments/device-plugin.yaml\\nkubectl<span class=\"w\"> </span>apply<span class=\"w\"> </span>-f<span class=\"w\"> </span>https://raw.githubusercontent.com/furiosa-ai/furiosa-sdk/v0.7.0/kubernetes/deployments/npu-feature-discovery.yaml\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       After executing the above command, you can check whether the installed daemonset is functioning normally with the\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         kubectl\\n        </span>\\n        <span class=\"pre\">\\n         get\\n        </span>\\n        <span class=\"pre\">\\n         daemonset\\n        </span>\\n        <span class=\"pre\">\\n         -n\\n        </span>\\n        <span class=\"pre\">\\n         kube-system\\n        </span>\\n       </code>\\n       command.\\nFor reference, the DaemonSet is distributed only to nodes equipped with NPUs, and uses\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         alpha.furiosa.ai/npu.family=warboy\\n        </span>\\n       </code>\\n       information that the Node Feature Discovery (\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         feature.node.kubernetes.io/pci-1ed2.present=true\\n        </span>\\n       </code>\\n       ) attaches to each node.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"w\"> </span>$<span class=\"w\"> </span>kubectl<span class=\"w\"> </span>get<span class=\"w\"> </span>daemonset<span class=\"w\"> </span>-n<span class=\"w\"> </span>kube-system\\nNAME<span class=\"w\">                           </span>DESIRED<span class=\"w\">   </span>CURRENT<span class=\"w\">   </span>READY<span class=\"w\">   </span>UP-TO-DATE<span class=\"w\">   </span>AVAILABLE<span class=\"w\">   </span>NODE<span class=\"w\"> </span>SELECTOR<span class=\"w\">                                      </span>AGE\\n<span class=\"w\"> </span>furiosa-device-plugin<span class=\"w\">          </span><span class=\"m\">3</span><span class=\"w\">         </span><span class=\"m\">3</span><span class=\"w\">         </span><span class=\"m\">3</span><span class=\"w\">       </span><span class=\"m\">3</span><span class=\"w\">            </span><span class=\"m\">3</span><span class=\"w\">           </span>feature.node.kubernetes.io/pci-1ed2.present<span class=\"o\">=</span><span class=\"nb\">true</span><span class=\"w\">   </span>128m\\n<span class=\"w\"> </span>furiosa-npu-feature-discovery<span class=\"w\">  </span><span class=\"m\">3</span><span class=\"w\">         </span><span class=\"m\">3</span><span class=\"w\">         </span><span class=\"m\">3</span><span class=\"w\">       </span><span class=\"m\">3</span><span class=\"w\">            </span><span class=\"m\">3</span><span class=\"w\">           </span>feature.node.kubernetes.io/pci-1ed2.present<span class=\"o\">=</span><span class=\"nb\">true</span><span class=\"w\">   </span>162m\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       The metadata attached by the Node Feature Discovery is shown in the following table.\\n      </p>\\n      <span id=\"k8snodelabels\">\\n      </span>\\n      <table class=\"docutils align-default\" id=\"id1\">\\n       <caption>\\n        <span class=\"caption-text\">\\n         NPU Node Labels\\n        </span>\\n        <a class=\"headerlink\" href=\"#id1\" title=\"Permalink to this table\">\\n         \\uf0c1\\n        </a>\\n       </caption>\\n       <colgroup>\\n        <col style=\"width: 33%\"/>\\n        <col style=\"width: 33%\"/>\\n        <col style=\"width: 33%\"/>\\n       </colgroup>\\n       <thead>\\n        <tr class=\"row-odd\">\\n         <th class=\"head\">\\n          <p>\\n           Label\\n          </p>\\n         </th>\\n         <th class=\"head\">\\n          <p>\\n           Value\\n          </p>\\n         </th>\\n         <th class=\"head\">\\n          <p>\\n           Description\\n          </p>\\n         </th>\\n        </tr>\\n       </thead>\\n       <tbody>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           beta.furiosa.ai/npu.count\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           1\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           The number of NPUs e x b number of NPUs attached to node\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           beta.furiosa.ai/npu.product\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           warboy, warboyB0\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           NPU Product Name (Code)\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           beta.furiosa.ai/npu.family\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           warboy, renegade\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           NPU Architecture (Family)\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           beta.furiosa.ai/machine.vendor\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           (depends on machine)\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Machine Manufacturer\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           beta.furiosa.ai/machine.name\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           (depends on machine)\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           The Nmae of Machine (Code)\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           beta.furiosa.ai/driver.version\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           1.3.0\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           NPU Device Driver Version\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           beta.furiosa.ai/driver.version.major\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           1\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Major Version Number of NPU Device Driver Version\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           beta.furiosa.ai/driver.version.minor\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           3\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Minor Version Number of NPU Device Driver\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           beta.furiosa.ai/driver.version.patch\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           0\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Patch Version Number of NPU Device Driver\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           beta.furiosa.ai/driver.reference\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           57ac7b0\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Build Commit Hash of NPU Device Driver\\n          </p>\\n         </td>\\n        </tr>\\n       </tbody>\\n      </table>\\n      <p>\\n       If you want to check node labels, then execute the\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         kubectl\\n        </span>\\n        <span class=\"pre\">\\n         get\\n        </span>\\n        <span class=\"pre\">\\n         nodes\\n        </span>\\n        <span class=\"pre\">\\n         --show-labels\\n        </span>\\n       </code>\\n       command. If you see labels which start with\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         beta.furiosa.ai\\n        </span>\\n       </code>\\n       Node Feature Discovery is successfully installed.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>kubectl<span class=\"w\"> </span>get<span class=\"w\"> </span>nodes<span class=\"w\"> </span>--show-labels\\n\\nwarboy-node01<span class=\"w\">     </span>Ready<span class=\"w\">   </span>&lt;none&gt;<span class=\"w\">  </span>65d<span class=\"w\">   </span>v1.20.10<span class=\"w\">   </span>beta.furiosa.ai/npu.count<span class=\"o\">=</span><span class=\"m\">1</span>,beta.furiosa.ai/npu.product<span class=\"o\">=</span>warboy...,kubernetes.io/os<span class=\"o\">=</span>linux\\nwarboy-node02<span class=\"w\">     </span>Ready<span class=\"w\">   </span>&lt;none&gt;<span class=\"w\">  </span>12d<span class=\"w\">   </span>v1.20.10<span class=\"w\">   </span>beta.furiosa.ai/npu.count<span class=\"o\">=</span><span class=\"m\">1</span>,beta.furiosa.ai/npu.product<span class=\"o\">=</span>warboy...,kubernetes.io/os<span class=\"o\">=</span>linux\\n</pre>\\n       </div>\\n      </div>\\n      <section id=\"device-plugin-configuration\">\\n       <h3>\\n        Device Plugin Configuration\\n        <a class=\"headerlink\" href=\"#device-plugin-configuration\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Execution options for Device Plugin can be set by the argument of command line or configuration file.\\n       </p>\\n       <ol class=\"arabic simple\">\\n        <li>\\n         <p>\\n          Command Line Arguments\\n         </p>\\n        </li>\\n       </ol>\\n       <p>\\n        The option can be set by the\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          k8s-device-plugin\\n         </span>\\n        </code>\\n        command as follows.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>$<span class=\"w\"> </span>k8s-device-plugin<span class=\"w\"> </span>--interval<span class=\"w\"> </span><span class=\"m\">10</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        For the Pod or DaemonSet specification command line arguments can be set as follows.\\n       </p>\\n       <div class=\"highlight-yaml notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"nt\">apiVersion</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">v1</span>\\n<span class=\"nt\">kind</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">Pod</span>\\n<span class=\"nt\">metadata</span><span class=\"p\">:</span>\\n<span class=\"w\">  </span><span class=\"nt\">name</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">furiosa-device-plugin</span>\\n<span class=\"w\">  </span><span class=\"nt\">namespace</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">kube-system</span>\\n<span class=\"nt\">spec</span><span class=\"p\">:</span>\\n<span class=\"w\">  </span><span class=\"nt\">containers</span><span class=\"p\">:</span>\\n<span class=\"w\">    </span><span class=\"p p-Indicator\">-</span><span class=\"w\"> </span><span class=\"nt\">name</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">device-plugin</span>\\n<span class=\"w\">      </span><span class=\"nt\">image</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">ghcr.io/furiosa-ai/k8s-device-plugin:latest</span>\\n<span class=\"w\">      </span><span class=\"nt\">command</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p p-Indicator\">[</span><span class=\"s\">\"/usr/bin/k8s-device-plugin\"</span><span class=\"p p-Indicator\">]</span>\\n<span class=\"w\">      </span><span class=\"nt\">args</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p p-Indicator\">[</span><span class=\"s\">\"--interval\"</span><span class=\"p p-Indicator\">,</span><span class=\"w\"> </span><span class=\"s\">\"10\"</span><span class=\"p p-Indicator\">]</span>\\n<span class=\"c1\"># (the reset is omitted)</span>\\n</pre>\\n        </div>\\n       </div>\\n       <table class=\"docutils align-default\" id=\"id2\">\\n        <caption>\\n         <span class=\"caption-text\">\\n          arguments of k8s-device-plugin\\n         </span>\\n         <a class=\"headerlink\" href=\"#id2\" title=\"Permalink to this table\">\\n          \\uf0c1\\n         </a>\\n        </caption>\\n        <colgroup>\\n         <col style=\"width: 20%\"/>\\n         <col style=\"width: 60%\"/>\\n         <col style=\"width: 20%\"/>\\n        </colgroup>\\n        <thead>\\n         <tr class=\"row-odd\">\\n          <th class=\"head\">\\n           <p>\\n            Item\\n           </p>\\n          </th>\\n          <th class=\"head\">\\n           <p>\\n            Explanation\\n           </p>\\n          </th>\\n          <th class=\"head\">\\n           <p>\\n            Default Value\\n           </p>\\n          </th>\\n         </tr>\\n        </thead>\\n        <tbody>\\n         <tr class=\"row-even\">\\n          <td>\\n           <p>\\n            default-pe\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            default core type when pod is allocated (Fusion/Single)\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            Fusion\\n           </p>\\n          </td>\\n         </tr>\\n         <tr class=\"row-odd\">\\n          <td>\\n           <p>\\n            interval\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            interval for searching device (seconds)\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            10\\n           </p>\\n          </td>\\n         </tr>\\n         <tr class=\"row-even\">\\n          <td>\\n           <p>\\n            disabled-devices\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            devices not for allocations (several devices can be designated using comma)\\n           </p>\\n          </td>\\n          <td>\\n          </td>\\n         </tr>\\n         <tr class=\"row-odd\">\\n          <td>\\n           <p>\\n            plugin-dir\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            directory path of kubelet device-plugin\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            /var/lib/kubelet/device-plugins\\n           </p>\\n          </td>\\n         </tr>\\n         <tr class=\"row-even\">\\n          <td>\\n           <p>\\n            socket-name\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            file name of socket created under &lt;plugin-dir&gt;\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            furiosa-npu\\n           </p>\\n          </td>\\n         </tr>\\n         <tr class=\"row-odd\">\\n          <td>\\n           <p>\\n            resource-name\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            name of NPU resource registered for k8s node\\n           </p>\\n          </td>\\n          <td>\\n           <p>\\n            beta.furiosa.ai/npu\\n           </p>\\n          </td>\\n         </tr>\\n        </tbody>\\n       </table>\\n       <ol class=\"arabic simple\" start=\"2\">\\n        <li>\\n         <p>\\n          Setting Configuration File\\n         </p>\\n        </li>\\n       </ol>\\n       <p>\\n        You may set configuration file by executing\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          k8s-device-plugin\\n         </span>\\n        </code>\\n        command with argument\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          config-file\\n         </span>\\n        </code>\\n        .\\nIf\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          config-file\\n         </span>\\n        </code>\\n        is set then the other arguments are not permitted.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>$<span class=\"w\"> </span>k8s-device-plugin<span class=\"w\"> </span>--config-file<span class=\"w\"> </span>/etc/furiosa/device-plugin.conf\\n</pre>\\n        </div>\\n       </div>\\n       <div class=\"literal-block-wrapper docutils container\" id=\"id3\">\\n        <div class=\"code-block-caption\">\\n         <span class=\"caption-text\">\\n          /etc/furiosa/device-plugin.conf\\n         </span>\\n         <a class=\"headerlink\" href=\"#id3\" title=\"Permalink to this code\">\\n          \\uf0c1\\n         </a>\\n        </div>\\n        <div class=\"highlight-yaml notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span><span class=\"nt\">interval</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">10</span>\\n<span class=\"nt\">defaultPe</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">Fusion</span>\\n<span class=\"nt\">disabledDevices</span><span class=\"p\">:</span><span class=\"w\">             </span><span class=\"c1\"># device npu1 equipped in warboy-node01 will not be used</span>\\n<span class=\"w\">  </span><span class=\"p p-Indicator\">-</span><span class=\"w\"> </span><span class=\"nt\">devName</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">npu1</span>\\n<span class=\"w\">    </span><span class=\"nt\">nodeName</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">warboy-node01</span>\\n<span class=\"nt\">pluginDir</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/var/lib/kubelet/device-plugins</span>\\n<span class=\"nt\">socketName</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">furiosa-npu</span>\\n<span class=\"nt\">resourceName</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">beta.furiosa.ai/npu</span>\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n       <p>\\n        Configuration file is a text file with Yaml format. The modification of file contents is applied to Device Plugin immediately. Updated configuration is recorded on log of Device Plugin.\\n(but, modifications on\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          pluginDir\\n         </span>\\n        </code>\\n        ,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          socketName\\n         </span>\\n        </code>\\n        , or\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          resourceName\\n         </span>\\n        </code>\\n        require reboot.)\\n       </p>\\n       <p>\\n        <a class=\"reference internal\" href=\"#installingdevicepluginandnfd\">\\n         <span class=\"std std-ref\">\\n          3. Installing Device Plugin and NPU Feature Discovery\\n         </span>\\n        </a>\\n        provides\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          device-plugin.yaml\\n         </span>\\n        </code>\\n        which is default configuration file based on ConfigMap.\\nIf you want to modify execution options of Device Plugin, modify ConfigMap. Once modified ConfigMap is applied to Pod, Device Plugin reads the ConfigMap and then reflects modification.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>$<span class=\"w\"> </span>kubectl<span class=\"w\"> </span>edit<span class=\"w\"> </span>configmap<span class=\"w\"> </span>npu-device-plugin<span class=\"w\"> </span>-n<span class=\"w\"> </span>kube-system\\n</pre>\\n        </div>\\n       </div>\\n       <div class=\"literal-block-wrapper docutils container\" id=\"id4\">\\n        <div class=\"code-block-caption\">\\n         <span class=\"caption-text\">\\n          configmap/npu-device-plugin\\n         </span>\\n         <a class=\"headerlink\" href=\"#id4\" title=\"Permalink to this code\">\\n          \\uf0c1\\n         </a>\\n        </div>\\n        <div class=\"highlight-yaml notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span><span class=\"nt\">apiVersion</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">v1</span>\\n<span class=\"nt\">data</span><span class=\"p\">:</span>\\n<span class=\"w\">  </span><span class=\"nt\">config.yaml</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p p-Indicator\">|</span>\\n<span class=\"w\">    </span><span class=\"no\">defaultPe: Fusion</span>\\n<span class=\"w\">    </span><span class=\"no\">interval: 15</span>\\n<span class=\"w\">    </span><span class=\"no\">disabledDevices:</span>\\n<span class=\"w\">      </span><span class=\"no\">- devName: npu2</span>\\n<span class=\"w\">        </span><span class=\"no\">nodeName: npu-001</span>\\n<span class=\"nt\">kind</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">ConfigMap</span>\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n      </section>\\n     </section>\\n     <section id=\"creating-a-pod-with-npus\">\\n      <h2>\\n       4. Creating a Pod with NPUs\\n       <a class=\"headerlink\" href=\"#creating-a-pod-with-npus\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       To allocate NPU to a Pod, add as shown below to\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         spec.containers[].resources.limits\\n        </span>\\n       </code>\\n       .\\n      </p>\\n      <div class=\"highlight-yaml notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"nt\">resources</span><span class=\"p\">:</span>\\n<span class=\"w\">    </span><span class=\"nt\">limits</span><span class=\"p\">:</span>\\n<span class=\"w\">        </span><span class=\"nt\">beta.furiosa.ai/npu</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s\">\"1\"</span><span class=\"w\"> </span><span class=\"c1\"># requesting 1 NPU</span>\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/blob/v0.7.0/kubernetes/deployments/pod-example.yaml\">\\n        Full example\\n       </a>\\n       for Pod creation is as follows.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span>cat<span class=\"w\"> </span>&gt;<span class=\"w\"> </span>npu-pod.yaml<span class=\"w\"> </span><span class=\"s\">&lt;&lt;EOL</span>\\n<span class=\"s\">apiVersion: v1</span>\\n<span class=\"s\">kind: Pod</span>\\n<span class=\"s\">metadata:</span>\\n<span class=\"s\">  name: npu-pod</span>\\n<span class=\"s\">spec:</span>\\n<span class=\"s\">  containers:</span>\\n<span class=\"s\">    - name: npu-pod</span>\\n<span class=\"s\">      image: ubuntu:focal</span>\\n<span class=\"s\">      resources:</span>\\n<span class=\"s\">        limits:</span>\\n<span class=\"s\">          cpu: \"4\"</span>\\n<span class=\"s\">          memory: \"8Gi\"</span>\\n<span class=\"s\">          beta.furiosa.ai/npu: \"1\"</span>\\n<span class=\"s\">        requests:</span>\\n<span class=\"s\">          cpu: \"4\"</span>\\n<span class=\"s\">          memory: \"8Gi\"</span>\\n<span class=\"s\">          beta.furiosa.ai/npu: \"1\"</span>\\n<span class=\"s\">EOL</span>\\n\\n$<span class=\"w\"> </span>kubectl<span class=\"w\"> </span>apply<span class=\"w\"> </span>-f<span class=\"w\"> </span>npu-pod.yaml\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       After Pod creation, you can check NPU allocation as follows.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span>kubectl<span class=\"w\"> </span>get<span class=\"w\"> </span>pods<span class=\"w\"> </span>npu-pod<span class=\"w\"> </span>-o<span class=\"w\"> </span>yaml<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>grep<span class=\"w\"> </span>alpha.furiosa.ai/npu\\n<span class=\"w\">    </span>beta.furiosa.ai/npu:<span class=\"w\"> </span><span class=\"s2\">\"1\"</span>\\n<span class=\"w\">    </span>beta.furiosa.ai/npu:<span class=\"w\"> </span><span class=\"s2\">\"1\"</span>\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       The SDK application automatically recognizes the allocated NPU device.\\nIf there are multiple NPU devices on a node, you can check which device is allocated as follows:\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span>kubectl<span class=\"w\"> </span><span class=\"nb\">exec</span><span class=\"w\"> </span>npu-pod<span class=\"w\"> </span>-it<span class=\"w\"> </span>--<span class=\"w\"> </span>/bin/bash\\nroot@npu-pod:/#<span class=\"w\"> </span><span class=\"nb\">echo</span><span class=\"w\"> </span><span class=\"nv\">$NPU_DEVNAME</span>\\nnpu0pe0-1\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       If furiosa-toolkit is installed in the Pod, you can check for more detailed device information using the\\nfuriosactl command as shown below.\\n      </p>\\n      <p>\\n       See\\n       <a class=\"reference internal\" href=\"installation.html#setupaptrepository\">\\n        <span class=\"std std-ref\">\\n         APT server configuration\\n        </span>\\n       </a>\\n       for installation guide using APT.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>root@npu-pod:/#<span class=\"w\"> </span>furiosactl\\nfuriosactl<span class=\"w\"> </span>controls<span class=\"w\"> </span>the<span class=\"w\"> </span>FURIOSA<span class=\"w\"> </span>NPU.\\n\\nFind<span class=\"w\"> </span>more<span class=\"w\"> </span>information<span class=\"w\"> </span>at:<span class=\"w\"> </span>https://furiosa.ai/\\n\\nBasic<span class=\"w\"> </span>Commands:\\n<span class=\"w\">    </span>version<span class=\"w\">    </span>Print<span class=\"w\"> </span>the<span class=\"w\"> </span>furiosactl<span class=\"w\"> </span>version<span class=\"w\"> </span>information\\n<span class=\"w\">    </span>info<span class=\"w\">       </span>Show<span class=\"w\"> </span>information<span class=\"w\"> </span>one<span class=\"w\"> </span>or<span class=\"w\"> </span>many<span class=\"w\"> </span>NPU<span class=\"o\">(</span>s<span class=\"o\">)</span>\\n<span class=\"w\">    </span>config<span class=\"w\">     </span>Get/Set<span class=\"w\"> </span>configuration<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>NPU<span class=\"w\"> </span>environment\\n\\nUsage:\\n<span class=\"w\">    </span>furiosactl<span class=\"w\"> </span>COMMAND\\n\\nroot@npu-pod:/#<span class=\"w\"> </span>furiosactl<span class=\"w\"> </span>info\\n+------+------------------+-------+--------+--------------+---------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>NPU<span class=\"w\">  </span><span class=\"p\">|</span><span class=\"w\"> </span>Name<span class=\"w\">             </span><span class=\"p\">|</span><span class=\"w\"> </span>Temp.<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>Power<span class=\"w\">  </span><span class=\"p\">|</span><span class=\"w\"> </span>PCI-BDF<span class=\"w\">      </span><span class=\"p\">|</span><span class=\"w\"> </span>PCI-DEV<span class=\"w\"> </span><span class=\"p\">|</span>\\n+------+------------------+-------+--------+--------------+---------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>npu0<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>FuriosaAI<span class=\"w\"> </span>Warboy<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\">  </span><span class=\"m\">40</span>°C<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">1</span>.37<span class=\"w\"> </span>W<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0000</span>:01:00.0<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">509</span>:0<span class=\"w\">   </span><span class=\"p\">|</span>\\n+------+------------------+-------+--------+--------------+---------+\\n</pre>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"npu-monitoring\">\\n      <h2>\\n       5. NPU monitoring\\n       <a class=\"headerlink\" href=\"#npu-monitoring\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       If you install\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         npu-metrics-exporter\\n        </span>\\n       </code>\\n       , its daemon set and service will be created in your kubernetes cluster.\\nThe Pod that is executed through DaemonSet outputs various NPU status information that may be\\nuseful for monitoring. The data is expressed in Prometheus format. If Prometheus\\nis installed, and service discovery is active, Prometheus will automatically collect\\ndata through the Exporter.\\n      </p>\\n      <p>\\n       The collected data may be reviewed with visualization tools such as Grafana.\\n      </p>\\n      <table class=\"docutils align-default\" id=\"id5\">\\n       <caption>\\n        <span class=\"caption-text\">\\n         npu-metrics-exporter collection category list\\n        </span>\\n        <a class=\"headerlink\" href=\"#id5\" title=\"Permalink to this table\">\\n         \\uf0c1\\n        </a>\\n       </caption>\\n       <colgroup>\\n        <col style=\"width: 50%\"/>\\n        <col style=\"width: 50%\"/>\\n       </colgroup>\\n       <thead>\\n        <tr class=\"row-odd\">\\n         <th class=\"head\">\\n          <p>\\n           Name\\n          </p>\\n         </th>\\n         <th class=\"head\">\\n          <p>\\n           Details\\n          </p>\\n         </th>\\n        </tr>\\n       </thead>\\n       <tbody>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           furiosa_npu_alive\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           NPU operation status (1:normal)\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           furiosa_npu_uptime\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           NPU operation time (s)\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           furiosa_npu_error\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Number of detected NPU errors\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           furiosa_npu_hw_temperature\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Temperature of each NPU components (°mC)\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           furiosa_npu_hw_power\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           NPU instantaneous power usage (µW)\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           furiosa_npu_hw_voltage\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           NPU instantaenous voltage (mV)\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           furiosa_npu_hw_current\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           NPU instantaneous current (mA)\\n          </p>\\n         </td>\\n        </tr>\\n       </tbody>\\n      </table>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"serving.html\" rel=\"prev\" title=\"Model Server (Serving Framework)\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"vm_support.html\" rel=\"next\" title=\"Configuring Warboy Pass-through for Virtual Machine\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='3e1275e1-f809-4d7a-beb7-ec8b8b5de8f5', link=Url('https://furiosa-ai.github.io/docs/latest/en/software/quantization.html'), name='quantization', parent='', child=[], description='\\n\\n\\n* Model Quantization\\n* [View page source](../_sources/software/quantization.rst.txt)\\n\\n---\\n\\n\\n\\nModel Quantization\\n[\\uf0c1](#model-quantization \"Permalink to this heading\")\\n=======================================================================\\n\\nFuriosa SDK and first generation Warboy support INT8 models.\\nTo support floating point models, Furiosa SDK provides quantization tools to convert\\nFP16 or FP32 floating point data type models into INT8 data type models.\\nQuantization is a common technique used to increase model processing performance or accelerate hardware.\\nUsing the quantization tool provied by Furiosa SDK, a greater variety of models can be accelerated by deploying the NPU.\\n\\nQuantization method supported by Furiosa SDK is based on\\n*post-training 8-bit quantization*\\n, and follows\\n[Tensorflow Lite 8-bit quantization specification](https://www.tensorflow.org/lite/performance/quantization_spec)\\n.\\n\\nHow It Works\\n[\\uf0c1](#how-it-works \"Permalink to this heading\")\\n-----------------------------------------------------------\\n\\nAs shown in the diagram below, the quantization tool receives the ONNX model as input,\\nperforms quantization through the following three steps, and outputs the quantized ONNX model.\\n\\n1. Graph Optimization\\n2. Calibration\\n3. Quantization\\n\\nIn the graph optimization process, the topological structure of the graph is changed by adding or replacing\\noperators in the model through analysis of the original model network structure,\\nso that the model can process quantized data with a minimal drop in accuracy.\\n\\nIn the calibration process, the data used to train the model is required in order to calibrate the weights of the model.\\n\\n\\nAccuracy of Quantized Models\\n[\\uf0c1](#accuracy-of-quantized-models \"Permalink to this heading\")\\n-------------------------------------------------------------------------------------------\\n\\nThe table below compares the accuracy of the original floating-point models with that of the quantized models obtained using the quantizer and various calibration methods provided by Furiosa SDK:\\n\\n\\nQuantization Accuracy\\n\\n[\\uf0c1](#id1 \"Permalink to this table\")\\n\\n| Model | FP Accuracy | INT8 Accuracy (Calibration Method) | INT8 Accuracy ÷ FP Accuracy |\\n| --- | --- | --- | --- |\\n| ConvNext-B | 85.8% | 80.376% (Asymmetric MSE) | 93.678% |\\n| EfficientNet-B0 | 77.698% | 73.556% (Asymmetric 99.99%-Percentile) | 94.669% |\\n| EfficientNetV2-S | 84.228% | 83.566% (Asymmetric 99.99%-Percentile) | 99.214% |\\n| ResNet50 v1.5 | 76.456% | 76.228% (Asymmetric MSE) | 99.702% |\\n| RetinaNet | mAP 0.3757 | mAP 0.37373 (Symmetric Entropy) | 99.476% |\\n| YOLOX-l | mAP 0.497 | mAP 0.48524 (Asymmetric 99.99%-Percentile) | 97.634% |\\n| YOLOv5-l | mAP 0.490 | mAP 0.47443 (Asymmetric MSE) | 96.822% |\\n| YOLOv5-m | mAP 0.454 | mAP 0.43963 (Asymmetric SQNR) | 96.835% |\\n\\n\\nModel Quantization APIs\\n[\\uf0c1](#model-quantization-apis \"Permalink to this heading\")\\n---------------------------------------------------------------------------------\\n\\nYou can use the APU and command line tool provided in this SDK to convert an ONNX model into an 8bit quantized model.\\n\\nRefer to the links below for further instructions.\\n\\n* [Python SDK example: How to use Furiosa SDK from start to finish](https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/HowToUseFuriosaSDKFromStartToFinish.ipynb)\\n* [Python SDK Quantization example](https://github.com/furiosa-ai/furiosa-sdk/tree/main/examples/quantizers)\\n* [Python reference - furiosa.quantizer](https://furiosa-ai.github.io/docs/latest/en/api/python/furiosa.quantizer.html)\\n\\n\\n\\n\\n[Previous](compiler.html \"Compiler\")\\n[Next](performance.html \"Performance Optimization\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Model Quantization * [View page source](../_sources/software/quantization.rst.txt)\\n---\\nModel Quantization [\\uf0c1](#model-quantization \"Permalink to this heading\") =======================================================================\\nFuriosa SDK and first generation Warboy support INT8 models. To support floating point models, Furiosa SDK provides quantization tools to convert FP16 or FP32 floating point data type models into INT8 data type models. Quantization is a common technique used to increase model processing performance or accelerate hardware. Using the quantization tool provied by Furiosa SDK, a greater variety of models can be accelerated by deploying the NPU.\\nQuantization method supported by Furiosa SDK is based on *post-training 8-bit quantization* , and follows [Tensorflow Lite 8-bit quantization specification](https://www.tensorflow.org/lite/performance/quantization_spec) .\\nHow It Works [\\uf0c1](#how-it-works \"Permalink to this heading\") -----------------------------------------------------------\\nAs shown in the diagram below, the quantization tool receives the ONNX model as input, performs quantization through the following three steps, and outputs the quantized ONNX model.\\n1. Graph Optimization 2. Calibration 3. Quantization\\nIn the graph optimization process, the topological structure of the graph is changed by adding or replacing operators in the model through analysis of the original model network structure, so that the model can process quantized data with a minimal drop in accuracy.\\nIn the calibration process, the data used to train the model is required in order to calibrate the weights of the model.\\nAccuracy of Quantized Models [\\uf0c1](#accuracy-of-quantized-models \"Permalink to this heading\") -------------------------------------------------------------------------------------------\\nThe table below compares the accuracy of the original floating-point models with that of the quantized models obtained using the quantizer and various calibration methods provided by Furiosa SDK:\\nQuantization Accuracy\\n[\\uf0c1](#id1 \"Permalink to this table\")\\n| Model | FP Accuracy | INT8 Accuracy (Calibration Method) | INT8 Accuracy ÷ FP Accuracy | | --- | --- | --- | --- | | ConvNext-B | 85.8% | 80.376% (Asymmetric MSE) | 93.678% | | EfficientNet-B0 | 77.698% | 73.556% (Asymmetric 99.99%-Percentile) | 94.669% | | EfficientNetV2-S | 84.228% | 83.566% (Asymmetric 99.99%-Percentile) | 99.214% | | ResNet50 v1.5 | 76.456% | 76.228% (Asymmetric MSE) | 99.702% | | RetinaNet | mAP 0.3757 | mAP 0.37373 (Symmetric Entropy) | 99.476% | | YOLOX-l | mAP 0.497 | mAP 0.48524 (Asymmetric 99.99%-Percentile) | 97.634% | | YOLOv5-l | mAP 0.490 | mAP 0.47443 (Asymmetric MSE) | 96.822% | | YOLOv5-m | mAP 0.454 | mAP 0.43963 (Asymmetric SQNR) | 96.835% |\\nModel Quantization APIs [\\uf0c1](#model-quantization-apis \"Permalink to this heading\") ---------------------------------------------------------------------------------\\nYou can use the APU and command line tool provided in this SDK to convert an ONNX model into an 8bit quantized model.\\nRefer to the links below for further instructions.\\n* [Python SDK example: How to use Furiosa SDK from start to finish](https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/HowToUseFuriosaSDKFromStartToFinish.ipynb) * [Python SDK Quantization example](https://github.com/furiosa-ai/furiosa-sdk/tree/main/examples/quantizers) * [Python reference - furiosa.quantizer](https://furiosa-ai.github.io/docs/latest/en/api/python/furiosa.quantizer.html)\\n[Previous](compiler.html \"Compiler\") [Next](performance.html \"Performance Optimization\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Model Quantization\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/quantization.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"model-quantization\">\\n     <span id=\"modelquantization\">\\n     </span>\\n     <h1>\\n      Model Quantization\\n      <a class=\"headerlink\" href=\"#model-quantization\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      Furiosa SDK and first generation Warboy support INT8 models.\\nTo support floating point models, Furiosa SDK provides quantization tools to convert\\nFP16 or FP32 floating point data type models into INT8 data type models.\\nQuantization is a common technique used to increase model processing performance or accelerate hardware.\\nUsing the quantization tool provied by Furiosa SDK, a greater variety of models can be accelerated by deploying the NPU.\\n     </p>\\n     <p>\\n      Quantization method supported by Furiosa SDK is based on\\n      <em>\\n       post-training 8-bit quantization\\n      </em>\\n      , and follows\\n      <a class=\"reference external\" href=\"https://www.tensorflow.org/lite/performance/quantization_spec\">\\n       Tensorflow Lite 8-bit quantization specification\\n      </a>\\n      .\\n     </p>\\n     <section id=\"how-it-works\">\\n      <h2>\\n       How It Works\\n       <a class=\"headerlink\" href=\"#how-it-works\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       As shown in the diagram below, the quantization tool receives the ONNX model as input,\\nperforms quantization through the following three steps, and outputs the quantized ONNX model.\\n      </p>\\n      <ol class=\"arabic simple\">\\n       <li>\\n        <p>\\n         Graph Optimization\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Calibration\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Quantization\\n        </p>\\n       </li>\\n      </ol>\\n      <figure class=\"align-center\">\\n       <img alt=\"Quantization Process\" class=\"with-shadow\" src=\"../_images/nux-quantizer_quantization_pipepline-edd29681.png\"/>\\n      </figure>\\n      <p>\\n       In the graph optimization process, the topological structure of the graph is changed by adding or replacing\\noperators in the model through analysis of the original model network structure,\\nso that the model can process quantized data with a minimal drop in accuracy.\\n      </p>\\n      <p>\\n       In the calibration process, the data used to train the model is required in order to calibrate the weights of the model.\\n      </p>\\n     </section>\\n     <section id=\"accuracy-of-quantized-models\">\\n      <h2>\\n       Accuracy of Quantized Models\\n       <a class=\"headerlink\" href=\"#accuracy-of-quantized-models\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       The table below compares the accuracy of the original floating-point models with that of the quantized models obtained using the quantizer and various calibration methods provided by Furiosa SDK:\\n      </p>\\n      <span id=\"quantizationaccuracytable\">\\n      </span>\\n      <table class=\"docutils align-default\" id=\"id1\">\\n       <caption>\\n        <span class=\"caption-text\">\\n         Quantization Accuracy\\n        </span>\\n        <a class=\"headerlink\" href=\"#id1\" title=\"Permalink to this table\">\\n         \\uf0c1\\n        </a>\\n       </caption>\\n       <thead>\\n        <tr class=\"row-odd\">\\n         <th class=\"head\">\\n          <p>\\n           Model\\n          </p>\\n         </th>\\n         <th class=\"head\">\\n          <p>\\n           FP Accuracy\\n          </p>\\n         </th>\\n         <th class=\"head\">\\n          <p>\\n           INT8 Accuracy (Calibration Method)\\n          </p>\\n         </th>\\n         <th class=\"head\">\\n          <p>\\n           INT8 Accuracy ÷ FP Accuracy\\n          </p>\\n         </th>\\n        </tr>\\n       </thead>\\n       <tbody>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           ConvNext-B\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           85.8%\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           80.376% (Asymmetric MSE)\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           93.678%\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           EfficientNet-B0\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           77.698%\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           73.556% (Asymmetric 99.99%-Percentile)\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           94.669%\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           EfficientNetV2-S\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           84.228%\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           83.566% (Asymmetric 99.99%-Percentile)\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           99.214%\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           ResNet50 v1.5\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           76.456%\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           76.228% (Asymmetric MSE)\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           99.702%\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           RetinaNet\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           mAP 0.3757\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           mAP 0.37373 (Symmetric Entropy)\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           99.476%\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           YOLOX-l\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           mAP 0.497\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           mAP 0.48524 (Asymmetric 99.99%-Percentile)\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           97.634%\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           YOLOv5-l\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           mAP 0.490\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           mAP 0.47443 (Asymmetric MSE)\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           96.822%\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           YOLOv5-m\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           mAP 0.454\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           mAP 0.43963 (Asymmetric SQNR)\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           96.835%\\n          </p>\\n         </td>\\n        </tr>\\n       </tbody>\\n      </table>\\n     </section>\\n     <section id=\"model-quantization-apis\">\\n      <h2>\\n       Model Quantization APIs\\n       <a class=\"headerlink\" href=\"#model-quantization-apis\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       You can use the APU and command line tool provided in this SDK to convert an ONNX model into an 8bit quantized model.\\n      </p>\\n      <p>\\n       Refer to the links below for further instructions.\\n      </p>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/HowToUseFuriosaSDKFromStartToFinish.ipynb\">\\n          Python SDK example: How to use Furiosa SDK from start to finish\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/tree/main/examples/quantizers\">\\n          Python SDK Quantization example\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference external\" href=\"https://furiosa-ai.github.io/docs/latest/en/api/python/furiosa.quantizer.html\">\\n          Python reference - furiosa.quantizer\\n         </a>\\n        </p>\\n       </li>\\n      </ul>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"compiler.html\" rel=\"prev\" title=\"Compiler\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"performance.html\" rel=\"next\" title=\"Performance Optimization\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='2114c344-3971-4d26-8c09-a001327ce7bb', link=Url('https://furiosa-ai.github.io/docs/latest/en/software/c-sdk.html'), name='c-sdk', parent='', child=[], description='\\n\\n\\n* C SDK installation and user guide\\n* [View page source](../_sources/software/c-sdk.rst.txt)\\n\\n---\\n\\n\\n\\nC SDK installation and user guide\\n[\\uf0c1](#c-sdk-installation-and-user-guide \"Permalink to this heading\")\\n=====================================================================================================\\n\\nWe explain here how to write FuriosaAI NPU applications using C programming language.\\nThe C SDK provides a C ABI-based static library and C header file. Using these, you can write applications in C, C++, or other languages that support C ABI.\\n\\nThe provided C SDK is relatively lower-level than\\n[Python SDK](python-sdk.html#pythonsdk)\\n. It can be used when lower latency and higher performance are required, or when Python runtime cannot be used.\\n\\nWarning\\n\\n`furiosa-libnux-dev`\\nand the current C API are being deprecated in the future release.\\n\\nAs substitute of the current API, new C API based on the next-generation runtime called FuriosaRT\\nwill be introduced with more features in the future release.\\n\\n\\nC SDK installation\\n[\\uf0c1](#c-sdk-installation \"Permalink to this heading\")\\n-----------------------------------------------------------------------\\n\\nThe minimum requirements for C SDK are as follows.\\n\\n* Ubuntu 20.04 LTS (Debian bullseye) or higher\\n* System administrator privileges (root)\\n* [FuriosaAI SDK required packages](installation.html#requiredpackages)\\n\\nIn order to install and use C SDK, you must install the driver, firmware, and runtime library in accordance with\\nthe\\n[Required Package Installation](installation.html#requiredpackages)\\nguide.\\n\\nOnce you have installed the required packages, follow the instructions below to install C SDK.\\n\\nInstallation using APT server\\n\\n\\nTo use FuriosaAI APT, refer to\\n[APT server configuration](installation.html#setupaptrepository)\\nand complete the authentication setting for server connection.\\n\\n```\\napt-get update && apt-get install -y furiosa-libnux-dev\\n\\n```\\n\\n\\n\\n\\n\\nCompiling with C SDK\\n[\\uf0c1](#compiling-with-c-sdk \"Permalink to this heading\")\\n---------------------------------------------------------------------------\\n\\nOnce you install the package as above, you can compile using the C SDK.\\n\\nC header files and static libraries are located in the\\n`/usr/include/furiosa`\\nand\\n`/usr/lib/x86_64-linux-gnu`\\ndirectories respectively.\\nThey are the system paths that gcc looks to find C headers and libraries by default,\\nso you can simply compile C applications with only\\n`-lnux`\\noption as follows:\\n\\n```\\ngcc example.c -lnux\\n\\n```\\n\\nAlso, you can find C SDK examples and C API reference at\\n[C Language SDK Reference](https://furiosa-ai.github.io/docs/v0.10.0/en/api/c/index.html)\\n.\\n\\n\\n\\n\\n\\n[Previous](python-sdk.html \"Python SDK installation and user guide\")\\n[Next](cli.html \"Command Line Tools\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* C SDK installation and user guide * [View page source](../_sources/software/c-sdk.rst.txt)\\n---\\nC SDK installation and user guide [\\uf0c1](#c-sdk-installation-and-user-guide \"Permalink to this heading\") =====================================================================================================\\nWe explain here how to write FuriosaAI NPU applications using C programming language. The C SDK provides a C ABI-based static library and C header file. Using these, you can write applications in C, C++, or other languages that support C ABI.\\nThe provided C SDK is relatively lower-level than [Python SDK](python-sdk.html#pythonsdk) . It can be used when lower latency and higher performance are required, or when Python runtime cannot be used.\\nWarning  `furiosa-libnux-dev` and the current C API are being deprecated in the future release.\\nAs substitute of the current API, new C API based on the next-generation runtime called FuriosaRT will be introduced with more features in the future release.\\nC SDK installation [\\uf0c1](#c-sdk-installation \"Permalink to this heading\") -----------------------------------------------------------------------\\nThe minimum requirements for C SDK are as follows.\\n* Ubuntu 20.04 LTS (Debian bullseye) or higher * System administrator privileges (root) * [FuriosaAI SDK required packages](installation.html#requiredpackages)\\nIn order to install and use C SDK, you must install the driver, firmware, and runtime library in accordance with the [Required Package Installation](installation.html#requiredpackages) guide.\\nOnce you have installed the required packages, follow the instructions below to install C SDK.\\nInstallation using APT server\\nTo use FuriosaAI APT, refer to [APT server configuration](installation.html#setupaptrepository) and complete the authentication setting for server connection.\\n``` apt-get update && apt-get install -y furiosa-libnux-dev\\n```\\nCompiling with C SDK [\\uf0c1](#compiling-with-c-sdk \"Permalink to this heading\") ---------------------------------------------------------------------------\\nOnce you install the package as above, you can compile using the C SDK.\\nC header files and static libraries are located in the `/usr/include/furiosa` and `/usr/lib/x86_64-linux-gnu` directories respectively. They are the system paths that gcc looks to find C headers and libraries by default, so you can simply compile C applications with only `-lnux` option as follows:\\n``` gcc example.c -lnux\\n```\\nAlso, you can find C SDK examples and C API reference at [C Language SDK Reference](https://furiosa-ai.github.io/docs/v0.10.0/en/api/c/index.html) .\\n[Previous](python-sdk.html \"Python SDK installation and user guide\") [Next](cli.html \"Command Line Tools\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     C SDK installation and user guide\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/c-sdk.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"c-sdk-installation-and-user-guide\">\\n     <span id=\"csdk\">\\n     </span>\\n     <h1>\\n      C SDK installation and user guide\\n      <a class=\"headerlink\" href=\"#c-sdk-installation-and-user-guide\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      We explain here how to write FuriosaAI NPU applications using C programming language.\\nThe C SDK provides a C ABI-based static library and C header file. Using these, you can write applications in C, C++, or other languages that support C ABI.\\n     </p>\\n     <p>\\n      The provided C SDK is relatively lower-level than\\n      <a class=\"reference internal\" href=\"python-sdk.html#pythonsdk\">\\n       <span class=\"std std-ref\">\\n        Python SDK\\n       </span>\\n      </a>\\n      . It can be used when lower latency and higher performance are required, or when Python runtime cannot be used.\\n     </p>\\n     <div class=\"admonition warning\">\\n      <p class=\"admonition-title\">\\n       Warning\\n      </p>\\n      <p>\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         furiosa-libnux-dev\\n        </span>\\n       </code>\\n       and the current C API are being deprecated in the future release.\\n      </p>\\n      <p>\\n       As substitute of the current API, new C API based on the next-generation runtime called FuriosaRT\\nwill be introduced with more features in the future release.\\n      </p>\\n     </div>\\n     <section id=\"c-sdk-installation\">\\n      <h2>\\n       C SDK installation\\n       <a class=\"headerlink\" href=\"#c-sdk-installation\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       The minimum requirements for C SDK are as follows.\\n      </p>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         Ubuntu 20.04 LTS (Debian bullseye) or higher\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         System administrator privileges (root)\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"installation.html#requiredpackages\">\\n          <span class=\"std std-ref\">\\n           FuriosaAI SDK required packages\\n          </span>\\n         </a>\\n        </p>\\n       </li>\\n      </ul>\\n      <p>\\n       In order to install and use C SDK, you must install the driver, firmware, and runtime library in accordance with\\nthe\\n       <a class=\"reference internal\" href=\"installation.html#requiredpackages\">\\n        <span class=\"std std-ref\">\\n         Required Package Installation\\n        </span>\\n       </a>\\n       guide.\\n      </p>\\n      <p>\\n       Once you have installed the required packages, follow the instructions below to install C SDK.\\n      </p>\\n      <div class=\"sphinx-tabs docutils container\">\\n       <div aria-label=\"Tabbed content\" role=\"tablist\">\\n        <button aria-controls=\"panel-0-0-0\" aria-selected=\"true\" class=\"sphinx-tabs-tab\" id=\"tab-0-0-0\" name=\"0-0\" role=\"tab\" tabindex=\"0\">\\n         Installation using APT server\\n        </button>\\n       </div>\\n       <div aria-labelledby=\"tab-0-0-0\" class=\"sphinx-tabs-panel\" id=\"panel-0-0-0\" name=\"0-0\" role=\"tabpanel\" tabindex=\"0\">\\n        <p>\\n         To use FuriosaAI APT, refer to\\n         <a class=\"reference internal\" href=\"installation.html#setupaptrepository\">\\n          <span class=\"std std-ref\">\\n           APT server configuration\\n          </span>\\n         </a>\\n         and complete the authentication setting for server connection.\\n        </p>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>apt-get<span class=\"w\"> </span>update<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span>apt-get<span class=\"w\"> </span>install<span class=\"w\"> </span>-y<span class=\"w\"> </span>furiosa-libnux-dev\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"compiling-with-c-sdk\">\\n      <h2>\\n       Compiling with C SDK\\n       <a class=\"headerlink\" href=\"#compiling-with-c-sdk\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       Once you install the package as above, you can compile using the C SDK.\\n      </p>\\n      <p>\\n       C header files and static libraries are located in the\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         /usr/include/furiosa\\n        </span>\\n       </code>\\n       and\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         /usr/lib/x86_64-linux-gnu\\n        </span>\\n       </code>\\n       directories respectively.\\nThey are the system paths that gcc looks to find C headers and libraries by default,\\nso you can simply compile C applications with only\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         -lnux\\n        </span>\\n       </code>\\n       option as follows:\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>gcc<span class=\"w\"> </span>example.c<span class=\"w\"> </span>-lnux\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       Also, you can find C SDK examples and C API reference at\\n       <a class=\"reference external\" href=\"https://furiosa-ai.github.io/docs/v0.10.0/en/api/c/index.html\">\\n        C Language SDK Reference\\n       </a>\\n       .\\n      </p>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"python-sdk.html\" rel=\"prev\" title=\"Python SDK installation and user guide\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"cli.html\" rel=\"next\" title=\"Command Line Tools\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='2a739e9e-c42e-4c07-82d5-41cf614dd6a9', link=Url('https://furiosa-ai.github.io/docs/latest/en/software/installation.html'), name='installation', parent='', child=[], description='\\n\\n\\n* Driver, Firmware, and Runtime Installation\\n* [View page source](../_sources/software/installation.rst.txt)\\n\\n---\\n\\n\\n\\nDriver, Firmware, and Runtime Installation\\n[\\uf0c1](#driver-firmware-and-runtime-installation \"Permalink to this heading\")\\n=====================================================================================================================\\n\\nHere, we explain how to install the packages necessary to use\\nthe various SW components provided by FuriosaAI.\\nThe required packages are composed of kernel drivers, firmware, and runtime library,\\nand they can be easily installed via the APT package manager.\\n\\nNote\\n\\nYou will be able to login\\n[FuriosaAI IAM](https://iam.furiosa.ai)\\nand create a new API key\\nupon registration to the FuriosaAI evaluation program.\\nCurrently, the request for registration can be done through\\n[contact\\n@\\n\\nfuriosa\\n.\\n\\nai](mailto:contact%40furiosa.ai)\\n.\\n\\n\\nMinimum requirements for SDK installation\\n[\\uf0c1](#minimum-requirements-for-sdk-installation \"Permalink to this heading\")\\n---------------------------------------------------------------------------------------------------------------------\\n\\n* Ubuntu 20.04 LTS (Focal Fossa)/Debian bullseye\\n  or higher\\n* Administrator privileges on system (root)\\n* Internet-accessible network\\n\\nAPT server configuration\\n[\\uf0c1](#apt-server-configuration \"Permalink to this heading\")\\n-----------------------------------------------------------------------------------\\n\\nIn order to use the APT server as provided by FuriosaAI, the APT server must be configured\\non Ubuntu or Debian Linux as delineated below.\\n\\n1. Install the necessary packages to access HTTPS-based APT server.\\n\\n```\\nsudo apt update\\nsudo apt install -y ca-certificates apt-transport-https gnupg wget\\n\\n```\\n\\n2. Register the FuriosaAI public Signing key.\\n\\n```\\nmkdir -p /etc/apt/keyrings && \\\\\\nwget -q -O- https://archive.furiosa.ai/furiosa-apt-key.gpg \\\\\\n| gpg --dearmor \\\\\\n| sudo tee /etc/apt/keyrings/furiosa-apt-key.gpg > /dev/null\\n\\n```\\n\\n3. Generate a new API key from\\n   [FuriosaAI IAM](https://iam.furiosa.ai)\\n   , and configure the API key as follows:\\n\\n```\\nsudo tee -a /etc/apt/auth.conf.d/furiosa.conf > /dev/null <<EOT\\n  machine archive.furiosa.ai\\n  login [KEY (ID)]\\n  password [PASSWORD]\\nEOT\\n\\nsudo chmod 400 /etc/apt/auth.conf.d/furiosa.conf\\n\\n```\\n\\n4. Configure the APT server according to the explanation given in the Linux distribution version tab.\\n\\nUbuntu 20.04 (Debian Bullseye)\\n\\nUbuntu 22.04 (Debian Bookworm)\\n\\n\\nRegister the APT server through the command below.\\n\\n```\\nsudo tee -a /etc/apt/sources.list.d/furiosa.list <<EOT\\ndeb [arch=amd64 signed-by=/etc/apt/keyrings/furiosa-apt-key.gpg] https://archive.furiosa.ai/ubuntu focal restricted\\nEOT\\n\\n```\\n\\n\\n\\nRegister the APT server through the command below.\\n\\n```\\nsudo tee -a /etc/apt/sources.list.d/furiosa.list <<EOT\\ndeb [arch=amd64 signed-by=/etc/apt/keyrings/furiosa-apt-key.gpg] https://archive.furiosa.ai/ubuntu jammy restricted\\nEOT\\n\\n```\\n\\n\\n\\n\\n\\nInstalling required packages.\\n[\\uf0c1](#installing-required-packages \"Permalink to this heading\")\\n--------------------------------------------------------------------------------------------\\n\\nIf you have registered the APT server as above, or registered on the download site,\\nyou will be able to install the required packages - NPU kernel driver, firmware, and runtime.\\n\\nInstallation using APT server\\n\\n\\n```\\nsudo apt-get update && sudo apt-get install -y furiosa-driver-warboy furiosa-libnux\\n\\n```\\n\\n\\n\\n\\n### Adding a user to the `furiosa` Group [\\uf0c1](#adding-a-user-to-the-furiosa-group \"Permalink to this heading\")\\n\\nLinux is a multi-user operating system that enables file and device access for both the owner and users within a specific group.\\nThe NPU device driver creates a group called\\n`furiosa`\\nand restricts access to NPU devices exclusively to users who are members of the\\n`furiosa`\\ngroup.\\nTo add a user to a member of\\n`furiosa`\\ngroup, please run as follows:\\n\\n```\\nsudo usermod -aG furiosa <username>\\n\\n```\\n\\nReplace <username> with the name of the user you want to add to the\\n`furiosa`\\ngroup.\\nFor example, in order to add the current user (i.e.,\\n`$USER`\\n) to the\\n`furiosa`\\ngroup, you can run as follows:\\n\\n```\\nsudo usermod -aG furiosa $USER\\n\\n```\\n\\nUpon logging out and logging back in, the change to the group membership will take effect.\\n\\n\\n### Holding/unholding installed version [\\uf0c1](#holding-unholding-installed-version \"Permalink to this heading\")\\n\\nFollowing package installation, in order to maintain a stable operating environment,\\nthere may be a need to hold the installed packages versions. By using the command below,\\nyou will be able to hold the currently installed versions.\\n\\n```\\nsudo apt-mark hold furiosa-driver-warboy furiosa-libhal-warboy furiosa-libnux libonnxruntime\\n\\n```\\n\\nIn order to unhold and update the current package versions, designate the package\\nthat you wish to unhold with the command\\n`apt-mark\\n\\nunhold`\\n.\\nHere, you can state the name of the package, thereby unholding selectively\\na specific package. In order to show the properties of an already held package,\\nuse the command\\n`apt-mark\\n\\nshowhold`\\n.\\n\\n```\\nsudo apt-mark unhold furiosa-driver-warboy furiosa-libhal-warboy furiosa-libnux libonnxruntime\\n\\n```\\n\\n\\n\\n### Installing a specific version [\\uf0c1](#installing-a-specific-version \"Permalink to this heading\")\\n\\nIf you need to install a specific version,\\nyou may designate the version that you want and install as follows.\\n\\n1. Check available versions through\\n   `apt\\n   \\n   list`\\n   .\\n\\n```\\nsudo apt list -a furiosa-libnux\\n\\n```\\n\\n2. State the package name and version as options in the command\\n   `apt-get\\n   \\n   install`\\n\\n```\\nsudo apt-get install -y furiosa-libnux=0.9.1-?\\n\\n```\\n\\n\\n\\n\\nNPU Firmware Update\\n[\\uf0c1](#npu-firmware-update \"Permalink to this heading\")\\n-------------------------------------------------------------------------\\n\\n\\n\\n\\n\\n[Previous](intro.html \"FuriosaAI SW Stack Introduction\")\\n[Next](python-sdk.html \"Python SDK installation and user guide\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Driver, Firmware, and Runtime Installation * [View page source](../_sources/software/installation.rst.txt)\\n---\\nDriver, Firmware, and Runtime Installation [\\uf0c1](#driver-firmware-and-runtime-installation \"Permalink to this heading\") =====================================================================================================================\\nHere, we explain how to install the packages necessary to use the various SW components provided by FuriosaAI. The required packages are composed of kernel drivers, firmware, and runtime library, and they can be easily installed via the APT package manager.\\nNote\\nYou will be able to login [FuriosaAI IAM](https://iam.furiosa.ai) and create a new API key upon registration to the FuriosaAI evaluation program. Currently, the request for registration can be done through [contact @\\nfuriosa .\\nai](mailto:contact%40furiosa.ai) .\\nMinimum requirements for SDK installation [\\uf0c1](#minimum-requirements-for-sdk-installation \"Permalink to this heading\") ---------------------------------------------------------------------------------------------------------------------\\n* Ubuntu 20.04 LTS (Focal Fossa)/Debian bullseye   or higher * Administrator privileges on system (root) * Internet-accessible network\\nAPT server configuration [\\uf0c1](#apt-server-configuration \"Permalink to this heading\") -----------------------------------------------------------------------------------\\nIn order to use the APT server as provided by FuriosaAI, the APT server must be configured on Ubuntu or Debian Linux as delineated below.\\n1. Install the necessary packages to access HTTPS-based APT server.\\n``` sudo apt update sudo apt install -y ca-certificates apt-transport-https gnupg wget\\n```\\n2. Register the FuriosaAI public Signing key.\\n``` mkdir -p /etc/apt/keyrings && \\\\ wget -q -O- https://archive.furiosa.ai/furiosa-apt-key.gpg \\\\ | gpg --dearmor \\\\ | sudo tee /etc/apt/keyrings/furiosa-apt-key.gpg > /dev/null\\n```\\n3. Generate a new API key from    [FuriosaAI IAM](https://iam.furiosa.ai)    , and configure the API key as follows:\\n``` sudo tee -a /etc/apt/auth.conf.d/furiosa.conf > /dev/null <<EOT   machine archive.furiosa.ai   login [KEY (ID)]   password [PASSWORD] EOT\\nsudo chmod 400 /etc/apt/auth.conf.d/furiosa.conf\\n```\\n4. Configure the APT server according to the explanation given in the Linux distribution version tab.\\nUbuntu 20.04 (Debian Bullseye)\\nUbuntu 22.04 (Debian Bookworm)\\nRegister the APT server through the command below.\\n``` sudo tee -a /etc/apt/sources.list.d/furiosa.list <<EOT deb [arch=amd64 signed-by=/etc/apt/keyrings/furiosa-apt-key.gpg] https://archive.furiosa.ai/ubuntu focal restricted EOT\\n```\\nRegister the APT server through the command below.\\n``` sudo tee -a /etc/apt/sources.list.d/furiosa.list <<EOT deb [arch=amd64 signed-by=/etc/apt/keyrings/furiosa-apt-key.gpg] https://archive.furiosa.ai/ubuntu jammy restricted EOT\\n```\\nInstalling required packages. [\\uf0c1](#installing-required-packages \"Permalink to this heading\") --------------------------------------------------------------------------------------------\\nIf you have registered the APT server as above, or registered on the download site, you will be able to install the required packages - NPU kernel driver, firmware, and runtime.\\nInstallation using APT server\\n``` sudo apt-get update && sudo apt-get install -y furiosa-driver-warboy furiosa-libnux\\n```\\n### Adding a user to the `furiosa` Group [\\uf0c1](#adding-a-user-to-the-furiosa-group \"Permalink to this heading\")\\nLinux is a multi-user operating system that enables file and device access for both the owner and users within a specific group. The NPU device driver creates a group called `furiosa` and restricts access to NPU devices exclusively to users who are members of the `furiosa` group. To add a user to a member of `furiosa` group, please run as follows:\\n``` sudo usermod -aG furiosa <username>\\n```\\nReplace <username> with the name of the user you want to add to the `furiosa` group. For example, in order to add the current user (i.e., `$USER` ) to the `furiosa` group, you can run as follows:\\n``` sudo usermod -aG furiosa $USER\\n```\\nUpon logging out and logging back in, the change to the group membership will take effect.\\n### Holding/unholding installed version [\\uf0c1](#holding-unholding-installed-version \"Permalink to this heading\")\\nFollowing package installation, in order to maintain a stable operating environment, there may be a need to hold the installed packages versions. By using the command below, you will be able to hold the currently installed versions.\\n``` sudo apt-mark hold furiosa-driver-warboy furiosa-libhal-warboy furiosa-libnux libonnxruntime\\n```\\nIn order to unhold and update the current package versions, designate the package that you wish to unhold with the command `apt-mark\\nunhold` .\\nHere, you can state the name of the package, thereby unholding selectively a specific package. In order to show the properties of an already held package, use the command `apt-mark\\nshowhold` .\\n``` sudo apt-mark unhold furiosa-driver-warboy furiosa-libhal-warboy furiosa-libnux libonnxruntime\\n```\\n### Installing a specific version [\\uf0c1](#installing-a-specific-version \"Permalink to this heading\")\\nIf you need to install a specific version, you may designate the version that you want and install as follows.\\n1. Check available versions through    `apt        list`    .\\n``` sudo apt list -a furiosa-libnux\\n```\\n2. State the package name and version as options in the command    `apt-get        install`\\n``` sudo apt-get install -y furiosa-libnux=0.9.1-?\\n```\\nNPU Firmware Update [\\uf0c1](#npu-firmware-update \"Permalink to this heading\") -------------------------------------------------------------------------\\n[Previous](intro.html \"FuriosaAI SW Stack Introduction\") [Next](python-sdk.html \"Python SDK installation and user guide\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Driver, Firmware, and Runtime Installation\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/installation.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"driver-firmware-and-runtime-installation\">\\n     <span id=\"requiredpackages\">\\n     </span>\\n     <h1>\\n      Driver, Firmware, and Runtime Installation\\n      <a class=\"headerlink\" href=\"#driver-firmware-and-runtime-installation\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      Here, we explain how to install the packages necessary to use\\nthe various SW components provided by FuriosaAI.\\nThe required packages are composed of kernel drivers, firmware, and runtime library,\\nand they can be easily installed via the APT package manager.\\n     </p>\\n     <div class=\"admonition note\">\\n      <p class=\"admonition-title\">\\n       Note\\n      </p>\\n      <p>\\n       You will be able to login\\n       <a class=\"reference external\" href=\"https://iam.furiosa.ai\">\\n        FuriosaAI IAM\\n       </a>\\n       and create a new API key\\nupon registration to the FuriosaAI evaluation program.\\nCurrently, the request for registration can be done through\\n       <a class=\"reference external\" href=\"mailto:contact%40furiosa.ai\">\\n        contact\\n        <span>\\n         @\\n        </span>\\n        furiosa\\n        <span>\\n         .\\n        </span>\\n        ai\\n       </a>\\n       .\\n      </p>\\n     </div>\\n     <section id=\"minimum-requirements-for-sdk-installation\">\\n      <span id=\"minimumrequirements\">\\n      </span>\\n      <h2>\\n       Minimum requirements for SDK installation\\n       <a class=\"headerlink\" href=\"#minimum-requirements-for-sdk-installation\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         Ubuntu 20.04 LTS (Focal Fossa)/Debian bullseye\\nor higher\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Administrator privileges on system (root)\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Internet-accessible network\\n        </p>\\n       </li>\\n      </ul>\\n     </section>\\n     <section id=\"apt-server-configuration\">\\n      <span id=\"setupaptrepository\">\\n      </span>\\n      <h2>\\n       APT server configuration\\n       <a class=\"headerlink\" href=\"#apt-server-configuration\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       In order to use the APT server as provided by FuriosaAI, the APT server must be configured\\non Ubuntu or Debian Linux as delineated below.\\n      </p>\\n      <ol class=\"arabic simple\">\\n       <li>\\n        <p>\\n         Install the necessary packages to access HTTPS-based APT server.\\n        </p>\\n       </li>\\n      </ol>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>sudo<span class=\"w\"> </span>apt<span class=\"w\"> </span>update\\nsudo<span class=\"w\"> </span>apt<span class=\"w\"> </span>install<span class=\"w\"> </span>-y<span class=\"w\"> </span>ca-certificates<span class=\"w\"> </span>apt-transport-https<span class=\"w\"> </span>gnupg<span class=\"w\"> </span>wget\\n</pre>\\n       </div>\\n      </div>\\n      <ol class=\"arabic simple\" start=\"2\">\\n       <li>\\n        <p>\\n         Register the FuriosaAI public Signing key.\\n        </p>\\n       </li>\\n      </ol>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>mkdir<span class=\"w\"> </span>-p<span class=\"w\"> </span>/etc/apt/keyrings<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span><span class=\"se\">\\\\</span>\\nwget<span class=\"w\"> </span>-q<span class=\"w\"> </span>-O-<span class=\"w\"> </span>https://archive.furiosa.ai/furiosa-apt-key.gpg<span class=\"w\"> </span><span class=\"se\">\\\\</span>\\n<span class=\"p\">|</span><span class=\"w\"> </span>gpg<span class=\"w\"> </span>--dearmor<span class=\"w\"> </span><span class=\"se\">\\\\</span>\\n<span class=\"p\">|</span><span class=\"w\"> </span>sudo<span class=\"w\"> </span>tee<span class=\"w\"> </span>/etc/apt/keyrings/furiosa-apt-key.gpg<span class=\"w\"> </span>&gt;<span class=\"w\"> </span>/dev/null\\n</pre>\\n       </div>\\n      </div>\\n      <ol class=\"arabic simple\" start=\"3\">\\n       <li>\\n        <p>\\n         Generate a new API key from\\n         <a class=\"reference external\" href=\"https://iam.furiosa.ai\">\\n          FuriosaAI IAM\\n         </a>\\n         , and configure the API key as follows:\\n        </p>\\n       </li>\\n      </ol>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>sudo<span class=\"w\"> </span>tee<span class=\"w\"> </span>-a<span class=\"w\"> </span>/etc/apt/auth.conf.d/furiosa.conf<span class=\"w\"> </span>&gt;<span class=\"w\"> </span>/dev/null<span class=\"w\"> </span><span class=\"s\">&lt;&lt;EOT</span>\\n<span class=\"s\">  machine archive.furiosa.ai</span>\\n<span class=\"s\">  login [KEY (ID)]</span>\\n<span class=\"s\">  password [PASSWORD]</span>\\n<span class=\"s\">EOT</span>\\n\\nsudo<span class=\"w\"> </span>chmod<span class=\"w\"> </span><span class=\"m\">400</span><span class=\"w\"> </span>/etc/apt/auth.conf.d/furiosa.conf\\n</pre>\\n       </div>\\n      </div>\\n      <ol class=\"arabic simple\" start=\"4\">\\n       <li>\\n        <p>\\n         Configure the APT server according to the explanation given in the Linux distribution version tab.\\n        </p>\\n       </li>\\n      </ol>\\n      <div class=\"sphinx-tabs docutils container\">\\n       <div aria-label=\"Tabbed content\" role=\"tablist\">\\n        <button aria-controls=\"panel-0-0-0\" aria-selected=\"true\" class=\"sphinx-tabs-tab\" id=\"tab-0-0-0\" name=\"0-0\" role=\"tab\" tabindex=\"0\">\\n         Ubuntu 20.04 (Debian Bullseye)\\n        </button>\\n        <button aria-controls=\"panel-0-0-1\" aria-selected=\"false\" class=\"sphinx-tabs-tab\" id=\"tab-0-0-1\" name=\"0-1\" role=\"tab\" tabindex=\"-1\">\\n         Ubuntu 22.04 (Debian Bookworm)\\n        </button>\\n       </div>\\n       <div aria-labelledby=\"tab-0-0-0\" class=\"sphinx-tabs-panel\" id=\"panel-0-0-0\" name=\"0-0\" role=\"tabpanel\" tabindex=\"0\">\\n        <p>\\n         Register the APT server through the command below.\\n        </p>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>sudo<span class=\"w\"> </span>tee<span class=\"w\"> </span>-a<span class=\"w\"> </span>/etc/apt/sources.list.d/furiosa.list<span class=\"w\"> </span><span class=\"s\">&lt;&lt;EOT</span>\\n<span class=\"s\">deb [arch=amd64 signed-by=/etc/apt/keyrings/furiosa-apt-key.gpg] https://archive.furiosa.ai/ubuntu focal restricted</span>\\n<span class=\"s\">EOT</span>\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n       <div aria-labelledby=\"tab-0-0-1\" class=\"sphinx-tabs-panel\" hidden=\"true\" id=\"panel-0-0-1\" name=\"0-1\" role=\"tabpanel\" tabindex=\"0\">\\n        <p>\\n         Register the APT server through the command below.\\n        </p>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>sudo<span class=\"w\"> </span>tee<span class=\"w\"> </span>-a<span class=\"w\"> </span>/etc/apt/sources.list.d/furiosa.list<span class=\"w\"> </span><span class=\"s\">&lt;&lt;EOT</span>\\n<span class=\"s\">deb [arch=amd64 signed-by=/etc/apt/keyrings/furiosa-apt-key.gpg] https://archive.furiosa.ai/ubuntu jammy restricted</span>\\n<span class=\"s\">EOT</span>\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"installing-required-packages\">\\n      <span id=\"installlinuxpackages\">\\n      </span>\\n      <h2>\\n       Installing required packages.\\n       <a class=\"headerlink\" href=\"#installing-required-packages\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       If you have registered the APT server as above, or registered on the download site,\\nyou will be able to install the required packages - NPU kernel driver, firmware, and runtime.\\n      </p>\\n      <div class=\"sphinx-tabs docutils container\">\\n       <div aria-label=\"Tabbed content\" role=\"tablist\">\\n        <button aria-controls=\"panel-1-1-0\" aria-selected=\"true\" class=\"sphinx-tabs-tab\" id=\"tab-1-1-0\" name=\"1-0\" role=\"tab\" tabindex=\"0\">\\n         Installation using APT server\\n        </button>\\n       </div>\\n       <div aria-labelledby=\"tab-1-1-0\" class=\"sphinx-tabs-panel\" id=\"panel-1-1-0\" name=\"1-0\" role=\"tabpanel\" tabindex=\"0\">\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>sudo<span class=\"w\"> </span>apt-get<span class=\"w\"> </span>update<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span>sudo<span class=\"w\"> </span>apt-get<span class=\"w\"> </span>install<span class=\"w\"> </span>-y<span class=\"w\"> </span>furiosa-driver-warboy<span class=\"w\"> </span>furiosa-libnux\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n      </div>\\n      <section id=\"adding-a-user-to-the-furiosa-group\">\\n       <span id=\"addusertofuriosagroup\">\\n       </span>\\n       <h3>\\n        Adding a user to the\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa\\n         </span>\\n        </code>\\n        Group\\n        <a class=\"headerlink\" href=\"#adding-a-user-to-the-furiosa-group\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Linux is a multi-user operating system that enables file and device access for both the owner and users within a specific group.\\nThe NPU device driver creates a group called\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa\\n         </span>\\n        </code>\\n        and restricts access to NPU devices exclusively to users who are members of the\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa\\n         </span>\\n        </code>\\n        group.\\nTo add a user to a member of\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa\\n         </span>\\n        </code>\\n        group, please run as follows:\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>sudo<span class=\"w\"> </span>usermod<span class=\"w\"> </span>-aG<span class=\"w\"> </span>furiosa<span class=\"w\"> </span>&lt;username&gt;\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        Replace &lt;username&gt; with the name of the user you want to add to the\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa\\n         </span>\\n        </code>\\n        group.\\nFor example, in order to add the current user (i.e.,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          $USER\\n         </span>\\n        </code>\\n        ) to the\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa\\n         </span>\\n        </code>\\n        group, you can run as follows:\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>sudo<span class=\"w\"> </span>usermod<span class=\"w\"> </span>-aG<span class=\"w\"> </span>furiosa<span class=\"w\"> </span><span class=\"nv\">$USER</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        Upon logging out and logging back in, the change to the group membership will take effect.\\n       </p>\\n      </section>\\n      <section id=\"holding-unholding-installed-version\">\\n       <span id=\"holdingaptversion\">\\n       </span>\\n       <h3>\\n        Holding/unholding installed version\\n        <a class=\"headerlink\" href=\"#holding-unholding-installed-version\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Following package installation, in order to maintain a stable operating environment,\\nthere may be a need to hold the installed packages versions. By using the command below,\\nyou will be able to hold the currently installed versions.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>sudo<span class=\"w\"> </span>apt-mark<span class=\"w\"> </span>hold<span class=\"w\"> </span>furiosa-driver-warboy<span class=\"w\"> </span>furiosa-libhal-warboy<span class=\"w\"> </span>furiosa-libnux<span class=\"w\"> </span>libonnxruntime\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        In order to unhold and update the current package versions, designate the package\\nthat you wish to unhold with the command\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          apt-mark\\n         </span>\\n         <span class=\"pre\">\\n          unhold\\n         </span>\\n        </code>\\n        .\\nHere, you can state the name of the package, thereby unholding selectively\\na specific package. In order to show the properties of an already held package,\\nuse the command\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          apt-mark\\n         </span>\\n         <span class=\"pre\">\\n          showhold\\n         </span>\\n        </code>\\n        .\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>sudo<span class=\"w\"> </span>apt-mark<span class=\"w\"> </span>unhold<span class=\"w\"> </span>furiosa-driver-warboy<span class=\"w\"> </span>furiosa-libhal-warboy<span class=\"w\"> </span>furiosa-libnux<span class=\"w\"> </span>libonnxruntime\\n</pre>\\n        </div>\\n       </div>\\n      </section>\\n      <section id=\"installing-a-specific-version\">\\n       <span id=\"installspecificversion\">\\n       </span>\\n       <h3>\\n        Installing a specific version\\n        <a class=\"headerlink\" href=\"#installing-a-specific-version\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        If you need to install a specific version,\\nyou may designate the version that you want and install as follows.\\n       </p>\\n       <ol class=\"arabic simple\">\\n        <li>\\n         <p>\\n          Check available versions through\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            apt\\n           </span>\\n           <span class=\"pre\">\\n            list\\n           </span>\\n          </code>\\n          .\\n         </p>\\n        </li>\\n       </ol>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>sudo<span class=\"w\"> </span>apt<span class=\"w\"> </span>list<span class=\"w\"> </span>-a<span class=\"w\"> </span>furiosa-libnux\\n</pre>\\n        </div>\\n       </div>\\n       <ol class=\"arabic simple\" start=\"2\">\\n        <li>\\n         <p>\\n          State the package name and version as options in the command\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            apt-get\\n           </span>\\n           <span class=\"pre\">\\n            install\\n           </span>\\n          </code>\\n         </p>\\n        </li>\\n       </ol>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>sudo<span class=\"w\"> </span>apt-get<span class=\"w\"> </span>install<span class=\"w\"> </span>-y<span class=\"w\"> </span>furiosa-libnux<span class=\"o\">=</span><span class=\"m\">0</span>.9.1-?\\n</pre>\\n        </div>\\n       </div>\\n      </section>\\n     </section>\\n     <section id=\"npu-firmware-update\">\\n      <span id=\"upgradefirmware\">\\n      </span>\\n      <h2>\\n       NPU Firmware Update\\n       <a class=\"headerlink\" href=\"#npu-firmware-update\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"intro.html\" rel=\"prev\" title=\"FuriosaAI SW Stack Introduction\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"python-sdk.html\" rel=\"next\" title=\"Python SDK installation and user guide\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='8bcf3c20-b1d0-44c9-bedc-9e24a3ce9586', link=Url('https://furiosa-ai.github.io/docs/latest/en/software/cli.html'), name='cli', parent='', child=[], description='\\n\\n\\n* Command Line Tools\\n* [View page source](../_sources/software/cli.rst.txt)\\n\\n---\\n\\n\\n\\nCommand Line Tools\\n[\\uf0c1](#command-line-tools \"Permalink to this heading\")\\n=======================================================================\\n\\nThrough the command line tools, Furiosa SDK provides functions such as monitoring NPU device information, compiling models, and checking compatibility between models and SDKs. This section explains how to install and use each command line tool.\\n\\nfuriosa-toolkit\\n[\\uf0c1](#furiosa-toolkit \"Permalink to this heading\")\\n-----------------------------------------------------------------\\n\\n`furiosa-toolkit`\\nprovides a command line tool that enables users to manage and check the information of NPU devices.\\n\\n### furiosa-toolkit installation [\\uf0c1](#furiosa-toolkit-installation \"Permalink to this heading\")\\n\\nTo use this command line tool, you first need to install the kernel driver as shown in\\n[Driver, Firmware, and Runtime Installation](installation.html#requiredpackages)\\n.\\nSubsequently, follow the instructions below to install furiosa-toolkit.\\n\\nInstallation using APT server\\n\\n\\n```\\nsudo apt-get install -y furiosa-toolkit\\n\\n```\\n\\n\\n\\n\\n\\n### furiosactl [\\uf0c1](#furiosactl \"Permalink to this heading\")\\n\\nThe furiosactl command provides a variety of subcommands and has the ability to obtain information or control the device.\\n\\n```\\nfuriosactl <sub command> [option] ..\\n\\n```\\n\\n\\n#### `furiosactl info` [\\uf0c1](#furiosactl-info \"Permalink to this heading\")\\n\\nAfter installing the kernel driver, you can use the\\n`furiosactl`\\ncommand to check whether the NPU device is recognized.\\nCurrently, this command provides the\\n`furiosactl\\n\\ninfo`\\ncommand to output temperature, power consumption and PCI information of the NPU device.\\nIf the device is not visible with this command after mounting it on the machine,\\n[Driver, Firmware, and Runtime Installation](installation.html#requiredpackages)\\nto install the driver.\\nIf you add the\\n`--full`\\noption to the\\n`info`\\ncommand, you can see the device’s UUID and serial number information together.\\n\\n```\\n$ furiosactl info\\n+------+--------+----------------+-------+--------+--------------+\\n| NPU  | Name   | Firmware       | Temp. | Power  | PCI-BDF      |\\n+------+--------+----------------+-------+--------+--------------+\\n| npu1 | warboy | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 |\\n+------+--------+----------------+-------+--------+--------------+\\n\\n$ furiosactl info --full\\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n| NPU  | Name   | UUID                                 | S/N               | Firmware       | Temp. | Power  | PCI-BDF      | PCI-DEV |\\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n| npu1 | warboy | 00000000-0000-0000-0000-000000000000 | WBYB0000000000000 | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 | 511:0   |\\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n\\n```\\n\\n\\n\\n#### `furiosactl list` [\\uf0c1](#furiosactl-list \"Permalink to this heading\")\\n\\nThe\\n`list`\\nsubcommand provides information about the device files available on the NPU device.\\nYou can also check whether each core present in the NPU is in use or idle.\\n\\n```\\nfuriosactl list\\n+------+------------------------------+-----------------------------------+\\n| NPU  | Cores                        | DEVFILES                          |\\n+------+------------------------------+-----------------------------------+\\n| npu1 | 0 (available), 1 (available) | npu1, npu1pe0, npu1pe1, npu1pe0-1 |\\n+------+------------------------------+-----------------------------------+\\n\\n```\\n\\n\\n\\n#### `furiosactl ps` [\\uf0c1](#furiosactl-ps \"Permalink to this heading\")\\n\\nThe\\n`ps`\\nsubcommand prints information about the OS process currently occupying the NPU device.\\n\\n```\\n$ furiosactl ps\\n+-----------+--------+------------------------------------------------------------+\\n| NPU       | PID    | CMD                                                        |\\n+-----------+--------+------------------------------------------------------------+\\n| npu0pe0-1 | 132529 | /usr/bin/python3 /usr/local/bin/uvicorn image_classify:app |\\n+-----------+--------+------------------------------------------------------------+\\n\\n```\\n\\n\\n\\n#### `furiosactl top` (experimental) [\\uf0c1](#furiosactl-top-experimental \"Permalink to this heading\")\\n\\nThe\\n`top`\\nsubcommand is used to view utilization by NPU unit over time.\\nThe output has the following meaning\\nBy default, utilization is calculated every 1 second, but you can set the calculation interval yourself with the\\n`--interval`\\noption. (unit: ms)\\n\\n\\nfuriosa top fields\\n\\n[\\uf0c1](#id1 \"Permalink to this table\")\\n\\n\\n| Item | Description |\\n| --- | --- |\\n| Datetime | Observation time |\\n| PID | Process ID that is using the NPU |\\n| Device | NPU device in use |\\n| NPU(%) | Percentage of time the NPU was used during the observation time. |\\n| Comp(%) | Percentage of time the NPU was used for computation during the observation time |\\n| I/O (%) | Percentage of time the NPU was used for I/O out of the time the NPU was used |\\n| Command | Executed command line of the process |\\n\\n```\\n$ furiosactl top --interval 200\\nNOTE: furiosa top is under development. Usage and output formats may change.\\nPlease enter Ctrl+C to stop.\\nDatetime                        PID       Device        NPU(%)   Comp(%)   I/O(%)   Command\\n2023-03-21T09:45:56.699483936Z  152616    npu1pe0-1      19.06    100.00     0.00   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n2023-03-21T09:45:56.906443888Z  152616    npu1pe0-1      51.09     93.05     6.95   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n2023-03-21T09:45:57.110489333Z  152616    npu1pe0-1      46.40     97.98     2.02   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n2023-03-21T09:45:57.316060982Z  152616    npu1pe0-1      51.43    100.00     0.00   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n2023-03-21T09:45:57.521140588Z  152616    npu1pe0-1      54.28     94.10     5.90   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n2023-03-21T09:45:57.725910558Z  152616    npu1pe0-1      48.93     98.93     1.07   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n2023-03-21T09:45:57.935041998Z  152616    npu1pe0-1      47.91    100.00     0.00   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n2023-03-21T09:45:58.13929122Z   152616    npu1pe0-1      49.06     94.94     5.06   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n\\n```\\n\\n\\n\\n\\n\\nfuriosa-bench (Benchmark Tool)\\n[\\uf0c1](#furiosa-bench-benchmark-tool \"Permalink to this heading\")\\n---------------------------------------------------------------------------------------------\\n\\n`furiosa-bench`\\ncommand carries out a benchmark with a ONNX or TFLite model and a workload using furiosa-runtime. A benchmark result includes tail latency and QPS.\\n\\nThe arguments of the command are as follows:\\n\\n```\\n$ furiosa-bench --help\\nUSAGE:\\n  furiosa-bench [OPTIONS] <model-path>\\n\\n  OPTIONS:\\n      -b, --batch <number>                       Sets the number of batch size, which should be exponents of two [default: 1]\\n      -o, --output <bench-result-path>           Create json file that has information about the benchmark\\n      -C, --compiler-config <compiler-config>    Sets a file path for compiler configuration (YAML format)\\n      -d, --devices <devices>                    Designates NPU devices to be used (e.g., \"warboy(2)*1\" or \"npu0pe0-1\")\\n      -h, --help                                 Prints help information\\n      -t, --io-threads <number>                  Sets the number of I/O Threads [default: 1]\\n          --duration <min-duration>              Sets the minimum test time in seconds. Both min_query_count and min_duration should be met to finish the test\\n                                                [default: 0]\\n      -n, --queries <min-query-count>            Sets the minimum number of test queries. Both min_query_count and min_duration_ms should be met to finish the\\n                                                test [default: 1]\\n      -T, --trace-output <trace-output>          Sets a file path for profiling result (Chrome Trace JSON format)\\n      -V, --version                              Prints version information\\n      -v, --verbose                              Print verbose log\\n      -w, --workers <number>                     Sets the number of workers [default: 1]\\n          --workload <workload>                  Sets the bench workload which can be either latency-oriented (L) or throughput-oriented (T) [default: L]\\n\\n  ARGS:\\n      <model-path>\\n\\n```\\n\\nMODEL\\\\_PATH is the file path of ONNX, TFLite or ENF (format produced by\\n[furiosa-compiler](compiler.html#compilercli)\\n).\\n\\nThe following is an example usage of furiosa-bench without an output path option (i.e.,\\n`--output`\\n):\\n\\n```\\n$ furiosa-bench mnist-8.onnx --workload L -n 1000 -w 8 -t 2\\n\\n  ======================================================================\\n  This benchmark was executed with latency-workload which prioritizes latency of individual queries over throughput.\\n  1000 queries executed with batch size 1\\n  Latency stats are as follows\\n  QPS(Throughput): 34.40/s\\n\\n  Per-query latency:\\n  Min latency (us)    : 8399\\n  Max latency (us)    : 307568\\n  Mean latency (us)   : 29040\\n  50th percentile (us): 19329\\n  95th percentile (us): 62797\\n  99th percentile (us): 79874\\n  99th percentile (us): 307568\\n\\n```\\n\\nIf an output path is specified, furiosa-bench will save a JSON document as follows:\\n\\n```\\n$ furiosa-bench mnist-8.onnx --workload L -n 1000 -w 8 -t 2 -o mnist.json\\n$ cat mnist.json\\n\\n  {\\n      \"model_data\": {\\n          \"path\": \"./mnist-8.onnx\",\\n          \"md5\": \"d7cd24a0a76cd492f31065301d468c3d  ./mnist-8.onnx\"\\n      },\\n      \"compiler_version\": \"0.10.0-dev (rev: 2d862de8a built_at: 2023-07-13T20:05:04Z)\",\\n      \"hal_version\": \"Version: 0.12.0-2+nightly-230716\",\\n      \"git_revision\": \"fe6f77a\",\\n      \"result\": {\\n          \"mode\": \"Latency\",\\n          \"total run time\": \"30025 us\",\\n          \"total num queries\": 1000,\\n          \"batch size\": 1,\\n          \"qps\": \"33.31/s\",\\n          \"latency stats\": {\\n              \"min\": \"8840 us\",\\n              \"max\": \"113254 us\",\\n              \"mean\": \"29989 us\",\\n              \"50th percentile\": \"18861 us\",\\n              \"95th percentile\": \"64927 us\",\\n              \"99th percentile\": \"87052 us\",\\n              \"99.9th percentile\": \"113254 us\"\\n          }\\n      }\\n  }\\n\\n```\\n\\n\\n\\nfuriosa\\n[\\uf0c1](#furiosa \"Permalink to this heading\")\\n-------------------------------------------------\\n\\nThe\\n`furiosa`\\ncommand is a meta-command line tool that can be used by installing the\\nPython SDK <PythonSDK>\\n\\n.\\nAdditional subcommands are also added when the extension package is installed.\\n\\nIf the Python execution environment is not prepared, refer to\\n[Python execution environment setup](python-sdk.html#setuppython)\\n.\\n\\nInstalling command line tool.\\n\\n```\\n$ pip install furiosa-sdk\\n\\n```\\n\\nVerifying installation.\\n\\n```\\n$ furiosa compile --version\\nlibnpu.so --- v2.0, built @ fe1fca3\\n0.5.0 (rev: 49b97492a built at 2021-12-07 04:07:08) (wrapper: None)\\n\\n```\\n\\n\\n### furiosa compile [\\uf0c1](#furiosa-compile \"Permalink to this heading\")\\n\\nThe\\n`compile`\\ncommand compiles models such as\\n[ONNX](https://onnx.ai/)\\nand\\n[TFLite](https://www.tensorflow.org/lite)\\n, generating programs that utilize FuriosaAI NPU.\\n\\nDetailed explanations and options can be found in the\\n[furiosa-compiler](compiler.html#compilercli)\\npage.\\n\\n\\n### furiosa litmus (Model Compatibility Checker) [\\uf0c1](#furiosa-litmus-model-compatibility-checker \"Permalink to this heading\")\\n\\nThe\\n`litmus`\\nis a tool to check quickly if an\\n[ONNX](https://onnx.ai/)\\nmodel can work normally with Furiosa SDK using NPU.\\n`litmus`\\ngoes through all usage steps of Furiosa SDK, including quantization, compilation, and inferences on FuriosaAI NPU.\\n`litmus`\\nis also a useful bug reporting tool. If you specify\\n`--dump`\\noption,\\n`litmus`\\nwill collect logs and environment information and dump an archive file.\\nThe archive file can be used to report issues.\\n\\nThe steps executed by\\n`litmus`\\ncommand are as follows.\\n\\n> * Step1: Load an input model and check it is a valid model.\\n> * Step2: Quantize the model with random calibration.\\n> * Step3: Compile the quantized model.\\n> * Step4: Inference the compiled model using\\n>   `furiosa-bench`\\n>   . This step is skipped if\\n>   `furiosa-bench`\\n>   was not installed.\\n\\nUsage:\\n\\n```\\nfuriosa-litmus [-h] [--dump OUTPUT_PREFIX] [--skip-quantization] [--target-npu TARGET_NPU] [-v] model_path\\n\\n```\\n\\nA simple example using\\n`litmus`\\ncommand is as follows.\\n\\n```\\n$ furiosa litmus model.onnx\\nlibfuriosa_hal.so --- v0.11.0, built @ 43c901f\\nINFO:furiosa.common.native:loaded native library libfuriosa_compiler.so.0.10.0 (0.10.0-dev d7548b7f6)\\nfuriosa-quantizer 0.10.0 (rev. 9ecebb6) furiosa-litmus 0.10.0 (rev. 9ecebb6)\\n[Step 1] Checking if the model can be loaded and optimized ...\\n[Step 1] Passed\\n[Step 2] Checking if the model can be quantized ...\\n[Step 2] Passed\\n[Step 3] Checking if the model can be compiled for the NPU family [warboy-2pe] ...\\n[1/6] 🔍   Compiling from onnx to dfg\\nDone in 0.09272794s\\n[2/6] 🔍   Compiling from dfg to ldfg\\n▪▪▪▪▪ [1/3] Splitting graph(LAS)...Done in 9.034934s\\n▪▪▪▪▪ [2/3] Lowering graph(LAS)...Done in 20.140083s\\n▪▪▪▪▪ [3/3] Optimizing graph...Done in 0.019548794s\\nDone in 29.196825s\\n[3/6] 🔍   Compiling from ldfg to cdfg\\nDone in 0.001701888s\\n[4/6] 🔍   Compiling from cdfg to gir\\nDone in 0.015205072s\\n[5/6] 🔍   Compiling from gir to lir\\nDone in 0.0038304s\\n[6/6] 🔍   Compiling from lir to enf\\nDone in 0.020943863s\\n✨  Finished in 29.331545s\\n[Step 3] Passed\\n[Step 4] Perform inference once for data collection... (Optional)\\n✨  Finished in 0.000001198s\\n======================================================================\\nThis benchmark was executed with latency-workload which prioritizes latency of individual queries over throughput.\\n1 queries executed with batch size 1\\nLatency stats are as follows\\nQPS(Throughput): 125.00/s\\n\\nPer-query latency:\\nMin latency (us)    : 7448\\nMax latency (us)    : 7448\\nMean latency (us)   : 7448\\n50th percentile (us): 7448\\n95th percentile (us): 7448\\n99th percentile (us): 7448\\n99th percentile (us): 7448\\n[Step 4] Finished\\n\\n```\\n\\nIf you have quantized model already, you can skip Step1 and Step2 with\\n`--skip-quantization`\\noption.\\n\\n```\\n$ furiosa litmus --skip-quantization quantized-model.onnx\\nlibfuriosa_hal.so --- v0.11.0, built @ 43c901f\\nINFO:furiosa.common.native:loaded native library libfuriosa_compiler.so.0.10.0 (0.10.0-dev d7548b7f6)\\nfuriosa-quantizer 0.10.0 (rev. 9ecebb6) furiosa-litmus 0.10.0 (rev. 9ecebb6)\\n[Step 1] Skip model loading and optimization\\n[Step 2] Skip model quantization\\n[Step 1 & Step 2] Load quantized model ...\\n[Step 3] Checking if the model can be compiled for the NPU family [warboy-2pe] ...\\n...\\n\\n```\\n\\nYou can use the\\n`--dump\\n\\n<path>`\\noption to create a\\n<path>-<unix\\\\_epoch>.zip\\n\\nfile that contains metadata necessary for analysis, such as compilation logs, runtime logs, software versions, and execution environments.\\nIf you have any problems, you can get support through\\n[FuriosaAI customer service center](https://furiosa-ai.atlassian.net/servicedesk/customer/portals)\\nwith this zip file.\\n\\n```\\n$ furiosa litmus --dump archive model.onnx\\nlibfuriosa_hal.so --- v0.11.0, built @ 43c901f\\nINFO:furiosa.common.native:loaded native library libfuriosa_compiler.so.0.10.0 (0.10.0-dev d7548b7f6)\\nfuriosa-quantizer 0.10.0 (rev. 9ecebb6) furiosa-litmus 0.10.0 (rev. 9ecebb6)\\n[Step 1] Checking if the model can be loaded and optimized ...\\n[Step 1] Passed\\n...\\n\\n$ zipinfo -1 archive-1690438803.zip\\narchive-16904388032l4hoi3h/meta.yaml\\narchive-16904388032l4hoi3h/compiler/compiler.log\\narchive-16904388032l4hoi3h/compiler/memory-analysis.html\\narchive-16904388032l4hoi3h/compiler/model.dot\\narchive-16904388032l4hoi3h/runtime/trace.json\\n\\n```\\n\\n\\n\\n\\n\\n\\n\\n[Previous](c-sdk.html \"C SDK installation and user guide\")\\n[Next](compiler.html \"Compiler\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Command Line Tools * [View page source](../_sources/software/cli.rst.txt)\\n---\\nCommand Line Tools [\\uf0c1](#command-line-tools \"Permalink to this heading\") =======================================================================\\nThrough the command line tools, Furiosa SDK provides functions such as monitoring NPU device information, compiling models, and checking compatibility between models and SDKs. This section explains how to install and use each command line tool.\\nfuriosa-toolkit [\\uf0c1](#furiosa-toolkit \"Permalink to this heading\") -----------------------------------------------------------------  `furiosa-toolkit` provides a command line tool that enables users to manage and check the information of NPU devices.\\n### furiosa-toolkit installation [\\uf0c1](#furiosa-toolkit-installation \"Permalink to this heading\")\\nTo use this command line tool, you first need to install the kernel driver as shown in [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) .\\nSubsequently, follow the instructions below to install furiosa-toolkit.\\nInstallation using APT server\\n``` sudo apt-get install -y furiosa-toolkit\\n```\\n### furiosactl [\\uf0c1](#furiosactl \"Permalink to this heading\")\\nThe furiosactl command provides a variety of subcommands and has the ability to obtain information or control the device.\\n``` furiosactl <sub command> [option] ..\\n```\\n#### `furiosactl info` [\\uf0c1](#furiosactl-info \"Permalink to this heading\")\\nAfter installing the kernel driver, you can use the `furiosactl` command to check whether the NPU device is recognized. Currently, this command provides the `furiosactl\\ninfo` command to output temperature, power consumption and PCI information of the NPU device. If the device is not visible with this command after mounting it on the machine, [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) to install the driver. If you add the `--full` option to the `info` command, you can see the device’s UUID and serial number information together.\\n``` $ furiosactl info +------+--------+----------------+-------+--------+--------------+ | NPU  | Name   | Firmware       | Temp. | Power  | PCI-BDF      | +------+--------+----------------+-------+--------+--------------+ | npu1 | warboy | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 | +------+--------+----------------+-------+--------+--------------+\\n$ furiosactl info --full +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+ | NPU  | Name   | UUID                                 | S/N               | Firmware       | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+ | npu1 | warboy | 00000000-0000-0000-0000-000000000000 | WBYB0000000000000 | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 | 511:0   | +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n```\\n#### `furiosactl list` [\\uf0c1](#furiosactl-list \"Permalink to this heading\")\\nThe `list` subcommand provides information about the device files available on the NPU device. You can also check whether each core present in the NPU is in use or idle.\\n``` furiosactl list +------+------------------------------+-----------------------------------+ | NPU  | Cores                        | DEVFILES                          | +------+------------------------------+-----------------------------------+ | npu1 | 0 (available), 1 (available) | npu1, npu1pe0, npu1pe1, npu1pe0-1 | +------+------------------------------+-----------------------------------+\\n```\\n#### `furiosactl ps` [\\uf0c1](#furiosactl-ps \"Permalink to this heading\")\\nThe `ps` subcommand prints information about the OS process currently occupying the NPU device.\\n``` $ furiosactl ps +-----------+--------+------------------------------------------------------------+ | NPU       | PID    | CMD                                                        | +-----------+--------+------------------------------------------------------------+ | npu0pe0-1 | 132529 | /usr/bin/python3 /usr/local/bin/uvicorn image_classify:app | +-----------+--------+------------------------------------------------------------+\\n```\\n#### `furiosactl top` (experimental) [\\uf0c1](#furiosactl-top-experimental \"Permalink to this heading\")\\nThe `top` subcommand is used to view utilization by NPU unit over time. The output has the following meaning By default, utilization is calculated every 1 second, but you can set the calculation interval yourself with the `--interval` option. (unit: ms)\\nfuriosa top fields\\n[\\uf0c1](#id1 \"Permalink to this table\")\\n| Item | Description | | --- | --- | | Datetime | Observation time | | PID | Process ID that is using the NPU | | Device | NPU device in use | | NPU(%) | Percentage of time the NPU was used during the observation time. | | Comp(%) | Percentage of time the NPU was used for computation during the observation time | | I/O (%) | Percentage of time the NPU was used for I/O out of the time the NPU was used | | Command | Executed command line of the process |\\n``` $ furiosactl top --interval 200 NOTE: furiosa top is under development. Usage and output formats may change. Please enter Ctrl+C to stop. Datetime                        PID       Device        NPU(%)   Comp(%)   I/O(%)   Command 2023-03-21T09:45:56.699483936Z  152616    npu1pe0-1      19.06    100.00     0.00   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:56.906443888Z  152616    npu1pe0-1      51.09     93.05     6.95   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.110489333Z  152616    npu1pe0-1      46.40     97.98     2.02   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.316060982Z  152616    npu1pe0-1      51.43    100.00     0.00   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.521140588Z  152616    npu1pe0-1      54.28     94.10     5.90   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.725910558Z  152616    npu1pe0-1      48.93     98.93     1.07   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.935041998Z  152616    npu1pe0-1      47.91    100.00     0.00   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:58.13929122Z   152616    npu1pe0-1      49.06     94.94     5.06   ./npu_runtime_test -n 10000 results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n```\\nfuriosa-bench (Benchmark Tool) [\\uf0c1](#furiosa-bench-benchmark-tool \"Permalink to this heading\") ---------------------------------------------------------------------------------------------  `furiosa-bench` command carries out a benchmark with a ONNX or TFLite model and a workload using furiosa-runtime. A benchmark result includes tail latency and QPS.\\nThe arguments of the command are as follows:\\n``` $ furiosa-bench --help USAGE:   furiosa-bench [OPTIONS] <model-path>\\n  OPTIONS:       -b, --batch <number>                       Sets the number of batch size, which should be exponents of two [default: 1]       -o, --output <bench-result-path>           Create json file that has information about the benchmark       -C, --compiler-config <compiler-config>    Sets a file path for compiler configuration (YAML format)       -d, --devices <devices>                    Designates NPU devices to be used (e.g., \"warboy(2)*1\" or \"npu0pe0-1\")       -h, --help                                 Prints help information       -t, --io-threads <number>                  Sets the number of I/O Threads [default: 1]           --duration <min-duration>              Sets the minimum test time in seconds. Both min_query_count and min_duration should be met to finish the test                                                 [default: 0]       -n, --queries <min-query-count>            Sets the minimum number of test queries. Both min_query_count and min_duration_ms should be met to finish the                                                 test [default: 1]       -T, --trace-output <trace-output>          Sets a file path for profiling result (Chrome Trace JSON format)       -V, --version                              Prints version information       -v, --verbose                              Print verbose log       -w, --workers <number>                     Sets the number of workers [default: 1]           --workload <workload>                  Sets the bench workload which can be either latency-oriented (L) or throughput-oriented (T) [default: L]\\n  ARGS:       <model-path>\\n```\\nMODEL\\\\_PATH is the file path of ONNX, TFLite or ENF (format produced by [furiosa-compiler](compiler.html#compilercli) ).\\nThe following is an example usage of furiosa-bench without an output path option (i.e., `--output` ):\\n``` $ furiosa-bench mnist-8.onnx --workload L -n 1000 -w 8 -t 2\\n  ======================================================================   This benchmark was executed with latency-workload which prioritizes latency of individual queries over throughput.   1000 queries executed with batch size 1   Latency stats are as follows   QPS(Throughput): 34.40/s\\n  Per-query latency:   Min latency (us)    : 8399   Max latency (us)    : 307568   Mean latency (us)   : 29040   50th percentile (us): 19329   95th percentile (us): 62797   99th percentile (us): 79874   99th percentile (us): 307568\\n```\\nIf an output path is specified, furiosa-bench will save a JSON document as follows:\\n``` $ furiosa-bench mnist-8.onnx --workload L -n 1000 -w 8 -t 2 -o mnist.json $ cat mnist.json\\n  {       \"model_data\": {           \"path\": \"./mnist-8.onnx\",           \"md5\": \"d7cd24a0a76cd492f31065301d468c3d  ./mnist-8.onnx\"       },       \"compiler_version\": \"0.10.0-dev (rev: 2d862de8a built_at: 2023-07-13T20:05:04Z)\",       \"hal_version\": \"Version: 0.12.0-2+nightly-230716\",       \"git_revision\": \"fe6f77a\",       \"result\": {           \"mode\": \"Latency\",           \"total run time\": \"30025 us\",           \"total num queries\": 1000,           \"batch size\": 1,           \"qps\": \"33.31/s\",           \"latency stats\": {               \"min\": \"8840 us\",               \"max\": \"113254 us\",               \"mean\": \"29989 us\",               \"50th percentile\": \"18861 us\",               \"95th percentile\": \"64927 us\",               \"99th percentile\": \"87052 us\",               \"99.9th percentile\": \"113254 us\"           }       }   }\\n```\\nfuriosa [\\uf0c1](#furiosa \"Permalink to this heading\") -------------------------------------------------\\nThe `furiosa` command is a meta-command line tool that can be used by installing the Python SDK <PythonSDK>\\n. Additional subcommands are also added when the extension package is installed.\\nIf the Python execution environment is not prepared, refer to [Python execution environment setup](python-sdk.html#setuppython) .\\nInstalling command line tool.\\n``` $ pip install furiosa-sdk\\n```\\nVerifying installation.\\n``` $ furiosa compile --version libnpu.so --- v2.0, built @ fe1fca3 0.5.0 (rev: 49b97492a built at 2021-12-07 04:07:08) (wrapper: None)\\n```\\n### furiosa compile [\\uf0c1](#furiosa-compile \"Permalink to this heading\")\\nThe `compile` command compiles models such as [ONNX](https://onnx.ai/) and [TFLite](https://www.tensorflow.org/lite) , generating programs that utilize FuriosaAI NPU.\\nDetailed explanations and options can be found in the [furiosa-compiler](compiler.html#compilercli) page.\\n### furiosa litmus (Model Compatibility Checker) [\\uf0c1](#furiosa-litmus-model-compatibility-checker \"Permalink to this heading\")\\nThe `litmus` is a tool to check quickly if an [ONNX](https://onnx.ai/) model can work normally with Furiosa SDK using NPU. `litmus` goes through all usage steps of Furiosa SDK, including quantization, compilation, and inferences on FuriosaAI NPU. `litmus` is also a useful bug reporting tool. If you specify `--dump` option, `litmus` will collect logs and environment information and dump an archive file. The archive file can be used to report issues.\\nThe steps executed by `litmus` command are as follows.\\n> * Step1: Load an input model and check it is a valid model. > * Step2: Quantize the model with random calibration. > * Step3: Compile the quantized model. > * Step4: Inference the compiled model using >   `furiosa-bench` >   . This step is skipped if >   `furiosa-bench` >   was not installed.\\nUsage:\\n``` furiosa-litmus [-h] [--dump OUTPUT_PREFIX] [--skip-quantization] [--target-npu TARGET_NPU] [-v] model_path\\n```\\nA simple example using `litmus` command is as follows.\\n``` $ furiosa litmus model.onnx libfuriosa_hal.so --- v0.11.0, built @ 43c901f INFO:furiosa.common.native:loaded native library libfuriosa_compiler.so.0.10.0 (0.10.0-dev d7548b7f6) furiosa-quantizer 0.10.0 (rev. 9ecebb6) furiosa-litmus 0.10.0 (rev. 9ecebb6) [Step 1] Checking if the model can be loaded and optimized ... [Step 1] Passed [Step 2] Checking if the model can be quantized ... [Step 2] Passed [Step 3] Checking if the model can be compiled for the NPU family [warboy-2pe] ... [1/6] 🔍   Compiling from onnx to dfg Done in 0.09272794s [2/6] 🔍   Compiling from dfg to ldfg ▪▪▪▪▪ [1/3] Splitting graph(LAS)...Done in 9.034934s ▪▪▪▪▪ [2/3] Lowering graph(LAS)...Done in 20.140083s ▪▪▪▪▪ [3/3] Optimizing graph...Done in 0.019548794s Done in 29.196825s [3/6] 🔍   Compiling from ldfg to cdfg Done in 0.001701888s [4/6] 🔍   Compiling from cdfg to gir Done in 0.015205072s [5/6] 🔍   Compiling from gir to lir Done in 0.0038304s [6/6] 🔍   Compiling from lir to enf Done in 0.020943863s ✨  Finished in 29.331545s [Step 3] Passed [Step 4] Perform inference once for data collection... (Optional) ✨  Finished in 0.000001198s ====================================================================== This benchmark was executed with latency-workload which prioritizes latency of individual queries over throughput. 1 queries executed with batch size 1 Latency stats are as follows QPS(Throughput): 125.00/s\\nPer-query latency: Min latency (us)    : 7448 Max latency (us)    : 7448 Mean latency (us)   : 7448 50th percentile (us): 7448 95th percentile (us): 7448 99th percentile (us): 7448 99th percentile (us): 7448 [Step 4] Finished\\n```\\nIf you have quantized model already, you can skip Step1 and Step2 with `--skip-quantization` option.\\n``` $ furiosa litmus --skip-quantization quantized-model.onnx libfuriosa_hal.so --- v0.11.0, built @ 43c901f INFO:furiosa.common.native:loaded native library libfuriosa_compiler.so.0.10.0 (0.10.0-dev d7548b7f6) furiosa-quantizer 0.10.0 (rev. 9ecebb6) furiosa-litmus 0.10.0 (rev. 9ecebb6) [Step 1] Skip model loading and optimization [Step 2] Skip model quantization [Step 1 & Step 2] Load quantized model ... [Step 3] Checking if the model can be compiled for the NPU family [warboy-2pe] ... ...\\n```\\nYou can use the `--dump\\n<path>` option to create a <path>-<unix\\\\_epoch>.zip\\nfile that contains metadata necessary for analysis, such as compilation logs, runtime logs, software versions, and execution environments. If you have any problems, you can get support through [FuriosaAI customer service center](https://furiosa-ai.atlassian.net/servicedesk/customer/portals) with this zip file.\\n``` $ furiosa litmus --dump archive model.onnx libfuriosa_hal.so --- v0.11.0, built @ 43c901f INFO:furiosa.common.native:loaded native library libfuriosa_compiler.so.0.10.0 (0.10.0-dev d7548b7f6) furiosa-quantizer 0.10.0 (rev. 9ecebb6) furiosa-litmus 0.10.0 (rev. 9ecebb6) [Step 1] Checking if the model can be loaded and optimized ... [Step 1] Passed ...\\n$ zipinfo -1 archive-1690438803.zip archive-16904388032l4hoi3h/meta.yaml archive-16904388032l4hoi3h/compiler/compiler.log archive-16904388032l4hoi3h/compiler/memory-analysis.html archive-16904388032l4hoi3h/compiler/model.dot archive-16904388032l4hoi3h/runtime/trace.json\\n```\\n[Previous](c-sdk.html \"C SDK installation and user guide\") [Next](compiler.html \"Compiler\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Command Line Tools\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/cli.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"command-line-tools\">\\n     <h1>\\n      Command Line Tools\\n      <a class=\"headerlink\" href=\"#command-line-tools\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      Through the command line tools, Furiosa SDK provides functions such as monitoring NPU device information, compiling models, and checking compatibility between models and SDKs. This section explains how to install and use each command line tool.\\n     </p>\\n     <section id=\"furiosa-toolkit\">\\n      <span id=\"toolkit\">\\n      </span>\\n      <h2>\\n       furiosa-toolkit\\n       <a class=\"headerlink\" href=\"#furiosa-toolkit\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         furiosa-toolkit\\n        </span>\\n       </code>\\n       provides a command line tool that enables users to manage and check the information of NPU devices.\\n      </p>\\n      <section id=\"furiosa-toolkit-installation\">\\n       <h3>\\n        furiosa-toolkit installation\\n        <a class=\"headerlink\" href=\"#furiosa-toolkit-installation\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        To use this command line tool, you first need to install the kernel driver as shown in\\n        <a class=\"reference internal\" href=\"installation.html#requiredpackages\">\\n         <span class=\"std std-ref\">\\n          Driver, Firmware, and Runtime Installation\\n         </span>\\n        </a>\\n        .\\nSubsequently, follow the instructions below to install furiosa-toolkit.\\n       </p>\\n       <div class=\"sphinx-tabs docutils container\">\\n        <div aria-label=\"Tabbed content\" role=\"tablist\">\\n         <button aria-controls=\"panel-0-0-0\" aria-selected=\"true\" class=\"sphinx-tabs-tab\" id=\"tab-0-0-0\" name=\"0-0\" role=\"tab\" tabindex=\"0\">\\n          Installation using APT server\\n         </button>\\n        </div>\\n        <div aria-labelledby=\"tab-0-0-0\" class=\"sphinx-tabs-panel\" id=\"panel-0-0-0\" name=\"0-0\" role=\"tabpanel\" tabindex=\"0\">\\n         <div class=\"highlight-sh notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span>sudo<span class=\"w\"> </span>apt-get<span class=\"w\"> </span>install<span class=\"w\"> </span>-y<span class=\"w\"> </span>furiosa-toolkit\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </div>\\n      </section>\\n      <section id=\"furiosactl\">\\n       <h3>\\n        furiosactl\\n        <a class=\"headerlink\" href=\"#furiosactl\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        The furiosactl command provides a variety of subcommands and has the ability to obtain information or control the device.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>furiosactl<span class=\"w\"> </span>&lt;sub<span class=\"w\"> </span>command&gt;<span class=\"w\"> </span><span class=\"o\">[</span>option<span class=\"o\">]</span><span class=\"w\"> </span>..\\n</pre>\\n        </div>\\n       </div>\\n       <section id=\"furiosactl-info\">\\n        <h4>\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           furiosactl\\n          </span>\\n          <span class=\"pre\">\\n           info\\n          </span>\\n         </code>\\n         <a class=\"headerlink\" href=\"#furiosactl-info\" title=\"Permalink to this heading\">\\n          \\uf0c1\\n         </a>\\n        </h4>\\n        <p>\\n         After installing the kernel driver, you can use the\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           furiosactl\\n          </span>\\n         </code>\\n         command to check whether the NPU device is recognized.\\nCurrently, this command provides the\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           furiosactl\\n          </span>\\n          <span class=\"pre\">\\n           info\\n          </span>\\n         </code>\\n         command to output temperature, power consumption and PCI information of the NPU device.\\nIf the device is not visible with this command after mounting it on the machine,\\n         <a class=\"reference internal\" href=\"installation.html#requiredpackages\">\\n          <span class=\"std std-ref\">\\n           Driver, Firmware, and Runtime Installation\\n          </span>\\n         </a>\\n         to install the driver.\\nIf you add the\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           --full\\n          </span>\\n         </code>\\n         option to the\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           info\\n          </span>\\n         </code>\\n         command, you can see the device’s UUID and serial number information together.\\n        </p>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>$<span class=\"w\"> </span>furiosactl<span class=\"w\"> </span>info\\n+------+--------+----------------+-------+--------+--------------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>NPU<span class=\"w\">  </span><span class=\"p\">|</span><span class=\"w\"> </span>Name<span class=\"w\">   </span><span class=\"p\">|</span><span class=\"w\"> </span>Firmware<span class=\"w\">       </span><span class=\"p\">|</span><span class=\"w\"> </span>Temp.<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>Power<span class=\"w\">  </span><span class=\"p\">|</span><span class=\"w\"> </span>PCI-BDF<span class=\"w\">      </span><span class=\"p\">|</span>\\n+------+--------+----------------+-------+--------+--------------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>npu1<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>warboy<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">1</span>.6.0,<span class=\"w\"> </span>3c10fd3<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\">  </span><span class=\"m\">54</span>°C<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>.99<span class=\"w\"> </span>W<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0000</span>:44:00.0<span class=\"w\"> </span><span class=\"p\">|</span>\\n+------+--------+----------------+-------+--------+--------------+\\n\\n$<span class=\"w\"> </span>furiosactl<span class=\"w\"> </span>info<span class=\"w\"> </span>--full\\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>NPU<span class=\"w\">  </span><span class=\"p\">|</span><span class=\"w\"> </span>Name<span class=\"w\">   </span><span class=\"p\">|</span><span class=\"w\"> </span>UUID<span class=\"w\">                                 </span><span class=\"p\">|</span><span class=\"w\"> </span>S/N<span class=\"w\">               </span><span class=\"p\">|</span><span class=\"w\"> </span>Firmware<span class=\"w\">       </span><span class=\"p\">|</span><span class=\"w\"> </span>Temp.<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>Power<span class=\"w\">  </span><span class=\"p\">|</span><span class=\"w\"> </span>PCI-BDF<span class=\"w\">      </span><span class=\"p\">|</span><span class=\"w\"> </span>PCI-DEV<span class=\"w\"> </span><span class=\"p\">|</span>\\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>npu1<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>warboy<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">00000000</span>-0000-0000-0000-000000000000<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>WBYB0000000000000<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">1</span>.6.0,<span class=\"w\"> </span>3c10fd3<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\">  </span><span class=\"m\">54</span>°C<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span>.99<span class=\"w\"> </span>W<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0000</span>:44:00.0<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">511</span>:0<span class=\"w\">   </span><span class=\"p\">|</span>\\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n</pre>\\n         </div>\\n        </div>\\n       </section>\\n       <section id=\"furiosactl-list\">\\n        <h4>\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           furiosactl\\n          </span>\\n          <span class=\"pre\">\\n           list\\n          </span>\\n         </code>\\n         <a class=\"headerlink\" href=\"#furiosactl-list\" title=\"Permalink to this heading\">\\n          \\uf0c1\\n         </a>\\n        </h4>\\n        <p>\\n         The\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           list\\n          </span>\\n         </code>\\n         subcommand provides information about the device files available on the NPU device.\\nYou can also check whether each core present in the NPU is in use or idle.\\n        </p>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>furiosactl<span class=\"w\"> </span>list\\n+------+------------------------------+-----------------------------------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>NPU<span class=\"w\">  </span><span class=\"p\">|</span><span class=\"w\"> </span>Cores<span class=\"w\">                        </span><span class=\"p\">|</span><span class=\"w\"> </span>DEVFILES<span class=\"w\">                          </span><span class=\"p\">|</span>\\n+------+------------------------------+-----------------------------------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>npu1<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">0</span><span class=\"w\"> </span><span class=\"o\">(</span>available<span class=\"o\">)</span>,<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span><span class=\"o\">(</span>available<span class=\"o\">)</span><span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>npu1,<span class=\"w\"> </span>npu1pe0,<span class=\"w\"> </span>npu1pe1,<span class=\"w\"> </span>npu1pe0-1<span class=\"w\"> </span><span class=\"p\">|</span>\\n+------+------------------------------+-----------------------------------+\\n</pre>\\n         </div>\\n        </div>\\n       </section>\\n       <section id=\"furiosactl-ps\">\\n        <h4>\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           furiosactl\\n          </span>\\n          <span class=\"pre\">\\n           ps\\n          </span>\\n         </code>\\n         <a class=\"headerlink\" href=\"#furiosactl-ps\" title=\"Permalink to this heading\">\\n          \\uf0c1\\n         </a>\\n        </h4>\\n        <p>\\n         The\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           ps\\n          </span>\\n         </code>\\n         subcommand prints information about the OS process currently occupying the NPU device.\\n        </p>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>$<span class=\"w\"> </span>furiosactl<span class=\"w\"> </span>ps\\n+-----------+--------+------------------------------------------------------------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>NPU<span class=\"w\">       </span><span class=\"p\">|</span><span class=\"w\"> </span>PID<span class=\"w\">    </span><span class=\"p\">|</span><span class=\"w\"> </span>CMD<span class=\"w\">                                                        </span><span class=\"p\">|</span>\\n+-----------+--------+------------------------------------------------------------+\\n<span class=\"p\">|</span><span class=\"w\"> </span>npu0pe0-1<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span><span class=\"m\">132529</span><span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>/usr/bin/python3<span class=\"w\"> </span>/usr/local/bin/uvicorn<span class=\"w\"> </span>image_classify:app<span class=\"w\"> </span><span class=\"p\">|</span>\\n+-----------+--------+------------------------------------------------------------+\\n</pre>\\n         </div>\\n        </div>\\n       </section>\\n       <section id=\"furiosactl-top-experimental\">\\n        <h4>\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           furiosactl\\n          </span>\\n          <span class=\"pre\">\\n           top\\n          </span>\\n         </code>\\n         (experimental)\\n         <a class=\"headerlink\" href=\"#furiosactl-top-experimental\" title=\"Permalink to this heading\">\\n          \\uf0c1\\n         </a>\\n        </h4>\\n        <p>\\n         The\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           top\\n          </span>\\n         </code>\\n         subcommand is used to view utilization by NPU unit over time.\\nThe output has the following meaning\\nBy default, utilization is calculated every 1 second, but you can set the calculation interval yourself with the\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           --interval\\n          </span>\\n         </code>\\n         option. (unit: ms)\\n        </p>\\n        <table class=\"docutils align-default\" id=\"id1\">\\n         <caption>\\n          <span class=\"caption-text\">\\n           furiosa top fields\\n          </span>\\n          <a class=\"headerlink\" href=\"#id1\" title=\"Permalink to this table\">\\n           \\uf0c1\\n          </a>\\n         </caption>\\n         <colgroup>\\n          <col style=\"width: 20%\"/>\\n          <col style=\"width: 80%\"/>\\n         </colgroup>\\n         <thead>\\n          <tr class=\"row-odd\">\\n           <th class=\"head\">\\n            <p>\\n             Item\\n            </p>\\n           </th>\\n           <th class=\"head\">\\n            <p>\\n             Description\\n            </p>\\n           </th>\\n          </tr>\\n         </thead>\\n         <tbody>\\n          <tr class=\"row-even\">\\n           <td>\\n            <p>\\n             Datetime\\n            </p>\\n           </td>\\n           <td>\\n            <p>\\n             Observation time\\n            </p>\\n           </td>\\n          </tr>\\n          <tr class=\"row-odd\">\\n           <td>\\n            <p>\\n             PID\\n            </p>\\n           </td>\\n           <td>\\n            <p>\\n             Process ID that is using the NPU\\n            </p>\\n           </td>\\n          </tr>\\n          <tr class=\"row-even\">\\n           <td>\\n            <p>\\n             Device\\n            </p>\\n           </td>\\n           <td>\\n            <p>\\n             NPU device in use\\n            </p>\\n           </td>\\n          </tr>\\n          <tr class=\"row-odd\">\\n           <td>\\n            <p>\\n             NPU(%)\\n            </p>\\n           </td>\\n           <td>\\n            <p>\\n             Percentage of time the NPU was used during the observation time.\\n            </p>\\n           </td>\\n          </tr>\\n          <tr class=\"row-even\">\\n           <td>\\n            <p>\\n             Comp(%)\\n            </p>\\n           </td>\\n           <td>\\n            <p>\\n             Percentage of time the NPU was used for computation during the observation time\\n            </p>\\n           </td>\\n          </tr>\\n          <tr class=\"row-odd\">\\n           <td>\\n            <p>\\n             I/O (%)\\n            </p>\\n           </td>\\n           <td>\\n            <p>\\n             Percentage of time the NPU was used for I/O out of the time the NPU was used\\n            </p>\\n           </td>\\n          </tr>\\n          <tr class=\"row-even\">\\n           <td>\\n            <p>\\n             Command\\n            </p>\\n           </td>\\n           <td>\\n            <p>\\n             Executed command line of the process\\n            </p>\\n           </td>\\n          </tr>\\n         </tbody>\\n        </table>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>$<span class=\"w\"> </span>furiosactl<span class=\"w\"> </span>top<span class=\"w\"> </span>--interval<span class=\"w\"> </span><span class=\"m\">200</span>\\nNOTE:<span class=\"w\"> </span>furiosa<span class=\"w\"> </span>top<span class=\"w\"> </span>is<span class=\"w\"> </span>under<span class=\"w\"> </span>development.<span class=\"w\"> </span>Usage<span class=\"w\"> </span>and<span class=\"w\"> </span>output<span class=\"w\"> </span>formats<span class=\"w\"> </span>may<span class=\"w\"> </span>change.\\nPlease<span class=\"w\"> </span>enter<span class=\"w\"> </span>Ctrl+C<span class=\"w\"> </span>to<span class=\"w\"> </span>stop.\\nDatetime<span class=\"w\">                        </span>PID<span class=\"w\">       </span>Device<span class=\"w\">        </span>NPU<span class=\"o\">(</span>%<span class=\"o\">)</span><span class=\"w\">   </span>Comp<span class=\"o\">(</span>%<span class=\"o\">)</span><span class=\"w\">   </span>I/O<span class=\"o\">(</span>%<span class=\"o\">)</span><span class=\"w\">   </span>Command\\n<span class=\"m\">2023</span>-03-21T09:45:56.699483936Z<span class=\"w\">  </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">19</span>.06<span class=\"w\">    </span><span class=\"m\">100</span>.00<span class=\"w\">     </span><span class=\"m\">0</span>.00<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n<span class=\"m\">2023</span>-03-21T09:45:56.906443888Z<span class=\"w\">  </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">51</span>.09<span class=\"w\">     </span><span class=\"m\">93</span>.05<span class=\"w\">     </span><span class=\"m\">6</span>.95<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n<span class=\"m\">2023</span>-03-21T09:45:57.110489333Z<span class=\"w\">  </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">46</span>.40<span class=\"w\">     </span><span class=\"m\">97</span>.98<span class=\"w\">     </span><span class=\"m\">2</span>.02<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n<span class=\"m\">2023</span>-03-21T09:45:57.316060982Z<span class=\"w\">  </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">51</span>.43<span class=\"w\">    </span><span class=\"m\">100</span>.00<span class=\"w\">     </span><span class=\"m\">0</span>.00<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n<span class=\"m\">2023</span>-03-21T09:45:57.521140588Z<span class=\"w\">  </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">54</span>.28<span class=\"w\">     </span><span class=\"m\">94</span>.10<span class=\"w\">     </span><span class=\"m\">5</span>.90<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n<span class=\"m\">2023</span>-03-21T09:45:57.725910558Z<span class=\"w\">  </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">48</span>.93<span class=\"w\">     </span><span class=\"m\">98</span>.93<span class=\"w\">     </span><span class=\"m\">1</span>.07<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n<span class=\"m\">2023</span>-03-21T09:45:57.935041998Z<span class=\"w\">  </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">47</span>.91<span class=\"w\">    </span><span class=\"m\">100</span>.00<span class=\"w\">     </span><span class=\"m\">0</span>.00<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n<span class=\"m\">2023</span>-03-21T09:45:58.13929122Z<span class=\"w\">   </span><span class=\"m\">152616</span><span class=\"w\">    </span>npu1pe0-1<span class=\"w\">      </span><span class=\"m\">49</span>.06<span class=\"w\">     </span><span class=\"m\">94</span>.94<span class=\"w\">     </span><span class=\"m\">5</span>.06<span class=\"w\">   </span>./npu_runtime_test<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">10000</span><span class=\"w\"> </span>results/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n</pre>\\n         </div>\\n        </div>\\n       </section>\\n      </section>\\n     </section>\\n     <section id=\"furiosa-bench-benchmark-tool\">\\n      <span id=\"furiosabench\">\\n      </span>\\n      <h2>\\n       furiosa-bench (Benchmark Tool)\\n       <a class=\"headerlink\" href=\"#furiosa-bench-benchmark-tool\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         furiosa-bench\\n        </span>\\n       </code>\\n       command carries out a benchmark with a ONNX or TFLite model and a workload using furiosa-runtime. A benchmark result includes tail latency and QPS.\\n      </p>\\n      <p>\\n       The arguments of the command are as follows:\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span>furiosa-bench<span class=\"w\"> </span>--help\\nUSAGE:\\n<span class=\"w\">  </span>furiosa-bench<span class=\"w\"> </span><span class=\"o\">[</span>OPTIONS<span class=\"o\">]</span><span class=\"w\"> </span>&lt;model-path&gt;\\n\\n<span class=\"w\">  </span>OPTIONS:\\n<span class=\"w\">      </span>-b,<span class=\"w\"> </span>--batch<span class=\"w\"> </span>&lt;number&gt;<span class=\"w\">                       </span>Sets<span class=\"w\"> </span>the<span class=\"w\"> </span>number<span class=\"w\"> </span>of<span class=\"w\"> </span>batch<span class=\"w\"> </span>size,<span class=\"w\"> </span>which<span class=\"w\"> </span>should<span class=\"w\"> </span>be<span class=\"w\"> </span>exponents<span class=\"w\"> </span>of<span class=\"w\"> </span>two<span class=\"w\"> </span><span class=\"o\">[</span>default:<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">]</span>\\n<span class=\"w\">      </span>-o,<span class=\"w\"> </span>--output<span class=\"w\"> </span>&lt;bench-result-path&gt;<span class=\"w\">           </span>Create<span class=\"w\"> </span>json<span class=\"w\"> </span>file<span class=\"w\"> </span>that<span class=\"w\"> </span>has<span class=\"w\"> </span>information<span class=\"w\"> </span>about<span class=\"w\"> </span>the<span class=\"w\"> </span>benchmark\\n<span class=\"w\">      </span>-C,<span class=\"w\"> </span>--compiler-config<span class=\"w\"> </span>&lt;compiler-config&gt;<span class=\"w\">    </span>Sets<span class=\"w\"> </span>a<span class=\"w\"> </span>file<span class=\"w\"> </span>path<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>compiler<span class=\"w\"> </span>configuration<span class=\"w\"> </span><span class=\"o\">(</span>YAML<span class=\"w\"> </span>format<span class=\"o\">)</span>\\n<span class=\"w\">      </span>-d,<span class=\"w\"> </span>--devices<span class=\"w\"> </span>&lt;devices&gt;<span class=\"w\">                    </span>Designates<span class=\"w\"> </span>NPU<span class=\"w\"> </span>devices<span class=\"w\"> </span>to<span class=\"w\"> </span>be<span class=\"w\"> </span>used<span class=\"w\"> </span><span class=\"o\">(</span>e.g.,<span class=\"w\"> </span><span class=\"s2\">\"warboy(2)*1\"</span><span class=\"w\"> </span>or<span class=\"w\"> </span><span class=\"s2\">\"npu0pe0-1\"</span><span class=\"o\">)</span>\\n<span class=\"w\">      </span>-h,<span class=\"w\"> </span>--help<span class=\"w\">                                 </span>Prints<span class=\"w\"> </span><span class=\"nb\">help</span><span class=\"w\"> </span>information\\n<span class=\"w\">      </span>-t,<span class=\"w\"> </span>--io-threads<span class=\"w\"> </span>&lt;number&gt;<span class=\"w\">                  </span>Sets<span class=\"w\"> </span>the<span class=\"w\"> </span>number<span class=\"w\"> </span>of<span class=\"w\"> </span>I/O<span class=\"w\"> </span>Threads<span class=\"w\"> </span><span class=\"o\">[</span>default:<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">]</span>\\n<span class=\"w\">          </span>--duration<span class=\"w\"> </span>&lt;min-duration&gt;<span class=\"w\">              </span>Sets<span class=\"w\"> </span>the<span class=\"w\"> </span>minimum<span class=\"w\"> </span><span class=\"nb\">test</span><span class=\"w\"> </span><span class=\"nb\">time</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span>seconds.<span class=\"w\"> </span>Both<span class=\"w\"> </span>min_query_count<span class=\"w\"> </span>and<span class=\"w\"> </span>min_duration<span class=\"w\"> </span>should<span class=\"w\"> </span>be<span class=\"w\"> </span>met<span class=\"w\"> </span>to<span class=\"w\"> </span>finish<span class=\"w\"> </span>the<span class=\"w\"> </span><span class=\"nb\">test</span>\\n<span class=\"w\">                                                </span><span class=\"o\">[</span>default:<span class=\"w\"> </span><span class=\"m\">0</span><span class=\"o\">]</span>\\n<span class=\"w\">      </span>-n,<span class=\"w\"> </span>--queries<span class=\"w\"> </span>&lt;min-query-count&gt;<span class=\"w\">            </span>Sets<span class=\"w\"> </span>the<span class=\"w\"> </span>minimum<span class=\"w\"> </span>number<span class=\"w\"> </span>of<span class=\"w\"> </span><span class=\"nb\">test</span><span class=\"w\"> </span>queries.<span class=\"w\"> </span>Both<span class=\"w\"> </span>min_query_count<span class=\"w\"> </span>and<span class=\"w\"> </span>min_duration_ms<span class=\"w\"> </span>should<span class=\"w\"> </span>be<span class=\"w\"> </span>met<span class=\"w\"> </span>to<span class=\"w\"> </span>finish<span class=\"w\"> </span>the\\n<span class=\"w\">                                                </span><span class=\"nb\">test</span><span class=\"w\"> </span><span class=\"o\">[</span>default:<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">]</span>\\n<span class=\"w\">      </span>-T,<span class=\"w\"> </span>--trace-output<span class=\"w\"> </span>&lt;trace-output&gt;<span class=\"w\">          </span>Sets<span class=\"w\"> </span>a<span class=\"w\"> </span>file<span class=\"w\"> </span>path<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>profiling<span class=\"w\"> </span>result<span class=\"w\"> </span><span class=\"o\">(</span>Chrome<span class=\"w\"> </span>Trace<span class=\"w\"> </span>JSON<span class=\"w\"> </span>format<span class=\"o\">)</span>\\n<span class=\"w\">      </span>-V,<span class=\"w\"> </span>--version<span class=\"w\">                              </span>Prints<span class=\"w\"> </span>version<span class=\"w\"> </span>information\\n<span class=\"w\">      </span>-v,<span class=\"w\"> </span>--verbose<span class=\"w\">                              </span>Print<span class=\"w\"> </span>verbose<span class=\"w\"> </span>log\\n<span class=\"w\">      </span>-w,<span class=\"w\"> </span>--workers<span class=\"w\"> </span>&lt;number&gt;<span class=\"w\">                     </span>Sets<span class=\"w\"> </span>the<span class=\"w\"> </span>number<span class=\"w\"> </span>of<span class=\"w\"> </span>workers<span class=\"w\"> </span><span class=\"o\">[</span>default:<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">]</span>\\n<span class=\"w\">          </span>--workload<span class=\"w\"> </span>&lt;workload&gt;<span class=\"w\">                  </span>Sets<span class=\"w\"> </span>the<span class=\"w\"> </span>bench<span class=\"w\"> </span>workload<span class=\"w\"> </span>which<span class=\"w\"> </span>can<span class=\"w\"> </span>be<span class=\"w\"> </span>either<span class=\"w\"> </span>latency-oriented<span class=\"w\"> </span><span class=\"o\">(</span>L<span class=\"o\">)</span><span class=\"w\"> </span>or<span class=\"w\"> </span>throughput-oriented<span class=\"w\"> </span><span class=\"o\">(</span>T<span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">[</span>default:<span class=\"w\"> </span>L<span class=\"o\">]</span>\\n\\n<span class=\"w\">  </span>ARGS:\\n<span class=\"w\">      </span>&lt;model-path&gt;\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       MODEL_PATH is the file path of ONNX, TFLite or ENF (format produced by\\n       <a class=\"reference internal\" href=\"compiler.html#compilercli\">\\n        <span class=\"std std-ref\">\\n         furiosa-compiler\\n        </span>\\n       </a>\\n       ).\\n      </p>\\n      <p>\\n       The following is an example usage of furiosa-bench without an output path option (i.e.,\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         --output\\n        </span>\\n       </code>\\n       ):\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span>furiosa-bench<span class=\"w\"> </span>mnist-8.onnx<span class=\"w\"> </span>--workload<span class=\"w\"> </span>L<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">1000</span><span class=\"w\"> </span>-w<span class=\"w\"> </span><span class=\"m\">8</span><span class=\"w\"> </span>-t<span class=\"w\"> </span><span class=\"nv\">2</span>\\n\\n<span class=\"w\">  </span><span class=\"o\">======================================================================</span>\\n<span class=\"w\">  </span>This<span class=\"w\"> </span>benchmark<span class=\"w\"> </span>was<span class=\"w\"> </span>executed<span class=\"w\"> </span>with<span class=\"w\"> </span>latency-workload<span class=\"w\"> </span>which<span class=\"w\"> </span>prioritizes<span class=\"w\"> </span>latency<span class=\"w\"> </span>of<span class=\"w\"> </span>individual<span class=\"w\"> </span>queries<span class=\"w\"> </span>over<span class=\"w\"> </span>throughput.\\n<span class=\"w\">  </span><span class=\"m\">1000</span><span class=\"w\"> </span>queries<span class=\"w\"> </span>executed<span class=\"w\"> </span>with<span class=\"w\"> </span>batch<span class=\"w\"> </span>size<span class=\"w\"> </span><span class=\"m\">1</span>\\n<span class=\"w\">  </span>Latency<span class=\"w\"> </span>stats<span class=\"w\"> </span>are<span class=\"w\"> </span>as<span class=\"w\"> </span>follows\\n<span class=\"w\">  </span>QPS<span class=\"o\">(</span>Throughput<span class=\"o\">)</span>:<span class=\"w\"> </span><span class=\"m\">34</span>.40/s\\n\\n<span class=\"w\">  </span>Per-query<span class=\"w\"> </span>latency:\\n<span class=\"w\">  </span>Min<span class=\"w\"> </span>latency<span class=\"w\"> </span><span class=\"o\">(</span>us<span class=\"o\">)</span><span class=\"w\">    </span>:<span class=\"w\"> </span><span class=\"m\">8399</span>\\n<span class=\"w\">  </span>Max<span class=\"w\"> </span>latency<span class=\"w\"> </span><span class=\"o\">(</span>us<span class=\"o\">)</span><span class=\"w\">    </span>:<span class=\"w\"> </span><span class=\"m\">307568</span>\\n<span class=\"w\">  </span>Mean<span class=\"w\"> </span>latency<span class=\"w\"> </span><span class=\"o\">(</span>us<span class=\"o\">)</span><span class=\"w\">   </span>:<span class=\"w\"> </span><span class=\"m\">29040</span>\\n<span class=\"w\">  </span>50th<span class=\"w\"> </span>percentile<span class=\"w\"> </span><span class=\"o\">(</span>us<span class=\"o\">)</span>:<span class=\"w\"> </span><span class=\"m\">19329</span>\\n<span class=\"w\">  </span>95th<span class=\"w\"> </span>percentile<span class=\"w\"> </span><span class=\"o\">(</span>us<span class=\"o\">)</span>:<span class=\"w\"> </span><span class=\"m\">62797</span>\\n<span class=\"w\">  </span>99th<span class=\"w\"> </span>percentile<span class=\"w\"> </span><span class=\"o\">(</span>us<span class=\"o\">)</span>:<span class=\"w\"> </span><span class=\"m\">79874</span>\\n<span class=\"w\">  </span>99th<span class=\"w\"> </span>percentile<span class=\"w\"> </span><span class=\"o\">(</span>us<span class=\"o\">)</span>:<span class=\"w\"> </span><span class=\"m\">307568</span>\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       If an output path is specified, furiosa-bench will save a JSON document as follows:\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span>furiosa-bench<span class=\"w\"> </span>mnist-8.onnx<span class=\"w\"> </span>--workload<span class=\"w\"> </span>L<span class=\"w\"> </span>-n<span class=\"w\"> </span><span class=\"m\">1000</span><span class=\"w\"> </span>-w<span class=\"w\"> </span><span class=\"m\">8</span><span class=\"w\"> </span>-t<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span>-o<span class=\"w\"> </span>mnist.json\\n$<span class=\"w\"> </span>cat<span class=\"w\"> </span>mnist.json\\n\\n<span class=\"w\">  </span><span class=\"o\">{</span>\\n<span class=\"w\">      </span><span class=\"s2\">\"model_data\"</span>:<span class=\"w\"> </span><span class=\"o\">{</span>\\n<span class=\"w\">          </span><span class=\"s2\">\"path\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"./mnist-8.onnx\"</span>,\\n<span class=\"w\">          </span><span class=\"s2\">\"md5\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"d7cd24a0a76cd492f31065301d468c3d  ./mnist-8.onnx\"</span>\\n<span class=\"w\">      </span><span class=\"o\">}</span>,\\n<span class=\"w\">      </span><span class=\"s2\">\"compiler_version\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"0.10.0-dev (rev: 2d862de8a built_at: 2023-07-13T20:05:04Z)\"</span>,\\n<span class=\"w\">      </span><span class=\"s2\">\"hal_version\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"Version: 0.12.0-2+nightly-230716\"</span>,\\n<span class=\"w\">      </span><span class=\"s2\">\"git_revision\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"fe6f77a\"</span>,\\n<span class=\"w\">      </span><span class=\"s2\">\"result\"</span>:<span class=\"w\"> </span><span class=\"o\">{</span>\\n<span class=\"w\">          </span><span class=\"s2\">\"mode\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"Latency\"</span>,\\n<span class=\"w\">          </span><span class=\"s2\">\"total run time\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"30025 us\"</span>,\\n<span class=\"w\">          </span><span class=\"s2\">\"total num queries\"</span>:<span class=\"w\"> </span><span class=\"m\">1000</span>,\\n<span class=\"w\">          </span><span class=\"s2\">\"batch size\"</span>:<span class=\"w\"> </span><span class=\"m\">1</span>,\\n<span class=\"w\">          </span><span class=\"s2\">\"qps\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"33.31/s\"</span>,\\n<span class=\"w\">          </span><span class=\"s2\">\"latency stats\"</span>:<span class=\"w\"> </span><span class=\"o\">{</span>\\n<span class=\"w\">              </span><span class=\"s2\">\"min\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"8840 us\"</span>,\\n<span class=\"w\">              </span><span class=\"s2\">\"max\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"113254 us\"</span>,\\n<span class=\"w\">              </span><span class=\"s2\">\"mean\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"29989 us\"</span>,\\n<span class=\"w\">              </span><span class=\"s2\">\"50th percentile\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"18861 us\"</span>,\\n<span class=\"w\">              </span><span class=\"s2\">\"95th percentile\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"64927 us\"</span>,\\n<span class=\"w\">              </span><span class=\"s2\">\"99th percentile\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"87052 us\"</span>,\\n<span class=\"w\">              </span><span class=\"s2\">\"99.9th percentile\"</span>:<span class=\"w\"> </span><span class=\"s2\">\"113254 us\"</span>\\n<span class=\"w\">          </span><span class=\"o\">}</span>\\n<span class=\"w\">      </span><span class=\"o\">}</span>\\n<span class=\"w\">  </span><span class=\"o\">}</span>\\n</pre>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"furiosa\">\\n      <h2>\\n       furiosa\\n       <a class=\"headerlink\" href=\"#furiosa\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       The\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         furiosa\\n        </span>\\n       </code>\\n       command is a meta-command line tool that can be used by installing the\\n       <cite>\\n        Python SDK &lt;PythonSDK&gt;\\n       </cite>\\n       .\\nAdditional subcommands are also added when the extension package is installed.\\n      </p>\\n      <p>\\n       If the Python execution environment is not prepared, refer to\\n       <a class=\"reference internal\" href=\"python-sdk.html#setuppython\">\\n        <span class=\"std std-ref\">\\n         Python execution environment setup\\n        </span>\\n       </a>\\n       .\\n      </p>\\n      <p>\\n       Installing command line tool.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>furiosa-sdk\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       Verifying installation.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>$<span class=\"w\"> </span>furiosa<span class=\"w\"> </span>compile<span class=\"w\"> </span>--version\\nlibnpu.so<span class=\"w\"> </span>---<span class=\"w\"> </span>v2.0,<span class=\"w\"> </span>built<span class=\"w\"> </span>@<span class=\"w\"> </span>fe1fca3\\n<span class=\"m\">0</span>.5.0<span class=\"w\"> </span><span class=\"o\">(</span>rev:<span class=\"w\"> </span>49b97492a<span class=\"w\"> </span>built<span class=\"w\"> </span>at<span class=\"w\"> </span><span class=\"m\">2021</span>-12-07<span class=\"w\"> </span><span class=\"m\">04</span>:07:08<span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span>wrapper:<span class=\"w\"> </span>None<span class=\"o\">)</span>\\n</pre>\\n       </div>\\n      </div>\\n      <section id=\"furiosa-compile\">\\n       <h3>\\n        furiosa compile\\n        <a class=\"headerlink\" href=\"#furiosa-compile\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        The\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          compile\\n         </span>\\n        </code>\\n        command compiles models such as\\n        <a class=\"reference external\" href=\"https://onnx.ai/\">\\n         ONNX\\n        </a>\\n        and\\n        <a class=\"reference external\" href=\"https://www.tensorflow.org/lite\">\\n         TFLite\\n        </a>\\n        , generating programs that utilize FuriosaAI NPU.\\n       </p>\\n       <p>\\n        Detailed explanations and options can be found in the\\n        <a class=\"reference internal\" href=\"compiler.html#compilercli\">\\n         <span class=\"std std-ref\">\\n          furiosa-compiler\\n         </span>\\n        </a>\\n        page.\\n       </p>\\n      </section>\\n      <section id=\"furiosa-litmus-model-compatibility-checker\">\\n       <span id=\"litmus\">\\n       </span>\\n       <h3>\\n        furiosa litmus (Model Compatibility Checker)\\n        <a class=\"headerlink\" href=\"#furiosa-litmus-model-compatibility-checker\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        The\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          litmus\\n         </span>\\n        </code>\\n        is a tool to check quickly if an\\n        <a class=\"reference external\" href=\"https://onnx.ai/\">\\n         ONNX\\n        </a>\\n        model can work normally with Furiosa SDK using NPU.\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          litmus\\n         </span>\\n        </code>\\n        goes through all usage steps of Furiosa SDK, including quantization, compilation, and inferences on FuriosaAI NPU.\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          litmus\\n         </span>\\n        </code>\\n        is also a useful bug reporting tool. If you specify\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          --dump\\n         </span>\\n        </code>\\n        option,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          litmus\\n         </span>\\n        </code>\\n        will collect logs and environment information and dump an archive file.\\nThe archive file can be used to report issues.\\n       </p>\\n       <p>\\n        The steps executed by\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          litmus\\n         </span>\\n        </code>\\n        command are as follows.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <ul class=\"simple\">\\n          <li>\\n           <p>\\n            Step1: Load an input model and check it is a valid model.\\n           </p>\\n          </li>\\n          <li>\\n           <p>\\n            Step2: Quantize the model with random calibration.\\n           </p>\\n          </li>\\n          <li>\\n           <p>\\n            Step3: Compile the quantized model.\\n           </p>\\n          </li>\\n          <li>\\n           <p>\\n            Step4: Inference the compiled model using\\n            <code class=\"docutils literal notranslate\">\\n             <span class=\"pre\">\\n              furiosa-bench\\n             </span>\\n            </code>\\n            . This step is skipped if\\n            <code class=\"docutils literal notranslate\">\\n             <span class=\"pre\">\\n              furiosa-bench\\n             </span>\\n            </code>\\n            was not installed.\\n           </p>\\n          </li>\\n         </ul>\\n        </div>\\n       </blockquote>\\n       <p>\\n        Usage:\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>furiosa-litmus<span class=\"w\"> </span><span class=\"o\">[</span>-h<span class=\"o\">]</span><span class=\"w\"> </span><span class=\"o\">[</span>--dump<span class=\"w\"> </span>OUTPUT_PREFIX<span class=\"o\">]</span><span class=\"w\"> </span><span class=\"o\">[</span>--skip-quantization<span class=\"o\">]</span><span class=\"w\"> </span><span class=\"o\">[</span>--target-npu<span class=\"w\"> </span>TARGET_NPU<span class=\"o\">]</span><span class=\"w\"> </span><span class=\"o\">[</span>-v<span class=\"o\">]</span><span class=\"w\"> </span>model_path\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        A simple example using\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          litmus\\n         </span>\\n        </code>\\n        command is as follows.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>$<span class=\"w\"> </span>furiosa<span class=\"w\"> </span>litmus<span class=\"w\"> </span>model.onnx\\nlibfuriosa_hal.so<span class=\"w\"> </span>---<span class=\"w\"> </span>v0.11.0,<span class=\"w\"> </span>built<span class=\"w\"> </span>@<span class=\"w\"> </span>43c901f\\nINFO:furiosa.common.native:loaded<span class=\"w\"> </span>native<span class=\"w\"> </span>library<span class=\"w\"> </span>libfuriosa_compiler.so.0.10.0<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">0</span>.10.0-dev<span class=\"w\"> </span>d7548b7f6<span class=\"o\">)</span>\\nfuriosa-quantizer<span class=\"w\"> </span><span class=\"m\">0</span>.10.0<span class=\"w\"> </span><span class=\"o\">(</span>rev.<span class=\"w\"> </span>9ecebb6<span class=\"o\">)</span><span class=\"w\"> </span>furiosa-litmus<span class=\"w\"> </span><span class=\"m\">0</span>.10.0<span class=\"w\"> </span><span class=\"o\">(</span>rev.<span class=\"w\"> </span>9ecebb6<span class=\"o\">)</span>\\n<span class=\"o\">[</span>Step<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">]</span><span class=\"w\"> </span>Checking<span class=\"w\"> </span><span class=\"k\">if</span><span class=\"w\"> </span>the<span class=\"w\"> </span>model<span class=\"w\"> </span>can<span class=\"w\"> </span>be<span class=\"w\"> </span>loaded<span class=\"w\"> </span>and<span class=\"w\"> </span>optimized<span class=\"w\"> </span>...\\n<span class=\"o\">[</span>Step<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">]</span><span class=\"w\"> </span>Passed\\n<span class=\"o\">[</span>Step<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"o\">]</span><span class=\"w\"> </span>Checking<span class=\"w\"> </span><span class=\"k\">if</span><span class=\"w\"> </span>the<span class=\"w\"> </span>model<span class=\"w\"> </span>can<span class=\"w\"> </span>be<span class=\"w\"> </span>quantized<span class=\"w\"> </span>...\\n<span class=\"o\">[</span>Step<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"o\">]</span><span class=\"w\"> </span>Passed\\n<span class=\"o\">[</span>Step<span class=\"w\"> </span><span class=\"m\">3</span><span class=\"o\">]</span><span class=\"w\"> </span>Checking<span class=\"w\"> </span><span class=\"k\">if</span><span class=\"w\"> </span>the<span class=\"w\"> </span>model<span class=\"w\"> </span>can<span class=\"w\"> </span>be<span class=\"w\"> </span>compiled<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>the<span class=\"w\"> </span>NPU<span class=\"w\"> </span>family<span class=\"w\"> </span><span class=\"o\">[</span>warboy-2pe<span class=\"o\">]</span><span class=\"w\"> </span>...\\n<span class=\"o\">[</span><span class=\"m\">1</span>/6<span class=\"o\">]</span><span class=\"w\"> </span>🔍<span class=\"w\">   </span>Compiling<span class=\"w\"> </span>from<span class=\"w\"> </span>onnx<span class=\"w\"> </span>to<span class=\"w\"> </span>dfg\\nDone<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">0</span>.09272794s\\n<span class=\"o\">[</span><span class=\"m\">2</span>/6<span class=\"o\">]</span><span class=\"w\"> </span>🔍<span class=\"w\">   </span>Compiling<span class=\"w\"> </span>from<span class=\"w\"> </span>dfg<span class=\"w\"> </span>to<span class=\"w\"> </span>ldfg\\n▪▪▪▪▪<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">1</span>/3<span class=\"o\">]</span><span class=\"w\"> </span>Splitting<span class=\"w\"> </span>graph<span class=\"o\">(</span>LAS<span class=\"o\">)</span>...Done<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">9</span>.034934s\\n▪▪▪▪▪<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">2</span>/3<span class=\"o\">]</span><span class=\"w\"> </span>Lowering<span class=\"w\"> </span>graph<span class=\"o\">(</span>LAS<span class=\"o\">)</span>...Done<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">20</span>.140083s\\n▪▪▪▪▪<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">3</span>/3<span class=\"o\">]</span><span class=\"w\"> </span>Optimizing<span class=\"w\"> </span>graph...Done<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">0</span>.019548794s\\nDone<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">29</span>.196825s\\n<span class=\"o\">[</span><span class=\"m\">3</span>/6<span class=\"o\">]</span><span class=\"w\"> </span>🔍<span class=\"w\">   </span>Compiling<span class=\"w\"> </span>from<span class=\"w\"> </span>ldfg<span class=\"w\"> </span>to<span class=\"w\"> </span>cdfg\\nDone<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">0</span>.001701888s\\n<span class=\"o\">[</span><span class=\"m\">4</span>/6<span class=\"o\">]</span><span class=\"w\"> </span>🔍<span class=\"w\">   </span>Compiling<span class=\"w\"> </span>from<span class=\"w\"> </span>cdfg<span class=\"w\"> </span>to<span class=\"w\"> </span>gir\\nDone<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">0</span>.015205072s\\n<span class=\"o\">[</span><span class=\"m\">5</span>/6<span class=\"o\">]</span><span class=\"w\"> </span>🔍<span class=\"w\">   </span>Compiling<span class=\"w\"> </span>from<span class=\"w\"> </span>gir<span class=\"w\"> </span>to<span class=\"w\"> </span>lir\\nDone<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">0</span>.0038304s\\n<span class=\"o\">[</span><span class=\"m\">6</span>/6<span class=\"o\">]</span><span class=\"w\"> </span>🔍<span class=\"w\">   </span>Compiling<span class=\"w\"> </span>from<span class=\"w\"> </span>lir<span class=\"w\"> </span>to<span class=\"w\"> </span>enf\\nDone<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">0</span>.020943863s\\n✨<span class=\"w\">  </span>Finished<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">29</span>.331545s\\n<span class=\"o\">[</span>Step<span class=\"w\"> </span><span class=\"m\">3</span><span class=\"o\">]</span><span class=\"w\"> </span>Passed\\n<span class=\"o\">[</span>Step<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"o\">]</span><span class=\"w\"> </span>Perform<span class=\"w\"> </span>inference<span class=\"w\"> </span>once<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>data<span class=\"w\"> </span>collection...<span class=\"w\"> </span><span class=\"o\">(</span>Optional<span class=\"o\">)</span>\\n✨<span class=\"w\">  </span>Finished<span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">0</span>.000001198s\\n<span class=\"o\">======================================================================</span>\\nThis<span class=\"w\"> </span>benchmark<span class=\"w\"> </span>was<span class=\"w\"> </span>executed<span class=\"w\"> </span>with<span class=\"w\"> </span>latency-workload<span class=\"w\"> </span>which<span class=\"w\"> </span>prioritizes<span class=\"w\"> </span>latency<span class=\"w\"> </span>of<span class=\"w\"> </span>individual<span class=\"w\"> </span>queries<span class=\"w\"> </span>over<span class=\"w\"> </span>throughput.\\n<span class=\"m\">1</span><span class=\"w\"> </span>queries<span class=\"w\"> </span>executed<span class=\"w\"> </span>with<span class=\"w\"> </span>batch<span class=\"w\"> </span>size<span class=\"w\"> </span><span class=\"m\">1</span>\\nLatency<span class=\"w\"> </span>stats<span class=\"w\"> </span>are<span class=\"w\"> </span>as<span class=\"w\"> </span>follows\\nQPS<span class=\"o\">(</span>Throughput<span class=\"o\">)</span>:<span class=\"w\"> </span><span class=\"m\">125</span>.00/s\\n\\nPer-query<span class=\"w\"> </span>latency:\\nMin<span class=\"w\"> </span>latency<span class=\"w\"> </span><span class=\"o\">(</span>us<span class=\"o\">)</span><span class=\"w\">    </span>:<span class=\"w\"> </span><span class=\"m\">7448</span>\\nMax<span class=\"w\"> </span>latency<span class=\"w\"> </span><span class=\"o\">(</span>us<span class=\"o\">)</span><span class=\"w\">    </span>:<span class=\"w\"> </span><span class=\"m\">7448</span>\\nMean<span class=\"w\"> </span>latency<span class=\"w\"> </span><span class=\"o\">(</span>us<span class=\"o\">)</span><span class=\"w\">   </span>:<span class=\"w\"> </span><span class=\"m\">7448</span>\\n50th<span class=\"w\"> </span>percentile<span class=\"w\"> </span><span class=\"o\">(</span>us<span class=\"o\">)</span>:<span class=\"w\"> </span><span class=\"m\">7448</span>\\n95th<span class=\"w\"> </span>percentile<span class=\"w\"> </span><span class=\"o\">(</span>us<span class=\"o\">)</span>:<span class=\"w\"> </span><span class=\"m\">7448</span>\\n99th<span class=\"w\"> </span>percentile<span class=\"w\"> </span><span class=\"o\">(</span>us<span class=\"o\">)</span>:<span class=\"w\"> </span><span class=\"m\">7448</span>\\n99th<span class=\"w\"> </span>percentile<span class=\"w\"> </span><span class=\"o\">(</span>us<span class=\"o\">)</span>:<span class=\"w\"> </span><span class=\"m\">7448</span>\\n<span class=\"o\">[</span>Step<span class=\"w\"> </span><span class=\"m\">4</span><span class=\"o\">]</span><span class=\"w\"> </span>Finished\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        If you have quantized model already, you can skip Step1 and Step2 with\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          --skip-quantization\\n         </span>\\n        </code>\\n        option.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>$<span class=\"w\"> </span>furiosa<span class=\"w\"> </span>litmus<span class=\"w\"> </span>--skip-quantization<span class=\"w\"> </span>quantized-model.onnx\\nlibfuriosa_hal.so<span class=\"w\"> </span>---<span class=\"w\"> </span>v0.11.0,<span class=\"w\"> </span>built<span class=\"w\"> </span>@<span class=\"w\"> </span>43c901f\\nINFO:furiosa.common.native:loaded<span class=\"w\"> </span>native<span class=\"w\"> </span>library<span class=\"w\"> </span>libfuriosa_compiler.so.0.10.0<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">0</span>.10.0-dev<span class=\"w\"> </span>d7548b7f6<span class=\"o\">)</span>\\nfuriosa-quantizer<span class=\"w\"> </span><span class=\"m\">0</span>.10.0<span class=\"w\"> </span><span class=\"o\">(</span>rev.<span class=\"w\"> </span>9ecebb6<span class=\"o\">)</span><span class=\"w\"> </span>furiosa-litmus<span class=\"w\"> </span><span class=\"m\">0</span>.10.0<span class=\"w\"> </span><span class=\"o\">(</span>rev.<span class=\"w\"> </span>9ecebb6<span class=\"o\">)</span>\\n<span class=\"o\">[</span>Step<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">]</span><span class=\"w\"> </span>Skip<span class=\"w\"> </span>model<span class=\"w\"> </span>loading<span class=\"w\"> </span>and<span class=\"w\"> </span>optimization\\n<span class=\"o\">[</span>Step<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"o\">]</span><span class=\"w\"> </span>Skip<span class=\"w\"> </span>model<span class=\"w\"> </span>quantization\\n<span class=\"o\">[</span>Step<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\"> </span><span class=\"p\">&amp;</span><span class=\"w\"> </span>Step<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"o\">]</span><span class=\"w\"> </span>Load<span class=\"w\"> </span>quantized<span class=\"w\"> </span>model<span class=\"w\"> </span>...\\n<span class=\"o\">[</span>Step<span class=\"w\"> </span><span class=\"m\">3</span><span class=\"o\">]</span><span class=\"w\"> </span>Checking<span class=\"w\"> </span><span class=\"k\">if</span><span class=\"w\"> </span>the<span class=\"w\"> </span>model<span class=\"w\"> </span>can<span class=\"w\"> </span>be<span class=\"w\"> </span>compiled<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>the<span class=\"w\"> </span>NPU<span class=\"w\"> </span>family<span class=\"w\"> </span><span class=\"o\">[</span>warboy-2pe<span class=\"o\">]</span><span class=\"w\"> </span>...\\n...\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        You can use the\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          --dump\\n         </span>\\n         <span class=\"pre\">\\n          &lt;path&gt;\\n         </span>\\n        </code>\\n        option to create a\\n        <cite>\\n         &lt;path&gt;-&lt;unix_epoch&gt;.zip\\n        </cite>\\n        file that contains metadata necessary for analysis, such as compilation logs, runtime logs, software versions, and execution environments.\\nIf you have any problems, you can get support through\\n        <a class=\"reference external\" href=\"https://furiosa-ai.atlassian.net/servicedesk/customer/portals\">\\n         FuriosaAI customer service center\\n        </a>\\n        with this zip file.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>$<span class=\"w\"> </span>furiosa<span class=\"w\"> </span>litmus<span class=\"w\"> </span>--dump<span class=\"w\"> </span>archive<span class=\"w\"> </span>model.onnx\\nlibfuriosa_hal.so<span class=\"w\"> </span>---<span class=\"w\"> </span>v0.11.0,<span class=\"w\"> </span>built<span class=\"w\"> </span>@<span class=\"w\"> </span>43c901f\\nINFO:furiosa.common.native:loaded<span class=\"w\"> </span>native<span class=\"w\"> </span>library<span class=\"w\"> </span>libfuriosa_compiler.so.0.10.0<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">0</span>.10.0-dev<span class=\"w\"> </span>d7548b7f6<span class=\"o\">)</span>\\nfuriosa-quantizer<span class=\"w\"> </span><span class=\"m\">0</span>.10.0<span class=\"w\"> </span><span class=\"o\">(</span>rev.<span class=\"w\"> </span>9ecebb6<span class=\"o\">)</span><span class=\"w\"> </span>furiosa-litmus<span class=\"w\"> </span><span class=\"m\">0</span>.10.0<span class=\"w\"> </span><span class=\"o\">(</span>rev.<span class=\"w\"> </span>9ecebb6<span class=\"o\">)</span>\\n<span class=\"o\">[</span>Step<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">]</span><span class=\"w\"> </span>Checking<span class=\"w\"> </span><span class=\"k\">if</span><span class=\"w\"> </span>the<span class=\"w\"> </span>model<span class=\"w\"> </span>can<span class=\"w\"> </span>be<span class=\"w\"> </span>loaded<span class=\"w\"> </span>and<span class=\"w\"> </span>optimized<span class=\"w\"> </span>...\\n<span class=\"o\">[</span>Step<span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">]</span><span class=\"w\"> </span>Passed\\n...\\n\\n$<span class=\"w\"> </span>zipinfo<span class=\"w\"> </span>-1<span class=\"w\"> </span>archive-1690438803.zip\\narchive-16904388032l4hoi3h/meta.yaml\\narchive-16904388032l4hoi3h/compiler/compiler.log\\narchive-16904388032l4hoi3h/compiler/memory-analysis.html\\narchive-16904388032l4hoi3h/compiler/model.dot\\narchive-16904388032l4hoi3h/runtime/trace.json\\n</pre>\\n        </div>\\n       </div>\\n      </section>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"c-sdk.html\" rel=\"prev\" title=\"C SDK installation and user guide\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"compiler.html\" rel=\"next\" title=\"Compiler\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='aa27e152-d0c0-4c00-99e9-54d1516e3a5b', link=Url('https://furiosa-ai.github.io/docs/latest/en/releases/0.6.0.html'), name='0', parent='', child=[], description='\\n\\n\\n* Release Notes - 0.6.0\\n* [View page source](../_sources/releases/0.6.0.rst.txt)\\n\\n---\\n\\n\\n\\nRelease Notes - 0.6.0\\n[\\uf0c1](#release-notes-0-6-0 \"Permalink to this heading\")\\n===========================================================================\\n\\nFuriosaAI SDK 0.6.0 is a major release.\\nIt includes 234 PRs on performance improvements, added functionalities, and bug fixes,\\nas well as approximately 900 commits.\\n\\nHow to upgrade\\n[\\uf0c1](#how-to-upgrade \"Permalink to this heading\")\\n---------------------------------------------------------------\\n\\nIf you are using the APT repositories, you easily upgrade with the instructions below:\\nMore detailed instructions can be found at\\n[Driver, Firmware, and Runtime Installation](../software/installation.html#requiredpackages)\\n.\\n\\n> ```\\n> apt-get update && \\\\\\n> apt-get install furiosa-driver-pdma furiosa-libnpu-warboy furiosa-libnux\\n> \\n> pip uninstall furiosa-sdk-quantizer furiosa-sdk-runtime furiosa-sdk-validator && \\\\\\n> pip install --upgrade furiosa-sdk\\n> \\n> ```\\n\\n\\nMajor changes\\n[\\uf0c1](#major-changes \"Permalink to this heading\")\\n-------------------------------------------------------------\\n\\nThe kernel driver (furiosa-driver-pdma) has been upgraded to 1.2.2, and the user-level driver (furiosa-libnpu-warboy)\\nhas been upgraded to 0.5.2, thereby providing more stable and higher NPU performance. Other major changes include the following:\\n\\n### Compiler [\\uf0c1](#compiler \"Permalink to this heading\")\\n\\n* Addition of NPU accelerated operators (see\\n  [List of Supported Operators for Warboy Acceleration](../npu/warboy.html#supportedoperators)\\n  for full list of accelerated operators)\\n  \\n  > + Space-to-depth (CRD mode)\\n  > + Transpose\\n  > + Slice (height axis only)\\n  > + Concat (height axis only)\\n  > + Grouped Convolution (if groups <= 128)\\n* Improvements to significantly reduce frequency of CPU tasks in models with\\n  operators that require large memory usage (reduced execution time)\\n\\n### Quantizer [\\uf0c1](#quantizer \"Permalink to this heading\")\\n\\n* Improve model quantization process to ensure idempotency\\n* Remove PyTorch reliance\\n* Improve code quality by removing multiple Pylint warnings\\n* Upgrade multiple library dependencies (e.g. Numpy -> 1.21.5, Pyyaml -> 6.0.0)\\n\\n### Python SDK [\\uf0c1](#python-sdk \"Permalink to this heading\")\\n\\n* Python SDK project structure change\\n  \\n  + furiosa-sdk-runtime -> furiosa-sdk\\n  + furiosa-sdk-quantizer -> furiosa-quantizer\\n  + furiosa-sdk-validator -> furiosa-litmus\\n* Validator, a package that checks for model compatibility with Furiosa SDK, is renamed to litmus. Installation instruction has also been updated accordingly.\\n\\nSee\\n[furiosa litmus (Model Compatibility Checker)](../software/cli.html#litmus)\\nfor more detailed usage instructions.\\n\\n> ```\\n> $ pip install \\'furiosa-sdk[litmus]\\'\\n> \\n> ```\\n\\n#### Furiosa Serving: Addition of FastAPI-based advanced serving library [\\uf0c1](#furiosa-serving-addition-of-fastapi-based-advanced-serving-library \"Permalink to this heading\")\\n\\nfuriosa-serving is based on FastAPI. It allows you to easily add Python-based business logic or image pre/postprocessing code,\\nbefore or after executing model inference API.\\n\\nYou can install using the following instructions.\\n\\n> ```\\n> $ pip install \\'furiosa-sdk[serving]\\'\\n> \\n> ```\\n\\nThe usage example is shown below. You can find more detailed instructions at\\n[furiosa-serving Github](https://github.com/furiosa-ai/furiosa-sdk/tree/main/python/furiosa-serving)\\n.\\n\\n> ```\\n> from typing import Dict\\n> \\n> from fastapi import File, UploadFile\\n> from furiosa.server.utils.thread import synchronous\\n> from furiosa.serving import ServeAPI, ServeModel\\n> import numpy as np\\n> \\n> \\n> serve = ServeAPI()\\n> \\n> \\n> model: ServeModel = synchronous(serve.model)(\\n>     \\'imagenet\\',\\n>     location=\\'./examples/assets/models/image_classification.onnx\\'\\n> )\\n> \\n> @model.post(\"/models/imagenet/infer\")\\n> async def infer(image: UploadFile = File(...)) -> Dict:\\n>     # Convert image to Numpy array with your preprocess() function\\n>     tensors: List[np.ndarray] = preprocess(image)\\n> \\n>     # Infer from ServeModel\\n>     result: List[np.ndarray] = await model.predict(tensors)\\n> \\n>     # Classify model from numpy array with your postprocess() function\\n>     response: Dict = postprocess(result)\\n> \\n>     return response\\n> \\n> ```\\n\\n\\n\\n\\n\\n\\n\\n[Previous](0.7.0.html \"Release Notes - 0.7.0\")\\n[Next](0.5.0.html \"Release Notes - 0.5.0\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Release Notes - 0.6.0 * [View page source](../_sources/releases/0.6.0.rst.txt)\\n---\\nRelease Notes - 0.6.0 [\\uf0c1](#release-notes-0-6-0 \"Permalink to this heading\") ===========================================================================\\nFuriosaAI SDK 0.6.0 is a major release. It includes 234 PRs on performance improvements, added functionalities, and bug fixes, as well as approximately 900 commits.\\nHow to upgrade [\\uf0c1](#how-to-upgrade \"Permalink to this heading\") ---------------------------------------------------------------\\nIf you are using the APT repositories, you easily upgrade with the instructions below: More detailed instructions can be found at [Driver, Firmware, and Runtime Installation](../software/installation.html#requiredpackages) .\\n> ``` > apt-get update && \\\\ > apt-get install furiosa-driver-pdma furiosa-libnpu-warboy furiosa-libnux >  > pip uninstall furiosa-sdk-quantizer furiosa-sdk-runtime furiosa-sdk-validator && \\\\ > pip install --upgrade furiosa-sdk >  > ```\\nMajor changes [\\uf0c1](#major-changes \"Permalink to this heading\") -------------------------------------------------------------\\nThe kernel driver (furiosa-driver-pdma) has been upgraded to 1.2.2, and the user-level driver (furiosa-libnpu-warboy) has been upgraded to 0.5.2, thereby providing more stable and higher NPU performance. Other major changes include the following:\\n### Compiler [\\uf0c1](#compiler \"Permalink to this heading\")\\n* Addition of NPU accelerated operators (see   [List of Supported Operators for Warboy Acceleration](../npu/warboy.html#supportedoperators)   for full list of accelerated operators)      > + Space-to-depth (CRD mode)   > + Transpose   > + Slice (height axis only)   > + Concat (height axis only)   > + Grouped Convolution (if groups <= 128) * Improvements to significantly reduce frequency of CPU tasks in models with   operators that require large memory usage (reduced execution time)\\n### Quantizer [\\uf0c1](#quantizer \"Permalink to this heading\")\\n* Improve model quantization process to ensure idempotency * Remove PyTorch reliance * Improve code quality by removing multiple Pylint warnings * Upgrade multiple library dependencies (e.g. Numpy -> 1.21.5, Pyyaml -> 6.0.0)\\n### Python SDK [\\uf0c1](#python-sdk \"Permalink to this heading\")\\n* Python SDK project structure change      + furiosa-sdk-runtime -> furiosa-sdk   + furiosa-sdk-quantizer -> furiosa-quantizer   + furiosa-sdk-validator -> furiosa-litmus * Validator, a package that checks for model compatibility with Furiosa SDK, is renamed to litmus. Installation instruction has also been updated accordingly.\\nSee [furiosa litmus (Model Compatibility Checker)](../software/cli.html#litmus) for more detailed usage instructions.\\n> ``` > $ pip install \\'furiosa-sdk[litmus]\\' >  > ```\\n#### Furiosa Serving: Addition of FastAPI-based advanced serving library [\\uf0c1](#furiosa-serving-addition-of-fastapi-based-advanced-serving-library \"Permalink to this heading\")\\nfuriosa-serving is based on FastAPI. It allows you to easily add Python-based business logic or image pre/postprocessing code, before or after executing model inference API.\\nYou can install using the following instructions.\\n> ``` > $ pip install \\'furiosa-sdk[serving]\\' >  > ```\\nThe usage example is shown below. You can find more detailed instructions at [furiosa-serving Github](https://github.com/furiosa-ai/furiosa-sdk/tree/main/python/furiosa-serving) .\\n> ``` > from typing import Dict >  > from fastapi import File, UploadFile > from furiosa.server.utils.thread import synchronous > from furiosa.serving import ServeAPI, ServeModel > import numpy as np >  >  > serve = ServeAPI() >  >  > model: ServeModel = synchronous(serve.model)( >     \\'imagenet\\', >     location=\\'./examples/assets/models/image_classification.onnx\\' > ) >  > @model.post(\"/models/imagenet/infer\") > async def infer(image: UploadFile = File(...)) -> Dict: >     # Convert image to Numpy array with your preprocess() function >     tensors: List[np.ndarray] = preprocess(image) >  >     # Infer from ServeModel >     result: List[np.ndarray] = await model.predict(tensors) >  >     # Classify model from numpy array with your postprocess() function >     response: Dict = postprocess(result) >  >     return response >  > ```\\n[Previous](0.7.0.html \"Release Notes - 0.7.0\") [Next](0.5.0.html \"Release Notes - 0.5.0\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Release Notes - 0.6.0\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/releases/0.6.0.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"release-notes-0-6-0\">\\n     <h1>\\n      Release Notes - 0.6.0\\n      <a class=\"headerlink\" href=\"#release-notes-0-6-0\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      FuriosaAI SDK 0.6.0 is a major release.\\nIt includes 234 PRs on performance improvements, added functionalities, and bug fixes,\\nas well as approximately 900 commits.\\n     </p>\\n     <section id=\"how-to-upgrade\">\\n      <h2>\\n       How to upgrade\\n       <a class=\"headerlink\" href=\"#how-to-upgrade\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       If you are using the APT repositories, you easily upgrade with the instructions below:\\nMore detailed instructions can be found at\\n       <a class=\"reference internal\" href=\"../software/installation.html#requiredpackages\">\\n        <span class=\"std std-ref\">\\n         Driver, Firmware, and Runtime Installation\\n        </span>\\n       </a>\\n       .\\n      </p>\\n      <blockquote>\\n       <div>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>apt-get<span class=\"w\"> </span>update<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span><span class=\"se\">\\\\</span>\\napt-get<span class=\"w\"> </span>install<span class=\"w\"> </span>furiosa-driver-pdma<span class=\"w\"> </span>furiosa-libnpu-warboy<span class=\"w\"> </span>furiosa-libnux\\n\\npip<span class=\"w\"> </span>uninstall<span class=\"w\"> </span>furiosa-sdk-quantizer<span class=\"w\"> </span>furiosa-sdk-runtime<span class=\"w\"> </span>furiosa-sdk-validator<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span><span class=\"se\">\\\\</span>\\npip<span class=\"w\"> </span>install<span class=\"w\"> </span>--upgrade<span class=\"w\"> </span>furiosa-sdk\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n      </blockquote>\\n     </section>\\n     <section id=\"major-changes\">\\n      <h2>\\n       Major changes\\n       <a class=\"headerlink\" href=\"#major-changes\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       The kernel driver (furiosa-driver-pdma) has been upgraded to 1.2.2, and the user-level driver (furiosa-libnpu-warboy)\\nhas been upgraded to 0.5.2, thereby providing more stable and higher NPU performance. Other major changes include the following:\\n      </p>\\n      <section id=\"compiler\">\\n       <h3>\\n        Compiler\\n        <a class=\"headerlink\" href=\"#compiler\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <ul>\\n        <li>\\n         <p>\\n          Addition of NPU accelerated operators (see\\n          <a class=\"reference internal\" href=\"../npu/warboy.html#supportedoperators\">\\n           <span class=\"std std-ref\">\\n            List of Supported Operators for Warboy Acceleration\\n           </span>\\n          </a>\\n          for full list of accelerated operators)\\n         </p>\\n         <blockquote>\\n          <div>\\n           <ul class=\"simple\">\\n            <li>\\n             <p>\\n              Space-to-depth (CRD mode)\\n             </p>\\n            </li>\\n            <li>\\n             <p>\\n              Transpose\\n             </p>\\n            </li>\\n            <li>\\n             <p>\\n              Slice (height axis only)\\n             </p>\\n            </li>\\n            <li>\\n             <p>\\n              Concat (height axis only)\\n             </p>\\n            </li>\\n            <li>\\n             <p>\\n              Grouped Convolution (if groups &lt;= 128)\\n             </p>\\n            </li>\\n           </ul>\\n          </div>\\n         </blockquote>\\n        </li>\\n        <li>\\n         <p>\\n          Improvements to significantly reduce frequency of CPU tasks in models with\\noperators that require large memory usage (reduced execution time)\\n         </p>\\n        </li>\\n       </ul>\\n      </section>\\n      <section id=\"quantizer\">\\n       <h3>\\n        Quantizer\\n        <a class=\"headerlink\" href=\"#quantizer\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          Improve model quantization process to ensure idempotency\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Remove PyTorch reliance\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Improve code quality by removing multiple Pylint warnings\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Upgrade multiple library dependencies (e.g. Numpy -&gt; 1.21.5, Pyyaml -&gt; 6.0.0)\\n         </p>\\n        </li>\\n       </ul>\\n      </section>\\n      <section id=\"python-sdk\">\\n       <h3>\\n        Python SDK\\n        <a class=\"headerlink\" href=\"#python-sdk\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          Python SDK project structure change\\n         </p>\\n         <ul>\\n          <li>\\n           <p>\\n            furiosa-sdk-runtime -&gt; furiosa-sdk\\n           </p>\\n          </li>\\n          <li>\\n           <p>\\n            furiosa-sdk-quantizer -&gt; furiosa-quantizer\\n           </p>\\n          </li>\\n          <li>\\n           <p>\\n            furiosa-sdk-validator -&gt; furiosa-litmus\\n           </p>\\n          </li>\\n         </ul>\\n        </li>\\n        <li>\\n         <p>\\n          Validator, a package that checks for model compatibility with Furiosa SDK, is renamed to litmus. Installation instruction has also been updated accordingly.\\n         </p>\\n        </li>\\n       </ul>\\n       <p>\\n        See\\n        <a class=\"reference internal\" href=\"../software/cli.html#litmus\">\\n         <span class=\"std std-ref\">\\n          furiosa litmus (Model Compatibility Checker)\\n         </span>\\n        </a>\\n        for more detailed usage instructions.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-sh notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span>$<span class=\"w\"> </span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span><span class=\"s1\">\\'furiosa-sdk[litmus]\\'</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <section id=\"furiosa-serving-addition-of-fastapi-based-advanced-serving-library\">\\n        <h4>\\n         Furiosa Serving: Addition of FastAPI-based advanced serving library\\n         <a class=\"headerlink\" href=\"#furiosa-serving-addition-of-fastapi-based-advanced-serving-library\" title=\"Permalink to this heading\">\\n          \\uf0c1\\n         </a>\\n        </h4>\\n        <p>\\n         furiosa-serving is based on FastAPI. It allows you to easily add Python-based business logic or image pre/postprocessing code,\\nbefore or after executing model inference API.\\n        </p>\\n        <p>\\n         You can install using the following instructions.\\n        </p>\\n        <blockquote>\\n         <div>\\n          <div class=\"highlight-sh notranslate\">\\n           <div class=\"highlight\">\\n            <pre><span></span>$<span class=\"w\"> </span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span><span class=\"s1\">\\'furiosa-sdk[serving]\\'</span>\\n</pre>\\n           </div>\\n          </div>\\n         </div>\\n        </blockquote>\\n        <p>\\n         The usage example is shown below. You can find more detailed instructions at\\n         <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/tree/main/python/furiosa-serving\">\\n          furiosa-serving Github\\n         </a>\\n         .\\n        </p>\\n        <blockquote>\\n         <div>\\n          <div class=\"highlight-python notranslate\">\\n           <div class=\"highlight\">\\n            <pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">typing</span> <span class=\"kn\">import</span> <span class=\"n\">Dict</span>\\n\\n<span class=\"kn\">from</span> <span class=\"nn\">fastapi</span> <span class=\"kn\">import</span> <span class=\"n\">File</span><span class=\"p\">,</span> <span class=\"n\">UploadFile</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.server.utils.thread</span> <span class=\"kn\">import</span> <span class=\"n\">synchronous</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">furiosa.serving</span> <span class=\"kn\">import</span> <span class=\"n\">ServeAPI</span><span class=\"p\">,</span> <span class=\"n\">ServeModel</span>\\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\\n\\n\\n<span class=\"n\">serve</span> <span class=\"o\">=</span> <span class=\"n\">ServeAPI</span><span class=\"p\">()</span>\\n\\n\\n<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">ServeModel</span> <span class=\"o\">=</span> <span class=\"n\">synchronous</span><span class=\"p\">(</span><span class=\"n\">serve</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">)(</span>\\n    <span class=\"s1\">\\'imagenet\\'</span><span class=\"p\">,</span>\\n    <span class=\"n\">location</span><span class=\"o\">=</span><span class=\"s1\">\\'./examples/assets/models/image_classification.onnx\\'</span>\\n<span class=\"p\">)</span>\\n\\n<span class=\"nd\">@model</span><span class=\"o\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"s2\">\"/models/imagenet/infer\"</span><span class=\"p\">)</span>\\n<span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">infer</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">UploadFile</span> <span class=\"o\">=</span> <span class=\"n\">File</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">))</span> <span class=\"o\">-&gt;</span> <span class=\"n\">Dict</span><span class=\"p\">:</span>\\n    <span class=\"c1\"># Convert image to Numpy array with your preprocess() function</span>\\n    <span class=\"n\">tensors</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">preprocess</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>\\n\\n    <span class=\"c1\"># Infer from ServeModel</span>\\n    <span class=\"n\">result</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">tensors</span><span class=\"p\">)</span>\\n\\n    <span class=\"c1\"># Classify model from numpy array with your postprocess() function</span>\\n    <span class=\"n\">response</span><span class=\"p\">:</span> <span class=\"n\">Dict</span> <span class=\"o\">=</span> <span class=\"n\">postprocess</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\\n\\n    <span class=\"k\">return</span> <span class=\"n\">response</span>\\n</pre>\\n           </div>\\n          </div>\\n         </div>\\n        </blockquote>\\n       </section>\\n      </section>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"0.7.0.html\" rel=\"prev\" title=\"Release Notes - 0.7.0\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"0.5.0.html\" rel=\"next\" title=\"Release Notes - 0.5.0\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='ab2af890-296a-47f5-89d9-ffc5cf4a924f', link=Url('https://furiosa-ai.github.io/docs/latest/en/software/python-sdk.html'), name='python-sdk', parent='', child=[], description='\\n\\n\\n* Python SDK installation and user guide\\n* [View page source](../_sources/software/python-sdk.rst.txt)\\n\\n---\\n\\n\\n\\nPython SDK installation and user guide\\n[\\uf0c1](#python-sdk-installation-and-user-guide \"Permalink to this heading\")\\n===============================================================================================================\\n\\nFuriosaAI Python SDK is a software development kit for writing Python applications that use the NPU.\\nWith the Python SDK, you can utilize various tools, libraries, and frameworks of the Python ecosystem that are most widely used in the AI/ML field for developing NPU applications.\\nPython SDK consists of various modules and provides an inference API, a quantization API, a command line tool, and a server program for serving.\\n\\nRequirements\\n[\\uf0c1](#requirements \"Permalink to this heading\")\\n-----------------------------------------------------------\\n\\n* Ubuntu 20.04 LTS (Debian bullseye) or higher\\n* [FuriosaAI SDK required packages](installation.html#requiredpackages)\\n* Python 3.8 or higher (See\\n  [Python execution environment setup](#setuppython)\\n  for setup Python environment)\\n* Latest version of pip\\n\\nTo install and use Python SDK, follow the\\n[Installing required packages](installation.html#requiredpackages)\\nguide.\\nYou need to install the required kernel driver, firmware, and runtime library.\\n\\n\\nPython execution environment setup\\n[\\uf0c1](#python-execution-environment-setup \"Permalink to this heading\")\\n-------------------------------------------------------------------------------------------------------\\n\\nPython SDK requires Python 3.8 or above. Here, we describe Python execution environment configuration.\\n\\nNote\\n\\nIf you are not using the FuriosaAI Python SDK, or if you are familiar with configuring a Python execution environment, you can skip this section.\\n\\nYou can check the Python version currently installed in your system with the command below.\\n\\n```\\npython --version\\nPython 3.8.10\\n\\n```\\n\\nIf the Python command does not exist, or if your Python version is below 3.8, configure the Python environment\\nby selecting one of the methods below.\\n\\n* [Python environment configuration with Conda](#condainstall)\\n  (recommended):\\n  [Conda](https://docs.conda.io/projects/conda/en/latest/index.html)\\n  allows you to\\n  configure a separate, isolated Python environment for specific Python applications.\\n  Conda therefore prevents the package dependency issues or Python version issues that users\\n  often encounter when installing Python applications.\\n* Configure the Python execution environment directly on the\\n  [Configuring Python environment using Linux packages](#setuppythononlinux)\\n  : Linux system.\\n  You can select this option if you are not concerned about conflicts with other Python execution environments.\\n\\n### Python environment configuration with Conda [\\uf0c1](#python-environment-configuration-with-conda \"Permalink to this heading\")\\n\\nConda makes it easy to configure a isolated Python environment for a specific Python application.\\nTo find out more about Conda, refer to readings available in\\n[Conda](https://docs.conda.io/projects/conda/en/latest/index.html)\\n.\\n\\nYou can get started by downloading the installer as shown below.\\nSelect\\nyes\\n\\nto all questions when running\\n`sh\\n\\n./Miniconda3-latest-Linux-x86_64.sh`\\n.\\n\\n```\\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\\nsh ./Miniconda3-latest-Linux-x86_64.sh\\nsource ~/.bashrc\\nconda --version\\n\\n```\\n\\n\\n#### Creating and activating isolated Python execution environment [\\uf0c1](#creating-and-activating-isolated-python-execution-environment \"Permalink to this heading\")\\n\\nAfter installing Anaconda, you can create an isolated Python execution environment and activate it as needed.\\n\\n1. If you want to use Python 3.8, create an execution environment with the name\\n   `furiosa-3.8`\\n   , by using the following command.\\n\\n```\\nconda create -n furiosa-3.8 python=3.8\\n\\n```\\n\\n2. The created Python 3.8 environment is activated with the\\n   `activate`\\n   command.\\n\\n```\\nconda activate furiosa-3.8\\n# version check\\npython --version\\n\\n```\\n\\n3. Once the Python execution environment is activated, install the Python SDK as explained in\\n   [Installing Python SDK package](#installpippackages)\\n   .\\n4. If you wish to terminate the Python execution environment, use the\\n   `deactivate`\\n   command.\\n\\n```\\n$ conda deactivate\\n\\n```\\n\\nAn environment created once can be used again at any time with the\\n`activate`\\ncommand.\\nPackages that have been installed do not need to be reinstalled after activation.\\n\\n\\n\\n### Configuring Python environment using Linux packages [\\uf0c1](#configuring-python-environment-using-linux-packages \"Permalink to this heading\")\\n\\n1. If you can configure the Python environment directly on the system, install the necessary packages as shown below.\\n\\n```\\nsudo apt install -y python3 python3-pip python-is-python3\\n\\n```\\n\\n2. Check the Python version to ensure proper installation.\\n\\n```\\npython --version\\nPython 3.8.10\\n\\n```\\n\\n\\n\\n\\nInstalling Python SDK package\\n[\\uf0c1](#installing-python-sdk-package \"Permalink to this heading\")\\n---------------------------------------------------------------------------------------------\\n\\nBefore installing the furiosa-sdk, you need to update Python’s package installer to the latest version.\\n\\n```\\npip install --upgrade pip setuptools wheel\\n\\n```\\n\\n\\nWarning\\n\\nIf you install the furiosa-sdk without updating to the latest version, you may encounter the following error.\\n\\n```\\nERROR: Could not find a version that satisfies the requirement furiosa-quantizer-impl==0.10.* (from furiosa-quantizer==0.10.*->furiosa-sdk) (from versions: none)\\nERROR: No matching distribution found for furiosa-quantizer-impl==0.10.* (from furiosa-quantizer==0.10.*->furiosa-sdk)\\n\\n```\\n\\n\\n\\nInstalling with PIP\\n\\nInstalling with the source code\\n\\n\\nFuriosaAI Python SDK package is uploaded on the\\n[pypi](https://pypi.org/)\\nrepository,\\nso you can easily install it as shown by using the\\n`pip`\\ncommand.\\n\\n```\\npip install furiosa-sdk\\n\\n```\\n\\nThe package contains a compiler command line interface and an inference API.\\nRefer to\\n[furiosa-compiler](compiler.html#compilercli)\\nand\\n[Tutorial and Code Examples](tutorials.html#tutorial)\\nfor detailed usage guides.\\n\\nAdditional functions are provided in the form of Python extra packages, and you can select and\\ninstall packages as you require from\\n[Extra packages](#pythonextrapackages)\\n.\\nFor example, if you need to install\\nserver`\\n\\nfor model serving and\\n`litmus`\\nto check the compatibility between model and SDK, specify the extension package as follows.\\n\\n```\\npip install \\'furiosa-sdk[server, litmus]\\'\\n\\n```\\n\\n\\n\\nDownload the source code from\\n[FuriosaAI Github repository](https://github.com/furiosa-ai/furiosa-sdk)\\nand install the packages in the following order.\\n\\n```\\ngit clone https://github.com/furiosa-ai/furiosa-sdk\\ncd furiosa-sdk/python\\npip install furiosa-runtime\\npip install furiosa-tools\\npip install furiosa-sdk\\n\\n```\\n\\nIf you wish to install extra packages, install the Python module in the subdirectory of furiosa-sdk/python.\\nFor example, if you want to install a model server, install it according to the order of dependencies as follows.\\n\\n```\\ncd furiosa-sdk/python\\npip install furiosa-server\\n\\n```\\n\\n\\n\\n\\n\\nExtra packages\\n[\\uf0c1](#extra-packages \"Permalink to this heading\")\\n---------------------------------------------------------------\\n\\n### Legacy Runtime/API [\\uf0c1](#legacy-runtime-api \"Permalink to this heading\")\\n\\nRather than the next-generation runtime and its API newly adoted since 0.10.0,\\nyou can install furiosa-sdk with the legacy runtime and API as follows:\\n\\n```\\npip install \\'furiosa-sdk[legacy]\\'\\n\\n```\\n\\n\\n\\n### FuriosaAI Models [\\uf0c1](#furiosaai-models \"Permalink to this heading\")\\n\\nIt can be executed directly on the NPU and provides optimized DNN model architecture, pre-trained\\nmodel image, among others, in the form of a Python module.\\nYou can install them with the following command.\\n\\n```\\npip install \\'furiosa-sdk[models]\\'\\n\\n```\\n\\n\\n\\n### Quantizer [\\uf0c1](#quantizer \"Permalink to this heading\")\\n\\nThe quantizer package provides a set of APIs for converting a model into a quantized model.\\nYou can find more information about the quantization function provided by the Furiosa SDK and the NPU\\nat\\n[Model Quantization](quantization.html#modelquantization)\\n.\\n\\n```\\npip install \\'furiosa-sdk[quantizer]\\'\\n\\n```\\n\\n\\n\\n### Model Server [\\uf0c1](#model-server \"Permalink to this heading\")\\n\\nProvides the function of accelerating DNN model with the NPU, and serving it with GRPC or Restful API.\\n\\n```\\npip install \\'furiosa-sdk[server]\\'\\n\\n```\\n\\n\\n\\n### Litmus [\\uf0c1](#litmus \"Permalink to this heading\")\\n\\nA tool to check whether the specified model is compatible with the Furiosa SDK.\\nHere, we simulate execution of processes such as model quantization and compilation.\\n\\n```\\npip install \\'furiosa-sdk[litmus]\\'\\n\\n```\\n\\n\\n\\n\\n\\n\\n\\n[Previous](installation.html \"Driver, Firmware, and Runtime Installation\")\\n[Next](c-sdk.html \"C SDK installation and user guide\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Python SDK installation and user guide * [View page source](../_sources/software/python-sdk.rst.txt)\\n---\\nPython SDK installation and user guide [\\uf0c1](#python-sdk-installation-and-user-guide \"Permalink to this heading\") ===============================================================================================================\\nFuriosaAI Python SDK is a software development kit for writing Python applications that use the NPU. With the Python SDK, you can utilize various tools, libraries, and frameworks of the Python ecosystem that are most widely used in the AI/ML field for developing NPU applications. Python SDK consists of various modules and provides an inference API, a quantization API, a command line tool, and a server program for serving.\\nRequirements [\\uf0c1](#requirements \"Permalink to this heading\") -----------------------------------------------------------\\n* Ubuntu 20.04 LTS (Debian bullseye) or higher * [FuriosaAI SDK required packages](installation.html#requiredpackages) * Python 3.8 or higher (See   [Python execution environment setup](#setuppython)   for setup Python environment) * Latest version of pip\\nTo install and use Python SDK, follow the [Installing required packages](installation.html#requiredpackages) guide. You need to install the required kernel driver, firmware, and runtime library.\\nPython execution environment setup [\\uf0c1](#python-execution-environment-setup \"Permalink to this heading\") -------------------------------------------------------------------------------------------------------\\nPython SDK requires Python 3.8 or above. Here, we describe Python execution environment configuration.\\nNote\\nIf you are not using the FuriosaAI Python SDK, or if you are familiar with configuring a Python execution environment, you can skip this section.\\nYou can check the Python version currently installed in your system with the command below.\\n``` python --version Python 3.8.10\\n```\\nIf the Python command does not exist, or if your Python version is below 3.8, configure the Python environment by selecting one of the methods below.\\n* [Python environment configuration with Conda](#condainstall)   (recommended):   [Conda](https://docs.conda.io/projects/conda/en/latest/index.html)   allows you to   configure a separate, isolated Python environment for specific Python applications.   Conda therefore prevents the package dependency issues or Python version issues that users   often encounter when installing Python applications. * Configure the Python execution environment directly on the   [Configuring Python environment using Linux packages](#setuppythononlinux)   : Linux system.   You can select this option if you are not concerned about conflicts with other Python execution environments.\\n### Python environment configuration with Conda [\\uf0c1](#python-environment-configuration-with-conda \"Permalink to this heading\")\\nConda makes it easy to configure a isolated Python environment for a specific Python application. To find out more about Conda, refer to readings available in [Conda](https://docs.conda.io/projects/conda/en/latest/index.html) .\\nYou can get started by downloading the installer as shown below. Select yes\\nto all questions when running `sh\\n./Miniconda3-latest-Linux-x86_64.sh` .\\n``` wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh sh ./Miniconda3-latest-Linux-x86_64.sh source ~/.bashrc conda --version\\n```\\n#### Creating and activating isolated Python execution environment [\\uf0c1](#creating-and-activating-isolated-python-execution-environment \"Permalink to this heading\")\\nAfter installing Anaconda, you can create an isolated Python execution environment and activate it as needed.\\n1. If you want to use Python 3.8, create an execution environment with the name    `furiosa-3.8`    , by using the following command.\\n``` conda create -n furiosa-3.8 python=3.8\\n```\\n2. The created Python 3.8 environment is activated with the    `activate`    command.\\n``` conda activate furiosa-3.8 # version check python --version\\n```\\n3. Once the Python execution environment is activated, install the Python SDK as explained in    [Installing Python SDK package](#installpippackages)    . 4. If you wish to terminate the Python execution environment, use the    `deactivate`    command.\\n``` $ conda deactivate\\n```\\nAn environment created once can be used again at any time with the `activate` command. Packages that have been installed do not need to be reinstalled after activation.\\n### Configuring Python environment using Linux packages [\\uf0c1](#configuring-python-environment-using-linux-packages \"Permalink to this heading\")\\n1. If you can configure the Python environment directly on the system, install the necessary packages as shown below.\\n``` sudo apt install -y python3 python3-pip python-is-python3\\n```\\n2. Check the Python version to ensure proper installation.\\n``` python --version Python 3.8.10\\n```\\nInstalling Python SDK package [\\uf0c1](#installing-python-sdk-package \"Permalink to this heading\") ---------------------------------------------------------------------------------------------\\nBefore installing the furiosa-sdk, you need to update Python’s package installer to the latest version.\\n``` pip install --upgrade pip setuptools wheel\\n```\\nWarning\\nIf you install the furiosa-sdk without updating to the latest version, you may encounter the following error.\\n``` ERROR: Could not find a version that satisfies the requirement furiosa-quantizer-impl==0.10.* (from furiosa-quantizer==0.10.*->furiosa-sdk) (from versions: none) ERROR: No matching distribution found for furiosa-quantizer-impl==0.10.* (from furiosa-quantizer==0.10.*->furiosa-sdk)\\n```\\nInstalling with PIP\\nInstalling with the source code\\nFuriosaAI Python SDK package is uploaded on the [pypi](https://pypi.org/) repository, so you can easily install it as shown by using the `pip` command.\\n``` pip install furiosa-sdk\\n```\\nThe package contains a compiler command line interface and an inference API. Refer to [furiosa-compiler](compiler.html#compilercli) and [Tutorial and Code Examples](tutorials.html#tutorial) for detailed usage guides.\\nAdditional functions are provided in the form of Python extra packages, and you can select and install packages as you require from [Extra packages](#pythonextrapackages) .\\nFor example, if you need to install server`\\nfor model serving and `litmus` to check the compatibility between model and SDK, specify the extension package as follows.\\n``` pip install \\'furiosa-sdk[server, litmus]\\'\\n```\\nDownload the source code from [FuriosaAI Github repository](https://github.com/furiosa-ai/furiosa-sdk) and install the packages in the following order.\\n``` git clone https://github.com/furiosa-ai/furiosa-sdk cd furiosa-sdk/python pip install furiosa-runtime pip install furiosa-tools pip install furiosa-sdk\\n```\\nIf you wish to install extra packages, install the Python module in the subdirectory of furiosa-sdk/python. For example, if you want to install a model server, install it according to the order of dependencies as follows.\\n``` cd furiosa-sdk/python pip install furiosa-server\\n```\\nExtra packages [\\uf0c1](#extra-packages \"Permalink to this heading\") ---------------------------------------------------------------\\n### Legacy Runtime/API [\\uf0c1](#legacy-runtime-api \"Permalink to this heading\")\\nRather than the next-generation runtime and its API newly adoted since 0.10.0, you can install furiosa-sdk with the legacy runtime and API as follows:\\n``` pip install \\'furiosa-sdk[legacy]\\'\\n```\\n### FuriosaAI Models [\\uf0c1](#furiosaai-models \"Permalink to this heading\")\\nIt can be executed directly on the NPU and provides optimized DNN model architecture, pre-trained model image, among others, in the form of a Python module. You can install them with the following command.\\n``` pip install \\'furiosa-sdk[models]\\'\\n```\\n### Quantizer [\\uf0c1](#quantizer \"Permalink to this heading\")\\nThe quantizer package provides a set of APIs for converting a model into a quantized model. You can find more information about the quantization function provided by the Furiosa SDK and the NPU at [Model Quantization](quantization.html#modelquantization) .\\n``` pip install \\'furiosa-sdk[quantizer]\\'\\n```\\n### Model Server [\\uf0c1](#model-server \"Permalink to this heading\")\\nProvides the function of accelerating DNN model with the NPU, and serving it with GRPC or Restful API.\\n``` pip install \\'furiosa-sdk[server]\\'\\n```\\n### Litmus [\\uf0c1](#litmus \"Permalink to this heading\")\\nA tool to check whether the specified model is compatible with the Furiosa SDK. Here, we simulate execution of processes such as model quantization and compilation.\\n``` pip install \\'furiosa-sdk[litmus]\\'\\n```\\n[Previous](installation.html \"Driver, Firmware, and Runtime Installation\") [Next](c-sdk.html \"C SDK installation and user guide\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Python SDK installation and user guide\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/python-sdk.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"python-sdk-installation-and-user-guide\">\\n     <span id=\"pythonsdk\">\\n     </span>\\n     <h1>\\n      Python SDK installation and user guide\\n      <a class=\"headerlink\" href=\"#python-sdk-installation-and-user-guide\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      FuriosaAI Python SDK is a software development kit for writing Python applications that use the NPU.\\nWith the Python SDK, you can utilize various tools, libraries, and frameworks of the Python ecosystem that are most widely used in the AI/ML field for developing NPU applications.\\nPython SDK consists of various modules and provides an inference API, a quantization API, a command line tool, and a server program for serving.\\n     </p>\\n     <section id=\"requirements\">\\n      <h2>\\n       Requirements\\n       <a class=\"headerlink\" href=\"#requirements\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         Ubuntu 20.04 LTS (Debian bullseye) or higher\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"installation.html#requiredpackages\">\\n          <span class=\"std std-ref\">\\n           FuriosaAI SDK required packages\\n          </span>\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Python 3.8 or higher (See\\n         <a class=\"reference internal\" href=\"#setuppython\">\\n          <span class=\"std std-ref\">\\n           Python execution environment setup\\n          </span>\\n         </a>\\n         for setup Python environment)\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Latest version of pip\\n        </p>\\n       </li>\\n      </ul>\\n      <p>\\n       To install and use Python SDK, follow the\\n       <a class=\"reference internal\" href=\"installation.html#requiredpackages\">\\n        <span class=\"std std-ref\">\\n         Installing required packages\\n        </span>\\n       </a>\\n       guide.\\nYou need to install the required kernel driver, firmware, and runtime library.\\n      </p>\\n     </section>\\n     <section id=\"python-execution-environment-setup\">\\n      <span id=\"setuppython\">\\n      </span>\\n      <h2>\\n       Python execution environment setup\\n       <a class=\"headerlink\" href=\"#python-execution-environment-setup\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       Python SDK requires Python 3.8 or above. Here, we describe Python execution environment configuration.\\n      </p>\\n      <div class=\"admonition note\">\\n       <p class=\"admonition-title\">\\n        Note\\n       </p>\\n       <p>\\n        If you are not using the FuriosaAI Python SDK, or if you are familiar with configuring a Python execution environment, you can skip this section.\\n       </p>\\n      </div>\\n      <p>\\n       You can check the Python version currently installed in your system with the command below.\\n      </p>\\n      <div class=\"highlight-default notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span><span class=\"n\">python</span> <span class=\"o\">--</span><span class=\"n\">version</span>\\n<span class=\"n\">Python</span> <span class=\"mf\">3.8.10</span>\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       If the Python command does not exist, or if your Python version is below 3.8, configure the Python environment\\nby selecting one of the methods below.\\n      </p>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"#condainstall\">\\n          <span class=\"std std-ref\">\\n           Python environment configuration with Conda\\n          </span>\\n         </a>\\n         (recommended):\\n         <a class=\"reference external\" href=\"https://docs.conda.io/projects/conda/en/latest/index.html\">\\n          Conda\\n         </a>\\n         allows you to\\nconfigure a separate, isolated Python environment for specific Python applications.\\nConda therefore prevents the package dependency issues or Python version issues that users\\noften encounter when installing Python applications.\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         Configure the Python execution environment directly on the\\n         <a class=\"reference internal\" href=\"#setuppythononlinux\">\\n          <span class=\"std std-ref\">\\n           Configuring Python environment using Linux packages\\n          </span>\\n         </a>\\n         : Linux system.\\nYou can select this option if you are not concerned about conflicts with other Python execution environments.\\n        </p>\\n       </li>\\n      </ul>\\n      <section id=\"python-environment-configuration-with-conda\">\\n       <span id=\"condainstall\">\\n       </span>\\n       <h3>\\n        Python environment configuration with Conda\\n        <a class=\"headerlink\" href=\"#python-environment-configuration-with-conda\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Conda makes it easy to configure a isolated Python environment for a specific Python application.\\nTo find out more about Conda, refer to readings available in\\n        <a class=\"reference external\" href=\"https://docs.conda.io/projects/conda/en/latest/index.html\">\\n         Conda\\n        </a>\\n        .\\n       </p>\\n       <p>\\n        You can get started by downloading the installer as shown below.\\nSelect\\n        <cite>\\n         yes\\n        </cite>\\n        to all questions when running\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          sh\\n         </span>\\n         <span class=\"pre\">\\n          ./Miniconda3-latest-Linux-x86_64.sh\\n         </span>\\n        </code>\\n        .\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"n\">wget</span> <span class=\"n\">https</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">repo</span><span class=\"o\">.</span><span class=\"n\">anaconda</span><span class=\"o\">.</span><span class=\"n\">com</span><span class=\"o\">/</span><span class=\"n\">miniconda</span><span class=\"o\">/</span><span class=\"n\">Miniconda3</span><span class=\"o\">-</span><span class=\"n\">latest</span><span class=\"o\">-</span><span class=\"n\">Linux</span><span class=\"o\">-</span><span class=\"n\">x86_64</span><span class=\"o\">.</span><span class=\"n\">sh</span>\\n<span class=\"n\">sh</span> <span class=\"o\">./</span><span class=\"n\">Miniconda3</span><span class=\"o\">-</span><span class=\"n\">latest</span><span class=\"o\">-</span><span class=\"n\">Linux</span><span class=\"o\">-</span><span class=\"n\">x86_64</span><span class=\"o\">.</span><span class=\"n\">sh</span>\\n<span class=\"n\">source</span> <span class=\"o\">~/.</span><span class=\"n\">bashrc</span>\\n<span class=\"n\">conda</span> <span class=\"o\">--</span><span class=\"n\">version</span>\\n</pre>\\n        </div>\\n       </div>\\n       <section id=\"creating-and-activating-isolated-python-execution-environment\">\\n        <h4>\\n         Creating and activating isolated Python execution environment\\n         <a class=\"headerlink\" href=\"#creating-and-activating-isolated-python-execution-environment\" title=\"Permalink to this heading\">\\n          \\uf0c1\\n         </a>\\n        </h4>\\n        <p>\\n         After installing Anaconda, you can create an isolated Python execution environment and activate it as needed.\\n        </p>\\n        <ol class=\"arabic simple\">\\n         <li>\\n          <p>\\n           If you want to use Python 3.8, create an execution environment with the name\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             furiosa-3.8\\n            </span>\\n           </code>\\n           , by using the following command.\\n          </p>\\n         </li>\\n        </ol>\\n        <div class=\"highlight-default notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span><span class=\"n\">conda</span> <span class=\"n\">create</span> <span class=\"o\">-</span><span class=\"n\">n</span> <span class=\"n\">furiosa</span><span class=\"o\">-</span><span class=\"mf\">3.8</span> <span class=\"n\">python</span><span class=\"o\">=</span><span class=\"mf\">3.8</span>\\n</pre>\\n         </div>\\n        </div>\\n        <ol class=\"arabic simple\" start=\"2\">\\n         <li>\\n          <p>\\n           The created Python 3.8 environment is activated with the\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             activate\\n            </span>\\n           </code>\\n           command.\\n          </p>\\n         </li>\\n        </ol>\\n        <div class=\"highlight-default notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span><span class=\"n\">conda</span> <span class=\"n\">activate</span> <span class=\"n\">furiosa</span><span class=\"o\">-</span><span class=\"mf\">3.8</span>\\n<span class=\"c1\"># version check</span>\\n<span class=\"n\">python</span> <span class=\"o\">--</span><span class=\"n\">version</span>\\n</pre>\\n         </div>\\n        </div>\\n        <ol class=\"arabic simple\" start=\"3\">\\n         <li>\\n          <p>\\n           Once the Python execution environment is activated, install the Python SDK as explained in\\n           <a class=\"reference internal\" href=\"#installpippackages\">\\n            <span class=\"std std-ref\">\\n             Installing Python SDK package\\n            </span>\\n           </a>\\n           .\\n          </p>\\n         </li>\\n         <li>\\n          <p>\\n           If you wish to terminate the Python execution environment, use the\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             deactivate\\n            </span>\\n           </code>\\n           command.\\n          </p>\\n         </li>\\n        </ol>\\n        <div class=\"highlight-default notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>$ conda deactivate\\n</pre>\\n         </div>\\n        </div>\\n        <p>\\n         An environment created once can be used again at any time with the\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           activate\\n          </span>\\n         </code>\\n         command.\\nPackages that have been installed do not need to be reinstalled after activation.\\n        </p>\\n       </section>\\n      </section>\\n      <section id=\"configuring-python-environment-using-linux-packages\">\\n       <span id=\"setuppythononlinux\">\\n       </span>\\n       <h3>\\n        Configuring Python environment using Linux packages\\n        <a class=\"headerlink\" href=\"#configuring-python-environment-using-linux-packages\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <ol class=\"arabic simple\">\\n        <li>\\n         <p>\\n          If you can configure the Python environment directly on the system, install the necessary packages as shown below.\\n         </p>\\n        </li>\\n       </ol>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"n\">sudo</span> <span class=\"n\">apt</span> <span class=\"n\">install</span> <span class=\"o\">-</span><span class=\"n\">y</span> <span class=\"n\">python3</span> <span class=\"n\">python3</span><span class=\"o\">-</span><span class=\"n\">pip</span> <span class=\"n\">python</span><span class=\"o\">-</span><span class=\"ow\">is</span><span class=\"o\">-</span><span class=\"n\">python3</span>\\n</pre>\\n        </div>\\n       </div>\\n       <ol class=\"arabic simple\" start=\"2\">\\n        <li>\\n         <p>\\n          Check the Python version to ensure proper installation.\\n         </p>\\n        </li>\\n       </ol>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"n\">python</span> <span class=\"o\">--</span><span class=\"n\">version</span>\\n<span class=\"n\">Python</span> <span class=\"mf\">3.8.10</span>\\n</pre>\\n        </div>\\n       </div>\\n      </section>\\n     </section>\\n     <section id=\"installing-python-sdk-package\">\\n      <span id=\"installpippackages\">\\n      </span>\\n      <h2>\\n       Installing Python SDK package\\n       <a class=\"headerlink\" href=\"#installing-python-sdk-package\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       Before installing the furiosa-sdk, you need to update Python’s package installer to the latest version.\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>--upgrade<span class=\"w\"> </span>pip<span class=\"w\"> </span>setuptools<span class=\"w\"> </span>wheel\\n</pre>\\n       </div>\\n      </div>\\n      <div class=\"admonition warning\">\\n       <p class=\"admonition-title\">\\n        Warning\\n       </p>\\n       <p>\\n        If you install the furiosa-sdk without updating to the latest version, you may encounter the following error.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>ERROR:<span class=\"w\"> </span>Could<span class=\"w\"> </span>not<span class=\"w\"> </span>find<span class=\"w\"> </span>a<span class=\"w\"> </span>version<span class=\"w\"> </span>that<span class=\"w\"> </span>satisfies<span class=\"w\"> </span>the<span class=\"w\"> </span>requirement<span class=\"w\"> </span>furiosa-quantizer-impl<span class=\"o\">==</span><span class=\"m\">0</span>.10.*<span class=\"w\"> </span><span class=\"o\">(</span>from<span class=\"w\"> </span>furiosa-quantizer<span class=\"o\">==</span><span class=\"m\">0</span>.10.*-&gt;furiosa-sdk<span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span>from<span class=\"w\"> </span>versions:<span class=\"w\"> </span>none<span class=\"o\">)</span>\\nERROR:<span class=\"w\"> </span>No<span class=\"w\"> </span>matching<span class=\"w\"> </span>distribution<span class=\"w\"> </span>found<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>furiosa-quantizer-impl<span class=\"o\">==</span><span class=\"m\">0</span>.10.*<span class=\"w\"> </span><span class=\"o\">(</span>from<span class=\"w\"> </span>furiosa-quantizer<span class=\"o\">==</span><span class=\"m\">0</span>.10.*-&gt;furiosa-sdk<span class=\"o\">)</span>\\n</pre>\\n        </div>\\n       </div>\\n      </div>\\n      <div class=\"sphinx-tabs docutils container\">\\n       <div aria-label=\"Tabbed content\" role=\"tablist\">\\n        <button aria-controls=\"panel-0-0-0\" aria-selected=\"true\" class=\"sphinx-tabs-tab\" id=\"tab-0-0-0\" name=\"0-0\" role=\"tab\" tabindex=\"0\">\\n         Installing with PIP\\n        </button>\\n        <button aria-controls=\"panel-0-0-1\" aria-selected=\"false\" class=\"sphinx-tabs-tab\" id=\"tab-0-0-1\" name=\"0-1\" role=\"tab\" tabindex=\"-1\">\\n         Installing with the source code\\n        </button>\\n       </div>\\n       <div aria-labelledby=\"tab-0-0-0\" class=\"sphinx-tabs-panel\" id=\"panel-0-0-0\" name=\"0-0\" role=\"tabpanel\" tabindex=\"0\">\\n        <p>\\n         FuriosaAI Python SDK package is uploaded on the\\n         <a class=\"reference external\" href=\"https://pypi.org/\">\\n          pypi\\n         </a>\\n         repository,\\nso you can easily install it as shown by using the\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           pip\\n          </span>\\n         </code>\\n         command.\\n        </p>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>furiosa-sdk\\n</pre>\\n         </div>\\n        </div>\\n        <p>\\n         The package contains a compiler command line interface and an inference API.\\nRefer to\\n         <a class=\"reference internal\" href=\"compiler.html#compilercli\">\\n          <span class=\"std std-ref\">\\n           furiosa-compiler\\n          </span>\\n         </a>\\n         and\\n         <a class=\"reference internal\" href=\"tutorials.html#tutorial\">\\n          <span class=\"std std-ref\">\\n           Tutorial and Code Examples\\n          </span>\\n         </a>\\n         for detailed usage guides.\\n        </p>\\n        <p>\\n         Additional functions are provided in the form of Python extra packages, and you can select and\\ninstall packages as you require from\\n         <a class=\"reference internal\" href=\"#pythonextrapackages\">\\n          <span class=\"std std-ref\">\\n           Extra packages\\n          </span>\\n         </a>\\n         .\\nFor example, if you need to install\\n         <cite>\\n          server`\\n         </cite>\\n         for model serving and\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           litmus\\n          </span>\\n         </code>\\n         to check the compatibility between model and SDK, specify the extension package as follows.\\n        </p>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span><span class=\"s1\">\\'furiosa-sdk[server, litmus]\\'</span>\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n       <div aria-labelledby=\"tab-0-0-1\" class=\"sphinx-tabs-panel\" hidden=\"true\" id=\"panel-0-0-1\" name=\"0-1\" role=\"tabpanel\" tabindex=\"0\">\\n        <p>\\n         Download the source code from\\n         <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk\">\\n          FuriosaAI Github repository\\n         </a>\\n         and install the packages in the following order.\\n        </p>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>git<span class=\"w\"> </span>clone<span class=\"w\"> </span>https://github.com/furiosa-ai/furiosa-sdk\\n<span class=\"nb\">cd</span><span class=\"w\"> </span>furiosa-sdk/python\\npip<span class=\"w\"> </span>install<span class=\"w\"> </span>furiosa-runtime\\npip<span class=\"w\"> </span>install<span class=\"w\"> </span>furiosa-tools\\npip<span class=\"w\"> </span>install<span class=\"w\"> </span>furiosa-sdk\\n</pre>\\n         </div>\\n        </div>\\n        <p>\\n         If you wish to install extra packages, install the Python module in the subdirectory of furiosa-sdk/python.\\nFor example, if you want to install a model server, install it according to the order of dependencies as follows.\\n        </p>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span><span class=\"nb\">cd</span><span class=\"w\"> </span>furiosa-sdk/python\\npip<span class=\"w\"> </span>install<span class=\"w\"> </span>furiosa-server\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"extra-packages\">\\n      <span id=\"pythonextrapackages\">\\n      </span>\\n      <h2>\\n       Extra packages\\n       <a class=\"headerlink\" href=\"#extra-packages\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <section id=\"legacy-runtime-api\">\\n       <h3>\\n        Legacy Runtime/API\\n        <a class=\"headerlink\" href=\"#legacy-runtime-api\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Rather than the next-generation runtime and its API newly adoted since 0.10.0,\\nyou can install furiosa-sdk with the legacy runtime and API as follows:\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span><span class=\"s1\">\\'furiosa-sdk[legacy]\\'</span>\\n</pre>\\n        </div>\\n       </div>\\n      </section>\\n      <section id=\"furiosaai-models\">\\n       <h3>\\n        FuriosaAI Models\\n        <a class=\"headerlink\" href=\"#furiosaai-models\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        It can be executed directly on the NPU and provides optimized DNN model architecture, pre-trained\\nmodel image, among others, in the form of a Python module.\\nYou can install them with the following command.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span><span class=\"s1\">\\'furiosa-sdk[models]\\'</span>\\n</pre>\\n        </div>\\n       </div>\\n      </section>\\n      <section id=\"quantizer\">\\n       <h3>\\n        Quantizer\\n        <a class=\"headerlink\" href=\"#quantizer\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        The quantizer package provides a set of APIs for converting a model into a quantized model.\\nYou can find more information about the quantization function provided by the Furiosa SDK and the NPU\\nat\\n        <a class=\"reference internal\" href=\"quantization.html#modelquantization\">\\n         <span class=\"std std-ref\">\\n          Model Quantization\\n         </span>\\n        </a>\\n        .\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span><span class=\"s1\">\\'furiosa-sdk[quantizer]\\'</span>\\n</pre>\\n        </div>\\n       </div>\\n      </section>\\n      <section id=\"model-server\">\\n       <h3>\\n        Model Server\\n        <a class=\"headerlink\" href=\"#model-server\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Provides the function of accelerating DNN model with the NPU, and serving it with GRPC or Restful API.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span><span class=\"s1\">\\'furiosa-sdk[server]\\'</span>\\n</pre>\\n        </div>\\n       </div>\\n      </section>\\n      <section id=\"litmus\">\\n       <h3>\\n        Litmus\\n        <a class=\"headerlink\" href=\"#litmus\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        A tool to check whether the specified model is compatible with the Furiosa SDK.\\nHere, we simulate execution of processes such as model quantization and compilation.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span><span class=\"s1\">\\'furiosa-sdk[litmus]\\'</span>\\n</pre>\\n        </div>\\n       </div>\\n      </section>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"installation.html\" rel=\"prev\" title=\"Driver, Firmware, and Runtime Installation\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"c-sdk.html\" rel=\"next\" title=\"C SDK installation and user guide\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='bff490c4-d18d-40e2-8cdc-089ac8f529ea', link=Url('https://furiosa-ai.github.io/docs/latest/en/software/intro.html'), name='intro', parent='', child=[], description='\\n\\n\\n* FuriosaAI SW Stack Introduction\\n* [View page source](../_sources/software/intro.rst.txt)\\n\\n---\\n\\n\\n\\nFuriosaAI SW Stack Introduction\\n[\\uf0c1](#furiosaai-sw-stack-introduction \"Permalink to this heading\")\\n=================================================================================================\\n\\nFuriosaAI provides various SW components to allow the\\nNPU to be used in various applications and environments.\\nHere, we outline the SW stack provided by FuriosaAI, explaining\\nthe roles of each component, together with guidelines and tutorials.\\n\\nThe above diagram demonstrates the SW stack provided by FuriosaAI, by layers.\\nAt the lowest level is the\\nIntroToWarboy\\n\\n, FuriosaAI’s first generation NPU.\\n\\nThe following outlines the key components.\\n\\nKernel Driver and Firmware\\n[\\uf0c1](#kernel-driver-and-firmware \"Permalink to this heading\")\\n---------------------------------------------------------------------------------------\\n\\nThe kernel driver allows the Linux operating system to\\nrecognize the NPU device and acknowledge it as a Linux device file.\\nIf the NPU is not recognized by the operating system, try reinstalling the driver.\\nThe firmware provides a low-level API for the NPU device based on the NPU device file\\nrecognized by the Linux operating system. The runtime and compiler control the NPU using the\\nlow-level API provided by the firmware, thereby executing and scheduling tasks for inference on the NPU\\nusing the compiled binary.\\n\\nThere is no need for the user to utilize kernel driver and firmware directly,\\nbut they must be installed for Furiosa SDK to work. Installation guide can be found in\\n[Driver, Firmware, and Runtime Installation](installation.html#requiredpackages)\\n.\\n\\n\\nCompiler\\n[\\uf0c1](#compiler \"Permalink to this heading\")\\n---------------------------------------------------\\n\\nThe compiler plays a key role in optimizing DNN models and generating executable code in the NPU.\\nCurrently, the compiler supports\\n[TFLite](https://www.tensorflow.org/lite)\\nand\\n[ONNX](https://onnx.ai/)\\nmodels,\\nand optimizes the models by introducing various latest research work and methods.\\n\\nThe compiler provided with\\nWarboy\\n\\nsupports NPU acceleration of various operators in the vision area.\\nFor operators that are not supported with acceleration on NPU, the compiler compiles them such that the CPU will be utilized.\\n\\nAdditionally, the compiler not only accelerates major vision models such as ResNet50, SSD-MobileNet, and EfficientNet, but also models designed by the users so long as supported operators are utilized - to generate code optimized for NPU.\\n\\nFor reference, operators supported by NPU acceleration can be found in\\n[List of Supported Operators for Warboy Acceleration](../npu/warboy.html#supportedoperators)\\n.\\n\\nThe compiler is embedded within runtime, so users do not need to install it separately.\\nIt can be used automatically in the process of creating a session through the Python/C SDK, or through\\n[furiosa-compiler](compiler.html#compilercli)\\n.\\n\\n\\nRuntime\\n[\\uf0c1](#runtime \"Permalink to this heading\")\\n-------------------------------------------------\\n\\nRuntime analyzes the executable program generated by the compiler, and actually executes the\\nDNN model inference task as described in the program. During compilation, the DNN model inference is optimized,\\nand split into a number of smaller tasks running on NPU and CPU. Runtime is responsible for balancing the available resources, scheduling these tasks in accordance with the workload, and controlling the NPU via firmware for tasks being executed on the NPU.\\n\\nRuntime functions are provided as APIs through the Python/C SDK, which will be described in the section below,\\nand installation instructions can be found in\\n[Driver, Firmware, and Runtime Installation](installation.html#requiredpackages)\\n.\\n\\n\\nPython SDK and C SDK\\n[\\uf0c1](#python-sdk-and-c-sdk \"Permalink to this heading\")\\n---------------------------------------------------------------------------\\n\\nPython and C SDK are packages that provide runtime functions as Python and C libraries as APIs, respectively.\\nThey provide an APIs that create objects called ‘session’, that allows the specified model to infer using the designated device, and enables high-performance inference in a blocking and asynchronous manner.\\nIf you need to write an application or service that utilizes the NPU, you can select and install one of the SDKs\\naccording to the programming language of the application you are using.\\nInstallation and usage of each SDK can be found in\\n[Python SDK installation and user guide](python-sdk.html#pythonsdk)\\nand\\n[C SDK installation and user guide](c-sdk.html#csdk)\\n.\\n\\n\\nModel quantizer API\\n[\\uf0c1](#model-quantizer-api \"Permalink to this heading\")\\n-------------------------------------------------------------------------\\n\\nFuriosaAI SDK and\\nWarboy\\n\\nsupport INT8 models, while models with\\nfloating point data as weights undergo quantization, and can be used in\\nWarboy\\n\\n.\\nTo facilitate this quantization process, Furiosa SDK provides a Model quantizer API.\\nMore information about the Model quantizer API provided by the Furiosa SDK can be found in\\n[Model Quantization](quantization.html#modelquantization)\\n.\\n\\n\\nModel Server\\n[\\uf0c1](#model-server \"Permalink to this heading\")\\n-----------------------------------------------------------\\n\\nThe model server exposes the DNN model as a GRPC or REST API.\\nModel formats such as\\n[TFLite](https://www.tensorflow.org/lite)\\nand\\n[ONNX](https://onnx.ai/)\\ncontain within them the data type and tensor shape or the input/output tensors. Using this information, the models are exposed through the commonly used\\n[Predict Protocol - Version 2](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md)\\n.\\n\\nWith the model server, users do not need to directly access the NPU through the library and Python/C SDK, but can access it through a remote API.\\nIn addition, horizontal scaling of services can be easily implemented by using multiple model servers serving the same model and using a load balancer.\\n\\nThe model server requires low latency and high throughput. Here, the scheduling function of the runtime is utilized.\\nInstallation and utilization of the model server can be found in\\n[Model Server (Serving Framework)](serving.html#modelserving)\\n.\\n\\n\\nKubernetes Support\\n[\\uf0c1](#kubernetes-support \"Permalink to this heading\")\\n-----------------------------------------------------------------------\\n\\nKubernetes, a platform for managing containerized workloads and services, is popular with many enterprises.\\nFuriosaAI SW stack also provides native Kubernetes support.\\nKubernetes Device Plugin enables the Kubernetes cluster to recognize FuriosaAI’s NPUs\\nand schedule them for workloads/services that require the NPU.\\nThis feature helps the allocation of resources when multiple workloads require NPU in a multi-tenant\\nenvironment such as Kubernetes, and enables efficient utilization of limited NPU resources.\\n\\nKubernetes Node Labeller adds the information of the physical NPU mounted\\non the node, participating in Kubernetes, as metadata to the Kubernetes node object.\\n\\nThis function allows the user to identify information of the NPU mounted on the node using Kubernetes API or command line tool, and to distribute workload to nodes that satisfy certain conditions by utilizing the Pod’s\\n`spec.nodeSelector`\\nor\\n`spec.nodeAffinity`\\n.\\n\\nInstallation and usage instructions for NPU support in the Kubernetes environment can be found in the\\n[Kubernetes Support](kubernetes_support.html#kubernetesintegration)\\npage.\\n\\n\\n\\n\\n\\n[Previous](../npu/warboy.html \"FuriosaAI Warboy\")\\n[Next](installation.html \"Driver, Firmware, and Runtime Installation\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* FuriosaAI SW Stack Introduction * [View page source](../_sources/software/intro.rst.txt)\\n---\\nFuriosaAI SW Stack Introduction [\\uf0c1](#furiosaai-sw-stack-introduction \"Permalink to this heading\") =================================================================================================\\nFuriosaAI provides various SW components to allow the NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials.\\nThe above diagram demonstrates the SW stack provided by FuriosaAI, by layers. At the lowest level is the IntroToWarboy\\n, FuriosaAI’s first generation NPU.\\nThe following outlines the key components.\\nKernel Driver and Firmware [\\uf0c1](#kernel-driver-and-firmware \"Permalink to this heading\") ---------------------------------------------------------------------------------------\\nThe kernel driver allows the Linux operating system to recognize the NPU device and acknowledge it as a Linux device file. If the NPU is not recognized by the operating system, try reinstalling the driver. The firmware provides a low-level API for the NPU device based on the NPU device file recognized by the Linux operating system. The runtime and compiler control the NPU using the low-level API provided by the firmware, thereby executing and scheduling tasks for inference on the NPU using the compiled binary.\\nThere is no need for the user to utilize kernel driver and firmware directly, but they must be installed for Furiosa SDK to work. Installation guide can be found in [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) .\\nCompiler [\\uf0c1](#compiler \"Permalink to this heading\") ---------------------------------------------------\\nThe compiler plays a key role in optimizing DNN models and generating executable code in the NPU. Currently, the compiler supports [TFLite](https://www.tensorflow.org/lite) and [ONNX](https://onnx.ai/) models, and optimizes the models by introducing various latest research work and methods.\\nThe compiler provided with Warboy\\nsupports NPU acceleration of various operators in the vision area. For operators that are not supported with acceleration on NPU, the compiler compiles them such that the CPU will be utilized.\\nAdditionally, the compiler not only accelerates major vision models such as ResNet50, SSD-MobileNet, and EfficientNet, but also models designed by the users so long as supported operators are utilized - to generate code optimized for NPU.\\nFor reference, operators supported by NPU acceleration can be found in [List of Supported Operators for Warboy Acceleration](../npu/warboy.html#supportedoperators) .\\nThe compiler is embedded within runtime, so users do not need to install it separately. It can be used automatically in the process of creating a session through the Python/C SDK, or through [furiosa-compiler](compiler.html#compilercli) .\\nRuntime [\\uf0c1](#runtime \"Permalink to this heading\") -------------------------------------------------\\nRuntime analyzes the executable program generated by the compiler, and actually executes the DNN model inference task as described in the program. During compilation, the DNN model inference is optimized, and split into a number of smaller tasks running on NPU and CPU. Runtime is responsible for balancing the available resources, scheduling these tasks in accordance with the workload, and controlling the NPU via firmware for tasks being executed on the NPU.\\nRuntime functions are provided as APIs through the Python/C SDK, which will be described in the section below, and installation instructions can be found in [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) .\\nPython SDK and C SDK [\\uf0c1](#python-sdk-and-c-sdk \"Permalink to this heading\") ---------------------------------------------------------------------------\\nPython and C SDK are packages that provide runtime functions as Python and C libraries as APIs, respectively. They provide an APIs that create objects called ‘session’, that allows the specified model to infer using the designated device, and enables high-performance inference in a blocking and asynchronous manner. If you need to write an application or service that utilizes the NPU, you can select and install one of the SDKs according to the programming language of the application you are using. Installation and usage of each SDK can be found in [Python SDK installation and user guide](python-sdk.html#pythonsdk) and [C SDK installation and user guide](c-sdk.html#csdk) .\\nModel quantizer API [\\uf0c1](#model-quantizer-api \"Permalink to this heading\") -------------------------------------------------------------------------\\nFuriosaAI SDK and Warboy\\nsupport INT8 models, while models with floating point data as weights undergo quantization, and can be used in Warboy\\n. To facilitate this quantization process, Furiosa SDK provides a Model quantizer API. More information about the Model quantizer API provided by the Furiosa SDK can be found in [Model Quantization](quantization.html#modelquantization) .\\nModel Server [\\uf0c1](#model-server \"Permalink to this heading\") -----------------------------------------------------------\\nThe model server exposes the DNN model as a GRPC or REST API. Model formats such as [TFLite](https://www.tensorflow.org/lite) and [ONNX](https://onnx.ai/) contain within them the data type and tensor shape or the input/output tensors. Using this information, the models are exposed through the commonly used [Predict Protocol - Version 2](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md) .\\nWith the model server, users do not need to directly access the NPU through the library and Python/C SDK, but can access it through a remote API. In addition, horizontal scaling of services can be easily implemented by using multiple model servers serving the same model and using a load balancer.\\nThe model server requires low latency and high throughput. Here, the scheduling function of the runtime is utilized. Installation and utilization of the model server can be found in [Model Server (Serving Framework)](serving.html#modelserving) .\\nKubernetes Support [\\uf0c1](#kubernetes-support \"Permalink to this heading\") -----------------------------------------------------------------------\\nKubernetes, a platform for managing containerized workloads and services, is popular with many enterprises. FuriosaAI SW stack also provides native Kubernetes support. Kubernetes Device Plugin enables the Kubernetes cluster to recognize FuriosaAI’s NPUs and schedule them for workloads/services that require the NPU. This feature helps the allocation of resources when multiple workloads require NPU in a multi-tenant environment such as Kubernetes, and enables efficient utilization of limited NPU resources.\\nKubernetes Node Labeller adds the information of the physical NPU mounted on the node, participating in Kubernetes, as metadata to the Kubernetes node object.\\nThis function allows the user to identify information of the NPU mounted on the node using Kubernetes API or command line tool, and to distribute workload to nodes that satisfy certain conditions by utilizing the Pod’s `spec.nodeSelector` or `spec.nodeAffinity` .\\nInstallation and usage instructions for NPU support in the Kubernetes environment can be found in the [Kubernetes Support](kubernetes_support.html#kubernetesintegration) page.\\n[Previous](../npu/warboy.html \"FuriosaAI Warboy\") [Next](installation.html \"Driver, Firmware, and Runtime Installation\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     FuriosaAI SW Stack Introduction\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/software/intro.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"furiosaai-sw-stack-introduction\">\\n     <h1>\\n      FuriosaAI SW Stack Introduction\\n      <a class=\"headerlink\" href=\"#furiosaai-sw-stack-introduction\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      FuriosaAI provides various SW components to allow the\\nNPU to be used in various applications and environments.\\nHere, we outline the SW stack provided by FuriosaAI, explaining\\nthe roles of each component, together with guidelines and tutorials.\\n     </p>\\n     <figure class=\"align-center\">\\n      <a class=\"with-shadow reference internal image-reference\" href=\"../_images/software_stack.jpg\">\\n       <img alt=\"FuriosaAI Software Stack\" class=\"with-shadow\" src=\"../_images/software_stack.jpg\" style=\"width: 500px;\"/>\\n      </a>\\n     </figure>\\n     <p>\\n      The above diagram demonstrates the SW stack provided by FuriosaAI, by layers.\\nAt the lowest level is the\\n      <span class=\"xref std std-ref\">\\n       IntroToWarboy\\n      </span>\\n      , FuriosaAI’s first generation NPU.\\n     </p>\\n     <p>\\n      The following outlines the key components.\\n     </p>\\n     <section id=\"kernel-driver-and-firmware\">\\n      <h2>\\n       Kernel Driver and Firmware\\n       <a class=\"headerlink\" href=\"#kernel-driver-and-firmware\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       The kernel driver allows the Linux operating system to\\nrecognize the NPU device and acknowledge it as a Linux device file.\\nIf the NPU is not recognized by the operating system, try reinstalling the driver.\\nThe firmware provides a low-level API for the NPU device based on the NPU device file\\nrecognized by the Linux operating system. The runtime and compiler control the NPU using the\\nlow-level API provided by the firmware, thereby executing and scheduling tasks for inference on the NPU\\nusing the compiled binary.\\n      </p>\\n      <p>\\n       There is no need for the user to utilize kernel driver and firmware directly,\\nbut they must be installed for Furiosa SDK to work. Installation guide can be found in\\n       <a class=\"reference internal\" href=\"installation.html#requiredpackages\">\\n        <span class=\"std std-ref\">\\n         Driver, Firmware, and Runtime Installation\\n        </span>\\n       </a>\\n       .\\n      </p>\\n     </section>\\n     <section id=\"compiler\">\\n      <h2>\\n       Compiler\\n       <a class=\"headerlink\" href=\"#compiler\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       The compiler plays a key role in optimizing DNN models and generating executable code in the NPU.\\nCurrently, the compiler supports\\n       <a class=\"reference external\" href=\"https://www.tensorflow.org/lite\">\\n        TFLite\\n       </a>\\n       and\\n       <a class=\"reference external\" href=\"https://onnx.ai/\">\\n        ONNX\\n       </a>\\n       models,\\nand optimizes the models by introducing various latest research work and methods.\\n      </p>\\n      <p>\\n       The compiler provided with\\n       <span class=\"xref std std-ref\">\\n        Warboy\\n       </span>\\n       supports NPU acceleration of various operators in the vision area.\\nFor operators that are not supported with acceleration on NPU, the compiler compiles them such that the CPU will be utilized.\\n      </p>\\n      <p>\\n       Additionally, the compiler not only accelerates major vision models such as ResNet50, SSD-MobileNet, and EfficientNet, but also models designed by the users so long as supported operators are utilized - to generate code optimized for NPU.\\n      </p>\\n      <p>\\n       For reference, operators supported by NPU acceleration can be found in\\n       <a class=\"reference internal\" href=\"../npu/warboy.html#supportedoperators\">\\n        <span class=\"std std-ref\">\\n         List of Supported Operators for Warboy Acceleration\\n        </span>\\n       </a>\\n       .\\n      </p>\\n      <p>\\n       The compiler is embedded within runtime, so users do not need to install it separately.\\nIt can be used automatically in the process of creating a session through the Python/C SDK, or through\\n       <a class=\"reference internal\" href=\"compiler.html#compilercli\">\\n        <span class=\"std std-ref\">\\n         furiosa-compiler\\n        </span>\\n       </a>\\n       .\\n      </p>\\n     </section>\\n     <section id=\"runtime\">\\n      <h2>\\n       Runtime\\n       <a class=\"headerlink\" href=\"#runtime\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       Runtime analyzes the executable program generated by the compiler, and actually executes the\\nDNN model inference task as described in the program. During compilation, the DNN model inference is optimized,\\nand split into a number of smaller tasks running on NPU and CPU. Runtime is responsible for balancing the available resources, scheduling these tasks in accordance with the workload, and controlling the NPU via firmware for tasks being executed on the NPU.\\n      </p>\\n      <p>\\n       Runtime functions are provided as APIs through the Python/C SDK, which will be described in the section below,\\nand installation instructions can be found in\\n       <a class=\"reference internal\" href=\"installation.html#requiredpackages\">\\n        <span class=\"std std-ref\">\\n         Driver, Firmware, and Runtime Installation\\n        </span>\\n       </a>\\n       .\\n      </p>\\n     </section>\\n     <section id=\"python-sdk-and-c-sdk\">\\n      <h2>\\n       Python SDK and C SDK\\n       <a class=\"headerlink\" href=\"#python-sdk-and-c-sdk\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       Python and C SDK are packages that provide runtime functions as Python and C libraries as APIs, respectively.\\nThey provide an APIs that create objects called ‘session’, that allows the specified model to infer using the designated device, and enables high-performance inference in a blocking and asynchronous manner.\\nIf you need to write an application or service that utilizes the NPU, you can select and install one of the SDKs\\naccording to the programming language of the application you are using.\\nInstallation and usage of each SDK can be found in\\n       <a class=\"reference internal\" href=\"python-sdk.html#pythonsdk\">\\n        <span class=\"std std-ref\">\\n         Python SDK installation and user guide\\n        </span>\\n       </a>\\n       and\\n       <a class=\"reference internal\" href=\"c-sdk.html#csdk\">\\n        <span class=\"std std-ref\">\\n         C SDK installation and user guide\\n        </span>\\n       </a>\\n       .\\n      </p>\\n     </section>\\n     <section id=\"model-quantizer-api\">\\n      <h2>\\n       Model quantizer API\\n       <a class=\"headerlink\" href=\"#model-quantizer-api\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       FuriosaAI SDK and\\n       <span class=\"xref std std-ref\">\\n        Warboy\\n       </span>\\n       support INT8 models, while models with\\nfloating point data as weights undergo quantization, and can be used in\\n       <span class=\"xref std std-ref\">\\n        Warboy\\n       </span>\\n       .\\nTo facilitate this quantization process, Furiosa SDK provides a Model quantizer API.\\nMore information about the Model quantizer API provided by the Furiosa SDK can be found in\\n       <a class=\"reference internal\" href=\"quantization.html#modelquantization\">\\n        <span class=\"std std-ref\">\\n         Model Quantization\\n        </span>\\n       </a>\\n       .\\n      </p>\\n     </section>\\n     <section id=\"model-server\">\\n      <h2>\\n       Model Server\\n       <a class=\"headerlink\" href=\"#model-server\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       The model server exposes the DNN model as a GRPC or REST API.\\nModel formats such as\\n       <a class=\"reference external\" href=\"https://www.tensorflow.org/lite\">\\n        TFLite\\n       </a>\\n       and\\n       <a class=\"reference external\" href=\"https://onnx.ai/\">\\n        ONNX\\n       </a>\\n       contain within them the data type and tensor shape or the input/output tensors. Using this information, the models are exposed through the commonly used\\n       <a class=\"reference external\" href=\"https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md\">\\n        Predict Protocol - Version 2\\n       </a>\\n       .\\n      </p>\\n      <p>\\n       With the model server, users do not need to directly access the NPU through the library and Python/C SDK, but can access it through a remote API.\\nIn addition, horizontal scaling of services can be easily implemented by using multiple model servers serving the same model and using a load balancer.\\n      </p>\\n      <p>\\n       The model server requires low latency and high throughput. Here, the scheduling function of the runtime is utilized.\\nInstallation and utilization of the model server can be found in\\n       <a class=\"reference internal\" href=\"serving.html#modelserving\">\\n        <span class=\"std std-ref\">\\n         Model Server (Serving Framework)\\n        </span>\\n       </a>\\n       .\\n      </p>\\n     </section>\\n     <section id=\"kubernetes-support\">\\n      <h2>\\n       Kubernetes Support\\n       <a class=\"headerlink\" href=\"#kubernetes-support\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       Kubernetes, a platform for managing containerized workloads and services, is popular with many enterprises.\\nFuriosaAI SW stack also provides native Kubernetes support.\\nKubernetes Device Plugin enables the Kubernetes cluster to recognize FuriosaAI’s NPUs\\nand schedule them for workloads/services that require the NPU.\\nThis feature helps the allocation of resources when multiple workloads require NPU in a multi-tenant\\nenvironment such as Kubernetes, and enables efficient utilization of limited NPU resources.\\n      </p>\\n      <p>\\n       Kubernetes Node Labeller adds the information of the physical NPU mounted\\non the node, participating in Kubernetes, as metadata to the Kubernetes node object.\\n      </p>\\n      <p>\\n       This function allows the user to identify information of the NPU mounted on the node using Kubernetes API or command line tool, and to distribute workload to nodes that satisfy certain conditions by utilizing the Pod’s\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         spec.nodeSelector\\n        </span>\\n       </code>\\n       or\\n       <code class=\"docutils literal notranslate\">\\n        <span class=\"pre\">\\n         spec.nodeAffinity\\n        </span>\\n       </code>\\n       .\\n      </p>\\n      <p>\\n       Installation and usage instructions for NPU support in the Kubernetes environment can be found in the\\n       <a class=\"reference internal\" href=\"kubernetes_support.html#kubernetesintegration\">\\n        <span class=\"std std-ref\">\\n         Kubernetes Support\\n        </span>\\n       </a>\\n       page.\\n      </p>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"../npu/warboy.html\" rel=\"prev\" title=\"FuriosaAI Warboy\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"installation.html\" rel=\"next\" title=\"Driver, Firmware, and Runtime Installation\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='c40d4e1b-c960-4ffb-883f-e35d53def7a1', link=Url('https://furiosa-ai.github.io/docs/latest/en/npu/warboy.html'), name='warboy', parent='', child=[], description='\\n\\n\\n* FuriosaAI Warboy\\n* [View page source](../_sources/npu/warboy.rst.txt)\\n\\n---\\n\\n\\n\\nFuriosaAI Warboy\\n[\\uf0c1](#furiosaai-warboy \"Permalink to this heading\")\\n===================================================================\\n\\nFuriosaAI’s first generation NPU Warboy is a chip with an architecture optimized for deep learning inference. It\\ndemonstrates high performance for deep learning inference while maintaining cost-efficiency. FuriosaAI Warboy is\\noptimized for inferences with low batch sizes; for inference requests with low batch sizes, all of the chip’s\\nresources are maximally utilized to achieve low latency. The large on-chip memory is also able to retain most major\\nCNN models, thereby eliminating memory bottlenecks, and achieving high energy efficiency.\\n\\nWarboy supports key CNN models used in various vision tasks, including\\nImage Classification, Object Detection, OCR, Super Resolution, and Pose Estimation.\\nIn particular, the chip demonstrates superior performance in computations such as depthwise/group convolution,\\nthat drive high accuracy and computational efficiency in state-of-the-art CNN models.\\n\\nWarboy delivers 64 TOPS performance and includes 32MB of SRAM.\\nWarboy consists of two processing elements (PE), which each delivers 32 TOPS performance and can be deployed independently.\\nWith a total performance of 64 TOPS, should there be a need to maximize response speed to models, the two PEs may undergo fusion,\\nto aggregate as a larger, single PE. Depending on the users’ model size or performance requirements the PEs may be 1) fused\\nso as to minimize response time, or 2) utilized independently to maximize throughput.\\n\\nFuriosaAI SDK provides the compiler, runtime software, and profiling tools for the FuriosaAI Warboy.\\nIt also supports the INT8 quantization scheme, used as a standard in TensorFLow and PyTorch, while providing tools to convert Floating Point models using Post Training Quantization.\\nWith the FuriosaAI SDK, users can compile trained or exported models in formats commonly used for inference (TFLite or ONNX), and accelerate them on FuriosaAI Warboy.\\n\\nHW Specifications\\n[\\uf0c1](#hw-specifications \"Permalink to this heading\")\\n---------------------------------------------------------------------\\n\\nThe chip is built with 5 billion transistors, dimensions of 180mm^2, clock speed of 2GHz, and delivers peak performance of 64 TOPS of INT8.\\nIt also supports a maximum of 4266 for LPDDR4x. Warboy has a DRAM bandwidth of 66GB/s, and supports PCIe Gen4 8x.\\n\\n\\nWarboy Hardware Specification\\n\\n[\\uf0c1](#id1 \"Permalink to this table\")\\n\\n\\n| Peak Performance | 64 TOPS |\\n| --- | --- |\\n| On-chip SRAM | 32 MB |\\n| Host Interface | PCIe Gen4 8-lane |\\n| Form Factor | Full-Height Half-Length (FHHL)  Half-Height Half-Length (HHHL) |\\n| Thermal Solution | Passive Fan  Active Fan |\\n| TDP | 40 - 60W (Configurable) |\\n| Operating Temperature | 0 ~ 50℃ |\\n| Clock Speed | 2.0 GHz |\\n| DDR Speed | 4266 Mbps |\\n| Memory Type | LPDDR4X |\\n| Memory Size | 16 GB (max. 32 GB) |\\n| Peak Memory Bandwidth | 66 GB/s |\\n\\n\\nList of Supported Operators for Warboy Acceleration\\n[\\uf0c1](#list-of-supported-operators-for-warboy-acceleration \"Permalink to this heading\")\\n-----------------------------------------------------------------------------------------------------------------------------------------\\n\\nFuriosaAI Warboy and SDK can accelerate the following operators, as supported by\\n[Tensorflow Lite](https://www.tensorflow.org/lite)\\nmodel and\\n[ONNX](https://onnx.ai/)\\n.\\n\\nThe names of the operators use\\n[ONNX](https://onnx.ai/)\\nas a reference.\\n\\nNote\\n\\nAny operators cannot be accelerated on Warboy, the operators will run on the CPU.\\nFor some operators, even if Warboy acceleration is supported, if certain conditions are not met, they may be split into several operators\\nor may run on the CPU. Some examples of this would be when the weight of the model is larger than Warboy memory, or if the Warboy memory\\nis not sufficient to process a certain computation.\\n\\n\\nOperators Accelerated on Warboy\\n\\n[\\uf0c1](#id2 \"Permalink to this table\")\\n\\n\\n| Name of operator | Additional details |\\n| --- | --- |\\n| [Add](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Add) |  |\\n| [AveragePool](https://github.com/onnx/onnx/blob/master/docs/Operators.md#AveragePool) |  |\\n| [BatchNormalization](https://github.com/onnx/onnx/blob/master/docs/Operators.md#batchnormalization) | Acceleration supported, only if after Conv |\\n| [Clip](https://github.com/onnx/onnx/blob/master/docs/Operators.md#clip) |  |\\n| [Concat](https://github.com/onnx/onnx/blob/master/docs/Operators.md#concat) | Acceleration supported, only for height axis |\\n| [Conv](https://github.com/onnx/onnx/blob/master/docs/Operators.md#conv) | Acceleration supported, only for group  <= 128 and dilation <= 12 |\\n| [ConvTranspose](https://github.com/onnx/onnx/blob/master/docs/Operators.md#convtranspose) |  |\\n| [DepthToSpace](https://github.com/onnx/onnx/blob/master/docs/Operators.md#depthtospace) |  |\\n| [Exp](https://github.com/onnx/onnx/blob/master/docs/Operators.md#exp) |  |\\n| [Expand](https://github.com/onnx/onnx/blob/master/docs/Operators.md#expand) |  |\\n| [Flatten](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Flatten) |  |\\n| [Gemm](https://github.com/onnx/onnx/blob/master/docs/Operators.md#gemm) |  |\\n| [LeakyRelu](https://github.com/onnx/onnx/blob/master/docs/Operators.md#leakyrelu) |  |\\n| [LpNormalization](https://github.com/onnx/onnx/blob/master/docs/Operators.md#lpnormalization) | Acceleration supported, only for p = 2 and batch <= 2 |\\n| [MatMul](https://github.com/onnx/onnx/blob/master/docs/Operators.md#matmul) |  |\\n| [MaxPool](https://github.com/onnx/onnx/blob/master/docs/Operators.md#maxpool) |  |\\n| [Mean](https://github.com/onnx/onnx/blob/master/docs/Operators.md#mean) |  |\\n| [Mul](https://github.com/onnx/onnx/blob/master/docs/Operators.md#mul) |  |\\n| [Pad](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Pad) |  |\\n| [ReduceL2](https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceL2) |  |\\n| [ReduceSum](https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceSum) |  |\\n| [Relu](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Relu) |  |\\n| [Reshape](https://github.com/onnx/onnx/blob/master/docs/Operators.md#reshape) |  |\\n| [Pow](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Pow) |  |\\n| [SpaceToDepth](https://github.com/onnx/onnx/blob/main/docs/Operators.md#SpaceToDepth) | Acceleration supported, only for mode=”CRD” and Furiosa SDK version 0.6.0 or higher |\\n| [Sigmoid](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Sigmoid) |  |\\n| [Slice](https://github.com/onnx/onnx/blob/master/docs/Operators.md#slice) | Acceleration supported, only for height axis |\\n| [Softmax](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Softmax) | Acceleration supported, only for batch <= 2 |\\n| [Softplus](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Softplus) |  |\\n| [Sub](https://github.com/onnx/onnx/blob/master/docs/Operators.md#sub) |  |\\n| [Split](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Split) |  |\\n| [Sqrt](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Sqrt) |  |\\n| [Transpose](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Transpose) |  |\\n| [Unsqueeze](https://github.com/onnx/onnx/blob/master/docs/Operators.md#unsqueeze) |  |\\n\\n\\nMLPerf\\n[\\uf0c1](#mlperf \"Permalink to this heading\")\\n-----------------------------------------------\\n\\nResults submitted to MLPerf can be found at\\n[MLPerf™ Inference Edge v2.0 Results](https://mlcommons.org/en/inference-edge-20/)\\n\\n### See Also [\\uf0c1](#see-also \"Permalink to this heading\")\\n\\n* [MLPerf™ Inference Edge v1.1 Results](https://mlcommons.org/en/inference-edge-11/)\\n* [MLPerf™ Inference Edge v0.5 Results](https://mlcommons.org/en/inference-edge-05/)\\n\\n\\n\\n\\n\\n[Previous](../index.html \"FuriosaAI NPU & SDK 0.10.1 Documents\")\\n[Next](../software/intro.html \"FuriosaAI SW Stack Introduction\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* FuriosaAI Warboy * [View page source](../_sources/npu/warboy.rst.txt)\\n---\\nFuriosaAI Warboy [\\uf0c1](#furiosaai-warboy \"Permalink to this heading\") ===================================================================\\nFuriosaAI’s first generation NPU Warboy is a chip with an architecture optimized for deep learning inference. It demonstrates high performance for deep learning inference while maintaining cost-efficiency. FuriosaAI Warboy is optimized for inferences with low batch sizes; for inference requests with low batch sizes, all of the chip’s resources are maximally utilized to achieve low latency. The large on-chip memory is also able to retain most major CNN models, thereby eliminating memory bottlenecks, and achieving high energy efficiency.\\nWarboy supports key CNN models used in various vision tasks, including Image Classification, Object Detection, OCR, Super Resolution, and Pose Estimation. In particular, the chip demonstrates superior performance in computations such as depthwise/group convolution, that drive high accuracy and computational efficiency in state-of-the-art CNN models.\\nWarboy delivers 64 TOPS performance and includes 32MB of SRAM. Warboy consists of two processing elements (PE), which each delivers 32 TOPS performance and can be deployed independently. With a total performance of 64 TOPS, should there be a need to maximize response speed to models, the two PEs may undergo fusion, to aggregate as a larger, single PE. Depending on the users’ model size or performance requirements the PEs may be 1) fused so as to minimize response time, or 2) utilized independently to maximize throughput.\\nFuriosaAI SDK provides the compiler, runtime software, and profiling tools for the FuriosaAI Warboy. It also supports the INT8 quantization scheme, used as a standard in TensorFLow and PyTorch, while providing tools to convert Floating Point models using Post Training Quantization. With the FuriosaAI SDK, users can compile trained or exported models in formats commonly used for inference (TFLite or ONNX), and accelerate them on FuriosaAI Warboy.\\nHW Specifications [\\uf0c1](#hw-specifications \"Permalink to this heading\") ---------------------------------------------------------------------\\nThe chip is built with 5 billion transistors, dimensions of 180mm^2, clock speed of 2GHz, and delivers peak performance of 64 TOPS of INT8. It also supports a maximum of 4266 for LPDDR4x. Warboy has a DRAM bandwidth of 66GB/s, and supports PCIe Gen4 8x.\\nWarboy Hardware Specification\\n[\\uf0c1](#id1 \"Permalink to this table\")\\n| Peak Performance | 64 TOPS | | --- | --- | | On-chip SRAM | 32 MB | | Host Interface | PCIe Gen4 8-lane | | Form Factor | Full-Height Half-Length (FHHL)  Half-Height Half-Length (HHHL) | | Thermal Solution | Passive Fan  Active Fan | | TDP | 40 - 60W (Configurable) | | Operating Temperature | 0 ~ 50℃ | | Clock Speed | 2.0 GHz | | DDR Speed | 4266 Mbps | | Memory Type | LPDDR4X | | Memory Size | 16 GB (max. 32 GB) | | Peak Memory Bandwidth | 66 GB/s |\\nList of Supported Operators for Warboy Acceleration [\\uf0c1](#list-of-supported-operators-for-warboy-acceleration \"Permalink to this heading\") -----------------------------------------------------------------------------------------------------------------------------------------\\nFuriosaAI Warboy and SDK can accelerate the following operators, as supported by [Tensorflow Lite](https://www.tensorflow.org/lite) model and [ONNX](https://onnx.ai/) .\\nThe names of the operators use [ONNX](https://onnx.ai/) as a reference.\\nNote\\nAny operators cannot be accelerated on Warboy, the operators will run on the CPU. For some operators, even if Warboy acceleration is supported, if certain conditions are not met, they may be split into several operators or may run on the CPU. Some examples of this would be when the weight of the model is larger than Warboy memory, or if the Warboy memory is not sufficient to process a certain computation.\\nOperators Accelerated on Warboy\\n[\\uf0c1](#id2 \"Permalink to this table\")\\n| Name of operator | Additional details | | --- | --- | | [Add](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Add) |  | | [AveragePool](https://github.com/onnx/onnx/blob/master/docs/Operators.md#AveragePool) |  | | [BatchNormalization](https://github.com/onnx/onnx/blob/master/docs/Operators.md#batchnormalization) | Acceleration supported, only if after Conv | | [Clip](https://github.com/onnx/onnx/blob/master/docs/Operators.md#clip) |  | | [Concat](https://github.com/onnx/onnx/blob/master/docs/Operators.md#concat) | Acceleration supported, only for height axis | | [Conv](https://github.com/onnx/onnx/blob/master/docs/Operators.md#conv) | Acceleration supported, only for group  <= 128 and dilation <= 12 | | [ConvTranspose](https://github.com/onnx/onnx/blob/master/docs/Operators.md#convtranspose) |  | | [DepthToSpace](https://github.com/onnx/onnx/blob/master/docs/Operators.md#depthtospace) |  | | [Exp](https://github.com/onnx/onnx/blob/master/docs/Operators.md#exp) |  | | [Expand](https://github.com/onnx/onnx/blob/master/docs/Operators.md#expand) |  | | [Flatten](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Flatten) |  | | [Gemm](https://github.com/onnx/onnx/blob/master/docs/Operators.md#gemm) |  | | [LeakyRelu](https://github.com/onnx/onnx/blob/master/docs/Operators.md#leakyrelu) |  | | [LpNormalization](https://github.com/onnx/onnx/blob/master/docs/Operators.md#lpnormalization) | Acceleration supported, only for p = 2 and batch <= 2 | | [MatMul](https://github.com/onnx/onnx/blob/master/docs/Operators.md#matmul) |  | | [MaxPool](https://github.com/onnx/onnx/blob/master/docs/Operators.md#maxpool) |  | | [Mean](https://github.com/onnx/onnx/blob/master/docs/Operators.md#mean) |  | | [Mul](https://github.com/onnx/onnx/blob/master/docs/Operators.md#mul) |  | | [Pad](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Pad) |  | | [ReduceL2](https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceL2) |  | | [ReduceSum](https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceSum) |  | | [Relu](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Relu) |  | | [Reshape](https://github.com/onnx/onnx/blob/master/docs/Operators.md#reshape) |  | | [Pow](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Pow) |  | | [SpaceToDepth](https://github.com/onnx/onnx/blob/main/docs/Operators.md#SpaceToDepth) | Acceleration supported, only for mode=”CRD” and Furiosa SDK version 0.6.0 or higher | | [Sigmoid](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Sigmoid) |  | | [Slice](https://github.com/onnx/onnx/blob/master/docs/Operators.md#slice) | Acceleration supported, only for height axis | | [Softmax](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Softmax) | Acceleration supported, only for batch <= 2 | | [Softplus](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Softplus) |  | | [Sub](https://github.com/onnx/onnx/blob/master/docs/Operators.md#sub) |  | | [Split](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Split) |  | | [Sqrt](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Sqrt) |  | | [Transpose](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Transpose) |  | | [Unsqueeze](https://github.com/onnx/onnx/blob/master/docs/Operators.md#unsqueeze) |  |\\nMLPerf [\\uf0c1](#mlperf \"Permalink to this heading\") -----------------------------------------------\\nResults submitted to MLPerf can be found at [MLPerf™ Inference Edge v2.0 Results](https://mlcommons.org/en/inference-edge-20/)\\n### See Also [\\uf0c1](#see-also \"Permalink to this heading\")\\n* [MLPerf™ Inference Edge v1.1 Results](https://mlcommons.org/en/inference-edge-11/) * [MLPerf™ Inference Edge v0.5 Results](https://mlcommons.org/en/inference-edge-05/)\\n[Previous](../index.html \"FuriosaAI NPU & SDK 0.10.1 Documents\") [Next](../software/intro.html \"FuriosaAI SW Stack Introduction\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     FuriosaAI Warboy\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/npu/warboy.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"furiosaai-warboy\">\\n     <span id=\"warboy\">\\n     </span>\\n     <h1>\\n      FuriosaAI Warboy\\n      <a class=\"headerlink\" href=\"#furiosaai-warboy\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      FuriosaAI’s first generation NPU Warboy is a chip with an architecture optimized for deep learning inference. It\\ndemonstrates high performance for deep learning inference while maintaining cost-efficiency. FuriosaAI Warboy is\\noptimized for inferences with low batch sizes; for inference requests with low batch sizes, all of the chip’s\\nresources are maximally utilized to achieve low latency. The large on-chip memory is also able to retain most major\\nCNN models, thereby eliminating memory bottlenecks, and achieving high energy efficiency.\\n     </p>\\n     <p>\\n      Warboy supports key CNN models used in various vision tasks, including\\nImage Classification, Object Detection, OCR, Super Resolution, and Pose Estimation.\\nIn particular, the chip demonstrates superior performance in computations such as depthwise/group convolution,\\nthat drive high accuracy and computational efficiency in state-of-the-art CNN models.\\n     </p>\\n     <p>\\n      Warboy delivers 64 TOPS performance and includes 32MB of SRAM.\\nWarboy consists of two processing elements (PE), which each delivers 32 TOPS performance and can be deployed independently.\\nWith a total performance of 64 TOPS, should there be a need to maximize response speed to models, the two PEs may undergo fusion,\\nto aggregate as a larger, single PE. Depending on the users’ model size or performance requirements the PEs may be 1) fused\\nso as to minimize response time, or 2) utilized independently to maximize throughput.\\n     </p>\\n     <p>\\n      FuriosaAI SDK provides the compiler, runtime software, and profiling tools for the FuriosaAI Warboy.\\nIt also supports the INT8 quantization scheme, used as a standard in TensorFLow and PyTorch, while providing tools to convert Floating Point models using Post Training Quantization.\\nWith the FuriosaAI SDK, users can compile trained or exported models in formats commonly used for inference (TFLite or ONNX), and accelerate them on FuriosaAI Warboy.\\n     </p>\\n     <section id=\"hw-specifications\">\\n      <h2>\\n       HW Specifications\\n       <a class=\"headerlink\" href=\"#hw-specifications\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       The chip is built with 5 billion transistors, dimensions of 180mm^2, clock speed of 2GHz, and delivers peak performance of 64 TOPS of INT8.\\nIt also supports a maximum of 4266 for LPDDR4x. Warboy has a DRAM bandwidth of 66GB/s, and supports PCIe Gen4 8x.\\n      </p>\\n      <table class=\"docutils align-center\" id=\"id1\">\\n       <caption>\\n        <span class=\"caption-text\">\\n         Warboy Hardware Specification\\n        </span>\\n        <a class=\"headerlink\" href=\"#id1\" title=\"Permalink to this table\">\\n         \\uf0c1\\n        </a>\\n       </caption>\\n       <colgroup>\\n        <col style=\"width: 40%\"/>\\n        <col style=\"width: 60%\"/>\\n       </colgroup>\\n       <tbody>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           Peak Performance\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           64 TOPS\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           On-chip SRAM\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           32 MB\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           Host Interface\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           PCIe Gen4 8-lane\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           Form Factor\\n          </p>\\n         </td>\\n         <td>\\n          <div class=\"line-block\">\\n           <div class=\"line\">\\n            Full-Height Half-Length (FHHL)\\n           </div>\\n           <div class=\"line\">\\n            Half-Height Half-Length (HHHL)\\n           </div>\\n          </div>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           Thermal Solution\\n          </p>\\n         </td>\\n         <td>\\n          <div class=\"line-block\">\\n           <div class=\"line\">\\n            Passive Fan\\n           </div>\\n           <div class=\"line\">\\n            Active Fan\\n           </div>\\n          </div>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           TDP\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           40 - 60W (Configurable)\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           Operating Temperature\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           0 ~ 50℃\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           Clock Speed\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           2.0 GHz\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           DDR Speed\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           4266 Mbps\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           Memory Type\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           LPDDR4X\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           Memory Size\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           16 GB (max. 32 GB)\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           Peak Memory Bandwidth\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           66 GB/s\\n          </p>\\n         </td>\\n        </tr>\\n       </tbody>\\n      </table>\\n     </section>\\n     <section id=\"list-of-supported-operators-for-warboy-acceleration\">\\n      <span id=\"supportedoperators\">\\n      </span>\\n      <h2>\\n       List of Supported Operators for Warboy Acceleration\\n       <a class=\"headerlink\" href=\"#list-of-supported-operators-for-warboy-acceleration\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       FuriosaAI Warboy and SDK can accelerate the following operators, as supported by\\n       <a class=\"reference external\" href=\"https://www.tensorflow.org/lite\">\\n        Tensorflow Lite\\n       </a>\\n       model and\\n       <a class=\"reference external\" href=\"https://onnx.ai/\">\\n        ONNX\\n       </a>\\n       .\\n      </p>\\n      <p>\\n       The names of the operators use\\n       <a class=\"reference external\" href=\"https://onnx.ai/\">\\n        ONNX\\n       </a>\\n       as a reference.\\n      </p>\\n      <div class=\"admonition note\">\\n       <p class=\"admonition-title\">\\n        Note\\n       </p>\\n       <p>\\n        Any operators cannot be accelerated on Warboy, the operators will run on the CPU.\\nFor some operators, even if Warboy acceleration is supported, if certain conditions are not met, they may be split into several operators\\nor may run on the CPU. Some examples of this would be when the weight of the model is larger than Warboy memory, or if the Warboy memory\\nis not sufficient to process a certain computation.\\n       </p>\\n      </div>\\n      <table class=\"docutils align-default\" id=\"id2\">\\n       <caption>\\n        <span class=\"caption-text\">\\n         Operators Accelerated on Warboy\\n        </span>\\n        <a class=\"headerlink\" href=\"#id2\" title=\"Permalink to this table\">\\n         \\uf0c1\\n        </a>\\n       </caption>\\n       <colgroup>\\n        <col style=\"width: 20%\"/>\\n        <col style=\"width: 80%\"/>\\n       </colgroup>\\n       <thead>\\n        <tr class=\"row-odd\">\\n         <th class=\"head\">\\n          <p>\\n           Name of operator\\n          </p>\\n         </th>\\n         <th class=\"head\">\\n          <p>\\n           Additional details\\n          </p>\\n         </th>\\n        </tr>\\n       </thead>\\n       <tbody>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#Add\">\\n            Add\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#AveragePool\">\\n            AveragePool\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#batchnormalization\">\\n            BatchNormalization\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Acceleration supported, only if after Conv\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#clip\">\\n            Clip\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#concat\">\\n            Concat\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Acceleration supported, only for height axis\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#conv\">\\n            Conv\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Acceleration supported, only for\\n           <cite>\\n            group\\n           </cite>\\n           &lt;= 128 and dilation &lt;= 12\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#convtranspose\">\\n            ConvTranspose\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#depthtospace\">\\n            DepthToSpace\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#exp\">\\n            Exp\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#expand\">\\n            Expand\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#Flatten\">\\n            Flatten\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#gemm\">\\n            Gemm\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#leakyrelu\">\\n            LeakyRelu\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#lpnormalization\">\\n            LpNormalization\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Acceleration supported, only for p = 2 and batch &lt;= 2\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#matmul\">\\n            MatMul\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#maxpool\">\\n            MaxPool\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#mean\">\\n            Mean\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#mul\">\\n            Mul\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#Pad\">\\n            Pad\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceL2\">\\n            ReduceL2\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceSum\">\\n            ReduceSum\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#Relu\">\\n            Relu\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#reshape\">\\n            Reshape\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#Pow\">\\n            Pow\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/main/docs/Operators.md#SpaceToDepth\">\\n            SpaceToDepth\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Acceleration supported, only for mode=”CRD” and Furiosa SDK version 0.6.0 or higher\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#Sigmoid\">\\n            Sigmoid\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#slice\">\\n            Slice\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Acceleration supported, only for height axis\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#Softmax\">\\n            Softmax\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n          <p>\\n           Acceleration supported, only for batch &lt;= 2\\n          </p>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#Softplus\">\\n            Softplus\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#sub\">\\n            Sub\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#Split\">\\n            Split\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#Sqrt\">\\n            Sqrt\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-even\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#Transpose\">\\n            Transpose\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n        <tr class=\"row-odd\">\\n         <td>\\n          <p>\\n           <a class=\"reference external\" href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#unsqueeze\">\\n            Unsqueeze\\n           </a>\\n          </p>\\n         </td>\\n         <td>\\n         </td>\\n        </tr>\\n       </tbody>\\n      </table>\\n     </section>\\n     <section id=\"mlperf\">\\n      <h2>\\n       MLPerf\\n       <a class=\"headerlink\" href=\"#mlperf\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       Results submitted to MLPerf can be found at\\n       <a class=\"reference external\" href=\"https://mlcommons.org/en/inference-edge-20/\">\\n        MLPerf™ Inference Edge v2.0 Results\\n       </a>\\n      </p>\\n      <figure class=\"align-center\">\\n       <a class=\"with-shadow reference internal image-reference\" href=\"../_images/mlperf_eng.png\">\\n        <img alt=\"Warboy MLPerf Results\" class=\"with-shadow\" src=\"../_images/mlperf_eng.png\" style=\"width: 800px;\"/>\\n       </a>\\n      </figure>\\n      <p>\\n      </p>\\n      <section id=\"see-also\">\\n       <h3>\\n        See Also\\n        <a class=\"headerlink\" href=\"#see-also\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          <a class=\"reference external\" href=\"https://mlcommons.org/en/inference-edge-11/\">\\n           MLPerf™ Inference Edge v1.1 Results\\n          </a>\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          <a class=\"reference external\" href=\"https://mlcommons.org/en/inference-edge-05/\">\\n           MLPerf™ Inference Edge v0.5 Results\\n          </a>\\n         </p>\\n        </li>\\n       </ul>\\n      </section>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"../index.html\" rel=\"prev\" title=\"FuriosaAI NPU &amp; SDK 0.10.1 Documents\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"../software/intro.html\" rel=\"next\" title=\"FuriosaAI SW Stack Introduction\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='9197c2b9-4a70-427e-910e-e57d9f467929', link=Url('https://furiosa-ai.github.io/docs/latest/en/releases/0.10.0.html'), name='0', parent='', child=[], description='\\n\\n\\n* Release Notes - 0.10.0\\n* [View page source](../_sources/releases/0.10.0.rst.txt)\\n\\n---\\n\\n\\n\\nRelease Notes - 0.10.0\\n[\\uf0c1](#release-notes-0-10-0 \"Permalink to this heading\")\\n=============================================================================\\n\\nFuriosa SDK 0.10.0 is a major release which includes the followings:\\n\\n* Adds the next generation runtime engine (FuriosaRT) with higher performance and multi-device features\\n* Improves usability of optimization for vision models by removing quantization operators from models\\n* Supports OpenMetrics format in Metrics Exporter and provide more metrics such as NPU utilization\\n* Improves furiosa-litmus to collect and dump from the diagnosis steps for reporting\\n* Removes Python dependencies from\\n  `furiosa-compiler`\\n  command\\n* Adds the new benchmark tool\\n  `furiosa-bench`\\n\\nThis release also includes a number of other feature additions, bug fixes, and performance improvements.\\n\\n\\nComponent versions\\n\\n[\\uf0c1](#id2 \"Permalink to this table\")\\n\\n\\n| Package Name | Version |\\n| --- | --- |\\n| NPU Driver | 1.9.2 |\\n| NPU Firmware Tools | 1.5.1 |\\n| NPU Firmware Image | 1.7.3 |\\n| HAL (Hardware Abstraction Layer) | 0.11.0 |\\n| Furiosa Compiler | 0.10.0 |\\n| Furiosa Quantizer | 0.10.0 |\\n| Furiosa Runtime | 0.10.0 |\\n| Python SDK (furiosa-server, furiosa-serving, ..) | 0.10.0 |\\n| NPU Toolkit (furiosactl) | 0.11.0 |\\n| NPU Device Plugin | 0.10.1 |\\n| NPU Feature Discovery | 0.2.0 |\\n\\nInstalling the latest SDK or Upgrading\\n[\\uf0c1](#installing-the-latest-sdk-or-upgrading \"Permalink to this heading\")\\n---------------------------------------------------------------------------------------------------------------\\n\\nIf you are using APT repository, the upgrade process is simple.\\nPlease run as follows. If you are not familiar with how to use FurioaAI’s APT repository,\\nplease find more detais from\\n[Driver, Firmware, and Runtime Installation](../software/installation.html#requiredpackages)\\n.\\n\\n```\\napt-get update && apt-get upgrade\\n\\n```\\n\\nYou can also upgrade specific packages as follows:\\n\\n```\\napt-get update && \\\\\\napt-get install -y furiosa-driver-warboy furiosa-libnux\\n\\n```\\n\\nYou can upgrade firmware as follows:\\n\\n```\\napt-get update && \\\\\\napt-get install -y furiosa-firmware-tools furiosa-firmware-image\\n\\n```\\n\\nYou can upgrade Python package as follows:\\n\\n```\\npip install --upgrade pip setuptools wheel\\npip install --upgrade furiosa-sdk\\n\\n```\\n\\n\\nWarning\\n\\nWhen installing or upgrading the furiosa-sdk without updating pip to the latest version,\\nyou may encounter the following errors.\\n\\n```\\nERROR: Could not find a version that satisfies the requirement furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk) (from versions: none)\\nERROR: No matching distribution found for furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk)\\n\\n```\\n\\n\\n\\n\\nMajor changes\\n[\\uf0c1](#major-changes \"Permalink to this heading\")\\n-------------------------------------------------------------\\n\\n### Next Generation Runtime Engine, FuriosaRT [\\uf0c1](#next-generation-runtime-engine-furiosart \"Permalink to this heading\")\\n\\nSDK 0.10.0 includes the next-generation runtime engine called\\nFuriosaRT\\n\\n.\\nFuriosaRT is a newly designed runtime library that offers more advanced features and\\nhigh performance in various workloads.\\nMany components, such as furiosa-litmus, furiosa-bench, and furiosa-seving,\\nare based on FuriosaRT, and the benefits of the new runtime engine are reflected in these components.\\nFuriosaRT provides the backward compatibility with the previous runtime and\\nincludes the following new features:\\n\\n#### New Runtime API [\\uf0c1](#new-runtime-api \"Permalink to this heading\")\\n\\nFuriosaRT introduces a native asynchronous\\nAPI based on Python’s asyncio <\\n<https://docs.python.org/3/library/asyncio.html>\\n>.\\nThe existing APIs were sufficient for batch applications,\\nbut it requires extra code to implement high-performance serving applications,\\nhandling many concurrent individual requests. The new API natively supports asynchronous execution.\\nWith the new API, users can easily write their applications running on existing web frameworks\\nsuch as\\n[FastAPI](https://fastapi.tiangolo.com/)\\n\\nThe new API introduced many advanced features, and you can learn more about the details at\\n[Furiosa SDK API Reference - furiosa.runtime](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html)\\n\\n\\n#### Multi-device Support and Improvement on Device Configuration [\\uf0c1](#multi-device-support-and-improvement-on-device-configuration \"Permalink to this heading\")\\n\\nFuriosaRT natively supports multiple devices with a single session. This feature leads\\nto high-performance inference using multiple devices without extra implementations.\\nFurthermore, FuriosaRT adopts more abstracted way to specify NPU devices.\\nBefore 0.9.0 release, users used to set device file names (e.g.,\\n`npu0pe0-1`\\n) explicitly\\nin the environment variable\\n`NPU_DEVNAME`\\nor\\n`session.create(..,\\n\\ndevice=”..”)`\\n.\\nThis way was inconvinient in many cases because users need\\nto find all available device files and specify them manually.\\n\\nFuriosaRT allows users to specify NPU arch, count of NPUs in a textutal representation.\\nThis representation is allowed in the new environment variable\\n`FURIOSA_DEVICES`\\nas follows:\\n\\n```\\nexport FURIOSA_DEVICES=\"warboy(2)*8\"\\n\\n```\\n\\nThe above example lets FuriosaRT to find 8 Warboys, each of which is configured as two PEs fusion in the system.\\n\\n```\\nexport FURIOSA_DEVICES=\"npu:0:0-1,npu:1:0-1\"\\n\\n```\\n\\nFor backward compatibility, FuriosaRT still supports\\n`NPU_DEVNAME`\\nenvironment variable.\\nHowever,\\n`NPU_DEVNAME`\\nwill be deprecated in a future release.\\n\\nYou can find more details about the device configuration at\\n[Furiosa SDK API Reference - Device Specification](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#device-specification)\\n.\\n\\n\\n#### Higher Throughput [\\uf0c1](#higher-throughput \"Permalink to this heading\")\\n\\nAccording to our benchmark, FuriosaRT shows significantly improved throughput\\ncompared to the previous runtime. In particular,\\n[worker\\\\_num](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#runner-api)\\nconfiguration became more effective in FuriosaRT. For example, in the previous runtime,\\nhigher than 2\\n`worker_num`\\ndid not show significant performance improvement in most cases.\\nHowever, in FuriosaRT, we observed that performance improvement is still significant even with\\n`worker_num\\n\\n>=\\n\\n10`\\n.\\nWe carried out benchmarking with Resnet50, YOLOv5m, YOLOv5L, SSD ResNet34, and SSD MobileNet models through\\n`furiosa-bench`\\ncommand introduced in this release. We observed that performance improvement\\nis significantly up to tens of percent depending on the model with\\n`worker_num\\n\\n>=\\n\\n4`\\n.\\n\\n\\n\\n### Model Server and Serving Framework [\\uf0c1](#model-server-and-serving-framework \"Permalink to this heading\")\\n\\n`furiosa-server`\\nand\\n`furioa-serving`\\nare a web server and a web framework respectively for serving models.\\nThe improvements of FuriosaRT are also reflected to the model server and serving framework.\\n\\n* [Multi-device Support and Improvement on Device Configuration](#release-0-10-0-deviceselector)\\n  can be used to configure multiple NPU devices in\\n  `furiosa-server`\\n  and\\n  `furioa-serving`\\n* New\\n  [asyncio](https://docs.python.org/3/library/asyncio.html)\\n  -based API that FuriosaRT offers is introduced to handle more concurrent requests with less resources.\\n* The model server and serving framework inherit the performance characteristics of FuriosaRT. Also, more\\n  `worker_num`\\n  can be used to improve the performance of the model server.\\n\\nPlease refer to\\n[Model Server (Serving Framework)](../software/serving.html#modelserving)\\nto learn more about the model server and serving framework.\\n\\n\\n### Model Quantization Tool [\\uf0c1](#model-quantization-tool \"Permalink to this heading\")\\n\\nThe furiosa-quantizer is a library that transforms trained models into quantized models\\nthrough the post-training quantization. In 0.10.0 release,\\nthe usability of the quantization tool has been improved,\\nso some parameters of the\\n`furiosa.quantizer.quantize()`\\nAPI have a few breaking changes.\\n\\n#### Motivation for Change [\\uf0c1](#motivation-for-change \"Permalink to this heading\")\\n\\n`furiosa.quantizer.quantize()`\\nfunction is a core function of the model quantization tool.\\n`furiosa.quantizer.quantize()`\\ntransforms an ONNX model into a quantized model and returns it. The function has\\nthe parameter\\n`with_quantize`\\nthat allows the model to accept directly the\\n`uint8`\\ntype\\ninstead of\\n`float32`\\n, also enabling skipping the quantization process for inferences\\nwhen the original data type (e.g., pixel values) is uint8.\\nThis option can result in significant performance improvements.\\nFor instance, YOLOv5 Large with this option can dramatically reduce the execution time\\nfrom 60.639 ms to 0.277 ms.\\n\\nSimilarly,\\n`normalized_pixel_outputs`\\noption allows to directly use\\n`unt8`\\ntype for outputs\\ninstead of\\n`float32`\\n. This option can be useful when the model output is an image in RGB format\\nor when it can be directly used as an integer value. This option shows significant performance boosts.\\n\\nIn certain applications, two options can reduce execution time by several times to hundreds of times.\\nHowever, there were the following limitations and feedback:\\n\\n* The parameter\\n  `normalized_pixel_outputs`\\n  was ambiguous in expressing the purpose clearly.\\n* `normalized_pixel_outputs`\\n  assumes the output tensor value ranged from 0 to 1 in floating-point, and it had limited in real application.\\n* `with_quantize`\\n  and\\n  `normalized_pixel_outputs`\\n  options only supported\\n  `uint8`\\n  type, and didn’t support\\n  `int8`\\n  type.\\n\\n#### What Changed [\\uf0c1](#what-changed \"Permalink to this heading\")\\n\\n* Removed the parameters\\n  `with_quantize`\\n  and\\n  `normalized_pixel_outputs`\\n  from\\n  `furiosa.quantizer.quantize()`\\n* Instead, added the class\\n  [ModelEditor](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.quantizer.html#furiosa.quantizer.ModelEditor)\\n  , allowing more options for model input/output types that offers following optimizations:\\n  \\n  + `convert_input_type(tensor_name,\\n    \\n    TensorType)`\\n    method takes a tensor name, removes the corresponding quantize operator, and changes the input type to a given\\n    `TensorType`\\n    .\\n  + `convert_output_type(tensor_name,\\n    \\n    TensorType,\\n    \\n    tensor_range)`\\n    method takes a tensor name, removes the corresponding dequantize operator, and changes the output type to\\n    `TensorType`\\n    , then modifies the scale of the model output to a given\\n    `tensor_range`\\n    .\\n* Since the\\n  `convert_{output,input}_type`\\n  methods are based on tensor names, users should be able to find tensor names from an original ONNX model.\\n\\nFor that,\\n`furiosa.quantizer`\\nmodule provides\\n`get_pure_input_names(ModelProto)`\\nand\\n`get_output_names(ModelProto)`\\nfunctions to retrieve tensor names from the original ONNX model.\\n\\nNote\\n\\nThe removal of\\n`with_quantize`\\n,\\n`normalized_pixel_outputs`\\nparameters from\\n`furiosa.quantizer.quantize()`\\nis a breaking change that requires modifying existing code.\\n\\nPlease refer to\\n[ModelEditor](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.quantizer.html#furiosa.quantizer.ModelEditor)\\nto learn more about the ModelEditor API and find examples from\\n[Tutorial and Code Examples](../software/tutorials.html#tutorial)\\n.\\n\\n\\n\\n### Compiler [\\uf0c1](#compiler \"Permalink to this heading\")\\n\\nSince this release, the compiler supports NPU acceleration for the\\nDequantize\\n\\noperator.\\nSo, the latency or throughput of models that include\\nDequantize\\n\\noperators can be enhanced.\\nMore details of this performance optimization can be found from\\n[Performance Optimization](../software/performance.html#performanceoptimization)\\n.\\n\\nSince 0.10.0, the default lifetime of compiler cache has increased from 2 days to 30 days.\\nPlease refer to\\n[Compiler Cache](../software/compiler.html#compilercache)\\nto learn the details of compiler cache feature.\\n\\n`furiosa-compiler`\\ncommand in 0.10.0 release also has the following improvements:\\n\\n* Add\\n  `furiosa-compiler`\\n  command in addition to\\n  `furiosa-compile`\\n  command\\n* `furiosa-compiler`\\n  and\\n  `furiosa-compile`\\n  commands as a native executable\\n  and do not require any Python runtime environment.\\n* `furiosa-compiler`\\n  is now available as an APT package, you can install via\\n  `apt\\n  \\n  install\\n  \\n  furiosa-compiler`\\n  .\\n* `furiosa\\n  \\n  compile`\\n  is kept for backward compatibility, and it will be removed in a future release.\\n\\nPlease visit\\n[furiosa-compiler](../software/compiler.html#compilercli)\\nto learn more about\\nfuriosa-compiler\\n\\ncommand.\\n\\n\\n### Performance Profiler [\\uf0c1](#performance-profiler \"Permalink to this heading\")\\n\\nThe performance profiler is a tool that helps users to analyze performance by measuring\\nthe actual execution time of inferences.\\nSince 0.10.0,\\n[Tracing via Profiler Context](../software/profiler.html#profilerenabledbycontext)\\nAPI provides the pause/resume features.\\n\\nThis feature allows users to skip unnecessary steps like pre/post processing or warming up times,\\nleading to the reduction of the profiling overhead and the size of the profile result files.\\nLiterally, calling\\n`profile.pause()`\\nmethod immediately stops the profliling, and\\n`profile.resume()`\\nresumes the profiling again.\\nThe profiler will not collect any profiling information between both method calls.\\nPlease refer to\\n[Pause/Resume of Profiler Context](../software/profiler.html#temporarilydisablingprofiler)\\nto learn more about the profiling API.\\n\\n\\n### furiosa-litmus [\\uf0c1](#furiosa-litmus \"Permalink to this heading\")\\n\\n`furiosa-litmus`\\nis a command-line tool that checks the compatibility of models\\nwith the NPU and Furiosa SDK. Since 0.10.0,\\n`furiosa-litmus`\\nhas a new feature to collect\\nlogs, profiling information, and an environment information for error reporting.\\nThis feature is enabled if\\n`--dump\\n\\n<OUTPUT_PREFIX>`\\noption is specified.\\nThe collected data is saved into a zip file named\\n`<OUTPUT_PREFIX>-<unix_epoch>.zip`\\n.\\n\\n```\\n$ furiosa-litmus <MODEL_PATH> --dump <OUTPUT_PREFIX>\\n\\n```\\n\\nThe collected information does not include the model itself\\nbut does contain only metadata of the model, memory usage, and environmental information\\n(e.g., Python version, SDK, compiler version, and dependency library versions).\\nYou can directly unzip the zip file to check the contents.\\nWhen reporting bugs, attaching this file will be very helpful for error dianosis and analysis.\\n\\n\\n### New Benchmark Tool ‘furiosa-bench’ [\\uf0c1](#new-benchmark-tool-furiosa-bench \"Permalink to this heading\")\\n\\nThe new benchmark tool,\\n`furiosa-bench`\\n, has been added since 0.10.0.\\n`furiosa-bench`\\ncommand offers various options to run a diverse workloads with certain runtime settings.\\nUsers can choose either latency-oriented or throughput-oriented workload, and can specify\\nthe number of devices, how long time to run, and runtime settings.\\n`furiosa-bench`\\naccepts\\nboth ONNX and Tflite models as well as an ENF file compiled by the furiosa-compiler.\\nMore details about the command can be found at\\n[furiosa-bench (Benchmark Tool)](../software/cli.html#furiosabench)\\n.\\n\\nAn example of a throughput benchmark\\n\\n```\\n$ furiosa-bench ./model.onnx --workload throughput -n 10000 --devices \"warboy(1)*2\" --workers 8 --batch 8\\n\\n```\\n\\nAn example of a latency benchmark\\n\\n```\\n$ furiosa-bench ./model.onnx --workload latency -n 10000 --devices \"warboy(2)*1\"\\n\\n```\\n\\n`furiosa-bench`\\ncan be installed through apt package manager as follows:\\n\\n```\\n$ apt install furiosa-bench\\n\\n```\\n\\n\\n\\n### furiosa-toolkit [\\uf0c1](#furiosa-toolkit \"Permalink to this heading\")\\n\\nfuriosa-toolkit is a collection of command line tools that provide NPU management and NPU device\\nmonitoring. Since 0.10.0,\\n`furiosa-toolkit`\\nincludes the following improvements:\\n\\n**Improvements of furiosactl**\\n\\nBefore 0.10.0, the sub-commands like\\n`list`\\n,\\n`info`\\nprint out a tabular text.\\nSince 0.10.0,\\n`furiosactl`\\nnewly provides\\n`--format`\\noption, allowing to\\nprint out the result in a structured format like\\n`json`\\nor\\n`yaml`\\n.\\nIt will be useful when a users implements a shell pipeline or a script to process\\nthe output of\\n`furiosactl`\\n.\\n\\n```\\n$ furiosactl info --format json\\n[{\"dev_name\":\"npu7\",\"product_name\":\"warboy\",\"device_uuid\":\"<device_uuid>\",\"device_sn\":\"<device_sn>\",\"firmware\":\"1.6.0, 7a3b908\",\"temperature\":\"47°C\",\"power\":\"0.99 W\",\"pci_bdf\":\"0000:d6:00.0\",\"pci_dev\":\"492:0\"}]\\n\\n$ furiosactl info --format yaml\\n- dev_name: npu7\\n  product_name: warboy\\n  device_uuid: <device_uuid>\\n  device_sn: <device_sn>\\n  firmware: 1.6.0, 7a3b908\\n  temperature: 47°C\\n  power: 0.98 W\\n  pci_bdf: 0000:d6:00.0\\n  pci_dev: 492:0\\n\\n```\\n\\nAlso, the subcommand\\n`info`\\nresults in two more metrics:\\n\\n* NPU Clock Frequency\\n* Entire power consumption of card\\n\\n**Improvements of furiosa-npu-metrics-exporter**\\n\\n`furiosa-npu-metrics-exporter`\\nis a HTTP server to export NPU metrics and status\\nin\\n[OpenMetrics](https://github.com/OpenObservability/OpenMetrics/blob/main/specification/OpenMetrics.md)\\nformat.\\nThe metrics that\\n`furiosa-npu-metrics-exporter`\\nexports can be collected by Prometheus and other OpenMetrics compatible collectors.\\n\\nSince 0.10.0,\\n`furiosa-npu-metrics-exporter`\\nincludes NPU clock frequency and NPU utilization\\nas metrics. NPU utilziation is still an experimental feature, and it is disabled by default.\\nTo enable this feature, you need to specify\\n`--enable-npu-utilization`\\noption as follows:\\n\\n```\\nfuriosa-npu-metrics-exporter --enable-npu-utilization\\n\\n```\\n\\nAdditionally,\\n`furiosa-npu-metrics-exporter`\\nis now available as an APT package\\nin addition to the docker image. You can install it as follows:\\n\\n```\\napt install furiosa-toolkit\\n\\n```\\n\\n\\n\\n\\n\\n\\n\\n[Previous](../api/python/furiosa.serving.processors.html \"furiosa.serving.processors package\")\\n[Next](0.9.0.html \"Release Notes - 0.9.0\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Release Notes - 0.10.0 * [View page source](../_sources/releases/0.10.0.rst.txt)\\n---\\nRelease Notes - 0.10.0 [\\uf0c1](#release-notes-0-10-0 \"Permalink to this heading\") =============================================================================\\nFuriosa SDK 0.10.0 is a major release which includes the followings:\\n* Adds the next generation runtime engine (FuriosaRT) with higher performance and multi-device features * Improves usability of optimization for vision models by removing quantization operators from models * Supports OpenMetrics format in Metrics Exporter and provide more metrics such as NPU utilization * Improves furiosa-litmus to collect and dump from the diagnosis steps for reporting * Removes Python dependencies from   `furiosa-compiler`   command * Adds the new benchmark tool   `furiosa-bench`  This release also includes a number of other feature additions, bug fixes, and performance improvements.\\nComponent versions\\n[\\uf0c1](#id2 \"Permalink to this table\")\\n| Package Name | Version | | --- | --- | | NPU Driver | 1.9.2 | | NPU Firmware Tools | 1.5.1 | | NPU Firmware Image | 1.7.3 | | HAL (Hardware Abstraction Layer) | 0.11.0 | | Furiosa Compiler | 0.10.0 | | Furiosa Quantizer | 0.10.0 | | Furiosa Runtime | 0.10.0 | | Python SDK (furiosa-server, furiosa-serving, ..) | 0.10.0 | | NPU Toolkit (furiosactl) | 0.11.0 | | NPU Device Plugin | 0.10.1 | | NPU Feature Discovery | 0.2.0 |\\nInstalling the latest SDK or Upgrading [\\uf0c1](#installing-the-latest-sdk-or-upgrading \"Permalink to this heading\") ---------------------------------------------------------------------------------------------------------------\\nIf you are using APT repository, the upgrade process is simple. Please run as follows. If you are not familiar with how to use FurioaAI’s APT repository, please find more detais from [Driver, Firmware, and Runtime Installation](../software/installation.html#requiredpackages) .\\n``` apt-get update && apt-get upgrade\\n```\\nYou can also upgrade specific packages as follows:\\n``` apt-get update && \\\\ apt-get install -y furiosa-driver-warboy furiosa-libnux\\n```\\nYou can upgrade firmware as follows:\\n``` apt-get update && \\\\ apt-get install -y furiosa-firmware-tools furiosa-firmware-image\\n```\\nYou can upgrade Python package as follows:\\n``` pip install --upgrade pip setuptools wheel pip install --upgrade furiosa-sdk\\n```\\nWarning\\nWhen installing or upgrading the furiosa-sdk without updating pip to the latest version, you may encounter the following errors.\\n``` ERROR: Could not find a version that satisfies the requirement furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk) (from versions: none) ERROR: No matching distribution found for furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk)\\n```\\nMajor changes [\\uf0c1](#major-changes \"Permalink to this heading\") -------------------------------------------------------------\\n### Next Generation Runtime Engine, FuriosaRT [\\uf0c1](#next-generation-runtime-engine-furiosart \"Permalink to this heading\")\\nSDK 0.10.0 includes the next-generation runtime engine called FuriosaRT\\n. FuriosaRT is a newly designed runtime library that offers more advanced features and high performance in various workloads. Many components, such as furiosa-litmus, furiosa-bench, and furiosa-seving, are based on FuriosaRT, and the benefits of the new runtime engine are reflected in these components. FuriosaRT provides the backward compatibility with the previous runtime and includes the following new features:\\n#### New Runtime API [\\uf0c1](#new-runtime-api \"Permalink to this heading\")\\nFuriosaRT introduces a native asynchronous API based on Python’s asyncio < <https://docs.python.org/3/library/asyncio.html> >. The existing APIs were sufficient for batch applications, but it requires extra code to implement high-performance serving applications, handling many concurrent individual requests. The new API natively supports asynchronous execution. With the new API, users can easily write their applications running on existing web frameworks such as [FastAPI](https://fastapi.tiangolo.com/)\\nThe new API introduced many advanced features, and you can learn more about the details at [Furiosa SDK API Reference - furiosa.runtime](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html)\\n#### Multi-device Support and Improvement on Device Configuration [\\uf0c1](#multi-device-support-and-improvement-on-device-configuration \"Permalink to this heading\")\\nFuriosaRT natively supports multiple devices with a single session. This feature leads to high-performance inference using multiple devices without extra implementations. Furthermore, FuriosaRT adopts more abstracted way to specify NPU devices. Before 0.9.0 release, users used to set device file names (e.g., `npu0pe0-1` ) explicitly in the environment variable `NPU_DEVNAME` or `session.create(..,\\ndevice=”..”)` .\\nThis way was inconvinient in many cases because users need to find all available device files and specify them manually.\\nFuriosaRT allows users to specify NPU arch, count of NPUs in a textutal representation. This representation is allowed in the new environment variable `FURIOSA_DEVICES` as follows:\\n``` export FURIOSA_DEVICES=\"warboy(2)*8\"\\n```\\nThe above example lets FuriosaRT to find 8 Warboys, each of which is configured as two PEs fusion in the system.\\n``` export FURIOSA_DEVICES=\"npu:0:0-1,npu:1:0-1\"\\n```\\nFor backward compatibility, FuriosaRT still supports `NPU_DEVNAME` environment variable. However, `NPU_DEVNAME` will be deprecated in a future release.\\nYou can find more details about the device configuration at [Furiosa SDK API Reference - Device Specification](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#device-specification) .\\n#### Higher Throughput [\\uf0c1](#higher-throughput \"Permalink to this heading\")\\nAccording to our benchmark, FuriosaRT shows significantly improved throughput compared to the previous runtime. In particular, [worker\\\\_num](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#runner-api) configuration became more effective in FuriosaRT. For example, in the previous runtime, higher than 2 `worker_num` did not show significant performance improvement in most cases. However, in FuriosaRT, we observed that performance improvement is still significant even with `worker_num\\n>=\\n10` .\\nWe carried out benchmarking with Resnet50, YOLOv5m, YOLOv5L, SSD ResNet34, and SSD MobileNet models through `furiosa-bench` command introduced in this release. We observed that performance improvement is significantly up to tens of percent depending on the model with `worker_num\\n>=\\n4` .\\n### Model Server and Serving Framework [\\uf0c1](#model-server-and-serving-framework \"Permalink to this heading\")  `furiosa-server` and `furioa-serving` are a web server and a web framework respectively for serving models. The improvements of FuriosaRT are also reflected to the model server and serving framework.\\n* [Multi-device Support and Improvement on Device Configuration](#release-0-10-0-deviceselector)   can be used to configure multiple NPU devices in   `furiosa-server`   and   `furioa-serving` * New   [asyncio](https://docs.python.org/3/library/asyncio.html)   -based API that FuriosaRT offers is introduced to handle more concurrent requests with less resources. * The model server and serving framework inherit the performance characteristics of FuriosaRT. Also, more   `worker_num`   can be used to improve the performance of the model server.\\nPlease refer to [Model Server (Serving Framework)](../software/serving.html#modelserving) to learn more about the model server and serving framework.\\n### Model Quantization Tool [\\uf0c1](#model-quantization-tool \"Permalink to this heading\")\\nThe furiosa-quantizer is a library that transforms trained models into quantized models through the post-training quantization. In 0.10.0 release, the usability of the quantization tool has been improved, so some parameters of the `furiosa.quantizer.quantize()` API have a few breaking changes.\\n#### Motivation for Change [\\uf0c1](#motivation-for-change \"Permalink to this heading\")  `furiosa.quantizer.quantize()` function is a core function of the model quantization tool. `furiosa.quantizer.quantize()` transforms an ONNX model into a quantized model and returns it. The function has the parameter `with_quantize` that allows the model to accept directly the `uint8` type instead of `float32` , also enabling skipping the quantization process for inferences when the original data type (e.g., pixel values) is uint8. This option can result in significant performance improvements. For instance, YOLOv5 Large with this option can dramatically reduce the execution time from 60.639 ms to 0.277 ms.\\nSimilarly, `normalized_pixel_outputs` option allows to directly use `unt8` type for outputs instead of `float32` . This option can be useful when the model output is an image in RGB format or when it can be directly used as an integer value. This option shows significant performance boosts.\\nIn certain applications, two options can reduce execution time by several times to hundreds of times. However, there were the following limitations and feedback:\\n* The parameter   `normalized_pixel_outputs`   was ambiguous in expressing the purpose clearly. * `normalized_pixel_outputs`   assumes the output tensor value ranged from 0 to 1 in floating-point, and it had limited in real application. * `with_quantize`   and   `normalized_pixel_outputs`   options only supported   `uint8`   type, and didn’t support   `int8`   type.\\n#### What Changed [\\uf0c1](#what-changed \"Permalink to this heading\")\\n* Removed the parameters   `with_quantize`   and   `normalized_pixel_outputs`   from   `furiosa.quantizer.quantize()` * Instead, added the class   [ModelEditor](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.quantizer.html#furiosa.quantizer.ModelEditor)   , allowing more options for model input/output types that offers following optimizations:      + `convert_input_type(tensor_name,          TensorType)`     method takes a tensor name, removes the corresponding quantize operator, and changes the input type to a given     `TensorType`     .   + `convert_output_type(tensor_name,          TensorType,          tensor_range)`     method takes a tensor name, removes the corresponding dequantize operator, and changes the output type to     `TensorType`     , then modifies the scale of the model output to a given     `tensor_range`     . * Since the   `convert_{output,input}_type`   methods are based on tensor names, users should be able to find tensor names from an original ONNX model.\\nFor that, `furiosa.quantizer` module provides `get_pure_input_names(ModelProto)` and `get_output_names(ModelProto)` functions to retrieve tensor names from the original ONNX model.\\nNote\\nThe removal of `with_quantize` , `normalized_pixel_outputs` parameters from `furiosa.quantizer.quantize()` is a breaking change that requires modifying existing code.\\nPlease refer to [ModelEditor](https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.quantizer.html#furiosa.quantizer.ModelEditor) to learn more about the ModelEditor API and find examples from [Tutorial and Code Examples](../software/tutorials.html#tutorial) .\\n### Compiler [\\uf0c1](#compiler \"Permalink to this heading\")\\nSince this release, the compiler supports NPU acceleration for the Dequantize\\noperator. So, the latency or throughput of models that include Dequantize\\noperators can be enhanced. More details of this performance optimization can be found from [Performance Optimization](../software/performance.html#performanceoptimization) .\\nSince 0.10.0, the default lifetime of compiler cache has increased from 2 days to 30 days. Please refer to [Compiler Cache](../software/compiler.html#compilercache) to learn the details of compiler cache feature.  `furiosa-compiler` command in 0.10.0 release also has the following improvements:\\n* Add   `furiosa-compiler`   command in addition to   `furiosa-compile`   command * `furiosa-compiler`   and   `furiosa-compile`   commands as a native executable   and do not require any Python runtime environment. * `furiosa-compiler`   is now available as an APT package, you can install via   `apt      install      furiosa-compiler`   . * `furiosa      compile`   is kept for backward compatibility, and it will be removed in a future release.\\nPlease visit [furiosa-compiler](../software/compiler.html#compilercli) to learn more about furiosa-compiler\\ncommand.\\n### Performance Profiler [\\uf0c1](#performance-profiler \"Permalink to this heading\")\\nThe performance profiler is a tool that helps users to analyze performance by measuring the actual execution time of inferences. Since 0.10.0, [Tracing via Profiler Context](../software/profiler.html#profilerenabledbycontext) API provides the pause/resume features.\\nThis feature allows users to skip unnecessary steps like pre/post processing or warming up times, leading to the reduction of the profiling overhead and the size of the profile result files. Literally, calling `profile.pause()` method immediately stops the profliling, and `profile.resume()` resumes the profiling again. The profiler will not collect any profiling information between both method calls. Please refer to [Pause/Resume of Profiler Context](../software/profiler.html#temporarilydisablingprofiler) to learn more about the profiling API.\\n### furiosa-litmus [\\uf0c1](#furiosa-litmus \"Permalink to this heading\")  `furiosa-litmus` is a command-line tool that checks the compatibility of models with the NPU and Furiosa SDK. Since 0.10.0, `furiosa-litmus` has a new feature to collect logs, profiling information, and an environment information for error reporting. This feature is enabled if `--dump\\n<OUTPUT_PREFIX>` option is specified. The collected data is saved into a zip file named `<OUTPUT_PREFIX>-<unix_epoch>.zip` .\\n``` $ furiosa-litmus <MODEL_PATH> --dump <OUTPUT_PREFIX>\\n```\\nThe collected information does not include the model itself but does contain only metadata of the model, memory usage, and environmental information (e.g., Python version, SDK, compiler version, and dependency library versions). You can directly unzip the zip file to check the contents. When reporting bugs, attaching this file will be very helpful for error dianosis and analysis.\\n### New Benchmark Tool ‘furiosa-bench’ [\\uf0c1](#new-benchmark-tool-furiosa-bench \"Permalink to this heading\")\\nThe new benchmark tool, `furiosa-bench` , has been added since 0.10.0. `furiosa-bench` command offers various options to run a diverse workloads with certain runtime settings. Users can choose either latency-oriented or throughput-oriented workload, and can specify the number of devices, how long time to run, and runtime settings. `furiosa-bench` accepts both ONNX and Tflite models as well as an ENF file compiled by the furiosa-compiler. More details about the command can be found at [furiosa-bench (Benchmark Tool)](../software/cli.html#furiosabench) .\\nAn example of a throughput benchmark\\n``` $ furiosa-bench ./model.onnx --workload throughput -n 10000 --devices \"warboy(1)*2\" --workers 8 --batch 8\\n```\\nAn example of a latency benchmark\\n``` $ furiosa-bench ./model.onnx --workload latency -n 10000 --devices \"warboy(2)*1\"\\n```  `furiosa-bench` can be installed through apt package manager as follows:\\n``` $ apt install furiosa-bench\\n```\\n### furiosa-toolkit [\\uf0c1](#furiosa-toolkit \"Permalink to this heading\")\\nfuriosa-toolkit is a collection of command line tools that provide NPU management and NPU device monitoring. Since 0.10.0, `furiosa-toolkit` includes the following improvements:\\n**Improvements of furiosactl**\\nBefore 0.10.0, the sub-commands like `list` , `info` print out a tabular text. Since 0.10.0, `furiosactl` newly provides `--format` option, allowing to print out the result in a structured format like `json` or `yaml` . It will be useful when a users implements a shell pipeline or a script to process the output of `furiosactl` .\\n``` $ furiosactl info --format json [{\"dev_name\":\"npu7\",\"product_name\":\"warboy\",\"device_uuid\":\"<device_uuid>\",\"device_sn\":\"<device_sn>\",\"firmware\":\"1.6.0, 7a3b908\",\"temperature\":\"47°C\",\"power\":\"0.99 W\",\"pci_bdf\":\"0000:d6:00.0\",\"pci_dev\":\"492:0\"}]\\n$ furiosactl info --format yaml - dev_name: npu7   product_name: warboy   device_uuid: <device_uuid>   device_sn: <device_sn>   firmware: 1.6.0, 7a3b908   temperature: 47°C   power: 0.98 W   pci_bdf: 0000:d6:00.0   pci_dev: 492:0\\n```\\nAlso, the subcommand `info` results in two more metrics:\\n* NPU Clock Frequency * Entire power consumption of card\\n**Improvements of furiosa-npu-metrics-exporter**  `furiosa-npu-metrics-exporter` is a HTTP server to export NPU metrics and status in [OpenMetrics](https://github.com/OpenObservability/OpenMetrics/blob/main/specification/OpenMetrics.md) format. The metrics that `furiosa-npu-metrics-exporter` exports can be collected by Prometheus and other OpenMetrics compatible collectors.\\nSince 0.10.0, `furiosa-npu-metrics-exporter` includes NPU clock frequency and NPU utilization as metrics. NPU utilziation is still an experimental feature, and it is disabled by default. To enable this feature, you need to specify `--enable-npu-utilization` option as follows:\\n``` furiosa-npu-metrics-exporter --enable-npu-utilization\\n```\\nAdditionally, `furiosa-npu-metrics-exporter` is now available as an APT package in addition to the docker image. You can install it as follows:\\n``` apt install furiosa-toolkit\\n```\\n[Previous](../api/python/furiosa.serving.processors.html \"furiosa.serving.processors package\") [Next](0.9.0.html \"Release Notes - 0.9.0\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Release Notes - 0.10.0\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/releases/0.10.0.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"release-notes-0-10-0\">\\n     <h1>\\n      Release Notes - 0.10.0\\n      <a class=\"headerlink\" href=\"#release-notes-0-10-0\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      Furiosa SDK 0.10.0 is a major release which includes the followings:\\n     </p>\\n     <ul class=\"simple\">\\n      <li>\\n       <p>\\n        Adds the next generation runtime engine (FuriosaRT) with higher performance and multi-device features\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Improves usability of optimization for vision models by removing quantization operators from models\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Supports OpenMetrics format in Metrics Exporter and provide more metrics such as NPU utilization\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Improves furiosa-litmus to collect and dump from the diagnosis steps for reporting\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Removes Python dependencies from\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-compiler\\n         </span>\\n        </code>\\n        command\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Adds the new benchmark tool\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-bench\\n         </span>\\n        </code>\\n       </p>\\n      </li>\\n     </ul>\\n     <p>\\n      This release also includes a number of other feature additions, bug fixes, and performance improvements.\\n     </p>\\n     <table class=\"docutils align-default\" id=\"id2\">\\n      <caption>\\n       <span class=\"caption-text\">\\n        Component versions\\n       </span>\\n       <a class=\"headerlink\" href=\"#id2\" title=\"Permalink to this table\">\\n        \\uf0c1\\n       </a>\\n      </caption>\\n      <colgroup>\\n       <col style=\"width: 80%\"/>\\n       <col style=\"width: 20%\"/>\\n      </colgroup>\\n      <thead>\\n       <tr class=\"row-odd\">\\n        <th class=\"head\">\\n         <p>\\n          Package Name\\n         </p>\\n        </th>\\n        <th class=\"head\">\\n         <p>\\n          Version\\n         </p>\\n        </th>\\n       </tr>\\n      </thead>\\n      <tbody>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          NPU Driver\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          1.9.2\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          NPU Firmware Tools\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          1.5.1\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          NPU Firmware Image\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          1.7.3\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          HAL (Hardware Abstraction Layer)\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.11.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          Furiosa Compiler\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.10.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          Furiosa Quantizer\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.10.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          Furiosa Runtime\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.10.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          Python SDK (furiosa-server, furiosa-serving, ..)\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.10.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          NPU Toolkit (furiosactl)\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.11.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          NPU Device Plugin\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.10.1\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          NPU Feature Discovery\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.2.0\\n         </p>\\n        </td>\\n       </tr>\\n      </tbody>\\n     </table>\\n     <section id=\"installing-the-latest-sdk-or-upgrading\">\\n      <h2>\\n       Installing the latest SDK or Upgrading\\n       <a class=\"headerlink\" href=\"#installing-the-latest-sdk-or-upgrading\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       If you are using APT repository, the upgrade process is simple.\\nPlease run as follows. If you are not familiar with how to use FurioaAI’s APT repository,\\nplease find more detais from\\n       <a class=\"reference internal\" href=\"../software/installation.html#requiredpackages\">\\n        <span class=\"std std-ref\">\\n         Driver, Firmware, and Runtime Installation\\n        </span>\\n       </a>\\n       .\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>apt-get<span class=\"w\"> </span>update<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span>apt-get<span class=\"w\"> </span>upgrade\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       You can also upgrade specific packages as follows:\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>apt-get<span class=\"w\"> </span>update<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span><span class=\"se\">\\\\</span>\\napt-get<span class=\"w\"> </span>install<span class=\"w\"> </span>-y<span class=\"w\"> </span>furiosa-driver-warboy<span class=\"w\"> </span>furiosa-libnux\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       You can upgrade firmware as follows:\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>apt-get<span class=\"w\"> </span>update<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span><span class=\"se\">\\\\</span>\\napt-get<span class=\"w\"> </span>install<span class=\"w\"> </span>-y<span class=\"w\"> </span>furiosa-firmware-tools<span class=\"w\"> </span>furiosa-firmware-image\\n</pre>\\n       </div>\\n      </div>\\n      <p>\\n       You can upgrade Python package as follows:\\n      </p>\\n      <div class=\"highlight-sh notranslate\">\\n       <div class=\"highlight\">\\n        <pre><span></span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>--upgrade<span class=\"w\"> </span>pip<span class=\"w\"> </span>setuptools<span class=\"w\"> </span>wheel\\npip<span class=\"w\"> </span>install<span class=\"w\"> </span>--upgrade<span class=\"w\"> </span>furiosa-sdk\\n</pre>\\n       </div>\\n      </div>\\n      <div class=\"admonition warning\">\\n       <p class=\"admonition-title\">\\n        Warning\\n       </p>\\n       <p>\\n        When installing or upgrading the furiosa-sdk without updating pip to the latest version,\\nyou may encounter the following errors.\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>ERROR:<span class=\"w\"> </span>Could<span class=\"w\"> </span>not<span class=\"w\"> </span>find<span class=\"w\"> </span>a<span class=\"w\"> </span>version<span class=\"w\"> </span>that<span class=\"w\"> </span>satisfies<span class=\"w\"> </span>the<span class=\"w\"> </span>requirement<span class=\"w\"> </span>furiosa-quantizer-impl<span class=\"o\">==</span><span class=\"m\">0</span>.9.*<span class=\"w\"> </span><span class=\"o\">(</span>from<span class=\"w\"> </span>furiosa-quantizer<span class=\"o\">==</span><span class=\"m\">0</span>.9.*-&gt;furiosa-sdk<span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span>from<span class=\"w\"> </span>versions:<span class=\"w\"> </span>none<span class=\"o\">)</span>\\nERROR:<span class=\"w\"> </span>No<span class=\"w\"> </span>matching<span class=\"w\"> </span>distribution<span class=\"w\"> </span>found<span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span>furiosa-quantizer-impl<span class=\"o\">==</span><span class=\"m\">0</span>.9.*<span class=\"w\"> </span><span class=\"o\">(</span>from<span class=\"w\"> </span>furiosa-quantizer<span class=\"o\">==</span><span class=\"m\">0</span>.9.*-&gt;furiosa-sdk<span class=\"o\">)</span>\\n</pre>\\n        </div>\\n       </div>\\n      </div>\\n     </section>\\n     <section id=\"major-changes\">\\n      <h2>\\n       Major changes\\n       <a class=\"headerlink\" href=\"#major-changes\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <section id=\"next-generation-runtime-engine-furiosart\">\\n       <h3>\\n        Next Generation Runtime Engine, FuriosaRT\\n        <a class=\"headerlink\" href=\"#next-generation-runtime-engine-furiosart\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        SDK 0.10.0 includes the next-generation runtime engine called\\n        <cite>\\n         FuriosaRT\\n        </cite>\\n        .\\nFuriosaRT is a newly designed runtime library that offers more advanced features and\\nhigh performance in various workloads.\\nMany components, such as furiosa-litmus, furiosa-bench, and furiosa-seving,\\nare based on FuriosaRT, and the benefits of the new runtime engine are reflected in these components.\\nFuriosaRT provides the backward compatibility with the previous runtime and\\nincludes the following new features:\\n       </p>\\n       <section id=\"new-runtime-api\">\\n        <h4>\\n         New Runtime API\\n         <a class=\"headerlink\" href=\"#new-runtime-api\" title=\"Permalink to this heading\">\\n          \\uf0c1\\n         </a>\\n        </h4>\\n        <p>\\n         FuriosaRT introduces a native asynchronous\\nAPI based on Python’s asyncio &lt;\\n         <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html\">\\n          https://docs.python.org/3/library/asyncio.html\\n         </a>\\n         &gt;.\\nThe existing APIs were sufficient for batch applications,\\nbut it requires extra code to implement high-performance serving applications,\\nhandling many concurrent individual requests. The new API natively supports asynchronous execution.\\nWith the new API, users can easily write their applications running on existing web frameworks\\nsuch as\\n         <a class=\"reference external\" href=\"https://fastapi.tiangolo.com/\">\\n          FastAPI\\n         </a>\\n        </p>\\n        <p>\\n         The new API introduced many advanced features, and you can learn more about the details at\\n         <a class=\"reference external\" href=\"https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html\">\\n          Furiosa SDK API Reference - furiosa.runtime\\n         </a>\\n        </p>\\n       </section>\\n       <section id=\"multi-device-support-and-improvement-on-device-configuration\">\\n        <span id=\"release-0-10-0-deviceselector\">\\n        </span>\\n        <h4>\\n         Multi-device Support and Improvement on Device Configuration\\n         <a class=\"headerlink\" href=\"#multi-device-support-and-improvement-on-device-configuration\" title=\"Permalink to this heading\">\\n          \\uf0c1\\n         </a>\\n        </h4>\\n        <p>\\n         FuriosaRT natively supports multiple devices with a single session. This feature leads\\nto high-performance inference using multiple devices without extra implementations.\\nFurthermore, FuriosaRT adopts more abstracted way to specify NPU devices.\\nBefore 0.9.0 release, users used to set device file names (e.g.,\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           npu0pe0-1\\n          </span>\\n         </code>\\n         ) explicitly\\nin the environment variable\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           NPU_DEVNAME\\n          </span>\\n         </code>\\n         or\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           session.create(..,\\n          </span>\\n          <span class=\"pre\">\\n           device=”..”)\\n          </span>\\n         </code>\\n         .\\nThis way was inconvinient in many cases because users need\\nto find all available device files and specify them manually.\\n        </p>\\n        <p>\\n         FuriosaRT allows users to specify NPU arch, count of NPUs in a textutal representation.\\nThis representation is allowed in the new environment variable\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           FURIOSA_DEVICES\\n          </span>\\n         </code>\\n         as follows:\\n        </p>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span><span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">FURIOSA_DEVICES</span><span class=\"o\">=</span><span class=\"s2\">\"warboy(2)*8\"</span>\\n</pre>\\n         </div>\\n        </div>\\n        <p>\\n         The above example lets FuriosaRT to find 8 Warboys, each of which is configured as two PEs fusion in the system.\\n        </p>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span><span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">FURIOSA_DEVICES</span><span class=\"o\">=</span><span class=\"s2\">\"npu:0:0-1,npu:1:0-1\"</span>\\n</pre>\\n         </div>\\n        </div>\\n        <p>\\n         For backward compatibility, FuriosaRT still supports\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           NPU_DEVNAME\\n          </span>\\n         </code>\\n         environment variable.\\nHowever,\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           NPU_DEVNAME\\n          </span>\\n         </code>\\n         will be deprecated in a future release.\\n        </p>\\n        <p>\\n         You can find more details about the device configuration at\\n         <a class=\"reference external\" href=\"https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#device-specification\">\\n          Furiosa SDK API Reference - Device Specification\\n         </a>\\n         .\\n        </p>\\n       </section>\\n       <section id=\"higher-throughput\">\\n        <h4>\\n         Higher Throughput\\n         <a class=\"headerlink\" href=\"#higher-throughput\" title=\"Permalink to this heading\">\\n          \\uf0c1\\n         </a>\\n        </h4>\\n        <p>\\n         According to our benchmark, FuriosaRT shows significantly improved throughput\\ncompared to the previous runtime. In particular,\\n         <a class=\"reference external\" href=\"https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.runtime.html#runner-api\">\\n          worker_num\\n         </a>\\n         configuration became more effective in FuriosaRT. For example, in the previous runtime,\\nhigher than 2\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           worker_num\\n          </span>\\n         </code>\\n         did not show significant performance improvement in most cases.\\nHowever, in FuriosaRT, we observed that performance improvement is still significant even with\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           worker_num\\n          </span>\\n          <span class=\"pre\">\\n           &gt;=\\n          </span>\\n          <span class=\"pre\">\\n           10\\n          </span>\\n         </code>\\n         .\\nWe carried out benchmarking with Resnet50, YOLOv5m, YOLOv5L, SSD ResNet34, and SSD MobileNet models through\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           furiosa-bench\\n          </span>\\n         </code>\\n         command introduced in this release. We observed that performance improvement\\nis significantly up to tens of percent depending on the model with\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           worker_num\\n          </span>\\n          <span class=\"pre\">\\n           &gt;=\\n          </span>\\n          <span class=\"pre\">\\n           4\\n          </span>\\n         </code>\\n         .\\n        </p>\\n       </section>\\n      </section>\\n      <section id=\"model-server-and-serving-framework\">\\n       <h3>\\n        Model Server and Serving Framework\\n        <a class=\"headerlink\" href=\"#model-server-and-serving-framework\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-server\\n         </span>\\n        </code>\\n        and\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furioa-serving\\n         </span>\\n        </code>\\n        are a web server and a web framework respectively for serving models.\\nThe improvements of FuriosaRT are also reflected to the model server and serving framework.\\n       </p>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          <a class=\"reference internal\" href=\"#release-0-10-0-deviceselector\">\\n           <span class=\"std std-ref\">\\n            Multi-device Support and Improvement on Device Configuration\\n           </span>\\n          </a>\\n          can be used to configure multiple NPU devices in\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            furiosa-server\\n           </span>\\n          </code>\\n          and\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            furioa-serving\\n           </span>\\n          </code>\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          New\\n          <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html\">\\n           asyncio\\n          </a>\\n          -based API that FuriosaRT offers is introduced to handle more concurrent requests with less resources.\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          The model server and serving framework inherit the performance characteristics of FuriosaRT. Also, more\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            worker_num\\n           </span>\\n          </code>\\n          can be used to improve the performance of the model server.\\n         </p>\\n        </li>\\n       </ul>\\n       <p>\\n        Please refer to\\n        <a class=\"reference internal\" href=\"../software/serving.html#modelserving\">\\n         <span class=\"std std-ref\">\\n          Model Server (Serving Framework)\\n         </span>\\n        </a>\\n        to learn more about the model server and serving framework.\\n       </p>\\n      </section>\\n      <section id=\"model-quantization-tool\">\\n       <h3>\\n        Model Quantization Tool\\n        <a class=\"headerlink\" href=\"#model-quantization-tool\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        The furiosa-quantizer is a library that transforms trained models into quantized models\\nthrough the post-training quantization. In 0.10.0 release,\\nthe usability of the quantization tool has been improved,\\nso some parameters of the\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa.quantizer.quantize()\\n         </span>\\n        </code>\\n        API have a few breaking changes.\\n       </p>\\n       <section id=\"motivation-for-change\">\\n        <h4>\\n         Motivation for Change\\n         <a class=\"headerlink\" href=\"#motivation-for-change\" title=\"Permalink to this heading\">\\n          \\uf0c1\\n         </a>\\n        </h4>\\n        <p>\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           furiosa.quantizer.quantize()\\n          </span>\\n         </code>\\n         function is a core function of the model quantization tool.\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           furiosa.quantizer.quantize()\\n          </span>\\n         </code>\\n         transforms an ONNX model into a quantized model and returns it. The function has\\nthe parameter\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           with_quantize\\n          </span>\\n         </code>\\n         that allows the model to accept directly the\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           uint8\\n          </span>\\n         </code>\\n         type\\ninstead of\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           float32\\n          </span>\\n         </code>\\n         , also enabling skipping the quantization process for inferences\\nwhen the original data type (e.g., pixel values) is uint8.\\nThis option can result in significant performance improvements.\\nFor instance, YOLOv5 Large with this option can dramatically reduce the execution time\\nfrom 60.639 ms to 0.277 ms.\\n        </p>\\n        <p>\\n         Similarly,\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           normalized_pixel_outputs\\n          </span>\\n         </code>\\n         option allows to directly use\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           unt8\\n          </span>\\n         </code>\\n         type for outputs\\ninstead of\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           float32\\n          </span>\\n         </code>\\n         . This option can be useful when the model output is an image in RGB format\\nor when it can be directly used as an integer value. This option shows significant performance boosts.\\n        </p>\\n        <p>\\n         In certain applications, two options can reduce execution time by several times to hundreds of times.\\nHowever, there were the following limitations and feedback:\\n        </p>\\n        <ul class=\"simple\">\\n         <li>\\n          <p>\\n           The parameter\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             normalized_pixel_outputs\\n            </span>\\n           </code>\\n           was ambiguous in expressing the purpose clearly.\\n          </p>\\n         </li>\\n         <li>\\n          <p>\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             normalized_pixel_outputs\\n            </span>\\n           </code>\\n           assumes the output tensor value ranged from 0 to 1 in floating-point, and it had limited in real application.\\n          </p>\\n         </li>\\n         <li>\\n          <p>\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             with_quantize\\n            </span>\\n           </code>\\n           and\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             normalized_pixel_outputs\\n            </span>\\n           </code>\\n           options only supported\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             uint8\\n            </span>\\n           </code>\\n           type, and didn’t support\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             int8\\n            </span>\\n           </code>\\n           type.\\n          </p>\\n         </li>\\n        </ul>\\n       </section>\\n       <section id=\"what-changed\">\\n        <h4>\\n         What Changed\\n         <a class=\"headerlink\" href=\"#what-changed\" title=\"Permalink to this heading\">\\n          \\uf0c1\\n         </a>\\n        </h4>\\n        <ul class=\"simple\">\\n         <li>\\n          <p>\\n           Removed the parameters\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             with_quantize\\n            </span>\\n           </code>\\n           and\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             normalized_pixel_outputs\\n            </span>\\n           </code>\\n           from\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             furiosa.quantizer.quantize()\\n            </span>\\n           </code>\\n          </p>\\n         </li>\\n         <li>\\n          <dl class=\"simple\">\\n           <dt>\\n            Instead, added the class\\n            <a class=\"reference external\" href=\"https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.quantizer.html#furiosa.quantizer.ModelEditor\">\\n             ModelEditor\\n            </a>\\n            , allowing more options for model input/output types that offers following optimizations:\\n           </dt>\\n           <dd>\\n            <ul>\\n             <li>\\n              <p>\\n               <code class=\"docutils literal notranslate\">\\n                <span class=\"pre\">\\n                 convert_input_type(tensor_name,\\n                </span>\\n                <span class=\"pre\">\\n                 TensorType)\\n                </span>\\n               </code>\\n               method takes a tensor name, removes the corresponding quantize operator, and changes the input type to a given\\n               <code class=\"docutils literal notranslate\">\\n                <span class=\"pre\">\\n                 TensorType\\n                </span>\\n               </code>\\n               .\\n              </p>\\n             </li>\\n             <li>\\n              <p>\\n               <code class=\"docutils literal notranslate\">\\n                <span class=\"pre\">\\n                 convert_output_type(tensor_name,\\n                </span>\\n                <span class=\"pre\">\\n                 TensorType,\\n                </span>\\n                <span class=\"pre\">\\n                 tensor_range)\\n                </span>\\n               </code>\\n               method takes a tensor name, removes the corresponding dequantize operator, and changes the output type to\\n               <code class=\"docutils literal notranslate\">\\n                <span class=\"pre\">\\n                 TensorType\\n                </span>\\n               </code>\\n               , then modifies the scale of the model output to a given\\n               <code class=\"docutils literal notranslate\">\\n                <span class=\"pre\">\\n                 tensor_range\\n                </span>\\n               </code>\\n               .\\n              </p>\\n             </li>\\n            </ul>\\n           </dd>\\n          </dl>\\n         </li>\\n         <li>\\n          <p>\\n           Since the\\n           <code class=\"docutils literal notranslate\">\\n            <span class=\"pre\">\\n             convert_{output,input}_type\\n            </span>\\n           </code>\\n           methods are based on tensor names, users should be able to find tensor names from an original ONNX model.\\n          </p>\\n         </li>\\n        </ul>\\n        <p>\\n         For that,\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           furiosa.quantizer\\n          </span>\\n         </code>\\n         module provides\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           get_pure_input_names(ModelProto)\\n          </span>\\n         </code>\\n         and\\n         <code class=\"docutils literal notranslate\">\\n          <span class=\"pre\">\\n           get_output_names(ModelProto)\\n          </span>\\n         </code>\\n         functions to retrieve tensor names from the original ONNX model.\\n        </p>\\n        <div class=\"admonition note\">\\n         <p class=\"admonition-title\">\\n          Note\\n         </p>\\n         <p>\\n          The removal of\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            with_quantize\\n           </span>\\n          </code>\\n          ,\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            normalized_pixel_outputs\\n           </span>\\n          </code>\\n          parameters from\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            furiosa.quantizer.quantize()\\n           </span>\\n          </code>\\n          is a breaking change that requires modifying existing code.\\n         </p>\\n        </div>\\n        <p>\\n         Please refer to\\n         <a class=\"reference external\" href=\"https://furiosa-ai.github.io/docs/v0.10.0/en/api/python/furiosa.quantizer.html#furiosa.quantizer.ModelEditor\">\\n          ModelEditor\\n         </a>\\n         to learn more about the ModelEditor API and find examples from\\n         <a class=\"reference internal\" href=\"../software/tutorials.html#tutorial\">\\n          <span class=\"std std-ref\">\\n           Tutorial and Code Examples\\n          </span>\\n         </a>\\n         .\\n        </p>\\n       </section>\\n      </section>\\n      <section id=\"compiler\">\\n       <h3>\\n        Compiler\\n        <a class=\"headerlink\" href=\"#compiler\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Since this release, the compiler supports NPU acceleration for the\\n        <cite>\\n         Dequantize\\n        </cite>\\n        operator.\\nSo, the latency or throughput of models that include\\n        <cite>\\n         Dequantize\\n        </cite>\\n        operators can be enhanced.\\nMore details of this performance optimization can be found from\\n        <a class=\"reference internal\" href=\"../software/performance.html#performanceoptimization\">\\n         <span class=\"std std-ref\">\\n          Performance Optimization\\n         </span>\\n        </a>\\n        .\\n       </p>\\n       <p>\\n        Since 0.10.0, the default lifetime of compiler cache has increased from 2 days to 30 days.\\nPlease refer to\\n        <a class=\"reference internal\" href=\"../software/compiler.html#compilercache\">\\n         <span class=\"std std-ref\">\\n          Compiler Cache\\n         </span>\\n        </a>\\n        to learn the details of compiler cache feature.\\n       </p>\\n       <p>\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-compiler\\n         </span>\\n        </code>\\n        command in 0.10.0 release also has the following improvements:\\n       </p>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          Add\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            furiosa-compiler\\n           </span>\\n          </code>\\n          command in addition to\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            furiosa-compile\\n           </span>\\n          </code>\\n          command\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            furiosa-compiler\\n           </span>\\n          </code>\\n          and\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            furiosa-compile\\n           </span>\\n          </code>\\n          commands as a native executable\\nand do not require any Python runtime environment.\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            furiosa-compiler\\n           </span>\\n          </code>\\n          is now available as an APT package, you can install via\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            apt\\n           </span>\\n           <span class=\"pre\">\\n            install\\n           </span>\\n           <span class=\"pre\">\\n            furiosa-compiler\\n           </span>\\n          </code>\\n          .\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            furiosa\\n           </span>\\n           <span class=\"pre\">\\n            compile\\n           </span>\\n          </code>\\n          is kept for backward compatibility, and it will be removed in a future release.\\n         </p>\\n        </li>\\n       </ul>\\n       <p>\\n        Please visit\\n        <a class=\"reference internal\" href=\"../software/compiler.html#compilercli\">\\n         <span class=\"std std-ref\">\\n          furiosa-compiler\\n         </span>\\n        </a>\\n        to learn more about\\n        <cite>\\n         furiosa-compiler\\n        </cite>\\n        command.\\n       </p>\\n      </section>\\n      <section id=\"performance-profiler\">\\n       <h3>\\n        Performance Profiler\\n        <a class=\"headerlink\" href=\"#performance-profiler\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        The performance profiler is a tool that helps users to analyze performance by measuring\\nthe actual execution time of inferences.\\nSince 0.10.0,\\n        <a class=\"reference internal\" href=\"../software/profiler.html#profilerenabledbycontext\">\\n         <span class=\"std std-ref\">\\n          Tracing via Profiler Context\\n         </span>\\n        </a>\\n        API provides the pause/resume features.\\n       </p>\\n       <p>\\n        This feature allows users to skip unnecessary steps like pre/post processing or warming up times,\\nleading to the reduction of the profiling overhead and the size of the profile result files.\\nLiterally, calling\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          profile.pause()\\n         </span>\\n        </code>\\n        method immediately stops the profliling, and\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          profile.resume()\\n         </span>\\n        </code>\\n        resumes the profiling again.\\nThe profiler will not collect any profiling information between both method calls.\\nPlease refer to\\n        <a class=\"reference internal\" href=\"../software/profiler.html#temporarilydisablingprofiler\">\\n         <span class=\"std std-ref\">\\n          Pause/Resume of Profiler Context\\n         </span>\\n        </a>\\n        to learn more about the profiling API.\\n       </p>\\n      </section>\\n      <section id=\"furiosa-litmus\">\\n       <h3>\\n        furiosa-litmus\\n        <a class=\"headerlink\" href=\"#furiosa-litmus\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-litmus\\n         </span>\\n        </code>\\n        is a command-line tool that checks the compatibility of models\\nwith the NPU and Furiosa SDK. Since 0.10.0,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-litmus\\n         </span>\\n        </code>\\n        has a new feature to collect\\nlogs, profiling information, and an environment information for error reporting.\\nThis feature is enabled if\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          --dump\\n         </span>\\n         <span class=\"pre\">\\n          &lt;OUTPUT_PREFIX&gt;\\n         </span>\\n        </code>\\n        option is specified.\\nThe collected data is saved into a zip file named\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          &lt;OUTPUT_PREFIX&gt;-&lt;unix_epoch&gt;.zip\\n         </span>\\n        </code>\\n        .\\n       </p>\\n       <div class=\"highlight-sh notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>$<span class=\"w\"> </span>furiosa-litmus<span class=\"w\"> </span>&lt;MODEL_PATH&gt;<span class=\"w\"> </span>--dump<span class=\"w\"> </span>&lt;OUTPUT_PREFIX&gt;\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        The collected information does not include the model itself\\nbut does contain only metadata of the model, memory usage, and environmental information\\n(e.g., Python version, SDK, compiler version, and dependency library versions).\\nYou can directly unzip the zip file to check the contents.\\nWhen reporting bugs, attaching this file will be very helpful for error dianosis and analysis.\\n       </p>\\n      </section>\\n      <section id=\"new-benchmark-tool-furiosa-bench\">\\n       <h3>\\n        New Benchmark Tool ‘furiosa-bench’\\n        <a class=\"headerlink\" href=\"#new-benchmark-tool-furiosa-bench\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        The new benchmark tool,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-bench\\n         </span>\\n        </code>\\n        , has been added since 0.10.0.\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-bench\\n         </span>\\n        </code>\\n        command offers various options to run a diverse workloads with certain runtime settings.\\nUsers can choose either latency-oriented or throughput-oriented workload, and can specify\\nthe number of devices, how long time to run, and runtime settings.\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-bench\\n         </span>\\n        </code>\\n        accepts\\nboth ONNX and Tflite models as well as an ENF file compiled by the furiosa-compiler.\\nMore details about the command can be found at\\n        <a class=\"reference internal\" href=\"../software/cli.html#furiosabench\">\\n         <span class=\"std std-ref\">\\n          furiosa-bench (Benchmark Tool)\\n         </span>\\n        </a>\\n        .\\n       </p>\\n       <p>\\n        An example of a throughput benchmark\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>$ furiosa-bench ./model.onnx --workload throughput -n 10000 --devices \"warboy(1)*2\" --workers 8 --batch 8\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        An example of a latency benchmark\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>$ furiosa-bench ./model.onnx --workload latency -n 10000 --devices \"warboy(2)*1\"\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-bench\\n         </span>\\n        </code>\\n        can be installed through apt package manager as follows:\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>$ apt install furiosa-bench\\n</pre>\\n        </div>\\n       </div>\\n      </section>\\n      <section id=\"furiosa-toolkit\">\\n       <h3>\\n        furiosa-toolkit\\n        <a class=\"headerlink\" href=\"#furiosa-toolkit\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        furiosa-toolkit is a collection of command line tools that provide NPU management and NPU device\\nmonitoring. Since 0.10.0,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-toolkit\\n         </span>\\n        </code>\\n        includes the following improvements:\\n       </p>\\n       <p>\\n        <strong>\\n         Improvements of furiosactl\\n        </strong>\\n       </p>\\n       <p>\\n        Before 0.10.0, the sub-commands like\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          list\\n         </span>\\n        </code>\\n        ,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          info\\n         </span>\\n        </code>\\n        print out a tabular text.\\nSince 0.10.0,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosactl\\n         </span>\\n        </code>\\n        newly provides\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          --format\\n         </span>\\n        </code>\\n        option, allowing to\\nprint out the result in a structured format like\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          json\\n         </span>\\n        </code>\\n        or\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          yaml\\n         </span>\\n        </code>\\n        .\\nIt will be useful when a users implements a shell pipeline or a script to process\\nthe output of\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosactl\\n         </span>\\n        </code>\\n        .\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span>$ furiosactl info --format json\\n[{\"dev_name\":\"npu7\",\"product_name\":\"warboy\",\"device_uuid\":\"&lt;device_uuid&gt;\",\"device_sn\":\"&lt;device_sn&gt;\",\"firmware\":\"1.6.0, 7a3b908\",\"temperature\":\"47°C\",\"power\":\"0.99 W\",\"pci_bdf\":\"0000:d6:00.0\",\"pci_dev\":\"492:0\"}]\\n\\n$ furiosactl info --format yaml\\n- dev_name: npu7\\n  product_name: warboy\\n  device_uuid: &lt;device_uuid&gt;\\n  device_sn: &lt;device_sn&gt;\\n  firmware: 1.6.0, 7a3b908\\n  temperature: 47°C\\n  power: 0.98 W\\n  pci_bdf: 0000:d6:00.0\\n  pci_dev: 492:0\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        Also, the subcommand\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          info\\n         </span>\\n        </code>\\n        results in two more metrics:\\n       </p>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          NPU Clock Frequency\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Entire power consumption of card\\n         </p>\\n        </li>\\n       </ul>\\n       <p>\\n        <strong>\\n         Improvements of furiosa-npu-metrics-exporter\\n        </strong>\\n       </p>\\n       <p>\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-npu-metrics-exporter\\n         </span>\\n        </code>\\n        is a HTTP server to export NPU metrics and status\\nin\\n        <a class=\"reference external\" href=\"https://github.com/OpenObservability/OpenMetrics/blob/main/specification/OpenMetrics.md\">\\n         OpenMetrics\\n        </a>\\n        format.\\nThe metrics that\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-npu-metrics-exporter\\n         </span>\\n        </code>\\n        exports can be collected by Prometheus and other OpenMetrics compatible collectors.\\n       </p>\\n       <p>\\n        Since 0.10.0,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-npu-metrics-exporter\\n         </span>\\n        </code>\\n        includes NPU clock frequency and NPU utilization\\nas metrics. NPU utilziation is still an experimental feature, and it is disabled by default.\\nTo enable this feature, you need to specify\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          --enable-npu-utilization\\n         </span>\\n        </code>\\n        option as follows:\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"n\">furiosa</span><span class=\"o\">-</span><span class=\"n\">npu</span><span class=\"o\">-</span><span class=\"n\">metrics</span><span class=\"o\">-</span><span class=\"n\">exporter</span> <span class=\"o\">--</span><span class=\"n\">enable</span><span class=\"o\">-</span><span class=\"n\">npu</span><span class=\"o\">-</span><span class=\"n\">utilization</span>\\n</pre>\\n        </div>\\n       </div>\\n       <p>\\n        Additionally,\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          furiosa-npu-metrics-exporter\\n         </span>\\n        </code>\\n        is now available as an APT package\\nin addition to the docker image. You can install it as follows:\\n       </p>\\n       <div class=\"highlight-default notranslate\">\\n        <div class=\"highlight\">\\n         <pre><span></span><span class=\"n\">apt</span> <span class=\"n\">install</span> <span class=\"n\">furiosa</span><span class=\"o\">-</span><span class=\"n\">toolkit</span>\\n</pre>\\n        </div>\\n       </div>\\n      </section>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"../api/python/furiosa.serving.processors.html\" rel=\"prev\" title=\"furiosa.serving.processors package\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"0.9.0.html\" rel=\"next\" title=\"Release Notes - 0.9.0\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='180d17f3-caa8-46a6-87f5-74a1c03de3a5', link=Url('https://furiosa-ai.github.io/docs/latest/en/'), name='', parent='', child=[], description='\\n\\n\\n* FuriosaAI NPU & SDK 0.10.1 Documents\\n* [View page source](_sources/index.rst.txt)\\n\\n---\\n\\n\\n\\nFuriosaAI NPU & SDK 0.10.1 Documents\\n[\\uf0c1](#furiosaai-npu-sdk-release-documents \"Permalink to this heading\")\\n==========================================================================================================\\n\\nThis document explains FuriosaAI NPU and its SDKs.\\n\\nNote\\n\\nFuriosaAI software components include kernel driver, firmware, runtime, C SDK, Python SDK,\\nand command lines tools. Currently, we offer them for only users who register\\n*Early\\nAccess Program (EAP)*\\nand agree to\\n*End User Licence Agreement (EULA)*\\n.\\nPlease contact\\n[contact\\n@\\n\\nfuriosa\\n.\\n\\nai](mailto:contact%40furiosa.ai)\\nto learn how to start the EAP.\\n\\n\\nFuriosaAI NPU\\n[\\uf0c1](#furiosaai-npu \"Permalink to this heading\")\\n-------------------------------------------------------------\\n\\n* [Introduction to FuriosaAI Warboy](npu/warboy.html)\\n  : HW specification, performance, and supported operators\\n\\nFuriosaAI Software\\n[\\uf0c1](#furiosaai-software \"Permalink to this heading\")\\n-----------------------------------------------------------------------\\n\\n* [FuriosaAI SW Stack Introduction](software/intro.html)\\n* [Driver, Firmware, and Runtime Installation](software/installation.html)\\n* [Python SDK installation and user guide](software/python-sdk.html)\\n* [C SDK installation and user guide](software/c-sdk.html)\\n* [Command Line Tools](software/cli.html)\\n* [Compiler](software/compiler.html)\\n* [Model Quantization](software/quantization.html)\\n* [FuriosaAI Model Zoo](https://furiosa-ai.github.io/furiosa-models/latest/)\\n* [Kubernetes Support](software/kubernetes_support.html)\\n* [Configuring Warboy Pass-through for Virtual Machine](software/vm_support.html)\\n\\n### FuriosaAI SDK Tutorial and Examples [\\uf0c1](#furiosaai-sdk-tutorial-and-examples \"Permalink to this heading\")\\n\\n* [Tutorial: How to use Furiosa SDK from Start to Finish](https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/HowToUseFuriosaSDKFromStartToFinish.ipynb)\\n* [Tutorial: Basic Inference API](https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/GettingStartedWithPythonSDK.ipynb)\\n* [Tutorial: Advanced Inference API](https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/AdvancedTopicsInInferenceAPIs.ipynb)\\n* [Example: Comparing Accuracy with CPU-based Inference](https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/InferenceAccuracyCheck.ipynb)\\n* [Example: Image Classification Inference](https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/Image_Classification.ipynb)\\n* [Example: SSD Object Detection Inference](https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/SSD_Object_Detection.ipynb)\\n* [Other Python SDK Examples](https://github.com/furiosa-ai/furiosa-sdk/tree/main/examples/inferences)\\n\\n### Serving, Model Deployment, MLOps [\\uf0c1](#serving-model-deployment-mlops \"Permalink to this heading\")\\n\\n* [Model Server (Serving Framework)](software/serving.html)\\n* [Kubernetes Support](software/kubernetes_support.html)\\n\\n\\nReferences\\n[\\uf0c1](#references \"Permalink to this heading\")\\n-------------------------------------------------------\\n\\n* [C Language SDK Reference](https://furiosa-ai.github.io/docs/v0.9.0/en/api/c/index.html)\\n* [Python SDK Reference](api/python/modules.html)\\n\\nOther Links\\n[\\uf0c1](#other-links \"Permalink to this heading\")\\n---------------------------------------------------------\\n\\n* [FuriosaAI Home](https://furiosa.ai)\\n* [FuriosaAI Customer Support Center](https://furiosa-ai.atlassian.net/servicedesk/customer/portals/)\\n* [Bug Report](customer-support/bugs.html#bugreport)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Next](npu/warboy.html \"FuriosaAI Warboy\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* FuriosaAI NPU & SDK 0.10.1 Documents * [View page source](_sources/index.rst.txt)\\n---\\nFuriosaAI NPU & SDK 0.10.1 Documents [\\uf0c1](#furiosaai-npu-sdk-release-documents \"Permalink to this heading\") ==========================================================================================================\\nThis document explains FuriosaAI NPU and its SDKs.\\nNote\\nFuriosaAI software components include kernel driver, firmware, runtime, C SDK, Python SDK, and command lines tools. Currently, we offer them for only users who register *Early Access Program (EAP)* and agree to *End User Licence Agreement (EULA)* .\\nPlease contact [contact @\\nfuriosa .\\nai](mailto:contact%40furiosa.ai) to learn how to start the EAP.\\nFuriosaAI NPU [\\uf0c1](#furiosaai-npu \"Permalink to this heading\") -------------------------------------------------------------\\n* [Introduction to FuriosaAI Warboy](npu/warboy.html)   : HW specification, performance, and supported operators\\nFuriosaAI Software [\\uf0c1](#furiosaai-software \"Permalink to this heading\") -----------------------------------------------------------------------\\n* [FuriosaAI SW Stack Introduction](software/intro.html) * [Driver, Firmware, and Runtime Installation](software/installation.html) * [Python SDK installation and user guide](software/python-sdk.html) * [C SDK installation and user guide](software/c-sdk.html) * [Command Line Tools](software/cli.html) * [Compiler](software/compiler.html) * [Model Quantization](software/quantization.html) * [FuriosaAI Model Zoo](https://furiosa-ai.github.io/furiosa-models/latest/) * [Kubernetes Support](software/kubernetes_support.html) * [Configuring Warboy Pass-through for Virtual Machine](software/vm_support.html)\\n### FuriosaAI SDK Tutorial and Examples [\\uf0c1](#furiosaai-sdk-tutorial-and-examples \"Permalink to this heading\")\\n* [Tutorial: How to use Furiosa SDK from Start to Finish](https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/HowToUseFuriosaSDKFromStartToFinish.ipynb) * [Tutorial: Basic Inference API](https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/GettingStartedWithPythonSDK.ipynb) * [Tutorial: Advanced Inference API](https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/AdvancedTopicsInInferenceAPIs.ipynb) * [Example: Comparing Accuracy with CPU-based Inference](https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/InferenceAccuracyCheck.ipynb) * [Example: Image Classification Inference](https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/Image_Classification.ipynb) * [Example: SSD Object Detection Inference](https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/SSD_Object_Detection.ipynb) * [Other Python SDK Examples](https://github.com/furiosa-ai/furiosa-sdk/tree/main/examples/inferences)\\n### Serving, Model Deployment, MLOps [\\uf0c1](#serving-model-deployment-mlops \"Permalink to this heading\")\\n* [Model Server (Serving Framework)](software/serving.html) * [Kubernetes Support](software/kubernetes_support.html)\\nReferences [\\uf0c1](#references \"Permalink to this heading\") -------------------------------------------------------\\n* [C Language SDK Reference](https://furiosa-ai.github.io/docs/v0.9.0/en/api/c/index.html) * [Python SDK Reference](api/python/modules.html)\\nOther Links [\\uf0c1](#other-links \"Permalink to this heading\") ---------------------------------------------------------\\n* [FuriosaAI Home](https://furiosa.ai) * [FuriosaAI Customer Support Center](https://furiosa-ai.atlassian.net/servicedesk/customer/portals/) * [Bug Report](customer-support/bugs.html#bugreport)\\n[Next](npu/warboy.html \"FuriosaAI Warboy\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"#\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     FuriosaAI NPU &amp; SDK 0.10.1 Documents\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"_sources/index.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"furiosaai-npu-sdk-release-documents\">\\n     <h1>\\n      FuriosaAI NPU &amp; SDK 0.10.1 Documents\\n      <a class=\"headerlink\" href=\"#furiosaai-npu-sdk-release-documents\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      This document explains FuriosaAI NPU and its SDKs.\\n     </p>\\n     <div class=\"admonition note\">\\n      <p class=\"admonition-title\">\\n       Note\\n      </p>\\n      <p>\\n       FuriosaAI software components include kernel driver, firmware, runtime, C SDK, Python SDK,\\nand command lines tools. Currently, we offer them for only users who register\\n       <em>\\n        Early\\nAccess Program (EAP)\\n       </em>\\n       and agree to\\n       <em>\\n        End User Licence Agreement (EULA)\\n       </em>\\n       .\\nPlease contact\\n       <a class=\"reference external\" href=\"mailto:contact%40furiosa.ai\">\\n        contact\\n        <span>\\n         @\\n        </span>\\n        furiosa\\n        <span>\\n         .\\n        </span>\\n        ai\\n       </a>\\n       to learn how to start the EAP.\\n      </p>\\n     </div>\\n     <section id=\"furiosaai-npu\">\\n      <h2>\\n       FuriosaAI NPU\\n       <a class=\"headerlink\" href=\"#furiosaai-npu\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"npu/warboy.html\">\\n          <span class=\"doc\">\\n           Introduction to FuriosaAI Warboy\\n          </span>\\n         </a>\\n         : HW specification, performance, and supported operators\\n        </p>\\n       </li>\\n      </ul>\\n     </section>\\n     <section id=\"furiosaai-software\">\\n      <h2>\\n       FuriosaAI Software\\n       <a class=\"headerlink\" href=\"#furiosaai-software\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"software/intro.html\">\\n          <span class=\"doc\">\\n           FuriosaAI SW Stack Introduction\\n          </span>\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"software/installation.html\">\\n          <span class=\"doc\">\\n           Driver, Firmware, and Runtime Installation\\n          </span>\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"software/python-sdk.html\">\\n          <span class=\"doc\">\\n           Python SDK installation and user guide\\n          </span>\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"software/c-sdk.html\">\\n          <span class=\"doc\">\\n           C SDK installation and user guide\\n          </span>\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"software/cli.html\">\\n          <span class=\"doc\">\\n           Command Line Tools\\n          </span>\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"software/compiler.html\">\\n          <span class=\"doc\">\\n           Compiler\\n          </span>\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"software/quantization.html\">\\n          <span class=\"doc\">\\n           Model Quantization\\n          </span>\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference external\" href=\"https://furiosa-ai.github.io/furiosa-models/latest/\">\\n          FuriosaAI Model Zoo\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"software/kubernetes_support.html\">\\n          <span class=\"doc\">\\n           Kubernetes Support\\n          </span>\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"software/vm_support.html\">\\n          <span class=\"doc\">\\n           Configuring Warboy Pass-through for Virtual Machine\\n          </span>\\n         </a>\\n        </p>\\n       </li>\\n      </ul>\\n      <section id=\"furiosaai-sdk-tutorial-and-examples\">\\n       <h3>\\n        FuriosaAI SDK Tutorial and Examples\\n        <a class=\"headerlink\" href=\"#furiosaai-sdk-tutorial-and-examples\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/HowToUseFuriosaSDKFromStartToFinish.ipynb\">\\n           Tutorial: How to use Furiosa SDK from Start to Finish\\n          </a>\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/GettingStartedWithPythonSDK.ipynb\">\\n           Tutorial: Basic Inference API\\n          </a>\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/AdvancedTopicsInInferenceAPIs.ipynb\">\\n           Tutorial: Advanced Inference API\\n          </a>\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/InferenceAccuracyCheck.ipynb\">\\n           Example: Comparing Accuracy with CPU-based Inference\\n          </a>\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/Image_Classification.ipynb\">\\n           Example: Image Classification Inference\\n          </a>\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/blob/main/examples/notebooks/SSD_Object_Detection.ipynb\">\\n           Example: SSD Object Detection Inference\\n          </a>\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          <a class=\"reference external\" href=\"https://github.com/furiosa-ai/furiosa-sdk/tree/main/examples/inferences\">\\n           Other Python SDK Examples\\n          </a>\\n         </p>\\n        </li>\\n       </ul>\\n      </section>\\n      <section id=\"serving-model-deployment-mlops\">\\n       <h3>\\n        Serving, Model Deployment, MLOps\\n        <a class=\"headerlink\" href=\"#serving-model-deployment-mlops\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          <a class=\"reference internal\" href=\"software/serving.html\">\\n           <span class=\"doc\">\\n            Model Server (Serving Framework)\\n           </span>\\n          </a>\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          <a class=\"reference internal\" href=\"software/kubernetes_support.html\">\\n           <span class=\"doc\">\\n            Kubernetes Support\\n           </span>\\n          </a>\\n         </p>\\n        </li>\\n       </ul>\\n      </section>\\n     </section>\\n     <section id=\"references\">\\n      <h2>\\n       References\\n       <a class=\"headerlink\" href=\"#references\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         <a class=\"reference external\" href=\"https://furiosa-ai.github.io/docs/v0.9.0/en/api/c/index.html\">\\n          C Language SDK Reference\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"api/python/modules.html\">\\n          <span class=\"doc\">\\n           Python SDK Reference\\n          </span>\\n         </a>\\n        </p>\\n       </li>\\n      </ul>\\n     </section>\\n     <section id=\"other-links\">\\n      <h2>\\n       Other Links\\n       <a class=\"headerlink\" href=\"#other-links\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <ul class=\"simple\">\\n       <li>\\n        <p>\\n         <a class=\"reference external\" href=\"https://furiosa.ai\">\\n          FuriosaAI Home\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference external\" href=\"https://furiosa-ai.atlassian.net/servicedesk/customer/portals/\">\\n          FuriosaAI Customer Support Center\\n         </a>\\n        </p>\\n       </li>\\n       <li>\\n        <p>\\n         <a class=\"reference internal\" href=\"customer-support/bugs.html#bugreport\">\\n          <span class=\"std std-ref\">\\n           Bug Report\\n          </span>\\n         </a>\\n        </p>\\n       </li>\\n      </ul>\\n      <div class=\"toctree-wrapper compound\">\\n      </div>\\n      <div class=\"toctree-wrapper compound\">\\n      </div>\\n      <div class=\"toctree-wrapper compound\">\\n      </div>\\n      <div class=\"toctree-wrapper compound\">\\n      </div>\\n      <div class=\"toctree-wrapper compound\">\\n      </div>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"npu/warboy.html\" rel=\"next\" title=\"FuriosaAI Warboy\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='f6a8c152-6f4c-4ac0-98ae-fbe3093522c6', link=Url('https://furiosa-ai.github.io/docs/latest/en/customer-support/bugs.html'), name='bugs', parent='', child=[], description='\\n\\n\\n* Bug Report\\n* [View page source](../_sources/customer-support/bugs.rst.txt)\\n\\n---\\n\\n\\n\\nBug Report\\n[\\uf0c1](#bug-report \"Permalink to this heading\")\\n=======================================================\\n\\nIf you encounter an unresolvable issue, you can file a bug report at\\n[FuriosaAI customer service center](https://furiosa-ai.atlassian.net/servicedesk/customer/portals)\\n.\\nThe following information should be included in a bug report.\\n\\n1. How to reproduce the bug\\n2. Log or screenshot of the bug\\n3. SDK version information\\n4. Compilation log, if model compilation failed\\n\\nBy default, when an error happens furiosa-sdk outputs the following message.\\nIf you see the following message, file a report\\nto the\\nBug Report\\n\\nsection of\\n[FuriosaAI customer service center](https://furiosa-ai.atlassian.net/servicedesk/customer/portals)\\nwith:\\n\\n1. The information given below the\\n   `Information\\n   \\n   Dump`\\n   , and\\n2. The compilation log file (this would be\\n   `/home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log`\\n   in the following example) outputted in the message.\\n\\n```\\nSaving the compilation log into /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log\\nUsing furiosa-compiler 0.5.0 (rev: 407c0c51f-modified built at 2021-11-18 22:32:34)\\n2021-11-22T06:30:28.392114Z  INFO Npu (npu0pe0) is being initialized\\n2021-11-22T06:30:28.397757Z  INFO NuxInner create with pes: [PeId(0)]\\n[1/6] 🔍   Compiling from onnx to dfg\\n2021-11-22T06:30:28.423026Z  INFO [Profiler] Received a termination signal.\\n2021-11-22T06:30:28.423371Z ERROR fail to compile the model: the static shape of tensor \\'input\\' contains an unsupported dimension value: Some(DimParam(\"batch_size\"))\\n================================================================================\\nInformation Dump\\n================================================================================\\n- Python version: 3.8.10 (default, Sep 28 2021, 16:10:42)  [GCC 9.3.0]\\n- furiosa-libnux path: libnux.so.0.5.0\\n- furiosa-libnux version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34)\\n- furiosa-compiler version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34)\\n- furiosa-sdk-runtime version: Furiosa SDK Runtime  (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\\n\\nPlease check the compiler log at /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log.\\nIf you have a problem, please report the log file to https://furiosa-ai.atlassian.net/servicedesk/customer/portals with the information dumped above.\\n================================================================================\\n\\n```\\n\\nIf you do not see a message as shown above, refer to the instructions below to collect the necessary information yourself\\nto file a bug report at\\n[FuriosaAI customer service center](https://furiosa-ai.atlassian.net/servicedesk/customer/portals)\\n.\\n\\nYou can find the Python runtime version information as shown.\\n\\n```\\n$ python --version\\nPython 3.8.6\\n\\n```\\n\\nYou can find the SDK version information as shown.\\n\\n```\\n$ python -c \"from furiosa import runtime;print(runtime.__full_version__)\"\\nloaded native library /usr/lib64/libnux.so (0.5.0 407c0c51f)\\nFuriosa SDK Runtime 0.5.0 (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\\n\\n```\\n\\n\\n\\n\\n\\n[Previous](../releases/0.5.0.html \"Release Notes - 0.5.0\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Bug Report * [View page source](../_sources/customer-support/bugs.rst.txt)\\n---\\nBug Report [\\uf0c1](#bug-report \"Permalink to this heading\") =======================================================\\nIf you encounter an unresolvable issue, you can file a bug report at [FuriosaAI customer service center](https://furiosa-ai.atlassian.net/servicedesk/customer/portals) .\\nThe following information should be included in a bug report.\\n1. How to reproduce the bug 2. Log or screenshot of the bug 3. SDK version information 4. Compilation log, if model compilation failed\\nBy default, when an error happens furiosa-sdk outputs the following message. If you see the following message, file a report to the Bug Report\\nsection of [FuriosaAI customer service center](https://furiosa-ai.atlassian.net/servicedesk/customer/portals) with:\\n1. The information given below the    `Information        Dump`    , and 2. The compilation log file (this would be    `/home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log`    in the following example) outputted in the message.\\n``` Saving the compilation log into /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log Using furiosa-compiler 0.5.0 (rev: 407c0c51f-modified built at 2021-11-18 22:32:34) 2021-11-22T06:30:28.392114Z  INFO Npu (npu0pe0) is being initialized 2021-11-22T06:30:28.397757Z  INFO NuxInner create with pes: [PeId(0)] [1/6] 🔍   Compiling from onnx to dfg 2021-11-22T06:30:28.423026Z  INFO [Profiler] Received a termination signal. 2021-11-22T06:30:28.423371Z ERROR fail to compile the model: the static shape of tensor \\'input\\' contains an unsupported dimension value: Some(DimParam(\"batch_size\")) ================================================================================ Information Dump ================================================================================ - Python version: 3.8.10 (default, Sep 28 2021, 16:10:42)  [GCC 9.3.0] - furiosa-libnux path: libnux.so.0.5.0 - furiosa-libnux version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34) - furiosa-compiler version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34) - furiosa-sdk-runtime version: Furiosa SDK Runtime  (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\\nPlease check the compiler log at /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log. If you have a problem, please report the log file to https://furiosa-ai.atlassian.net/servicedesk/customer/portals with the information dumped above. ================================================================================\\n```\\nIf you do not see a message as shown above, refer to the instructions below to collect the necessary information yourself to file a bug report at [FuriosaAI customer service center](https://furiosa-ai.atlassian.net/servicedesk/customer/portals) .\\nYou can find the Python runtime version information as shown.\\n``` $ python --version Python 3.8.6\\n```\\nYou can find the SDK version information as shown.\\n``` $ python -c \"from furiosa import runtime;print(runtime.__full_version__)\" loaded native library /usr/lib64/libnux.so (0.5.0 407c0c51f) Furiosa SDK Runtime 0.5.0 (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\\n```\\n[Previous](../releases/0.5.0.html \"Release Notes - 0.5.0\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Bug Report\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/customer-support/bugs.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"bug-report\">\\n     <span id=\"bugreport\">\\n     </span>\\n     <h1>\\n      Bug Report\\n      <a class=\"headerlink\" href=\"#bug-report\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      If you encounter an unresolvable issue, you can file a bug report at\\n      <a class=\"reference external\" href=\"https://furiosa-ai.atlassian.net/servicedesk/customer/portals\">\\n       FuriosaAI customer service center\\n      </a>\\n      .\\nThe following information should be included in a bug report.\\n     </p>\\n     <ol class=\"arabic simple\">\\n      <li>\\n       <p>\\n        How to reproduce the bug\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Log or screenshot of the bug\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        SDK version information\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        Compilation log, if model compilation failed\\n       </p>\\n      </li>\\n     </ol>\\n     <p>\\n      By default, when an error happens furiosa-sdk outputs the following message.\\nIf you see the following message, file a report\\nto the\\n      <cite>\\n       Bug Report\\n      </cite>\\n      section of\\n      <a class=\"reference external\" href=\"https://furiosa-ai.atlassian.net/servicedesk/customer/portals\">\\n       FuriosaAI customer service center\\n      </a>\\n      with:\\n     </p>\\n     <ol class=\"arabic simple\">\\n      <li>\\n       <p>\\n        The information given below the\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          Information\\n         </span>\\n         <span class=\"pre\">\\n          Dump\\n         </span>\\n        </code>\\n        , and\\n       </p>\\n      </li>\\n      <li>\\n       <p>\\n        The compilation log file (this would be\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log\\n         </span>\\n        </code>\\n        in the following example) outputted in the message.\\n       </p>\\n      </li>\\n     </ol>\\n     <div class=\"highlight-default notranslate\">\\n      <div class=\"highlight\">\\n       <pre><span></span>Saving the compilation log into /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log\\nUsing furiosa-compiler 0.5.0 (rev: 407c0c51f-modified built at 2021-11-18 22:32:34)\\n2021-11-22T06:30:28.392114Z  INFO Npu (npu0pe0) is being initialized\\n2021-11-22T06:30:28.397757Z  INFO NuxInner create with pes: [PeId(0)]\\n[1/6] 🔍   Compiling from onnx to dfg\\n2021-11-22T06:30:28.423026Z  INFO [Profiler] Received a termination signal.\\n2021-11-22T06:30:28.423371Z ERROR fail to compile the model: the static shape of tensor \\'input\\' contains an unsupported dimension value: Some(DimParam(\"batch_size\"))\\n================================================================================\\nInformation Dump\\n================================================================================\\n- Python version: 3.8.10 (default, Sep 28 2021, 16:10:42)  [GCC 9.3.0]\\n- furiosa-libnux path: libnux.so.0.5.0\\n- furiosa-libnux version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34)\\n- furiosa-compiler version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34)\\n- furiosa-sdk-runtime version: Furiosa SDK Runtime  (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\\n\\nPlease check the compiler log at /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log.\\nIf you have a problem, please report the log file to https://furiosa-ai.atlassian.net/servicedesk/customer/portals with the information dumped above.\\n================================================================================\\n</pre>\\n      </div>\\n     </div>\\n     <p>\\n      If you do not see a message as shown above, refer to the instructions below to collect the necessary information yourself\\nto file a bug report at\\n      <a class=\"reference external\" href=\"https://furiosa-ai.atlassian.net/servicedesk/customer/portals\">\\n       FuriosaAI customer service center\\n      </a>\\n      .\\n     </p>\\n     <p>\\n      You can find the Python runtime version information as shown.\\n     </p>\\n     <div class=\"highlight-default notranslate\">\\n      <div class=\"highlight\">\\n       <pre><span></span>$ python --version\\nPython 3.8.6\\n</pre>\\n      </div>\\n     </div>\\n     <p>\\n      You can find the SDK version information as shown.\\n     </p>\\n     <div class=\"highlight-default notranslate\">\\n      <div class=\"highlight\">\\n       <pre><span></span>$ python -c \"from furiosa import runtime;print(runtime.__full_version__)\"\\nloaded native library /usr/lib64/libnux.so (0.5.0 407c0c51f)\\nFuriosa SDK Runtime 0.5.0 (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\\n</pre>\\n      </div>\\n     </div>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"../releases/0.5.0.html\" rel=\"prev\" title=\"Release Notes - 0.5.0\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n'),\n",
       " Page(id='ca538c67-ce79-412f-bd1b-c0f73dd818e7', link=Url('https://furiosa-ai.github.io/docs/latest/en/releases/0.7.0.html'), name='0', parent='', child=[], description='\\n\\n\\n* Release Notes - 0.7.0\\n* [View page source](../_sources/releases/0.7.0.rst.txt)\\n\\n---\\n\\n\\n\\nRelease Notes - 0.7.0\\n[\\uf0c1](#release-notes-0-7-0 \"Permalink to this heading\")\\n===========================================================================\\n\\nFuriosa SDK 0.7.0 is a major release, and includes approximately 1,400 commits towards performance enhancement, added functions, and bug fixes.\\n\\n\\ncomponent version information\\n\\n[\\uf0c1](#id1 \"Permalink to this table\")\\n\\n\\n| Package name | Version |\\n| --- | --- |\\n| NPU Driver | 1.3.0 |\\n| HAL (Hardware Abstraction Layer) | 0.8.0 |\\n| Furiosa Compiler | 0.7.0 |\\n| Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..) | 0.7.0 |\\n| NPU Device Plugin | 0.10.0 |\\n| NPU Feature Discovery | 0.1.0 |\\n| NPU Management CLI (furiosactl) | 0.9.1 |\\n\\nHow to upgrade\\n[\\uf0c1](#how-to-upgrade \"Permalink to this heading\")\\n---------------------------------------------------------------\\n\\nThe upgrade is a simple process if you are using an APT repository.\\nDetailed information on APT repository setting and installation can be found in\\n[Driver, Firmware, and Runtime Installation](../software/installation.html#requiredpackages)\\n.\\n\\n> ```\\n> apt-get update && \\\\\\n> apt-get install -y furiosa-driver-pdma furiosa-libnux\\n> \\n> pip install --upgrade furiosa-sdk\\n> \\n> ```\\n\\n\\nKey changes\\n[\\uf0c1](#key-changes \"Permalink to this heading\")\\n---------------------------------------------------------\\n\\n### Compiler - More NPU acceleration supports [\\uf0c1](#compiler-more-npu-acceleration-supports \"Permalink to this heading\")\\n\\nThrough improvements in the compiler, more operators can be accelerated in various use cases.\\nAccelerated operators with its condition adopted by 0.7.0 release are following.\\nYou can find the entire list of accelerated operators at\\n[List of Supported Operators for Warboy Acceleration](../npu/warboy.html#supportedoperators)\\n.\\n\\n> * Added Linear and Nearest mode support for the Resize operator\\n> * Added DCR mode support for the SpaceToDepth operator\\n> * Added DCR mode support for the DepthToSpace operator\\n> * Added CHW axis support for the Pad operator\\n> * Added C axis support for the Slice operator\\n> * Added acceleration support for operators Tanh, Exp, and Log\\n> * Added C axis support for the Concat operator\\n> * Increased Dilation support to up to x12\\n> * Added acceleration support for operators Gelu, Erf, and Elu\\n\\n\\n### Compiler - Compiler Cache [\\uf0c1](#compiler-compiler-cache \"Permalink to this heading\")\\n\\nCompiler cache stores the compiled binary into a cache directory, and reuses the cache when the same model is compiled.\\nAlso, you can also use Redis as the compiler cache storage.\\nMore detailed instructions can be found in\\n[Compiler Cache](../software/compiler.html#compilercache)\\n.\\n\\n\\n### Compiler - Compiler Hint [\\uf0c1](#compiler-compiler-hint \"Permalink to this heading\")\\n\\nWhen running a function that includes compilation,\\nsuch as\\n`session.create()`\\n, a path that includes the compilation log is printed as follows.\\n\\n> ```\\n> Saving the compilation log into /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log\\n> \\n> ```\\n\\nSince 0.7.0, compilation logs contain compilation hints more helpful to understand the compilation process\\nand give some optimization opportunities.\\n\\nThe\\n`cat\\n\\n<log\\n\\nfile>\\n\\n|\\n\\ngrep\\n\\nHint`\\ncommand will show you only hint from the log.\\nThe hint informs why certain operators are not accelerated as shown in the below example.\\n\\n> ```\\n> cat /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log | grep Hint\\n> 2022-05-24T02:44:11.399402Z  WARN nux::session: Hint [19]: \\'LogSoftmax\\' cannot be accelerated yet\\n> 2022-05-24T02:44:11.399407Z  WARN nux::session: Hint [12]: groups should be bigger than 1\\n> 2022-05-24T02:44:11.399408Z  WARN nux::session: Hint [17]: Softmax with large batch (36 > 2) cannot be accelerated by Warboy\\n> \\n> ```\\n\\n\\n### Performance Profiling Tools [\\uf0c1](#performance-profiling-tools \"Permalink to this heading\")\\n\\nThe profiler had been an experimental and closed-beta feature. The release 0.7.0 includes the performance profiler\\nby default. It allow users to view the time taken in each step in the model inference process.\\nYou can activate the profiler through a shell environment variable or a profiler context in your Python code.\\n\\nMore details can be found in\\n[Performance Profiling](../software/profiler.html#profiling)\\n.\\n\\n\\n### Improvements/Bug fixes of Python SDK [\\uf0c1](#improvements-bug-fixes-of-python-sdk \"Permalink to this heading\")\\n\\n* Since 0.7.0,\\n  `session.create()`\\n  and\\n  `session.create_async()`\\n  can take the batch size.\\n* Fixed a bug that compiler options passed to\\n  `session.create()`\\n  and\\n  `session.create_async()`\\n  wasn’t effective.\\n\\nBelow is an example that uses batch size and compiler option.\\n\\n> ```\\n> config = {\\n>   \"without_quantize\": {\\n>       \"parameters\": [{\"input_min\": 0.0, \"input_max\": 255.0, \"permute\": [0, 2, 3, 1]}]\\n>   }\\n> }\\n> \\n> with session.create(\"model.onnx\", batch_size=2, compile_config=config) as sess:\\n>   outputs = sess.run(inputs)\\n> \\n> ```\\n\\n\\n### Improvements/Bug fixes of Quantization tools [\\uf0c1](#improvements-bug-fixes-of-quantization-tools \"Permalink to this heading\")\\n\\n* You can now infer published tensor shapes even if\\n  axes\\n  \\n  property is not designated in ONNX Squeeze operators below version OpSet 12\\n* Added support not just for Conv receiving tensors with NxCxHxW shapes as input, but also for Conv receiving tensors with NxCxD shapes\\n* Modified “Conv - BatchNormalization” subgraph to be fused to Conv even when Conv does not receive bias as input\\n* Modified to always quantize Sub, Concat, and Pow operators in QDQ format, regardless of whether operands have initial values, so that the model can be processed in a consistent way in the post-quantization process\\n* Modified to prevent ONNX Runtime related warnings in the quantization process and the result model\\n* Reinforced the inspection condition to not miss any cases where tensor shape information cannot be inferred\\n* Modified to allow random calibration not only for models that receive float32 data as inputs, but also for models that receive other decimal or integer types as inputs\\n* Modified to find and terminate in a stable manner when given an already quantized model\\n* Modified to adjust scale of weight appropriately if Conv data input or scale of weight is too small, such that scale of bias becomes 0\\n* Reinforced conditions for “Gather - MatMul” subgraph to be fused into Gather\\n* Dependent libraries updated to latest version\\n\\n### Device Plugin - Configuration file support [\\uf0c1](#device-plugin-configuration-file-support \"Permalink to this heading\")\\n\\nA function to set the execution option of the NPU Device Plugin used in Kubernetes with a file has been added. As before, option items can be entered as command-line arguments, or options can be specified by selecting a configuration file. Detailed instructions can be found in\\n[Kubernetes Support](../software/kubernetes_support.html#kubernetesintegration)\\n.\\n\\n\\n\\n\\n\\n\\n[Previous](0.8.0.html \"Release Notes - 0.8.0\")\\n[Next](0.6.0.html \"Release Notes - 0.6.0\")\\n\\n---\\n\\n\\n© Copyright 2023 FuriosaAI, Inc..\\n\\n\\nBuilt with\\n[Sphinx](https://www.sphinx-doc.org/)\\nusing a\\n[theme](https://github.com/readthedocs/sphinx_rtd_theme)\\nprovided by\\n[Read the Docs](https://readthedocs.org)\\n.\\n\\n\\n\\n', description_clean='* Release Notes - 0.7.0 * [View page source](../_sources/releases/0.7.0.rst.txt)\\n---\\nRelease Notes - 0.7.0 [\\uf0c1](#release-notes-0-7-0 \"Permalink to this heading\") ===========================================================================\\nFuriosa SDK 0.7.0 is a major release, and includes approximately 1,400 commits towards performance enhancement, added functions, and bug fixes.\\ncomponent version information\\n[\\uf0c1](#id1 \"Permalink to this table\")\\n| Package name | Version | | --- | --- | | NPU Driver | 1.3.0 | | HAL (Hardware Abstraction Layer) | 0.8.0 | | Furiosa Compiler | 0.7.0 | | Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..) | 0.7.0 | | NPU Device Plugin | 0.10.0 | | NPU Feature Discovery | 0.1.0 | | NPU Management CLI (furiosactl) | 0.9.1 |\\nHow to upgrade [\\uf0c1](#how-to-upgrade \"Permalink to this heading\") ---------------------------------------------------------------\\nThe upgrade is a simple process if you are using an APT repository. Detailed information on APT repository setting and installation can be found in [Driver, Firmware, and Runtime Installation](../software/installation.html#requiredpackages) .\\n> ``` > apt-get update && \\\\ > apt-get install -y furiosa-driver-pdma furiosa-libnux >  > pip install --upgrade furiosa-sdk >  > ```\\nKey changes [\\uf0c1](#key-changes \"Permalink to this heading\") ---------------------------------------------------------\\n### Compiler - More NPU acceleration supports [\\uf0c1](#compiler-more-npu-acceleration-supports \"Permalink to this heading\")\\nThrough improvements in the compiler, more operators can be accelerated in various use cases. Accelerated operators with its condition adopted by 0.7.0 release are following. You can find the entire list of accelerated operators at [List of Supported Operators for Warboy Acceleration](../npu/warboy.html#supportedoperators) .\\n> * Added Linear and Nearest mode support for the Resize operator > * Added DCR mode support for the SpaceToDepth operator > * Added DCR mode support for the DepthToSpace operator > * Added CHW axis support for the Pad operator > * Added C axis support for the Slice operator > * Added acceleration support for operators Tanh, Exp, and Log > * Added C axis support for the Concat operator > * Increased Dilation support to up to x12 > * Added acceleration support for operators Gelu, Erf, and Elu\\n### Compiler - Compiler Cache [\\uf0c1](#compiler-compiler-cache \"Permalink to this heading\")\\nCompiler cache stores the compiled binary into a cache directory, and reuses the cache when the same model is compiled. Also, you can also use Redis as the compiler cache storage. More detailed instructions can be found in [Compiler Cache](../software/compiler.html#compilercache) .\\n### Compiler - Compiler Hint [\\uf0c1](#compiler-compiler-hint \"Permalink to this heading\")\\nWhen running a function that includes compilation, such as `session.create()` , a path that includes the compilation log is printed as follows.\\n> ``` > Saving the compilation log into /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log >  > ```\\nSince 0.7.0, compilation logs contain compilation hints more helpful to understand the compilation process and give some optimization opportunities.\\nThe `cat\\n<log\\nfile>\\n|\\ngrep\\nHint` command will show you only hint from the log. The hint informs why certain operators are not accelerated as shown in the below example.\\n> ``` > cat /home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log | grep Hint > 2022-05-24T02:44:11.399402Z  WARN nux::session: Hint [19]: \\'LogSoftmax\\' cannot be accelerated yet > 2022-05-24T02:44:11.399407Z  WARN nux::session: Hint [12]: groups should be bigger than 1 > 2022-05-24T02:44:11.399408Z  WARN nux::session: Hint [17]: Softmax with large batch (36 > 2) cannot be accelerated by Warboy >  > ```\\n### Performance Profiling Tools [\\uf0c1](#performance-profiling-tools \"Permalink to this heading\")\\nThe profiler had been an experimental and closed-beta feature. The release 0.7.0 includes the performance profiler by default. It allow users to view the time taken in each step in the model inference process. You can activate the profiler through a shell environment variable or a profiler context in your Python code.\\nMore details can be found in [Performance Profiling](../software/profiler.html#profiling) .\\n### Improvements/Bug fixes of Python SDK [\\uf0c1](#improvements-bug-fixes-of-python-sdk \"Permalink to this heading\")\\n* Since 0.7.0,   `session.create()`   and   `session.create_async()`   can take the batch size. * Fixed a bug that compiler options passed to   `session.create()`   and   `session.create_async()`   wasn’t effective.\\nBelow is an example that uses batch size and compiler option.\\n> ``` > config = { >   \"without_quantize\": { >       \"parameters\": [{\"input_min\": 0.0, \"input_max\": 255.0, \"permute\": [0, 2, 3, 1]}] >   } > } >  > with session.create(\"model.onnx\", batch_size=2, compile_config=config) as sess: >   outputs = sess.run(inputs) >  > ```\\n### Improvements/Bug fixes of Quantization tools [\\uf0c1](#improvements-bug-fixes-of-quantization-tools \"Permalink to this heading\")\\n* You can now infer published tensor shapes even if   axes      property is not designated in ONNX Squeeze operators below version OpSet 12 * Added support not just for Conv receiving tensors with NxCxHxW shapes as input, but also for Conv receiving tensors with NxCxD shapes * Modified “Conv - BatchNormalization” subgraph to be fused to Conv even when Conv does not receive bias as input * Modified to always quantize Sub, Concat, and Pow operators in QDQ format, regardless of whether operands have initial values, so that the model can be processed in a consistent way in the post-quantization process * Modified to prevent ONNX Runtime related warnings in the quantization process and the result model * Reinforced the inspection condition to not miss any cases where tensor shape information cannot be inferred * Modified to allow random calibration not only for models that receive float32 data as inputs, but also for models that receive other decimal or integer types as inputs * Modified to find and terminate in a stable manner when given an already quantized model * Modified to adjust scale of weight appropriately if Conv data input or scale of weight is too small, such that scale of bias becomes 0 * Reinforced conditions for “Gather - MatMul” subgraph to be fused into Gather * Dependent libraries updated to latest version\\n### Device Plugin - Configuration file support [\\uf0c1](#device-plugin-configuration-file-support \"Permalink to this heading\")\\nA function to set the execution option of the NPU Device Plugin used in Kubernetes with a file has been added. As before, option items can be entered as command-line arguments, or options can be specified by selecting a configuration file. Detailed instructions can be found in [Kubernetes Support](../software/kubernetes_support.html#kubernetesintegration) .\\n[Previous](0.8.0.html \"Release Notes - 0.8.0\") [Next](0.6.0.html \"Release Notes - 0.6.0\")\\n---\\n© Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](https://readthedocs.org) .', html_content='<div class=\"wy-nav-content\">\\n <div class=\"rst-content\">\\n  <div aria-label=\"Page navigation\" role=\"navigation\">\\n   <ul class=\"wy-breadcrumbs\">\\n    <li>\\n     <a aria-label=\"Home\" class=\"icon icon-home\" href=\"../index.html\">\\n     </a>\\n    </li>\\n    <li class=\"breadcrumb-item active\">\\n     Release Notes - 0.7.0\\n    </li>\\n    <li class=\"wy-breadcrumbs-aside\">\\n     <a href=\"../_sources/releases/0.7.0.rst.txt\" rel=\"nofollow\">\\n      View page source\\n     </a>\\n    </li>\\n   </ul>\\n   <hr/>\\n  </div>\\n  <div class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\" role=\"main\">\\n   <div itemprop=\"articleBody\">\\n    <section id=\"release-notes-0-7-0\">\\n     <h1>\\n      Release Notes - 0.7.0\\n      <a class=\"headerlink\" href=\"#release-notes-0-7-0\" title=\"Permalink to this heading\">\\n       \\uf0c1\\n      </a>\\n     </h1>\\n     <p>\\n      Furiosa SDK 0.7.0 is a major release, and includes approximately 1,400 commits towards performance enhancement, added functions, and bug fixes.\\n     </p>\\n     <table class=\"docutils align-default\" id=\"id1\">\\n      <caption>\\n       <span class=\"caption-text\">\\n        component version information\\n       </span>\\n       <a class=\"headerlink\" href=\"#id1\" title=\"Permalink to this table\">\\n        \\uf0c1\\n       </a>\\n      </caption>\\n      <colgroup>\\n       <col style=\"width: 80%\"/>\\n       <col style=\"width: 20%\"/>\\n      </colgroup>\\n      <thead>\\n       <tr class=\"row-odd\">\\n        <th class=\"head\">\\n         <p>\\n          Package name\\n         </p>\\n        </th>\\n        <th class=\"head\">\\n         <p>\\n          Version\\n         </p>\\n        </th>\\n       </tr>\\n      </thead>\\n      <tbody>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          NPU Driver\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          1.3.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          HAL (Hardware Abstraction Layer)\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.8.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          Furiosa Compiler\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.7.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..)\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.7.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          NPU Device Plugin\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.10.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-odd\">\\n        <td>\\n         <p>\\n          NPU Feature Discovery\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.1.0\\n         </p>\\n        </td>\\n       </tr>\\n       <tr class=\"row-even\">\\n        <td>\\n         <p>\\n          NPU Management CLI (furiosactl)\\n         </p>\\n        </td>\\n        <td>\\n         <p>\\n          0.9.1\\n         </p>\\n        </td>\\n       </tr>\\n      </tbody>\\n     </table>\\n     <section id=\"how-to-upgrade\">\\n      <h2>\\n       How to upgrade\\n       <a class=\"headerlink\" href=\"#how-to-upgrade\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <p>\\n       The upgrade is a simple process if you are using an APT repository.\\nDetailed information on APT repository setting and installation can be found in\\n       <a class=\"reference internal\" href=\"../software/installation.html#requiredpackages\">\\n        <span class=\"std std-ref\">\\n         Driver, Firmware, and Runtime Installation\\n        </span>\\n       </a>\\n       .\\n      </p>\\n      <blockquote>\\n       <div>\\n        <div class=\"highlight-sh notranslate\">\\n         <div class=\"highlight\">\\n          <pre><span></span>apt-get<span class=\"w\"> </span>update<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span><span class=\"se\">\\\\</span>\\napt-get<span class=\"w\"> </span>install<span class=\"w\"> </span>-y<span class=\"w\"> </span>furiosa-driver-pdma<span class=\"w\"> </span>furiosa-libnux\\n\\npip<span class=\"w\"> </span>install<span class=\"w\"> </span>--upgrade<span class=\"w\"> </span>furiosa-sdk\\n</pre>\\n         </div>\\n        </div>\\n       </div>\\n      </blockquote>\\n     </section>\\n     <section id=\"key-changes\">\\n      <h2>\\n       Key changes\\n       <a class=\"headerlink\" href=\"#key-changes\" title=\"Permalink to this heading\">\\n        \\uf0c1\\n       </a>\\n      </h2>\\n      <section id=\"compiler-more-npu-acceleration-supports\">\\n       <h3>\\n        Compiler - More NPU acceleration supports\\n        <a class=\"headerlink\" href=\"#compiler-more-npu-acceleration-supports\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Through improvements in the compiler, more operators can be accelerated in various use cases.\\nAccelerated operators with its condition adopted by 0.7.0 release are following.\\nYou can find the entire list of accelerated operators at\\n        <a class=\"reference internal\" href=\"../npu/warboy.html#supportedoperators\">\\n         <span class=\"std std-ref\">\\n          List of Supported Operators for Warboy Acceleration\\n         </span>\\n        </a>\\n        .\\n       </p>\\n       <blockquote>\\n        <div>\\n         <ul class=\"simple\">\\n          <li>\\n           <p>\\n            Added Linear and Nearest mode support for the Resize operator\\n           </p>\\n          </li>\\n          <li>\\n           <p>\\n            Added DCR mode support for the SpaceToDepth operator\\n           </p>\\n          </li>\\n          <li>\\n           <p>\\n            Added DCR mode support for the DepthToSpace operator\\n           </p>\\n          </li>\\n          <li>\\n           <p>\\n            Added CHW axis support for the Pad operator\\n           </p>\\n          </li>\\n          <li>\\n           <p>\\n            Added C axis support for the Slice operator\\n           </p>\\n          </li>\\n          <li>\\n           <p>\\n            Added acceleration support for operators Tanh, Exp, and Log\\n           </p>\\n          </li>\\n          <li>\\n           <p>\\n            Added C axis support for the Concat operator\\n           </p>\\n          </li>\\n          <li>\\n           <p>\\n            Increased Dilation support to up to x12\\n           </p>\\n          </li>\\n          <li>\\n           <p>\\n            Added acceleration support for operators Gelu, Erf, and Elu\\n           </p>\\n          </li>\\n         </ul>\\n        </div>\\n       </blockquote>\\n      </section>\\n      <section id=\"compiler-compiler-cache\">\\n       <h3>\\n        Compiler - Compiler Cache\\n        <a class=\"headerlink\" href=\"#compiler-compiler-cache\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        Compiler cache stores the compiled binary into a cache directory, and reuses the cache when the same model is compiled.\\nAlso, you can also use Redis as the compiler cache storage.\\nMore detailed instructions can be found in\\n        <a class=\"reference internal\" href=\"../software/compiler.html#compilercache\">\\n         <span class=\"std std-ref\">\\n          Compiler Cache\\n         </span>\\n        </a>\\n        .\\n       </p>\\n      </section>\\n      <section id=\"compiler-compiler-hint\">\\n       <h3>\\n        Compiler - Compiler Hint\\n        <a class=\"headerlink\" href=\"#compiler-compiler-hint\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        When running a function that includes compilation,\\nsuch as\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          session.create()\\n         </span>\\n        </code>\\n        , a path that includes the compilation log is printed as follows.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-sh notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span>Saving<span class=\"w\"> </span>the<span class=\"w\"> </span>compilation<span class=\"w\"> </span>log<span class=\"w\"> </span>into<span class=\"w\"> </span>/home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n       <p>\\n        Since 0.7.0, compilation logs contain compilation hints more helpful to understand the compilation process\\nand give some optimization opportunities.\\n       </p>\\n       <p>\\n        The\\n        <code class=\"docutils literal notranslate\">\\n         <span class=\"pre\">\\n          cat\\n         </span>\\n         <span class=\"pre\">\\n          &lt;log\\n         </span>\\n         <span class=\"pre\">\\n          file&gt;\\n         </span>\\n         <span class=\"pre\">\\n          |\\n         </span>\\n         <span class=\"pre\">\\n          grep\\n         </span>\\n         <span class=\"pre\">\\n          Hint\\n         </span>\\n        </code>\\n        command will show you only hint from the log.\\nThe hint informs why certain operators are not accelerated as shown in the below example.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-sh notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span>cat<span class=\"w\"> </span>/home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>grep<span class=\"w\"> </span>Hint\\n<span class=\"m\">2022</span>-05-24T02:44:11.399402Z<span class=\"w\">  </span>WARN<span class=\"w\"> </span>nux::session:<span class=\"w\"> </span>Hint<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">19</span><span class=\"o\">]</span>:<span class=\"w\"> </span><span class=\"s1\">\\'LogSoftmax\\'</span><span class=\"w\"> </span>cannot<span class=\"w\"> </span>be<span class=\"w\"> </span>accelerated<span class=\"w\"> </span>yet\\n<span class=\"m\">2022</span>-05-24T02:44:11.399407Z<span class=\"w\">  </span>WARN<span class=\"w\"> </span>nux::session:<span class=\"w\"> </span>Hint<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">12</span><span class=\"o\">]</span>:<span class=\"w\"> </span>groups<span class=\"w\"> </span>should<span class=\"w\"> </span>be<span class=\"w\"> </span>bigger<span class=\"w\"> </span>than<span class=\"w\"> </span><span class=\"m\">1</span>\\n<span class=\"m\">2022</span>-05-24T02:44:11.399408Z<span class=\"w\">  </span>WARN<span class=\"w\"> </span>nux::session:<span class=\"w\"> </span>Hint<span class=\"w\"> </span><span class=\"o\">[</span><span class=\"m\">17</span><span class=\"o\">]</span>:<span class=\"w\"> </span>Softmax<span class=\"w\"> </span>with<span class=\"w\"> </span>large<span class=\"w\"> </span>batch<span class=\"w\"> </span><span class=\"o\">(</span><span class=\"m\">36</span><span class=\"w\"> </span>&gt;<span class=\"w\"> </span><span class=\"m\">2</span><span class=\"o\">)</span><span class=\"w\"> </span>cannot<span class=\"w\"> </span>be<span class=\"w\"> </span>accelerated<span class=\"w\"> </span>by<span class=\"w\"> </span>Warboy\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n      </section>\\n      <section id=\"performance-profiling-tools\">\\n       <h3>\\n        Performance Profiling Tools\\n        <a class=\"headerlink\" href=\"#performance-profiling-tools\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        The profiler had been an experimental and closed-beta feature. The release 0.7.0 includes the performance profiler\\nby default. It allow users to view the time taken in each step in the model inference process.\\nYou can activate the profiler through a shell environment variable or a profiler context in your Python code.\\n       </p>\\n       <p>\\n        More details can be found in\\n        <a class=\"reference internal\" href=\"../software/profiler.html#profiling\">\\n         <span class=\"std std-ref\">\\n          Performance Profiling\\n         </span>\\n        </a>\\n        .\\n       </p>\\n       <a class=\"with-shadow reference internal image-reference\" href=\"../_images/tracing.png\">\\n        <img alt=\"Tracing\" class=\"with-shadow align-center\" src=\"../_images/tracing.png\" style=\"width: 600px;\"/>\\n       </a>\\n       <p>\\n       </p>\\n      </section>\\n      <section id=\"improvements-bug-fixes-of-python-sdk\">\\n       <h3>\\n        Improvements/Bug fixes of Python SDK\\n        <a class=\"headerlink\" href=\"#improvements-bug-fixes-of-python-sdk\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          Since 0.7.0,\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            session.create()\\n           </span>\\n          </code>\\n          and\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            session.create_async()\\n           </span>\\n          </code>\\n          can take the batch size.\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Fixed a bug that compiler options passed to\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            session.create()\\n           </span>\\n          </code>\\n          and\\n          <code class=\"docutils literal notranslate\">\\n           <span class=\"pre\">\\n            session.create_async()\\n           </span>\\n          </code>\\n          wasn’t effective.\\n         </p>\\n        </li>\\n       </ul>\\n       <p>\\n        Below is an example that uses batch size and compiler option.\\n       </p>\\n       <blockquote>\\n        <div>\\n         <div class=\"highlight-python notranslate\">\\n          <div class=\"highlight\">\\n           <pre><span></span><span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\\n  <span class=\"s2\">\"without_quantize\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\\n      <span class=\"s2\">\"parameters\"</span><span class=\"p\">:</span> <span class=\"p\">[{</span><span class=\"s2\">\"input_min\"</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"s2\">\"input_max\"</span><span class=\"p\">:</span> <span class=\"mf\">255.0</span><span class=\"p\">,</span> <span class=\"s2\">\"permute\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]}]</span>\\n  <span class=\"p\">}</span>\\n<span class=\"p\">}</span>\\n\\n<span class=\"k\">with</span> <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"s2\">\"model.onnx\"</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">compile_config</span><span class=\"o\">=</span><span class=\"n\">config</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">sess</span><span class=\"p\">:</span>\\n  <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\\n</pre>\\n          </div>\\n         </div>\\n        </div>\\n       </blockquote>\\n      </section>\\n      <section id=\"improvements-bug-fixes-of-quantization-tools\">\\n       <h3>\\n        Improvements/Bug fixes of Quantization tools\\n        <a class=\"headerlink\" href=\"#improvements-bug-fixes-of-quantization-tools\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <ul class=\"simple\">\\n        <li>\\n         <p>\\n          You can now infer published tensor shapes even if\\n          <cite>\\n           axes\\n          </cite>\\n          property is not designated in ONNX Squeeze operators below version OpSet 12\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Added support not just for Conv receiving tensors with NxCxHxW shapes as input, but also for Conv receiving tensors with NxCxD shapes\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Modified “Conv - BatchNormalization” subgraph to be fused to Conv even when Conv does not receive bias as input\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Modified to always quantize Sub, Concat, and Pow operators in QDQ format, regardless of whether operands have initial values, so that the model can be processed in a consistent way in the post-quantization process\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Modified to prevent ONNX Runtime related warnings in the quantization process and the result model\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Reinforced the inspection condition to not miss any cases where tensor shape information cannot be inferred\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Modified to allow random calibration not only for models that receive float32 data as inputs, but also for models that receive other decimal or integer types as inputs\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Modified to find and terminate in a stable manner when given an already quantized model\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Modified to adjust scale of weight appropriately if Conv data input or scale of weight is too small, such that scale of bias becomes 0\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Reinforced conditions for “Gather - MatMul” subgraph to be fused into Gather\\n         </p>\\n        </li>\\n        <li>\\n         <p>\\n          Dependent libraries updated to latest version\\n         </p>\\n        </li>\\n       </ul>\\n      </section>\\n      <section id=\"device-plugin-configuration-file-support\">\\n       <h3>\\n        Device Plugin - Configuration file support\\n        <a class=\"headerlink\" href=\"#device-plugin-configuration-file-support\" title=\"Permalink to this heading\">\\n         \\uf0c1\\n        </a>\\n       </h3>\\n       <p>\\n        A function to set the execution option of the NPU Device Plugin used in Kubernetes with a file has been added. As before, option items can be entered as command-line arguments, or options can be specified by selecting a configuration file. Detailed instructions can be found in\\n        <a class=\"reference internal\" href=\"../software/kubernetes_support.html#kubernetesintegration\">\\n         <span class=\"std std-ref\">\\n          Kubernetes Support\\n         </span>\\n        </a>\\n        .\\n       </p>\\n      </section>\\n     </section>\\n    </section>\\n   </div>\\n  </div>\\n  <footer>\\n   <div aria-label=\"Footer\" class=\"rst-footer-buttons\" role=\"navigation\">\\n    <a accesskey=\"p\" class=\"btn btn-neutral float-left\" href=\"0.8.0.html\" rel=\"prev\" title=\"Release Notes - 0.8.0\">\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-left\">\\n     </span>\\n     Previous\\n    </a>\\n    <a accesskey=\"n\" class=\"btn btn-neutral float-right\" href=\"0.6.0.html\" rel=\"next\" title=\"Release Notes - 0.6.0\">\\n     Next\\n     <span aria-hidden=\"true\" class=\"fa fa-arrow-circle-right\">\\n     </span>\\n    </a>\\n   </div>\\n   <hr/>\\n   <div role=\"contentinfo\">\\n    <p>\\n     © Copyright 2023 FuriosaAI, Inc..\\n    </p>\\n   </div>\\n   Built with\\n   <a href=\"https://www.sphinx-doc.org/\">\\n    Sphinx\\n   </a>\\n   using a\\n   <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">\\n    theme\\n   </a>\\n   provided by\\n   <a href=\"https://readthedocs.org\">\\n    Read the Docs\\n   </a>\\n   .\\n  </footer>\\n </div>\\n</div>\\n')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version_name = 'warboy'\n",
    "data_dir = f'../../data/db/db-{version_name}_sdk.json'\n",
    "\n",
    "with open(data_dir, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    all_pages = [Page.model_validate_json(page) for page in data['sdk']]\n",
    "\n",
    "def find_page_with_url(url: str) -> Page:\n",
    "    for page in all_pages:\n",
    "        if str(page.link) == url:\n",
    "            return page\n",
    "    return None\n",
    "\n",
    "all_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라마인덱스 \"Document\"로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert page to llama index Document and TextNode\n",
    "docs = [convert_page_to_llama_index_document(page) for page in all_pages]\n",
    "nodes = [TextNode(id_=doc.id_, text=doc.text, metadata=doc.metadata) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 테스트 (link 인풋으로 직접 넣어주기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='Hello! How can I assist you today?', additional_kwargs={'model': 'llama3.1:70b', 'created_at': '2024-12-24T08:58:29.599469501Z', 'done': True, 'done_reason': 'stop', 'context': [128009, 128006, 882, 128007, 271, 15339, 128009, 128006, 78191, 128007, 271, 9906, 0, 2650, 649, 358, 7945, 499, 3432, 30], 'total_duration': 29463410216, 'load_duration': 28653532332, 'prompt_eval_count': 12, 'prompt_eval_duration': 114700000, 'eval_count': 10, 'eval_duration': 650808000}, raw={'model': 'llama3.1:70b', 'created_at': '2024-12-24T08:58:29.599469501Z', 'response': 'Hello! How can I assist you today?', 'done': True, 'done_reason': 'stop', 'context': [128009, 128006, 882, 128007, 271, 15339, 128009, 128006, 78191, 128007, 271, 9906, 0, 2650, 649, 358, 7945, 499, 3432, 30], 'total_duration': 29463410216, 'load_duration': 28653532332, 'prompt_eval_count': 12, 'prompt_eval_duration': 114700000, 'eval_count': 10, 'eval_duration': 650808000}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define LLM\n",
    "import os\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "llm = Ollama(model=\"llama3.1:70b\", request_timeout=600,temperature=0)\n",
    "llm.complete(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a highly knowledgeable assistant specializing in Furiosa's NPU SDK. Your task is to provide detailed and accurate responses to user queries about Furiosa SDK, including:\n",
    "\n",
    "1. Interpreting and explaining code examples.\n",
    "2. Providing guidance on CLI (Command Line Interface) commands and their usage.\n",
    "3. Offering detailed information about supported software and hardware configurations.\n",
    "\n",
    "For each query:\n",
    "- Extract key details from the question and the provided context.\n",
    "- Use the retrieved contents to generate a clear and step-by-step explanation.\n",
    "- Always include relevant examples or commands, where applicable, to enhance understanding.\n",
    "\n",
    "Make sure your response is concise but comprehensive, ensuring the user can act on your guidance immediately.\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Context:\n",
    "{retrieved_contents}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_simple = PromptTemplate(\n",
    "    template=\"\"\"You are a highly knowledgeable assistant specializing in Furiosa's NPU SDK. Your task is to provide a short responses to user queries.\n",
    "For each query, extract the most relevant information from the retrieved contents to generate a clear and compact explanation to the question. Make the response within 3 sentences.\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Context:\n",
    "{retrieved_contents}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m query_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is bert?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m link_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/furiosa_mlperf.html\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m document \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_page_to_llama_index_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_page_with_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink_in\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m* Query:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(query_in)\n",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m, in \u001b[0;36mconvert_page_to_llama_index_document\u001b[0;34m(page)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_page_to_llama_index_document\u001b[39m(page: Page) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CustomDocument:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CustomDocument(\n\u001b[0;32m---> 24\u001b[0m         doc_id\u001b[38;5;241m=\u001b[39m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m,\n\u001b[1;32m     25\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(page\u001b[38;5;241m.\u001b[39mlink),\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: page\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent_doc_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: page\u001b[38;5;241m.\u001b[39mparent,\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchild_doc_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(page\u001b[38;5;241m.\u001b[39mchild),\n\u001b[1;32m     30\u001b[0m         },\n\u001b[1;32m     31\u001b[0m         text\u001b[38;5;241m=\u001b[39mpage\u001b[38;5;241m.\u001b[39mdescription_clean,  \u001b[38;5;66;03m# 기본 text\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         page_content\u001b[38;5;241m=\u001b[39mpage\u001b[38;5;241m.\u001b[39mdescription_clean,  \u001b[38;5;66;03m# 추가 속성\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'id'"
     ]
    }
   ],
   "source": [
    "# Manual test \n",
    "query_in = \"What is bert?\"\n",
    "\n",
    "link_in = 'https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/furiosa_mlperf.html'\n",
    "document = convert_page_to_llama_index_document(find_page_with_url(link_in))\n",
    "\n",
    "print('* Query:')\n",
    "print(query_in)\n",
    "print(\"=\"*60)\n",
    "print('\\n')\n",
    "\n",
    "print('* link_gt:')\n",
    "print(link_in)\n",
    "print(\"=\"*60)\n",
    "print('\\n')\n",
    "\n",
    "print('* ChatBot response:')\n",
    "\n",
    "full_prompt = prompt.format(query=query_in, retrieved_contents=document.text)\n",
    "result = llm.complete(full_prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벡터DB 설정과 Retriever 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"dunzhang/stella_en_1.5B_v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "# Load retriever from data dir\n",
    "# Save from html nodes\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "vectordb_save_path = \"../../data/db/llama-index-resources/chroma\"\n",
    "collection_name = \"stella\"\n",
    "chroma_client = chromadb.PersistentClient(path=vectordb_save_path)\n",
    "chroma_collection = chroma_client.get_or_create_collection(collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# Save data\n",
    "index = VectorStoreIndex(nodes, storage_context=storage_context, embed_model=embed_model)\n",
    "# Load data\n",
    "# index = VectorStoreIndex.from_vector_store(\n",
    "#     vector_store, storage_context=storage_context, embed_model=embed_model,\n",
    "# )\n",
    "chroma_retriever = VectorIndexRetriever(index=index, similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load BM25Retriever\n",
    "- research에서 수집한 모든 페이지를 llama-index document로 변환 후 bm25에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce62971c42484114a148e9098b2e3467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcf52cc427d4e45a39bc930e2a8dc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fe70b848d945d9b2ed23dd3be51597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding newlines for mmindex:   0%|          | 0.00/244k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save llama index document to bm25\n",
    "# Save html nodes\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "import Stemmer\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes,\n",
    "    similarity_top_k=5,\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"en\",\n",
    ")\n",
    "bm25_save_path = \"../../data/db/llama-index-resources/bm25\"\n",
    "bm25_retriever.persist(bm25_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bm25\n",
    "import Stemmer\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_persist_dir(bm25_save_path)\n",
    "bm25_retriever.similarity_top_k = 5\n",
    "bm25_retriever.stemmer = Stemmer.Stemmer(\"english\")\n",
    "bm25_retriever.language = \"en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid\n",
    "- db: chroma + bm25 (두 방식이 상호보완적이므로)\n",
    "- normalize: dbsf\n",
    "- algorithm: Convex Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_dbsf(scores: List[str]):\n",
    "\tarr = np.array(scores)\n",
    "\tmean_value = np.mean(arr)\n",
    "\tstd_value = np.std(arr)\n",
    "\tmin_value = mean_value - 3 * std_value\n",
    "\tmax_value = mean_value + 3 * std_value\n",
    "\tnorm_score = (arr - min_value) / (max_value - min_value)\n",
    "\treturn norm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def hybrid_cc(lexical_results, semantic_results, top_k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Perform hybrid search using convex combination of BM25 and semantic scores.\n",
    "    \n",
    "    :param query: Search query (string)\n",
    "    :param alpha: Weight for BM25 scores (0 <= alpha <= 1). 1-alpha is weight for semantic scores.\n",
    "    \"\"\"\n",
    "    # Step 1: Perform BM25 Search\n",
    "    bm25_ids = np.array([result.id_ for result in lexical_results])\n",
    "    bm25_scores = np.array([result.score for result in lexical_results])\n",
    "    \n",
    "    # Step 2: Perform Semantic Search using ChromaRetriever\n",
    "    chroma_ids = np.array([result.id_ for result in semantic_results])\n",
    "    chroma_scores = np.array([result.score for result in semantic_results])\n",
    "    \n",
    "    # Step 3: Normalize the Scores\n",
    "    bm25_scores_norm = normalize_dbsf(bm25_scores)\n",
    "    chroma_scores_norm = normalize_dbsf(chroma_scores)\n",
    "\n",
    "    ids = [bm25_ids, chroma_ids]\n",
    "    scores = [bm25_scores_norm, chroma_scores_norm]\n",
    "    \n",
    "    df = pd.concat(\n",
    "\t\t[pd.Series(dict(zip(_id, score))) for _id, score in zip(ids, scores)], axis=1\n",
    "\t)\n",
    "    df.columns = [\"semantic\", \"lexical\"]\n",
    "    df = df.fillna(0)\n",
    "    df[\"weighted_sum\"] = df.mul((alpha, 1.0 - alpha)).sum(axis=1)\n",
    "    df = df.sort_values(by=\"weighted_sum\", ascending=False)\n",
    "\n",
    "    retrieved_ids, retrieved_scores = df.index.tolist()[:top_k], df[\"weighted_sum\"][:top_k].tolist()\n",
    "    retrieved_contents = []\n",
    "    for idx, id in enumerate(retrieved_ids):\n",
    "        content = next((node for node in lexical_results if node.id_ == id), None)\n",
    "        if content is not None:\n",
    "            content.score = retrieved_scores[idx]\n",
    "            retrieved_contents.append(content)\n",
    "            continue\n",
    "        content = next((node for node in semantic_results if node.id_ == id), None)\n",
    "        if content is not None:\n",
    "            content.score = retrieved_scores[idx]\n",
    "            retrieved_contents.append(content)\n",
    "\n",
    "    return retrieved_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cutoff\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "cutoff = SimilarityPostprocessor(similarity_cutoff=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단계 별 실행 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서부터 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_id                  cf227685-cc4e-420e-b21a-e7da166093e5\n",
      "link        https://furiosa-ai.github.io/docs/latest/en/so...\n",
      "question    What steps are necessary to ensure that a Warb...\n",
      "answer      First, enable IOMMU in both BIOS and Linux OS....\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "qa_dir = f'../../data/chatbot/qa-{version_name}_sdk.csv'\n",
    "\n",
    "qa_with_link = pd.read_csv(qa_dir, encoding=\"utf-8\", index_col=0)\n",
    "print(qa_with_link.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>link</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa</td>\n",
       "      <td>https://furiosa-ai.github.io/docs-dev/2024.1/e...</td>\n",
       "      <td>What are the planned features for Furiosa LLM'...</td>\n",
       "      <td>Planned features for Furiosa LLM include Tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa</td>\n",
       "      <td>https://furiosa-ai.github.io/docs-dev/2024.1/e...</td>\n",
       "      <td>What are the key features of Furiosa LLM that ...</td>\n",
       "      <td>Furiosa LLM features include a vLLM-compatible...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa</td>\n",
       "      <td>https://furiosa-ai.github.io/docs-dev/2024.1/e...</td>\n",
       "      <td>How does Furiosa LLM manage efficient KV cache...</td>\n",
       "      <td>Furiosa LLM manages efficient KV cache through...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dcd59fbc-fb76-4f34-b6ec-ea88a833b047</td>\n",
       "      <td>https://furiosa-ai.github.io/docs-dev/2024.1/e...</td>\n",
       "      <td>What are the default configuration values for ...</td>\n",
       "      <td>The default configuration values for deploying...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dcd59fbc-fb76-4f34-b6ec-ea88a833b047</td>\n",
       "      <td>https://furiosa-ai.github.io/docs-dev/2024.1/e...</td>\n",
       "      <td>What are the functionalities provided by the F...</td>\n",
       "      <td>The Furiosa device plugin discovers Furiosa NP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>a214fb49-b797-4d38-b877-597b6bb059eb</td>\n",
       "      <td>https://furiosa-ai.github.io/docs-dev/2024.1/e...</td>\n",
       "      <td>What command can be used to verify the install...</td>\n",
       "      <td>The command 'lspci -nn | grep FuriosaAI' can b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>a214fb49-b797-4d38-b877-597b6bb059eb</td>\n",
       "      <td>https://furiosa-ai.github.io/docs-dev/2024.1/e...</td>\n",
       "      <td>What are the necessary steps to upgrade the fi...</td>\n",
       "      <td>To upgrade the firmware of FuriosaAI devices, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1ba93fae-bf2e-42c1-a66d-dabbee880912</td>\n",
       "      <td>https://furiosa-ai.github.io/docs-dev/2024.1/e...</td>\n",
       "      <td>What are the constraints when specifying NPU r...</td>\n",
       "      <td>When specifying NPU resources, you can set NPU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1ba93fae-bf2e-42c1-a66d-dabbee880912</td>\n",
       "      <td>https://furiosa-ai.github.io/docs-dev/2024.1/e...</td>\n",
       "      <td>How does the deployment of Furiosa Feature Dis...</td>\n",
       "      <td>Furiosa Feature Discovery labels nodes based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1ba93fae-bf2e-42c1-a66d-dabbee880912</td>\n",
       "      <td>https://furiosa-ai.github.io/docs-dev/2024.1/e...</td>\n",
       "      <td>What steps must an administrator take to prepa...</td>\n",
       "      <td>An administrator must install prerequisites li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 page_id  \\\n",
       "0   3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa   \n",
       "1   3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa   \n",
       "2   3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa   \n",
       "3   dcd59fbc-fb76-4f34-b6ec-ea88a833b047   \n",
       "4   dcd59fbc-fb76-4f34-b6ec-ea88a833b047   \n",
       "..                                   ...   \n",
       "58  a214fb49-b797-4d38-b877-597b6bb059eb   \n",
       "59  a214fb49-b797-4d38-b877-597b6bb059eb   \n",
       "60  1ba93fae-bf2e-42c1-a66d-dabbee880912   \n",
       "61  1ba93fae-bf2e-42c1-a66d-dabbee880912   \n",
       "62  1ba93fae-bf2e-42c1-a66d-dabbee880912   \n",
       "\n",
       "                                                 link  \\\n",
       "0   https://furiosa-ai.github.io/docs-dev/2024.1/e...   \n",
       "1   https://furiosa-ai.github.io/docs-dev/2024.1/e...   \n",
       "2   https://furiosa-ai.github.io/docs-dev/2024.1/e...   \n",
       "3   https://furiosa-ai.github.io/docs-dev/2024.1/e...   \n",
       "4   https://furiosa-ai.github.io/docs-dev/2024.1/e...   \n",
       "..                                                ...   \n",
       "58  https://furiosa-ai.github.io/docs-dev/2024.1/e...   \n",
       "59  https://furiosa-ai.github.io/docs-dev/2024.1/e...   \n",
       "60  https://furiosa-ai.github.io/docs-dev/2024.1/e...   \n",
       "61  https://furiosa-ai.github.io/docs-dev/2024.1/e...   \n",
       "62  https://furiosa-ai.github.io/docs-dev/2024.1/e...   \n",
       "\n",
       "                                             question  \\\n",
       "0   What are the planned features for Furiosa LLM'...   \n",
       "1   What are the key features of Furiosa LLM that ...   \n",
       "2   How does Furiosa LLM manage efficient KV cache...   \n",
       "3   What are the default configuration values for ...   \n",
       "4   What are the functionalities provided by the F...   \n",
       "..                                                ...   \n",
       "58  What command can be used to verify the install...   \n",
       "59  What are the necessary steps to upgrade the fi...   \n",
       "60  What are the constraints when specifying NPU r...   \n",
       "61  How does the deployment of Furiosa Feature Dis...   \n",
       "62  What steps must an administrator take to prepa...   \n",
       "\n",
       "                                               answer  \n",
       "0   Planned features for Furiosa LLM include Tenso...  \n",
       "1   Furiosa LLM features include a vLLM-compatible...  \n",
       "2   Furiosa LLM manages efficient KV cache through...  \n",
       "3   The default configuration values for deploying...  \n",
       "4   The Furiosa device plugin discovers Furiosa NP...  \n",
       "..                                                ...  \n",
       "58  The command 'lspci -nn | grep FuriosaAI' can b...  \n",
       "59  To upgrade the firmware of FuriosaAI devices, ...  \n",
       "60  When specifying NPU resources, you can set NPU...  \n",
       "61  Furiosa Feature Discovery labels nodes based o...  \n",
       "62  An administrator must install prerequisites li...  \n",
       "\n",
       "[63 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_with_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What steps are necessary to ensure that a Warboy device is recognized and available within a virtual machine using QEMU-KVM?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_with_link['question'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer = 3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa \n",
      "\n",
      "----------\n",
      "[NodeWithScore(node=TextNode(id_='0acdfa06-7dff-4603-9a5c-dbc4e3310580', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/software_stack.html', 'title': 'software_stack', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='* [.rst](../_sources/overview/software_stack.rst \"Download source file\") * .pdf\\nFuriosaAI’s Software Stack ==========================\\nContents --------\\n* [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support)\\nFuriosaAI’s Software Stack [#](#furiosaai-s-software-stack \"Link to this heading\") ==================================================================================\\nFuriosaAI provides the streamlined software stack to allow FuriosaAI NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials. The above diagram demonstrates the SW stack provided by FuriosaAI, by layers.\\nThe following outlines the key components.\\nKernel Driver, Firmware, and PE Runtime [#](#kernel-driver-firmware-and-pe-runtime \"Link to this heading\") ----------------------------------------------------------------------------------------------------------\\nThe kernel device driver enables the Linux operating system to recognize NPU devices and expose them as Linux device files. The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE). PERT is responsible for communicating with the host’s runtime and scheduling, managing the resources of PEs to execute NPU tasks.\\nFuriosa Compiler [#](#furiosa-compiler \"Link to this heading\") --------------------------------------------------------------\\nFuriosa compiler analyzes, optimizes a model graph, and generates a NPU executable program. The operation passes involve graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. When `torch.compile()` backend, `FuriosaBackend` is used or `furiosa-llm` is used, the Furiosa Compiler is transparently used to generate NPU executable programs for Runtime.\\nFuriosa Runtime [#](#furiosa-runtime \"Link to this heading\") ------------------------------------------------------------\\nRuntime loads multiple executable NPU programs generated by Furiosa compiler, and run them on the NPU. A single model can be compiled into multiple executable programs according to model architectures and applications. Runtime is responsible for scheduling NPU programs and managing computation and memory resource on NPUs and CPUs. Also, Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs.\\nFuriosa Model Compressor (Quantizer) [#](#furiosa-model-compressor-quantizer \"Link to this heading\") ----------------------------------------------------------------------------------------------------\\nFuriosa Model Compressor is a library as well as toolkit for model calibration and quantization. Model quantization is a powerful technique to reduce memory footprint, computation cost, inference latency and power consumption. Furiosa Model Compressor provides post-training quantization methods, such as\\n* BF16 (W16A16) * INT8 Weight-Only (W8A16) * FP8 (W8A8) * INT8 SmoothQuant (W8A8) * INT4 Weight-Only (W4A16 AWQ / GPTQ) (Planned in release 2024.2)\\nFuriosa LLM [#](#furiosa-llm \"Link to this heading\") ----------------------------------------------------\\nFuriosa LLM provides a high-performance inference engine for LLM models, such as Llama 3.1 70B, 8B, GPT-J, and Bert. Furiosa LLM is designed to provide the state-of-the-art serving optimization for LLM models. The key features of Furiosa LLM include vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and OpenAI-compatible API server. You can find further information at [Furiosa LLM](../furiosa_llm/intro.html#furiosallm) .\\nKubernetes Support [#](#kubernetes-support \"Link to this heading\") ------------------------------------------------------------------\\nKubernetes, an open-source platform designed to manage containerized applications and services, is extensively adopted by various companies due to its robust capabilities for deploying, scaling, and automating containerized workloads. FuriosaAI software stack also offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment.\\nFuriosaAI’s device plugin enables Kubernetes clusters to recognize FuriosaAI’s NPUs and allows NPUs to be scheduled for workloads and services that require them. This feature allows users to easily deploy AI workloads with FuriosaAI NPUs on Kubernetes, enabling efficient resource utilization and scaling.\\nYou can find more information about Kubernetes support in the [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) .\\n[previous\\nFuriosaAI RNGD](rngd.html \"previous page\") [next\\nSupported Models](supported_models.html \"next page\")\\nContents\\n* [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=3.463642120361328), NodeWithScore(node=TextNode(id_='3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/intro.html', 'title': 'intro', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='* [.rst](../_sources/furiosa_llm/intro.rst \"Download source file\") * .pdf\\nFuriosa LLM ===========\\nFuriosa LLM [#](#furiosa-llm \"Link to this heading\") ====================================================\\nFuriosa LLM provides a high-performance inference engine for LLM models and Multi-Modal LLM models, Furiosa LLM is designed to provide the state-of-the-art serving optimization. The features of Furiosa LLM includes:\\n* vLLM-compatible API * Efficient KV cache management with PagedAttention * Continuous batching of incoming requests in serving * Quantization: INT4, INT8, FP8, GPTQ, AWQ * Data Parallelism and Pipeline Parallelism across multiple NPUs * Tensor Parallelism (planned in release 2024.2) across multiple NPUs * OpenAI-compatible API server * Various decoding algorithms, greedy search, beam search, top-k/top-p, speculative decoding (planned) * HuggingFace model integration and hub support * HuggingFace PEFT support (planned)\\n[previous\\nRunning MLPerf™ Inference Benchmark](../getting_started/furiosa_mlperf.html \"previous page\") [next\\nReferences](references.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=3.3194470405578613), NodeWithScore(node=TextNode(id_='a53038c2-0668-4963-875e-79abe9c99e2c', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/whatsnew/index.html', 'title': 'index', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='* [.rst](../_sources/whatsnew/index.rst \"Download source file\") * .pdf\\nWhat’s New ==========\\nContents --------\\n* [Furiosa SDK 2024.1.0 (2024-10-11)](#furiosa-sdk-2024-1-0-2024-10-11)   + [Highlights](#highlights)\\nWhat’s New [#](#what-s-new \"Link to this heading\") ==================================================\\nThis page describes the changes and functionality available in in the latest releases of Furiosa SDK 2024.1.0.\\nFuriosa SDK 2024.1.0 (2024-10-11) [#](#furiosa-sdk-2024-1-0-2024-10-11 \"Link to this heading\") ----------------------------------------------------------------------------------------------\\n2024.1.0 is the first SDK release for RNGD. This release is alpha release, and the features and APIs described in this document may change in the future.\\n### Highlights [#](#highlights \"Link to this heading\")\\n* Model Support: LLaMA 3.1 8B/70B, BERT Large, GPT-J 6B * Furiosa Quantizer supports the following quantization methods:      + BF16 (W16A16)   + INT8 Weight-Only (W8A16)   + FP8 (W8A8)   + INT8 SmoothQuant (W8A8) * Furiosa LLM      + Efficient KV cache management with PagedAttention   + Continuous batching support in serving   + OpenAI-compatible API server   + Greedy search and beam search   + Pipeline Parallelism and Data Parallelism across multiple NPUs * `furiosa-mlperf`   command      + Server and Offline scenarios   + BERT, GPT-J, LLaMA 3.1 benchmarks * System Management Interface      + System Management Interface Library and CLI for Furiosa NPU family * Cloud Native Toolkit      + Kubernetes integration for managing and monitoring the Furiosa NPU family\\nComponent version\\n[#](#id1 \"Link to this table\")\\n| Package name | Version | | --- | --- | | furiosa-compiler | 2024.1.0 | | furiosa-device-plugin | 2024.1.0 | | furiosa-driver-rngd | 2024.1.0 | | furiosa-feature-discovery | 2024.1.0 | | furiosa-firmware-image-tools | 2024.1.0 | | furiosa-firmware-image-rngd | 0.0.19 | | furiosa-libsmi | 2024.1.0 | | furiosa-llm | 2024.1.0 | | furiosa-llm-models | 2024.1.0 | | furiosa-mlperf | 2024.1.0 | | furiosa-mlperf-resources | 2024.1.0 | | furiosa-model-compressor | 2024.1.0 | | furiosa-model-compressor-impl | 2024.1.0 | | furiosa-native-compiler | 2024.1.0 | | furiosa-native-runtime | 2024.1.0 | | furiosa-smi | 2024.1.0 | | furiosa-torch-ext | 2024.1.0 |\\n[previous\\nSupported Models](../overview/supported_models.html \"previous page\") [next\\nRoadmap](../overview/roadmap.html \"next page\")\\nContents\\n* [Furiosa SDK 2024.1.0 (2024-10-11)](#furiosa-sdk-2024-1-0-2024-10-11)   + [Highlights](#highlights)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=3.19343900680542), NodeWithScore(node=TextNode(id_='d4927eaf-a1a0-40bd-9624-79db4213c5fc', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/roadmap.html', 'title': 'roadmap', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='* [.rst](../_sources/overview/roadmap.rst \"Download source file\") * .pdf\\nRoadmap =======\\nContents --------\\n* [Latest Recent Release](#latest-recent-release) * [Future Releases](#future-releases)   + [2024.2.0 (beta 0) - November, 2024](#beta-0-november-2024)   + [2024.3.0 (beta 1) - December, 2024](#beta-1-december-2024)\\nRoadmap [#](#roadmap \"Link to this heading\") ============================================\\nFurisaAI strives to deliver the releases for each month, while offering patch releases. This page shows the forward-looking roadmap of ongoing & upcoming projects and when they are expected to land, broken down by areas on [our software stack](software_stack.html#softwarestack) .\\nLatest Recent Release [#](#latest-recent-release \"Link to this heading\") ------------------------------------------------------------------------\\nThe latest release is 2024.1.0 (alpha) on October 11, 2024. You can find the release notes [here](../whatsnew/index.html#whatsnew) .\\nFuture Releases [#](#future-releases \"Link to this heading\") ------------------------------------------------------------\\nNote\\nThe roadmap is subject to change and may not reflect the final product.\\n### 2024.2.0 (beta 0) - November, 2024 [#](#beta-0-november-2024 \"Link to this heading\")\\n* Model support:      + Language Models: CodeLLaaMA2, Vicuna, Solar, EXAONE-3.0   + Vision Models: MobileNetV1, MobileNetV2, ResNet152, ResNet50, EfficientNet, YOLOv8m, .. * (Furiosa LLM) Tensor Parallelism support Phase 1: Intra-chip * Torch 2.4.1 support * CPU memory swapping in Furiosa LLM\\n### 2024.3.0 (beta 1) - December, 2024 [#](#beta-1-december-2024 \"Link to this heading\")\\n* Model support: TBD * (Furiosa LLM) Tensor Parallelism support Phase 2: Inter-chip * `torch.compile()`   backend * Huggingface Optimum integration\\n[previous\\nWhat’s New](../whatsnew/index.html \"previous page\") [next\\nInstalling Prerequisites](../getting_started/prerequisites.html \"next page\")\\nContents\\n* [Latest Recent Release](#latest-recent-release) * [Future Releases](#future-releases)   + [2024.2.0 (beta 0) - November, 2024](#beta-0-november-2024)   + [2024.3.0 (beta 1) - December, 2024](#beta-1-december-2024)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=3.088785171508789), NodeWithScore(node=TextNode(id_='13853744-dc18-4c98-b1c2-4f5806b28514', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/', 'title': '', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='* [.rst](_sources/index.rst \"Download source file\") * .pdf\\nFuriosaAI Developer Center ==========================\\nContents --------\\n* [Overview](#overview) * [Getting Started](#getting-started) * [Cloud Native Toolkit](#cloud-native-toolkit) * [Device Management](#device-management) * [Customer Support](#customer-support)\\nFuriosaAI Developer Center [#](#furiosaai-developer-center \"Link to this heading\") ==================================================================================\\nWelcome to the FuriosaAI Developer Center. FuriosaAI provides an streamlined software stack for deep learning model inference on FuriosaAI NPUs. This document provides a guide to easily perform the entire workflow of writing inference applications, from starting with PyTorch model to model quantization, serving, and production deployment.\\nWarning\\nThis document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future.\\nOverview [#](#overview \"Link to this heading\") ----------------------------------------------\\n* [FuriosaAI RNGD](overview/rngd.html#rngd)   : RNGD Hardware Specification, and features * [FuriosaAI’s Software Stack](overview/software_stack.html#softwarestack)   : An overview of the FuriosaAI software stack * [Supported Models](overview/supported_models.html#supportedmodels)   : A list of supported models * [What’s New](whatsnew/index.html#whatsnew)   : New features and changes in the latest release * [Roadmap](overview/roadmap.html#roadmap)   : The future roadmap of FuriosaAI Software Stack\\nGetting Started [#](#getting-started \"Link to this heading\") ------------------------------------------------------------\\n* [Installing Prerequisites](getting_started/prerequisites.html#installingprerequisites)   : How to install the prerequisites for FuriosaAI Software Stack * [Quick Start with Furiosa LLM](getting_started/furiosa_llm.html#gettingstartedfuriosallm) * [Running MLPerf™ Inference Benchmark](getting_started/furiosa_mlperf.html#gettingstartedfuriosamlperf)\\nCloud Native Toolkit [#](#cloud-native-toolkit \"Link to this heading\") ----------------------------------------------------------------------\\n* [Cloud Native Toolkit](cloud_native_toolkit/intro.html#cloudnativetoolkit)   : An overview of the Cloud Native Toolkit * [Kubernetes Support](cloud_native_toolkit/kubernetes.html#kubernetes)   : An overview of the Kubernetes Support\\nDevice Management [#](#device-management \"Link to this heading\") ----------------------------------------------------------------\\n* [furiosa-smi](device_management/furiosa_smi.html#furiosasmi)   : A command line utility for managing FuriosaAI NPUs\\nCustomer Support [#](#customer-support \"Link to this heading\") --------------------------------------------------------------\\n* [FuriosaAI Homepage](https://furiosa.ai) * [FuriosaAI Forum](https://furiosa-ai.discourse.group/) * [FuriosaAI Customer Portal](https://furiosa-ai.atlassian.net/servicedesk/customer/portals/) * [FuriosaAI Warboy SDK Document](https://furiosa-ai.github.io/docs/latest/en/)\\n[next\\nFuriosaAI RNGD](overview/rngd.html \"next page\")\\nContents\\n* [Overview](#overview) * [Getting Started](#getting-started) * [Cloud Native Toolkit](#cloud-native-toolkit) * [Device Management](#device-management) * [Customer Support](#customer-support)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=3.0831470489501953)]\n",
      "----------\n",
      "[NodeWithScore(node=TextNode(id_='3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/intro.html', 'title': 'intro', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='* [.rst](../_sources/furiosa_llm/intro.rst \"Download source file\") * .pdf\\nFuriosa LLM ===========\\nFuriosa LLM [#](#furiosa-llm \"Link to this heading\") ====================================================\\nFuriosa LLM provides a high-performance inference engine for LLM models and Multi-Modal LLM models, Furiosa LLM is designed to provide the state-of-the-art serving optimization. The features of Furiosa LLM includes:\\n* vLLM-compatible API * Efficient KV cache management with PagedAttention * Continuous batching of incoming requests in serving * Quantization: INT4, INT8, FP8, GPTQ, AWQ * Data Parallelism and Pipeline Parallelism across multiple NPUs * Tensor Parallelism (planned in release 2024.2) across multiple NPUs * OpenAI-compatible API server * Various decoding algorithms, greedy search, beam search, top-k/top-p, speculative decoding (planned) * HuggingFace model integration and hub support * HuggingFace PEFT support (planned)\\n[previous\\nRunning MLPerf™ Inference Benchmark](../getting_started/furiosa_mlperf.html \"previous page\") [next\\nReferences](references.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.672514574827812), NodeWithScore(node=TextNode(id_='a53038c2-0668-4963-875e-79abe9c99e2c', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/whatsnew/index.html', 'title': 'index', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='* [.rst](../_sources/whatsnew/index.rst \"Download source file\") * .pdf\\nWhat’s New ==========\\nContents --------\\n* [Furiosa SDK 2024.1.0 (2024-10-11)](#furiosa-sdk-2024-1-0-2024-10-11)   + [Highlights](#highlights)\\nWhat’s New [#](#what-s-new \"Link to this heading\") ==================================================\\nThis page describes the changes and functionality available in in the latest releases of Furiosa SDK 2024.1.0.\\nFuriosa SDK 2024.1.0 (2024-10-11) [#](#furiosa-sdk-2024-1-0-2024-10-11 \"Link to this heading\") ----------------------------------------------------------------------------------------------\\n2024.1.0 is the first SDK release for RNGD. This release is alpha release, and the features and APIs described in this document may change in the future.\\n### Highlights [#](#highlights \"Link to this heading\")\\n* Model Support: LLaMA 3.1 8B/70B, BERT Large, GPT-J 6B * Furiosa Quantizer supports the following quantization methods:      + BF16 (W16A16)   + INT8 Weight-Only (W8A16)   + FP8 (W8A8)   + INT8 SmoothQuant (W8A8) * Furiosa LLM      + Efficient KV cache management with PagedAttention   + Continuous batching support in serving   + OpenAI-compatible API server   + Greedy search and beam search   + Pipeline Parallelism and Data Parallelism across multiple NPUs * `furiosa-mlperf`   command      + Server and Offline scenarios   + BERT, GPT-J, LLaMA 3.1 benchmarks * System Management Interface      + System Management Interface Library and CLI for Furiosa NPU family * Cloud Native Toolkit      + Kubernetes integration for managing and monitoring the Furiosa NPU family\\nComponent version\\n[#](#id1 \"Link to this table\")\\n| Package name | Version | | --- | --- | | furiosa-compiler | 2024.1.0 | | furiosa-device-plugin | 2024.1.0 | | furiosa-driver-rngd | 2024.1.0 | | furiosa-feature-discovery | 2024.1.0 | | furiosa-firmware-image-tools | 2024.1.0 | | furiosa-firmware-image-rngd | 0.0.19 | | furiosa-libsmi | 2024.1.0 | | furiosa-llm | 2024.1.0 | | furiosa-llm-models | 2024.1.0 | | furiosa-mlperf | 2024.1.0 | | furiosa-mlperf-resources | 2024.1.0 | | furiosa-model-compressor | 2024.1.0 | | furiosa-model-compressor-impl | 2024.1.0 | | furiosa-native-compiler | 2024.1.0 | | furiosa-native-runtime | 2024.1.0 | | furiosa-smi | 2024.1.0 | | furiosa-torch-ext | 2024.1.0 |\\n[previous\\nSupported Models](../overview/supported_models.html \"previous page\") [next\\nRoadmap](../overview/roadmap.html \"next page\")\\nContents\\n* [Furiosa SDK 2024.1.0 (2024-10-11)](#furiosa-sdk-2024-1-0-2024-10-11)   + [Highlights](#highlights)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5389004366031906), NodeWithScore(node=TextNode(id_='d4927eaf-a1a0-40bd-9624-79db4213c5fc', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/roadmap.html', 'title': 'roadmap', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='* [.rst](../_sources/overview/roadmap.rst \"Download source file\") * .pdf\\nRoadmap =======\\nContents --------\\n* [Latest Recent Release](#latest-recent-release) * [Future Releases](#future-releases)   + [2024.2.0 (beta 0) - November, 2024](#beta-0-november-2024)   + [2024.3.0 (beta 1) - December, 2024](#beta-1-december-2024)\\nRoadmap [#](#roadmap \"Link to this heading\") ============================================\\nFurisaAI strives to deliver the releases for each month, while offering patch releases. This page shows the forward-looking roadmap of ongoing & upcoming projects and when they are expected to land, broken down by areas on [our software stack](software_stack.html#softwarestack) .\\nLatest Recent Release [#](#latest-recent-release \"Link to this heading\") ------------------------------------------------------------------------\\nThe latest release is 2024.1.0 (alpha) on October 11, 2024. You can find the release notes [here](../whatsnew/index.html#whatsnew) .\\nFuture Releases [#](#future-releases \"Link to this heading\") ------------------------------------------------------------\\nNote\\nThe roadmap is subject to change and may not reflect the final product.\\n### 2024.2.0 (beta 0) - November, 2024 [#](#beta-0-november-2024 \"Link to this heading\")\\n* Model support:      + Language Models: CodeLLaaMA2, Vicuna, Solar, EXAONE-3.0   + Vision Models: MobileNetV1, MobileNetV2, ResNet152, ResNet50, EfficientNet, YOLOv8m, .. * (Furiosa LLM) Tensor Parallelism support Phase 1: Intra-chip * Torch 2.4.1 support * CPU memory swapping in Furiosa LLM\\n### 2024.3.0 (beta 1) - December, 2024 [#](#beta-1-december-2024 \"Link to this heading\")\\n* Model support: TBD * (Furiosa LLM) Tensor Parallelism support Phase 2: Inter-chip * `torch.compile()`   backend * Huggingface Optimum integration\\n[previous\\nWhat’s New](../whatsnew/index.html \"previous page\") [next\\nInstalling Prerequisites](../getting_started/prerequisites.html \"next page\")\\nContents\\n* [Latest Recent Release](#latest-recent-release) * [Future Releases](#future-releases)   + [2024.2.0 (beta 0) - November, 2024](#beta-0-november-2024)   + [2024.3.0 (beta 1) - December, 2024](#beta-1-december-2024)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4198848701513558), NodeWithScore(node=TextNode(id_='0acdfa06-7dff-4603-9a5c-dbc4e3310580', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/software_stack.html', 'title': 'software_stack', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='* [.rst](../_sources/overview/software_stack.rst \"Download source file\") * .pdf\\nFuriosaAI’s Software Stack ==========================\\nContents --------\\n* [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support)\\nFuriosaAI’s Software Stack [#](#furiosaai-s-software-stack \"Link to this heading\") ==================================================================================\\nFuriosaAI provides the streamlined software stack to allow FuriosaAI NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials. The above diagram demonstrates the SW stack provided by FuriosaAI, by layers.\\nThe following outlines the key components.\\nKernel Driver, Firmware, and PE Runtime [#](#kernel-driver-firmware-and-pe-runtime \"Link to this heading\") ----------------------------------------------------------------------------------------------------------\\nThe kernel device driver enables the Linux operating system to recognize NPU devices and expose them as Linux device files. The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE). PERT is responsible for communicating with the host’s runtime and scheduling, managing the resources of PEs to execute NPU tasks.\\nFuriosa Compiler [#](#furiosa-compiler \"Link to this heading\") --------------------------------------------------------------\\nFuriosa compiler analyzes, optimizes a model graph, and generates a NPU executable program. The operation passes involve graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. When `torch.compile()` backend, `FuriosaBackend` is used or `furiosa-llm` is used, the Furiosa Compiler is transparently used to generate NPU executable programs for Runtime.\\nFuriosa Runtime [#](#furiosa-runtime \"Link to this heading\") ------------------------------------------------------------\\nRuntime loads multiple executable NPU programs generated by Furiosa compiler, and run them on the NPU. A single model can be compiled into multiple executable programs according to model architectures and applications. Runtime is responsible for scheduling NPU programs and managing computation and memory resource on NPUs and CPUs. Also, Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs.\\nFuriosa Model Compressor (Quantizer) [#](#furiosa-model-compressor-quantizer \"Link to this heading\") ----------------------------------------------------------------------------------------------------\\nFuriosa Model Compressor is a library as well as toolkit for model calibration and quantization. Model quantization is a powerful technique to reduce memory footprint, computation cost, inference latency and power consumption. Furiosa Model Compressor provides post-training quantization methods, such as\\n* BF16 (W16A16) * INT8 Weight-Only (W8A16) * FP8 (W8A8) * INT8 SmoothQuant (W8A8) * INT4 Weight-Only (W4A16 AWQ / GPTQ) (Planned in release 2024.2)\\nFuriosa LLM [#](#furiosa-llm \"Link to this heading\") ----------------------------------------------------\\nFuriosa LLM provides a high-performance inference engine for LLM models, such as Llama 3.1 70B, 8B, GPT-J, and Bert. Furiosa LLM is designed to provide the state-of-the-art serving optimization for LLM models. The key features of Furiosa LLM include vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and OpenAI-compatible API server. You can find further information at [Furiosa LLM](../furiosa_llm/intro.html#furiosallm) .\\nKubernetes Support [#](#kubernetes-support \"Link to this heading\") ------------------------------------------------------------------\\nKubernetes, an open-source platform designed to manage containerized applications and services, is extensively adopted by various companies due to its robust capabilities for deploying, scaling, and automating containerized workloads. FuriosaAI software stack also offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment.\\nFuriosaAI’s device plugin enables Kubernetes clusters to recognize FuriosaAI’s NPUs and allows NPUs to be scheduled for workloads and services that require them. This feature allows users to easily deploy AI workloads with FuriosaAI NPUs on Kubernetes, enabling efficient resource utilization and scaling.\\nYou can find more information about Kubernetes support in the [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) .\\n[previous\\nFuriosaAI RNGD](rngd.html \"previous page\") [next\\nSupported Models](supported_models.html \"next page\")\\nContents\\n* [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3842574746309207), NodeWithScore(node=TextNode(id_='13853744-dc18-4c98-b1c2-4f5806b28514', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/', 'title': '', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='* [.rst](_sources/index.rst \"Download source file\") * .pdf\\nFuriosaAI Developer Center ==========================\\nContents --------\\n* [Overview](#overview) * [Getting Started](#getting-started) * [Cloud Native Toolkit](#cloud-native-toolkit) * [Device Management](#device-management) * [Customer Support](#customer-support)\\nFuriosaAI Developer Center [#](#furiosaai-developer-center \"Link to this heading\") ==================================================================================\\nWelcome to the FuriosaAI Developer Center. FuriosaAI provides an streamlined software stack for deep learning model inference on FuriosaAI NPUs. This document provides a guide to easily perform the entire workflow of writing inference applications, from starting with PyTorch model to model quantization, serving, and production deployment.\\nWarning\\nThis document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future.\\nOverview [#](#overview \"Link to this heading\") ----------------------------------------------\\n* [FuriosaAI RNGD](overview/rngd.html#rngd)   : RNGD Hardware Specification, and features * [FuriosaAI’s Software Stack](overview/software_stack.html#softwarestack)   : An overview of the FuriosaAI software stack * [Supported Models](overview/supported_models.html#supportedmodels)   : A list of supported models * [What’s New](whatsnew/index.html#whatsnew)   : New features and changes in the latest release * [Roadmap](overview/roadmap.html#roadmap)   : The future roadmap of FuriosaAI Software Stack\\nGetting Started [#](#getting-started \"Link to this heading\") ------------------------------------------------------------\\n* [Installing Prerequisites](getting_started/prerequisites.html#installingprerequisites)   : How to install the prerequisites for FuriosaAI Software Stack * [Quick Start with Furiosa LLM](getting_started/furiosa_llm.html#gettingstartedfuriosallm) * [Running MLPerf™ Inference Benchmark](getting_started/furiosa_mlperf.html#gettingstartedfuriosamlperf)\\nCloud Native Toolkit [#](#cloud-native-toolkit \"Link to this heading\") ----------------------------------------------------------------------\\n* [Cloud Native Toolkit](cloud_native_toolkit/intro.html#cloudnativetoolkit)   : An overview of the Cloud Native Toolkit * [Kubernetes Support](cloud_native_toolkit/kubernetes.html#kubernetes)   : An overview of the Kubernetes Support\\nDevice Management [#](#device-management \"Link to this heading\") ----------------------------------------------------------------\\n* [furiosa-smi](device_management/furiosa_smi.html#furiosasmi)   : A command line utility for managing FuriosaAI NPUs\\nCustomer Support [#](#customer-support \"Link to this heading\") --------------------------------------------------------------\\n* [FuriosaAI Homepage](https://furiosa.ai) * [FuriosaAI Forum](https://furiosa-ai.discourse.group/) * [FuriosaAI Customer Portal](https://furiosa-ai.atlassian.net/servicedesk/customer/portals/) * [FuriosaAI Warboy SDK Document](https://furiosa-ai.github.io/docs/latest/en/)\\n[next\\nFuriosaAI RNGD](overview/rngd.html \"next page\")\\nContents\\n* [Overview](#overview) * [Getting Started](#getting-started) * [Cloud Native Toolkit](#cloud-native-toolkit) * [Device Management](#device-management) * [Customer Support](#customer-support)\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.165901851343843)]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Retrieve document\n",
    "print(f\"Answer = {qa_with_link['page_id'].values[0]} \\n\")\n",
    "print(\"-\"*10)\n",
    "\n",
    "## VectorDB\n",
    "semantic_results = chroma_retriever.retrieve(qa_with_link['question'].values[0])\n",
    "\n",
    "## BM25\n",
    "lexical_results = bm25_retriever.retrieve(qa_with_link['question'].values[0])\n",
    "print(lexical_results)\n",
    "print(\"-\"*10)\n",
    "\n",
    "## Hybrid\n",
    "retrieved_contents = hybrid_cc(semantic_results=semantic_results, lexical_results=lexical_results)\n",
    "print(retrieved_contents)\n",
    "print(\"-\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa', embedding=None, metadata={'source': 'https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/intro.html', 'title': 'intro', 'parent_doc_id': '', 'child_doc_ids': '[]'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='* [.rst](../_sources/furiosa_llm/intro.rst \"Download source file\") * .pdf\\nFuriosa LLM ===========\\nFuriosa LLM [#](#furiosa-llm \"Link to this heading\") ====================================================\\nFuriosa LLM provides a high-performance inference engine for LLM models and Multi-Modal LLM models, Furiosa LLM is designed to provide the state-of-the-art serving optimization. The features of Furiosa LLM includes:\\n* vLLM-compatible API * Efficient KV cache management with PagedAttention * Continuous batching of incoming requests in serving * Quantization: INT4, INT8, FP8, GPTQ, AWQ * Data Parallelism and Pipeline Parallelism across multiple NPUs * Tensor Parallelism (planned in release 2024.2) across multiple NPUs * OpenAI-compatible API server * Various decoding algorithms, greedy search, beam search, top-k/top-p, speculative decoding (planned) * HuggingFace model integration and hub support * HuggingFace PEFT support (planned)\\n[previous\\nRunning MLPerf™ Inference Benchmark](../getting_started/furiosa_mlperf.html \"previous page\") [next\\nReferences](references.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.672514574827812)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Cutoff\n",
    "retrieved_contents_filtered = cutoff.postprocess_nodes(retrieved_contents)\n",
    "retrieved_contents_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'* [.rst](../_sources/furiosa_llm/intro.rst \"Download source file\") * .pdf\\nFuriosa LLM ===========\\nFuriosa LLM [#](#furiosa-llm \"Link to this heading\") ====================================================\\nFuriosa LLM provides a high-performance inference engine for LLM models and Multi-Modal LLM models, Furiosa LLM is designed to provide the state-of-the-art serving optimization. The features of Furiosa LLM includes:\\n* vLLM-compatible API * Efficient KV cache management with PagedAttention * Continuous batching of incoming requests in serving * Quantization: INT4, INT8, FP8, GPTQ, AWQ * Data Parallelism and Pipeline Parallelism across multiple NPUs * Tensor Parallelism (planned in release 2024.2) across multiple NPUs * OpenAI-compatible API server * Various decoding algorithms, greedy search, beam search, top-k/top-p, speculative decoding (planned) * HuggingFace model integration and hub support * HuggingFace PEFT support (planned)\\n[previous\\nRunning MLPerf™ Inference Benchmark](../getting_started/furiosa_mlperf.html \"previous page\") [next\\nReferences](references.html \"next page\")\\nBy FuriosaAI, Inc.\\n© Copyright 2024, FuriosaAI, Inc..'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_contents_filtered[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 셀에 문제 있을 수 있음. 체크 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the planned features for Furiosa LLM's future releases, and how do they enhance its capabilities?\n",
      "**Planned Features for Furiosa LLM's Future Releases**\n",
      "\n",
      "Based on the provided context, I've extracted the key details about the planned features for Furiosa LLM's future releases. These features aim to enhance its capabilities and provide state-of-the-art serving optimization.\n",
      "\n",
      "**1. Tensor Parallelism (Release 2024.2)**\n",
      "\n",
      "Tensor Parallelism is a planned feature that will allow Furiosa LLM to parallelize tensor operations across multiple NPUs. This feature will enable more efficient processing of large models and improve overall performance.\n",
      "\n",
      "**Example:** To utilize Tensor Parallelism, you can use the `--tensor-parallelism` flag when running your model with Furiosa LLM. For instance:\n",
      "```bash\n",
      "furiosa_llm --model my_model --input input_data --tensor-parallelism\n",
      "```\n",
      "This will enable tensor parallelism for your model and distribute the computation across multiple NPUs.\n",
      "\n",
      "**2. Speculative Decoding (Planned)**\n",
      "\n",
      "Speculative decoding is a planned feature that will allow Furiosa LLM to perform speculative execution of decoding algorithms, such as greedy search, beam search, top-k/top-p, and others. This feature will improve the overall performance of the model by reducing the latency associated with decoding.\n",
      "\n",
      "**Example:** To use speculative decoding, you can specify the `--speculative-decoding` flag when running your model with Furiosa LLM. For instance:\n",
      "```bash\n",
      "furiosa_llm --model my_model --input input_data --speculative-decoding\n",
      "```\n",
      "This will enable speculative decoding for your model and improve its performance.\n",
      "\n",
      "**3. HuggingFace PEFT Support (Planned)**\n",
      "\n",
      "HuggingFace PEFT support is a planned feature that will allow Furiosa LLM to integrate with the HuggingFace PEFT (Parameter-Efficient Fine-Tuning) library. This feature will enable users to fine-tune their models using the PEFT library and leverage its capabilities.\n",
      "\n",
      "**Example:** To use HuggingFace PEFT support, you can specify the `--huggingface-peft` flag when running your model with Furiosa LLM. For instance:\n",
      "```bash\n",
      "furiosa_llm --model my_model --input input_data --huggingface-peft\n",
      "```\n",
      "This will enable HuggingFace PEFT support for your model and allow you to fine-tune it using the PEFT library.\n",
      "\n",
      "These planned features will enhance the capabilities of Furiosa LLM and provide users with more efficient and effective ways to deploy their models.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Complete prompt & Generate LLM answer\n",
    "print(qa_with_link['question'].values[0])\n",
    "if len(retrieved_contents) > 0:\n",
    "    full_prompt = prompt.format(query=qa_with_link['question'].values[0], retrieved_contents=retrieved_contents_filtered[0].text)\n",
    "    result = llm.complete(full_prompt)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the planned features for Furiosa LLM's future releases, and how do they enhance its capabilities?\n",
      "**Planned Features for Furiosa LLM's Future Releases**\n",
      "\n",
      "According to the provided context, several features are planned for future releases of Furiosa LLM, which will enhance its capabilities:\n",
      "\n",
      "1.  **Tensor Parallelism**: This feature is planned for release in 2024.2 and will allow for parallelization across multiple NPUs.\n",
      "2.  **Speculative Decoding**: This decoding algorithm is planned for a future release and will provide an additional option for users.\n",
      "3.  **HuggingFace PEFT Support**: This feature is also planned for a future release, which will enable support for HuggingFace's PEFT (Parameter-Efficient Fine-Tuning) technique.\n",
      "\n",
      "These features will further enhance Furiosa LLM's capabilities, providing users with more options for optimizing their models and improving performance.\n",
      "\n",
      "**Current Features**\n",
      "\n",
      "In addition to the planned features, Furiosa LLM currently provides several key features, including:\n",
      "\n",
      "*   vLLM-compatible API\n",
      "*   Efficient KV cache management with PagedAttention\n",
      "*   Continuous batching of incoming requests in serving\n",
      "*   Quantization: INT4, INT8, FP8, GPTQ, AWQ\n",
      "*   Data Parallelism and Pipeline Parallelism across multiple NPUs\n",
      "*   OpenAI-compatible API server\n",
      "*   Various decoding algorithms (greedy search, beam search, top-k/top-p)\n",
      "*   HuggingFace model integration and hub support\n",
      "\n",
      "These features make Furiosa LLM a powerful tool for optimizing and serving large language models.\n",
      "\n",
      "**Example Use Case**\n",
      "\n",
      "To utilize the current features of Furiosa LLM, you can use the following example command to serve a model using the OpenAI-compatible API server:\n",
      "\n",
      "```bash\n",
      "furiosa-llm --model-path /path/to/model --api-server openai\n",
      "```\n",
      "\n",
      "This command will start the Furiosa LLM server with the specified model and make it available through the OpenAI-compatible API.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Furiosa LLM is a powerful tool for optimizing and serving large language models. With its current features and planned future releases, it provides users with a wide range of options for improving performance and efficiency. By utilizing these features, users can unlock the full potential of their models and achieve state-of-the-art results.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Complete prompt & Generate LLM answer\n",
    "print(qa_with_link['question'].values[0])\n",
    "if len(retrieved_contents) > 0:\n",
    "    full_prompt = prompt.format(query=qa_with_link['question'].values[0], retrieved_contents=retrieved_contents_filtered)\n",
    "    result = llm.complete(full_prompt)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate responses for all qa dataset\n",
    "\n",
    "- rngd \n",
    "\n",
    "- warboy \n",
    "\n",
    "- faq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>link</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa</td>\n",
       "      <td>https://furiosa-ai.discourse.group/t/furiosaai...</td>\n",
       "      <td>How can I convert an ONNX file to an ENF file ...</td>\n",
       "      <td>You can use the following command: furiosa com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa</td>\n",
       "      <td>https://furiosa-ai.discourse.group/t/furiosaai...</td>\n",
       "      <td>Can I specify NPU 0 and NPU 1 separately when ...</td>\n",
       "      <td>Yes, you can execute tasks by specifying diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa</td>\n",
       "      <td>https://furiosa-ai.discourse.group/t/furiosaai...</td>\n",
       "      <td>Please provide a precise explanation of the ta...</td>\n",
       "      <td>Warboy only supports inference and does not su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dcd59fbc-fb76-4f34-b6ec-ea88a833b047</td>\n",
       "      <td>https://furiosa-ai.discourse.group/t/furiosaai...</td>\n",
       "      <td>What operators are supported by Warboy, and wh...</td>\n",
       "      <td>It is specialized in accelerating CNN-based mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dcd59fbc-fb76-4f34-b6ec-ea88a833b047</td>\n",
       "      <td>https://furiosa-ai.discourse.group/t/furiosaai...</td>\n",
       "      <td>Can accuracy drop after quantization? Are ther...</td>\n",
       "      <td>Accuracy may drop when quantizing an FP32 mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dcd59fbc-fb76-4f34-b6ec-ea88a833b047</td>\n",
       "      <td>https://furiosa-ai.discourse.group/t/furiosaai...</td>\n",
       "      <td>Why does the Insufficient Instruction Memory e...</td>\n",
       "      <td>Cause: This error occurs when the number of op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3bb3bd0a-7bdc-45a7-8e3e-e556a52d7eda</td>\n",
       "      <td>https://furiosa-ai.discourse.group/t/furiosaai...</td>\n",
       "      <td>Why does the Incompatible configuration runtim...</td>\n",
       "      <td>Cause: This error occurs when the SDK version ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3bb3bd0a-7bdc-45a7-8e3e-e556a52d7eda</td>\n",
       "      <td>https://furiosa-ai.discourse.group/t/furiosaai...</td>\n",
       "      <td>Why does the model inference time take longer ...</td>\n",
       "      <td>Cause: This occurs when the model uses operato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3bb3bd0a-7bdc-45a7-8e3e-e556a52d7eda</td>\n",
       "      <td>https://furiosa-ai.github.io/docs-dev/2024.1/e...</td>\n",
       "      <td>What are the specific subcommands provided by ...</td>\n",
       "      <td>The 'furiosa-mlperf' command provides subcomma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a3d94379-304a-4dbc-8300-39169378bfd5</td>\n",
       "      <td>https://furiosa-ai.github.io/docs-dev/2024.1/e...</td>\n",
       "      <td>What are the necessary components and steps re...</td>\n",
       "      <td>To launch the OpenAI-compatible Furiosa-LLM se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                page_id  \\\n",
       "0  3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa   \n",
       "1  3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa   \n",
       "2  3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa   \n",
       "3  dcd59fbc-fb76-4f34-b6ec-ea88a833b047   \n",
       "4  dcd59fbc-fb76-4f34-b6ec-ea88a833b047   \n",
       "5  dcd59fbc-fb76-4f34-b6ec-ea88a833b047   \n",
       "6  3bb3bd0a-7bdc-45a7-8e3e-e556a52d7eda   \n",
       "7  3bb3bd0a-7bdc-45a7-8e3e-e556a52d7eda   \n",
       "8  3bb3bd0a-7bdc-45a7-8e3e-e556a52d7eda   \n",
       "9  a3d94379-304a-4dbc-8300-39169378bfd5   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://furiosa-ai.discourse.group/t/furiosaai...   \n",
       "1  https://furiosa-ai.discourse.group/t/furiosaai...   \n",
       "2  https://furiosa-ai.discourse.group/t/furiosaai...   \n",
       "3  https://furiosa-ai.discourse.group/t/furiosaai...   \n",
       "4  https://furiosa-ai.discourse.group/t/furiosaai...   \n",
       "5  https://furiosa-ai.discourse.group/t/furiosaai...   \n",
       "6  https://furiosa-ai.discourse.group/t/furiosaai...   \n",
       "7  https://furiosa-ai.discourse.group/t/furiosaai...   \n",
       "8  https://furiosa-ai.github.io/docs-dev/2024.1/e...   \n",
       "9  https://furiosa-ai.github.io/docs-dev/2024.1/e...   \n",
       "\n",
       "                                            question  \\\n",
       "0  How can I convert an ONNX file to an ENF file ...   \n",
       "1  Can I specify NPU 0 and NPU 1 separately when ...   \n",
       "2  Please provide a precise explanation of the ta...   \n",
       "3  What operators are supported by Warboy, and wh...   \n",
       "4  Can accuracy drop after quantization? Are ther...   \n",
       "5  Why does the Insufficient Instruction Memory e...   \n",
       "6  Why does the Incompatible configuration runtim...   \n",
       "7  Why does the model inference time take longer ...   \n",
       "8  What are the specific subcommands provided by ...   \n",
       "9  What are the necessary components and steps re...   \n",
       "\n",
       "                                              answer  \n",
       "0  You can use the following command: furiosa com...  \n",
       "1  Yes, you can execute tasks by specifying diffe...  \n",
       "2  Warboy only supports inference and does not su...  \n",
       "3  It is specialized in accelerating CNN-based mo...  \n",
       "4  Accuracy may drop when quantizing an FP32 mode...  \n",
       "5  Cause: This error occurs when the number of op...  \n",
       "6  Cause: This error occurs when the SDK version ...  \n",
       "7  Cause: This occurs when the model uses operato...  \n",
       "8  The 'furiosa-mlperf' command provides subcomma...  \n",
       "9  To launch the OpenAI-compatible Furiosa-LLM se...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version_name = 'warboy'\n",
    "# qa_dir = f'../../data/chatbot/qa-{version_name}_sdk.csv'\n",
    "qa_dir = '../../data/handmade-faq/qa-warboy_sdk.csv'\n",
    "qa_with_link = pd.read_csv(qa_dir, encoding=\"iso-8859-1\", index_col=0)\n",
    "qa_with_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a highly knowledgeable assistant specializing in Furiosa's NPU SDK. Your task is to provide an short, but accurate responses to user queries.\n",
      "For each query, use the retrieved contents to generate a clear and compact explanation (1~3 sentences) to the question. Make sure your response is concise but comprehensive, ensuring the user can act on your guidance immediately. \n",
      "\n",
      "Question:\n",
      "{query}\n",
      "\n",
      "Context:\n",
      "{retrieved_contents}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_simple.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_simple = PromptTemplate(\n",
    "    template=\"\"\"You are a highly knowledgeable assistant specializing in Furiosa's NPU SDK. Your task is to provide short responses to user queries.\n",
    "For each query, extract the most relevant information from the retrieved contents to generate a clear and compact explanation to the question. Limit the response to within 3 sentences.\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Context:\n",
    "{retrieved_contents}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_compact = PromptTemplate(\n",
    "    template=\"\"\"Your task is to provide short 1-sentence answer to user queries based on the provided context. \n",
    "Question:\n",
    "{query}\n",
    "Context:\n",
    "{retrieved_contents}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I convert an ONNX file to an ENF file after quantization to later use it by simply opening a session?\n",
      "--------------------------------------------------------------------------------\n",
      "This text is a documentation for optimizing the performance of deep learning models on Furiosa AI's NPU (Neural Processing Unit). The optimization techniques are categorized into two levels: model level and runtime level.\n",
      "\n",
      "**Model Level Optimization**\n",
      "\n",
      "1.  **Knowledge Distillation**: This technique involves training a smaller student model to mimic the behavior of a larger teacher model. The student model is trained using the output of the teacher model as its target, which helps to transfer knowledge from the teacher model to the student model.\n",
      "2.  **Pruning**: Pruning involves removing redundant or unnecessary weights and connections in the neural network. This can help reduce the computational cost and memory requirements of the model.\n",
      "3.  **Quantization**: Quantization involves reducing the precision of the model's weights and activations from floating-point numbers to integers. This can help reduce the memory requirements and improve the performance of the model on NPU devices.\n",
      "4.  **Patch Size Optimization**: The patch size refers to the number of inputs that are processed together in a single batch. Increasing the patch size can improve the performance of the model, but it also increases the memory requirements.\n",
      "\n",
      "**Runtime Level Optimization**\n",
      "\n",
      "1.  **More Inference Concurrency (Number of Workers)**: Increasing the number of workers can help improve the performance of the model by allowing multiple inferences to be processed concurrently.\n",
      "2.  **Sync API vs Async APIs**: Sync API is a blocking API that waits for the completion of the inference, while Async APIs are non-blocking and allow multiple inferences to be requested simultaneously.\n",
      "\n",
      "**Async APIs**\n",
      "\n",
      "1.  **Queue API**: Queue API allows submitting inference requests without waiting for completion and waiting for the inference results asynchronously.\n",
      "2.  **Using Async/Await Syntax**: This syntax provides an easier way to implement a serving application using async/await API.\n",
      "\n",
      "Overall, these optimization techniques can help improve the performance of deep learning models on Furiosa AI's NPU devices.\n"
     ]
    }
   ],
   "source": [
    "question = qa_with_link['question'].values[0]\n",
    "print(question)\n",
    "print(\"--\"*40)\n",
    "\n",
    "# Step 1: VectorDB 검색\n",
    "semantic_results = chroma_retriever.retrieve(question)\n",
    "\n",
    "# Step 2: BM25 검색\n",
    "lexical_results = bm25_retriever.retrieve(question)\n",
    "\n",
    "# Step 3: Hybrid 검색\n",
    "retrieved_contents = hybrid_cc(semantic_results=semantic_results, lexical_results=lexical_results)\n",
    "\n",
    "# Step 4: Cutoff\n",
    "retrieved_contents_filtered = cutoff.postprocess_nodes(retrieved_contents)\n",
    "\n",
    "full_prompt_1 = prompt_compact.format(query=qa_with_link['question'].values[0], retrieved_contents=retrieved_contents_filtered[0].text[:200])\n",
    "result = llm.complete(full_prompt_1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to qa_with_generated_results-warboy_simple2.csv\n"
     ]
    }
   ],
   "source": [
    "version_name = 'warboy'\n",
    "# qa_dir = f'../../data/chatbot/qa-{version_name}_sdk.csv'\n",
    "# qa_with_link = pd.read_csv(qa_dir, encoding=\"utf-8\", index_col=0)\n",
    "\n",
    "qa_dir = '../../data/handmade-faq/qa-warboy_sdk.csv'\n",
    "qa_with_link = pd.read_csv(qa_dir, encoding=\"iso-8859-1\", index_col=0)\n",
    "\n",
    "# 새로운 컬럼 추가를 위해 DataFrame 복사\n",
    "qa_with_link['generated'] = \"\"  # 생성된 텍스트를 저장할 새로운 컬럼 초기화\n",
    "\n",
    "for idx, row in qa_with_link.iterrows():\n",
    "    question = row['question']\n",
    "    \n",
    "    # Step 1: VectorDB 검색\n",
    "    semantic_results = chroma_retriever.retrieve(question)\n",
    "\n",
    "    # Step 2: BM25 검색\n",
    "    lexical_results = bm25_retriever.retrieve(question)\n",
    "\n",
    "    # Step 3: Hybrid 검색\n",
    "    retrieved_contents = hybrid_cc(semantic_results=semantic_results, lexical_results=lexical_results)\n",
    "\n",
    "    # Step 4: Cutoff\n",
    "    retrieved_contents_filtered = cutoff.postprocess_nodes(retrieved_contents)\n",
    "\n",
    "    # Step 5: LLM 처리\n",
    "    if len(retrieved_contents_filtered) > 0:\n",
    "        # full_prompt = prompt.format(query=question, retrieved_contents=retrieved_contents_filtered[0].text)\n",
    "        full_prompt = prompt_simple.format(query=question, retrieved_contents=retrieved_contents_filtered[0].text)\n",
    "        result = llm.complete(full_prompt)\n",
    "        qa_with_link.at[idx, 'generated'] = result  # 결과를 'generated' 컬럼에 추가\n",
    "    else:\n",
    "        qa_with_link.at[idx, 'generated'] = \"No relevant content found\"  # 검색 결과가 없는 경우 기본 메시지 추가\n",
    "\n",
    "# 저장\n",
    "# output_file = f\"qa_with_generated_results-{version_name}.csv\"\n",
    "output_file = f\"qa_with_generated_results-{version_name}_simple3.csv\"\n",
    "qa_with_link.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SubBotResponse(BaseModel):\n",
    "    answer: str = Field(\n",
    "        default=\"\",\n",
    "        description=\"Answer of llm based on user question and given context\",\n",
    "    )\n",
    "    docs: List[str] = Field(\n",
    "        default=[],\n",
    "        description=\"List of reference_id of the metadata in Something to read.\",\n",
    "    )\n",
    "\n",
    "output_parser = PydanticOutputParser(output_cls=SubBotResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    ")\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "class RetrieverEvent(Event):\n",
    "    retrieved_nodes: List[NodeWithScore]\n",
    "\n",
    "class CutoffEvent(Event):\n",
    "    retrieved_nodes_with_score: List[NodeWithScore]\n",
    "\n",
    "class PostprocessEvent(Event):\n",
    "    retrieved_contents: List[dict]\n",
    "\n",
    "class PromptEvent(Event):\n",
    "    prompt: str\n",
    "\n",
    "class HybridFlow(Workflow):\n",
    "    llm = Ollama(model=\"llama3.1:70b\")\n",
    "\n",
    "    @step\n",
    "    async def retrieve(self, ctx: Context, ev: StartEvent) -> RetrieverEvent:\n",
    "        query = ev.question\n",
    "        ctx.data[\"query\"] = query\n",
    "        return RetrieverEvent(retrieved_nodes=hybrid_cc(query, 3, 0.18))\n",
    "\n",
    "    @step\n",
    "    async def cutoff(self, ev: RetrieverEvent) -> CutoffEvent:\n",
    "        retrieved_nodes = ev.retrieved_nodes\n",
    "        return CutoffEvent(\n",
    "            retrieved_nodes_with_score=cutoff.postprocess_nodes(retrieved_nodes)\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def postprocess(self, ev: CutoffEvent) -> PostprocessEvent:\n",
    "        retrieved_nodes_with_score = ev.retrieved_nodes_with_score\n",
    "        return PostprocessEvent(retrieved_contents=postprocess_nodes(retrieved_nodes_with_score))\n",
    "\n",
    "    @step\n",
    "    async def prompt(self, ctx: Context, ev: PostprocessEvent) -> PromptEvent:\n",
    "        query = ctx.data[\"query\"]\n",
    "        retrieved_contents = ev.retrieved_contents\n",
    "        return PromptEvent(\n",
    "            prompt=prompt.format(query=query, retrieved_contents=retrieved_contents)\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def generate(self, ev: PromptEvent) -> StopEvent:\n",
    "        prompt = ev.prompt\n",
    "        print(prompt)\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "        return StopEvent(result=str(response))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step retrieve\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'StartEvent' object has no attribute 'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m w \u001b[38;5;241m=\u001b[39m HybridFlow(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m w\u001b[38;5;241m.\u001b[39mrun(query\u001b[38;5;241m=\u001b[39mqa_with_link[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/langserve/lib/python3.11/site-packages/llama_index/core/workflow/workflow.py:323\u001b[0m, in \u001b[0;36mWorkflow.run.<locals>._run_workflow\u001b[0;34m()\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[1;32m    322\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mwrite_event_to_stream(StopEvent())\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[1;32m    326\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperation timed out after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/langserve/lib/python3.11/site-packages/llama_index/core/workflow/workflow.py:212\u001b[0m, in \u001b[0;36mWorkflow._start.<locals>._task\u001b[0;34m(name, queue, step, config)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mretry_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     delay \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mretry_policy\u001b[38;5;241m.\u001b[39mnext(\n\u001b[1;32m    215\u001b[0m         retry_start_at \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mtime(), attempts, e\n\u001b[1;32m    216\u001b[0m     )\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;66;03m# We're done retrying\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/langserve/lib/python3.11/site-packages/llama_index/core/workflow/workflow.py:208\u001b[0m, in \u001b[0;36mWorkflow._start.<locals>._task\u001b[0;34m(name, queue, step, config)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m         new_ev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# exit the retrying loop\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/langserve/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:297\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    290\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    291\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    295\u001b[0m )\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 297\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "Cell \u001b[0;32mIn[37], line 29\u001b[0m, in \u001b[0;36mHybridFlow.retrieve\u001b[0;34m(self, ctx, ev)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;129m@step\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, ctx: Context, ev: StartEvent) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RetrieverEvent:\n\u001b[0;32m---> 29\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[43mev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquestion\u001b[49m\n\u001b[1;32m     30\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RetrieverEvent(retrieved_nodes\u001b[38;5;241m=\u001b[39mhybrid_cc(query, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0.18\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/langserve/lib/python3.11/site-packages/llama_index/core/workflow/events.py:80\u001b[0m, in \u001b[0;36mEvent.__getattr__\u001b[0;34m(self, _Event__name)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[__name]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StartEvent' object has no attribute 'question'"
     ]
    }
   ],
   "source": [
    "w = HybridFlow(timeout=60, verbose=True)\n",
    "result = await w.run(query=qa_with_link['question'].values[1])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langserve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
