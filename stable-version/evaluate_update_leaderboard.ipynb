{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate and compare metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from metrics.evaluation_transition import evaluate_translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가를 위해 reference text 가져오기 \n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "translation_config = 'ko2en'\n",
    "target_lang = \"English\"  # 목표 언어\n",
    "\n",
    "data_dir = f\"../data/flores/\"\n",
    "result_dir = f\"../result/translate/{translation_config}/\"\n",
    "leaderboard_file = f\"../result/translate/{translation_config}/leaderboard.csv\"\n",
    "\n",
    "# 기존 리더보드 파일 로드 또는 새로 생성\n",
    "if os.path.exists(leaderboard_file):\n",
    "    leaderboard = pd.read_csv(leaderboard_file)\n",
    "else:\n",
    "    leaderboard = pd.DataFrame(columns=[\n",
    "        \"device-type\", \"device-name\", \"llm\", \"quantization\", \"calibration\",\n",
    "        \"BLEU\", \"METEOR\", \"BERTScore\", \"tps\"\n",
    "    ])\n",
    "    \n",
    "# Load reference data\n",
    "data_eng = load_text_file(f\"{data_dir}/devtest.eng_Latn\")\n",
    "data_kor = load_text_file(f\"{data_dir}/devtest.kor_Hang\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dudaji/miniconda3/envs/langserve/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/dudaji/miniconda3/envs/langserve/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1884347/774413979.py:58: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  leaderboard = pd.concat([leaderboard, new_entry], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and added GPU-A5000_qwen2.5:72b-Q4_K_M_calib-base.json to leaderboard.\n",
      "Processed and added NPU-RNGD_llama3.1-8B-Instruct-W8A8_calib-base.json to leaderboard.\n",
      "Processed and added GPU-A5000_llama3.1:70b-Q4_K_M_calib-base.json to leaderboard.\n",
      "Processed and added GPU-A5000_llama3.3:70b-Q4_K_M_calib-base.json to leaderboard.\n",
      "Processed and added GPU-A5000_llama3.1-Q4_K_M_calib-base.json to leaderboard.\n",
      "Leaderboard updated and saved to ../result/translate/ko2en/leaderboard.csv\n"
     ]
    }
   ],
   "source": [
    "active_metrics = ['BLEU', 'METEOR', 'BERTScore', 'tps']\n",
    "\n",
    "# Process each JSON file\n",
    "for filename in os.listdir(result_dir):\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    # Parse metadata from filename\n",
    "    name_head = filename.replace(\".json\", \"\")\n",
    "    metadata = {\n",
    "        \"device-type\": name_head.split(\"-\")[0],\n",
    "        \"device-name\": name_head.split(\"_\")[0].split(\"-\")[1],\n",
    "        \"llm\": name_head.split(\"_\")[1].split('-')[0],\n",
    "        \"quantization\": name_head.split(\"_calib\")[0].split('-')[-1],\n",
    "        \"calibration\": name_head.split(\"_calib-\")[1],\n",
    "    }\n",
    "\n",
    "    # Skip if already in leaderboard\n",
    "    if ((leaderboard[\"device-type\"] == metadata[\"device-type\"]) &\n",
    "        (leaderboard[\"device-name\"] == metadata[\"device-name\"]) &\n",
    "        (leaderboard[\"llm\"] == metadata[\"llm\"]) &\n",
    "        (leaderboard[\"quantization\"] == metadata[\"quantization\"]) &\n",
    "        (leaderboard[\"calibration\"] == metadata[\"calibration\"])).any():\n",
    "        print(f\"Skipping {filename}, already in leaderboard.\")\n",
    "        continue\n",
    "    \n",
    "    # Load translation results\n",
    "    with open(os.path.join(result_dir, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "        json_data = json.load(file)\n",
    "        translations = json_data.get(\"translations\", [])\n",
    "\n",
    "    # Evaluate translations\n",
    "    num_metrics = 4  # BLEU, METEOR, BERTScore, TPS\n",
    "    metrics = np.full((num_metrics, len(translations)), np.nan)\n",
    "\n",
    "    for i, result in enumerate(translations):\n",
    "        translation = result.get(\"translation\", \"\")\n",
    "        elapsed_time = result.get(\"elapsed_time\", 1e-6)  # Default time if not provided\n",
    "        ref_text = data_eng[i] if i < len(data_eng) else \"\"\n",
    "\n",
    "        # Evaluate translation\n",
    "        metric_result = evaluate_translation(\n",
    "            translation, ref_text, target_lang, elapsed_time, active_metrics\n",
    "        )\n",
    "        for j, metric_name in enumerate(active_metrics):\n",
    "            metrics[j, i] = metric_result.get(metric_name, np.nan)\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_metrics = {metric: np.nanmean(metrics[j, :]) for j, metric in enumerate(active_metrics)}\n",
    "\n",
    "    # Add to leaderboard\n",
    "    new_entry = pd.DataFrame([{\n",
    "        **metadata,\n",
    "        **avg_metrics\n",
    "    }])  # Create a DataFrame for the new entry\n",
    "\n",
    "    # Concatenate the new entry to the leaderboard\n",
    "    leaderboard = pd.concat([leaderboard, new_entry], ignore_index=True)\n",
    "    print(f\"Processed and added {filename} to leaderboard.\")\n",
    "\n",
    "# 리더보드 CSV 파일로 저장\n",
    "leaderboard.to_csv(leaderboard_file, index=False)\n",
    "print(f\"Leaderboard updated and saved to {leaderboard_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langserve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
