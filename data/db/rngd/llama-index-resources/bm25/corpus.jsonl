{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/furiosa_llm\/intro.html","title":"intro","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/furiosa_llm\/intro.html\", \"title\": \"intro\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/_sources\/furiosa_llm\/intro.rst \\\"Download source file\\\") * .pdf\\nFuriosa LLM ===========\\nFuriosa LLM [#](#furiosa-llm \\\"Link to this heading\\\") ====================================================\\nFuriosa LLM provides a high-performance inference engine for LLM models and Multi-Modal LLM models, Furiosa LLM is designed to provide the state-of-the-art serving optimization. The features of Furiosa LLM includes:\\n* vLLM-compatible API * Efficient KV cache management with PagedAttention * Continuous batching of incoming requests in serving * Quantization: INT4, INT8, FP8, GPTQ, AWQ * Data Parallelism and Pipeline Parallelism across multiple NPUs * Tensor Parallelism (planned in release 2024.2) across multiple NPUs * OpenAI-compatible API server * Various decoding algorithms, greedy search, beam search, top-k\/top-p, speculative decoding (planned) * HuggingFace model integration and hub support * HuggingFace PEFT support (planned)\\n[previous\\nRunning MLPerf\\u2122 Inference Benchmark](..\/getting_started\/furiosa_mlperf.html \\\"previous page\\\") [next\\nReferences](references.html \\\"next page\\\")\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/cloud_native_toolkit\/kubernetes\/device_plugin.html","title":"device_plugin","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"dcd59fbc-fb76-4f34-b6ec-ea88a833b047\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/cloud_native_toolkit\/kubernetes\/device_plugin.html\", \"title\": \"device_plugin\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/..\/_sources\/cloud_native_toolkit\/kubernetes\/device_plugin.rst \\\"Download source file\\\") * .pdf\\nInstalling Furiosa Device Plugin ================================\\nContents --------\\n* [Furiosa Device Plugin](#furiosa-device-plugin)   + [Configuration](#configuration)   + [Deploying Furiosa Device Plugin with Helm](#deploying-furiosa-device-plugin-with-helm)\\nInstalling Furiosa Device Plugin [#](#installing-furiosa-device-plugin \\\"Link to this heading\\\") ==============================================================================================\\nFuriosa Device Plugin [#](#furiosa-device-plugin \\\"Link to this heading\\\") ------------------------------------------------------------------------\\nThe Furiosa device plugin implements the [Kubernetes Device Plugin](https:\/\/kubernetes.io\/docs\/concepts\/extend-kubernetes\/compute-storage-net\/device-plugins\/) interface for FuriosaAI NPU devices, and its features are as follows:\\n* discovering the Furiosa NPU devices and register to a Kubernetes cluster. * tracking the health of the devices and report to a Kubernetes cluster. * running AI workload on the top of the Furiosa NPU devices within a Kubernetes cluster.\\n### Configuration [#](#configuration \\\"Link to this heading\\\")\\nThe Furiosa NPU can be integrated into the Kubernetes cluster in various configurations. A single NPU card can either be exposed as a single resource or partitioned into multiple resources. Partitioning into multiple resources allows for more granular control.\\nThe following table shows the available resource strategy:\\nResource Strategy\\n[#](#id1 \\\"Link to this table\\\")\\n| NPU Configuration | Resource Name | Resource Count Per Card | | --- | --- | --- | | legacy | beta.furiosa.ai\/npu | 1 | | generic | furiosa.ai\/rngd | 1 |\\nThe helm chart of Furiosa device plugin is available at [furiosa-ai\/helm-charts](https:\/\/github.com\/furiosa-ai\/helm-charts) .\\nFollowing shows default values of the helm chart.\\n``` config:   resourceStrategy: generic   debugMode: false   disabledDeviceUUIDListMap:\\n```\\n### Deploying Furiosa Device Plugin with Helm [#](#deploying-furiosa-device-plugin-with-helm \\\"Link to this heading\\\")\\nThe Furiosa device plugin helm chart is available at [furiosa-ai\/helm-charts](https:\/\/github.com\/furiosa-ai\/helm-charts) . To configure deployment as you need, you can modify `charts\/furiosa-device-plugin\/values.yaml` .\\n* If resourceStrategy is not specified, the default value is   `\\\"generic\\\"`   . * If debugMode is not specified, the default value is   `false`   . * If disabledDeviceUUIDListMap is not specified, the default value is empty list   `[]`   .\\nYou can deploy the Furiosa Device Plugin by running the following commands:\\n``` helm repo add furiosa https:\/\/furiosa-ai.github.io\/helm-charts helm repo update helm install furiosa-device-plugin furiosa\/furiosa-device-plugin -n kube-system\\n```\\n[previous\\nInstalling Furiosa Feature Discovery](feature_discovery.html \\\"previous page\\\") [next\\nInstalling Furiosa Metrics Exporter](metrics_exporter.html \\\"next page\\\")\\nContents\\n* [Furiosa Device Plugin](#furiosa-device-plugin)   + [Configuration](#configuration)   + [Deploying Furiosa Device Plugin with Helm](#deploying-furiosa-device-plugin-with-helm)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/getting_started\/furiosa_mlperf.html","title":"furiosa_mlperf","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"3bb3bd0a-7bdc-45a7-8e3e-e556a52d7eda\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/getting_started\/furiosa_mlperf.html\", \"title\": \"furiosa_mlperf\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/_sources\/getting_started\/furiosa_mlperf.rst \\\"Download source file\\\") * .pdf\\nRunning MLPerf\\u2122 Inference Benchmark ===================================\\nContents --------\\n* [Installing   `furiosa-mlperf`   command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1)   + [SYNOPSIS](#synopsis)   + [Examples](#examples) * [Running   `furiosa-mlperf`   in container environment](#running-furiosa-mlperf-in-container-environment)\\nRunning MLPerf\\u2122 Inference Benchmark [#](#running-mlperf-inference-benchmark \\\"Link to this heading\\\") ===================================================================================================\\nMLPerf\\u2122 is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems.\\nFuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf\\u2122 Inference Benchmark using the FuriosaAI Software Stack.\\nNote  `furiosa-mlperf` is based on MLPerf\\u2122 Inference Benchmark v4.1.\\nThe only exception is that we replaced the Llama2 benchmark with one using Llama 3.1.\\nInstalling `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command \\\"Link to this heading\\\") --------------------------------------------------------------------------------------------------\\nTo install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following:\\nThe minimum requirements for `furiosa-mlperf` are as follows:\\n* Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers (   [Setting up APT server](prerequisites.html#aptsetup)   ) * About 100GB storage space (only for the Llama 3.1 70B)\\nThen, please install the `furiosa-mlperf` package as follows:\\n``` sudo apt update sudo apt install -y furiosa-mlperf\\n```\\nThis command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` .\\nRunning MLPerf Inference Benchmark [#](#id1 \\\"Link to this heading\\\") -------------------------------------------------------------------\\n### SYNOPSIS [#](#synopsis \\\"Link to this heading\\\")\\nThe `furiosa-mlperf` command provides the following subcommands:\\n``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z)\\nUsage: furiosa-mlperf <COMMAND>\\nCommands:   bert-offline       Run BERT benchmark with offline scenario   bert-server        Run BERT benchmark with server scenario   gpt-j-offline      Run GPT-J benchmark with offline scenario   gpt-j-server       Run GPT-J benchmark with server scenario   llama-3.1-offline  Run Llama 3.1 benchmark with offline scenario   llama-3.1-server   Run Llama 3.1 benchmark with server scenario   help               Print this message or the help of the given subcommand(s)\\nOptions:   -h, --help     Print help   -V, --version  Print version\\n```\\n### Examples [#](#examples \\\"Link to this heading\\\")\\n* BERT benchmark      The BERT benchmark is based on running with a single RNGD.      + Server Scenario          To run BERT Large serving inference benchmark, you can use the following command:          ```     furiosa-mlperf bert-server .\/bert-large .\/bert-server-result --device-mesh \\\"npu:0:*\\\"          ```   + Offline Scenario          To run BERT Large offline inference benchmark, you can use the following command:          ```     furiosa-mlperf bert-offline .\/bert-large .\/bert-offline-result --device-mesh \\\"npu:0:*\\\"          ``` * GPT-J benchmark      The GPT-J benchmark is based on running with a single RNGD.      + Server Scenario          To run GPT-J 6B serving inference benchmark, you can use the following command:          ```     furiosa-mlperf gpt-j-server .\/gpt-j-6b .\/gpt-j-server-result          ```   + Offline Scenario          To run GPT-J 6B offline inference benchmark, you can use the following command:          ```     furiosa-mlperf gpt-j-offline .\/gpt-j-6b .\/gpt-j-offline-result          ``` * Llama 3.1 benchmark      The Llama 3.1 benchmark is based on running with four RNGDs.      + Server Scenario          To run Llama 3.1 70B serving inference benchmark, you can use the following command:          ```     furiosa-mlperf llama-3.1-server .\/llama-3.1-70b .\/llama-3.1-server-result          ```   + Offline Scenario          To run Llama 3.1 70B offline inference benchmark, you can use the following command:          ```     furiosa-mlperf llama-3.1-offline .\/llama-3.1-70b .\/llama-3.1-offline-result          ``` * Common      Once the process completes, it writes the results to a file in the specified results directory.   You can open this file to view a summary of the results.      ```   cat gpt-j-offline-result\/mlperf_log_summary.txt      ```         ```   ================================================   MLPerf Results Summary   ================================================   SUT name : GPT-J SUT   Scenario : Offline   Mode     : PerformanceOnly   Samples per second: 11.842   Tokens per second (inferred): 817.095   Result is : VALID     Min duration satisfied : Yes     Min queries satisfied : Yes     Early stopping satisfied: Yes      ```\\nRunning `furiosa-mlperf` in container environment [#](#running-furiosa-mlperf-in-container-environment \\\"Link to this heading\\\") ------------------------------------------------------------------------------------------------------------------------------\\nFuriosaAI provides a containerized version of the `furiosa-mlperf` command. If you use the containerized version, you can run the `furiosa-mlperf` command without installing the FuriosaAI Software Stack on your host system or you can run the command on Kubernetes environment.\\nTo run the `furiosa-mlperf` container, you can use the following command:\\n(Assumes model artifacts exist in `\/opt\/gpt-j-6b` directory)\\n``` $ docker run -it --rm --privileged \\\\   -v \/opt\/gpt-j-6b:gpt-j-6b furiosa-mlperf:2024.1.0 bash\\n(container) # furiosa-mlperf gpt-j-offline \/gpt-j-6b \/gpt-j-offline-result\\n```\\nTo run in a containerized environment, refer to the examples provided in this document.\\nWarning\\nThe example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](..\/cloud_native_toolkit\/intro.html#cloudnativetoolkit)\\n[previous\\nQuick Start with Furiosa LLM](furiosa_llm.html \\\"previous page\\\") [next\\nFuriosa LLM](..\/furiosa_llm\/intro.html \\\"next page\\\")\\nContents\\n* [Installing   `furiosa-mlperf`   command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1)   + [SYNOPSIS](#synopsis)   + [Examples](#examples) * [Running   `furiosa-mlperf`   in container environment](#running-furiosa-mlperf-in-container-environment)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/furiosa_llm\/furiosa-llm-serve.html","title":"furiosa-llm-serve","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"a3d94379-304a-4dbc-8300-39169378bfd5\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/furiosa_llm\/furiosa-llm-serve.html\", \"title\": \"furiosa-llm-serve\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/_sources\/furiosa_llm\/furiosa-llm-serve.rst \\\"Download source file\\\") * .pdf\\nOpenAI Compatible Server ========================\\nContents --------\\n* [Preparing Chat Templates](#preparing-chat-templates) * [Launching the Server](#launching-the-server)   + [Arguments for the serve command](#arguments-for-the-serve-command) * [Examples](#examples)   + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds)   + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api)\\nOpenAI Compatible Server [#](#openai-compatible-server \\\"Link to this heading\\\") ==============================================================================\\nThe `furiosa-llm` package includes an OpenAI-compatible server that can interact with OpenAI clients (and, of course, HTTP clients as well), supporting the `\/v1\/chat\/completions` and `\/v1\/completions` APIs. In this section, we will explain how to launch the OpenAI-compatible furiosa-llm\\nserver.\\nTip\\nYou can learn more about the OpenAI API at [Completions API](https:\/\/platform.openai.com\/docs\/api-reference\/completions) and [Chat API](https:\/\/platform.openai.com\/docs\/api-reference\/chat) .\\nTo launch the server, you must prepare: (1) the FuriosaAI LLM Engine artifact and (2) a chat template for the model. To download the FuriosaAI LLM Engine artifact, follow the instructions provided through our distribution channels or contact our sales team. To prepare the chat template, follow the instructions in the following section.\\nPreparing Chat Templates [#](#preparing-chat-templates \\\"Link to this heading\\\") ------------------------------------------------------------------------------\\nFuriosa SDK 2024.1.0 (alpha) uses Transformers v4.31.0, which does not include a chat template. Therefore, to support `\/v1\/chat\/completions` , you must provide a chat template yourself. This constraint will be removed in future releases.\\nWarning\\nThis document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future.\\nIf you have access to the Llama repositories on Hugging Face, you can obtain the chat template for Llama using the following commands:\\n``` # Prerequisite: create a separate environment to install the latest Transformers version pip install \\\"transformers>=4.34.0\\\" python - <<EOF from transformers import AutoTokenizer tok = AutoTokenizer.from_pretrained('meta-llama\/Meta-Llama-3.1-70B-Instruct') with open('chat_template.tpl', 'w') as f:     f.write(tok.chat_template) EOF\\n```\\nLaunching the Server [#](#launching-the-server \\\"Link to this heading\\\") ----------------------------------------------------------------------\\nYou can launch the server using the furiosa-llm serve\\ncommand.\\n### Arguments for the serve command [#](#arguments-for-the-serve-command \\\"Link to this heading\\\")\\n``` usage: furiosa-llm serve [-h] --model {furiosa-ai\/llama-3-1-70b,furiosa-ai\/llama-3-1-8b,furiosa-ai\/fake-llm} --artifact ARTIFACT [--host HOST] [--port PORT]                      --chat-template CHAT_TEMPLATE [--response-role RESPONSE_ROLE] [-pp PIPELINE_PARALLEL_SIZE] [-tp TENSOR_PARALLEL_SIZE] [--devices DEVICES]\\noptions: -h, --help            show this help message and exit --model {furiosa-ai\/llama-3-1-70b,furiosa-ai\/llama-3-1-8b,furiosa-ai\/fake-llm}                         The model to use. Currently only one model is supported per server. --artifact ARTIFACT   Path to Furiosa LLM Engine artifact --host HOST           Host to bind the server to --port PORT           Port to bind the server to --chat-template CHAT_TEMPLATE                         Path to chat template file (must be a jinja2 template) --response-role RESPONSE_ROLE                         Response role for \/v1\/chat\/completions API (default: 'assistant') -pp PIPELINE_PARALLEL_SIZE, --pipeline-parallel-size PIPELINE_PARALLEL_SIZE                         Number of pipeline stages. -tp TENSOR_PARALLEL_SIZE, --tensor-parallel-size TENSOR_PARALLEL_SIZE                         Number of tensor parallel replicas. --devices DEVICES     Devices to use (e.g. \\\"npu:0:*,npu:1:*\\\"). If unspecified, all available devices from the host will be used.\\n```\\nExamples [#](#examples \\\"Link to this heading\\\") ----------------------------------------------\\n### LLaMA-3.1-70B with 4 RNGDs [#](#llama-3-1-70b-with-4-rngds \\\"Link to this heading\\\")\\n``` furiosa-llm serve \\\\ --model {path to mlperf-llama-3-1-fp8-pp4} \\\\ -tp 4 -pp 4 --devices \\\"npu:0:*,npu:1:*,npu:2:*,npu:3:*\\\" \\\\ --chat-template {path to chat template}\\n```\\n### LLaMA-3.1-8B with Single RNGD [#](#llama-3-1-8b-with-single-rngd \\\"Link to this heading\\\")\\n``` furiosa-llm serve \\\\ --model {path to mlperf-llama-3-1-8b-fp8} \\\\ -tp 4 -pp 1 --devices \\\"npu:0:*\\\" \\\\  # You may choose arbitrary device index, if multiple devices are on host --chat-template {path to chat template}\\n```\\nUsing OpenAI Client [#](#using-openai-client \\\"Link to this heading\\\") --------------------------------------------------------------------\\nYou can use two APIs: `client.chat.completions` and `client.completions` . You can also set `stream=True` to receive a streaming response.\\nTip\\nYou can install the OpenAI Python client using the following command:\\n``` pip install openai\\n```\\n``` import openai\\nHOST = \\\"localhost:8000\\\" openai.api_base = f\\\"http:\/\/{HOST}\/v1\\\" openai.api_key = \\\"0000\\\"\\nstream_chat_completion = openai.ChatCompletion.create(     model=\\\"\\\",     messages=[         {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant.\\\"},         {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the largest animal in the world?\\\"},     ],     stream=True, )\\nfor completion in stream_chat_completion:     content = completion.choices[0].delta.get(\\\"content\\\")     if content:         print(content, end=\\\"\\\")\\n```\\nThe compatibility with OpenAI API [#](#the-compatibility-with-openai-api \\\"Link to this heading\\\") ------------------------------------------------------------------------------------------------\\nCurrently, `furiosa\\nserve` supports the following OpenAI API parameters: You can find more about each parameter at [Completions API](https:\/\/platform.openai.com\/docs\/api-reference\/completions) and [Chat API](https:\/\/platform.openai.com\/docs\/api-reference\/chat) .\\nWarning\\nPlease note that using `use_beam_search` with `stream` is not allowed because the beam search cannot determine the tokens until the end of the sequence.\\nIn 2024.1 release, `n` works only for beam search and it will be fixed in the next release.\\n* `n` * `temperature` * `top_p` * `top_k` * `early_stopping` * `length_penalty` * `max_tokens` * `min_tokens` * `use_beam_search` * `best_of` \\n[previous\\nFuriosa LLM](intro.html \\\"previous page\\\") [next\\nReferences](references.html \\\"next page\\\")\\nContents\\n* [Preparing Chat Templates](#preparing-chat-templates) * [Launching the Server](#launching-the-server)   + [Arguments for the serve command](#arguments-for-the-serve-command) * [Examples](#examples)   + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds)   + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/","title":"","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"13853744-dc18-4c98-b1c2-4f5806b28514\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/\", \"title\": \"\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](_sources\/index.rst \\\"Download source file\\\") * .pdf\\nFuriosaAI Developer Center ==========================\\nContents --------\\n* [Overview](#overview) * [Getting Started](#getting-started) * [Cloud Native Toolkit](#cloud-native-toolkit) * [Device Management](#device-management) * [Customer Support](#customer-support)\\nFuriosaAI Developer Center [#](#furiosaai-developer-center \\\"Link to this heading\\\") ==================================================================================\\nWelcome to the FuriosaAI Developer Center. FuriosaAI provides an streamlined software stack for deep learning model inference on FuriosaAI NPUs. This document provides a guide to easily perform the entire workflow of writing inference applications, from starting with PyTorch model to model quantization, serving, and production deployment.\\nWarning\\nThis document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future.\\nOverview [#](#overview \\\"Link to this heading\\\") ----------------------------------------------\\n* [FuriosaAI RNGD](overview\/rngd.html#rngd)   : RNGD Hardware Specification, and features * [FuriosaAI\\u2019s Software Stack](overview\/software_stack.html#softwarestack)   : An overview of the FuriosaAI software stack * [Supported Models](overview\/supported_models.html#supportedmodels)   : A list of supported models * [What\\u2019s New](whatsnew\/index.html#whatsnew)   : New features and changes in the latest release * [Roadmap](overview\/roadmap.html#roadmap)   : The future roadmap of FuriosaAI Software Stack\\nGetting Started [#](#getting-started \\\"Link to this heading\\\") ------------------------------------------------------------\\n* [Installing Prerequisites](getting_started\/prerequisites.html#installingprerequisites)   : How to install the prerequisites for FuriosaAI Software Stack * [Quick Start with Furiosa LLM](getting_started\/furiosa_llm.html#gettingstartedfuriosallm) * [Running MLPerf\\u2122 Inference Benchmark](getting_started\/furiosa_mlperf.html#gettingstartedfuriosamlperf)\\nCloud Native Toolkit [#](#cloud-native-toolkit \\\"Link to this heading\\\") ----------------------------------------------------------------------\\n* [Cloud Native Toolkit](cloud_native_toolkit\/intro.html#cloudnativetoolkit)   : An overview of the Cloud Native Toolkit * [Kubernetes Support](cloud_native_toolkit\/kubernetes.html#kubernetes)   : An overview of the Kubernetes Support\\nDevice Management [#](#device-management \\\"Link to this heading\\\") ----------------------------------------------------------------\\n* [furiosa-smi](device_management\/furiosa_smi.html#furiosasmi)   : A command line utility for managing FuriosaAI NPUs\\nCustomer Support [#](#customer-support \\\"Link to this heading\\\") --------------------------------------------------------------\\n* [FuriosaAI Homepage](https:\/\/furiosa.ai) * [FuriosaAI Forum](https:\/\/furiosa-ai.discourse.group\/) * [FuriosaAI Customer Portal](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals\/) * [FuriosaAI Warboy SDK Document](https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/)\\n[next\\nFuriosaAI RNGD](overview\/rngd.html \\\"next page\\\")\\nContents\\n* [Overview](#overview) * [Getting Started](#getting-started) * [Cloud Native Toolkit](#cloud-native-toolkit) * [Device Management](#device-management) * [Customer Support](#customer-support)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/cloud_native_toolkit\/kubernetes.html","title":"kubernetes","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"2b294227-f255-492c-b642-46e100e59705\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/cloud_native_toolkit\/kubernetes.html\", \"title\": \"kubernetes\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/_sources\/cloud_native_toolkit\/kubernetes.rst \\\"Download source file\\\") * .pdf\\nKubernetes Support ==================\\nKubernetes Support [#](#kubernetes-support \\\"Link to this heading\\\") ==================================================================\\nWe do support the following versions of Kubernetes and CRI runtime:\\n* Kubernetes: v1.24.0 or later * helm v3.0.0 or later * CRI Runtime:   [containerd](https:\/\/github.com\/containerd\/containerd)   or   [CRI-O](https:\/\/github.com\/cri-o\/cri-o)\\nNote\\nDocker is officially deprecated as a container runtime in Kubernetes. It is recommended to use containerd or CRI-O as a container runtime. Otherwise you may face unexpected issues with the device plugin. For more information, see [here](https:\/\/kubernetes.io\/blog\/2020\/12\/02\/dont-panic-kubernetes-and-docker\/) .\\nKubernetes Support\\n* [Installing Furiosa Feature Discovery](kubernetes\/feature_discovery.html) * [Installing Furiosa Device Plugin](kubernetes\/device_plugin.html) * [Installing Furiosa Metrics Exporter](kubernetes\/metrics_exporter.html) * [Scheduling NPUs](kubernetes\/scheduling_npus.html)\\n[previous\\nCloud Native Toolkit](intro.html \\\"previous page\\\") [next\\nInstalling Furiosa Feature Discovery](kubernetes\/feature_discovery.html \\\"next page\\\")\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/overview\/roadmap.html","title":"roadmap","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"d4927eaf-a1a0-40bd-9624-79db4213c5fc\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/overview\/roadmap.html\", \"title\": \"roadmap\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/_sources\/overview\/roadmap.rst \\\"Download source file\\\") * .pdf\\nRoadmap =======\\nContents --------\\n* [Latest Recent Release](#latest-recent-release) * [Future Releases](#future-releases)   + [2024.2.0 (beta 0) - November, 2024](#beta-0-november-2024)   + [2024.3.0 (beta 1) - December, 2024](#beta-1-december-2024)\\nRoadmap [#](#roadmap \\\"Link to this heading\\\") ============================================\\nFurisaAI strives to deliver the releases for each month, while offering patch releases. This page shows the forward-looking roadmap of ongoing & upcoming projects and when they are expected to land, broken down by areas on [our software stack](software_stack.html#softwarestack) .\\nLatest Recent Release [#](#latest-recent-release \\\"Link to this heading\\\") ------------------------------------------------------------------------\\nThe latest release is 2024.1.0 (alpha) on October 11, 2024. You can find the release notes [here](..\/whatsnew\/index.html#whatsnew) .\\nFuture Releases [#](#future-releases \\\"Link to this heading\\\") ------------------------------------------------------------\\nNote\\nThe roadmap is subject to change and may not reflect the final product.\\n### 2024.2.0 (beta 0) - November, 2024 [#](#beta-0-november-2024 \\\"Link to this heading\\\")\\n* Model support:      + Language Models: CodeLLaaMA2, Vicuna, Solar, EXAONE-3.0   + Vision Models: MobileNetV1, MobileNetV2, ResNet152, ResNet50, EfficientNet, YOLOv8m, .. * (Furiosa LLM) Tensor Parallelism support Phase 1: Intra-chip * Torch 2.4.1 support * CPU memory swapping in Furiosa LLM\\n### 2024.3.0 (beta 1) - December, 2024 [#](#beta-1-december-2024 \\\"Link to this heading\\\")\\n* Model support: TBD * (Furiosa LLM) Tensor Parallelism support Phase 2: Inter-chip * `torch.compile()`   backend * Huggingface Optimum integration\\n[previous\\nWhat\\u2019s New](..\/whatsnew\/index.html \\\"previous page\\\") [next\\nInstalling Prerequisites](..\/getting_started\/prerequisites.html \\\"next page\\\")\\nContents\\n* [Latest Recent Release](#latest-recent-release) * [Future Releases](#future-releases)   + [2024.2.0 (beta 0) - November, 2024](#beta-0-november-2024)   + [2024.3.0 (beta 1) - December, 2024](#beta-1-december-2024)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/overview\/supported_models.html","title":"supported_models","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"4f19d9ef-0eb7-40bf-a693-6e2d80ddb54a\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/overview\/supported_models.html\", \"title\": \"supported_models\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/_sources\/overview\/supported_models.rst \\\"Download source file\\\") * .pdf\\nSupported Models ================\\nContents --------\\n* [Decoder-only Models](#decoder-only-models) * [Encoder-only Models](#encoder-only-models)\\nSupported Models [#](#supported-models \\\"Link to this heading\\\") ==============================================================\\nFuriosaAI Software Stack supports a variety of Transformer-based models in HuggingFace Hub. The following is the list of model architectures that are currently supported by Furiosa SDK. If your model is based on the following architectures, you can use Furiosa SDK to compile, quantize, and run the model on FuriosaAI RNGD.\\nDecoder-only Models [#](#decoder-only-models \\\"Link to this heading\\\") --------------------------------------------------------------------\\nThe following models are supported for decoding only:\\nDecoder-only Models\\n[#](#id1 \\\"Link to this table\\\")\\n| Architecture | Model Name | Example HuggingFace Models | | --- | --- | --- | | `LlamaForCausalLM` | Llama 2, Llama 3.1 | `meta-llama\/Llama-2-70b-hf` , `meta-llama\/Meta-Llama-3.1-70B` , `meta-llama\/Meta-Llama-3-70B-Instruct` , `meta-llama\/Meta-Llama-3.1-8B` , `meta-llama\/Llama-3.1-8B-Instruct` , .. | | `GPTJForCausalLM` | GPT-J | `EleutherAI\/gpt-j-6b` |\\nEncoder-only Models [#](#encoder-only-models \\\"Link to this heading\\\") --------------------------------------------------------------------\\nEncoder-only Models\\n[#](#id2 \\\"Link to this table\\\")\\n| Architecture | Model Name | Example HuggingFace Models | | --- | --- | --- | | `BertForQuestionAnswering` | Bert | `google-bert\/bert-large-uncased` , `google-bert\/bert-base-uncased` , .. |\\n[previous\\nFuriosaAI\\u2019s Software Stack](software_stack.html \\\"previous page\\\") [next\\nWhat\\u2019s New](..\/whatsnew\/index.html \\\"next page\\\")\\nContents\\n* [Decoder-only Models](#decoder-only-models) * [Encoder-only Models](#encoder-only-models)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/overview\/software_stack.html","title":"software_stack","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"0acdfa06-7dff-4603-9a5c-dbc4e3310580\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/overview\/software_stack.html\", \"title\": \"software_stack\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/_sources\/overview\/software_stack.rst \\\"Download source file\\\") * .pdf\\nFuriosaAI\\u2019s Software Stack ==========================\\nContents --------\\n* [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support)\\nFuriosaAI\\u2019s Software Stack [#](#furiosaai-s-software-stack \\\"Link to this heading\\\") ==================================================================================\\nFuriosaAI provides the streamlined software stack to allow FuriosaAI NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials. The above diagram demonstrates the SW stack provided by FuriosaAI, by layers.\\nThe following outlines the key components.\\nKernel Driver, Firmware, and PE Runtime [#](#kernel-driver-firmware-and-pe-runtime \\\"Link to this heading\\\") ----------------------------------------------------------------------------------------------------------\\nThe kernel device driver enables the Linux operating system to recognize NPU devices and expose them as Linux device files. The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE). PERT is responsible for communicating with the host\\u2019s runtime and scheduling, managing the resources of PEs to execute NPU tasks.\\nFuriosa Compiler [#](#furiosa-compiler \\\"Link to this heading\\\") --------------------------------------------------------------\\nFuriosa compiler analyzes, optimizes a model graph, and generates a NPU executable program. The operation passes involve graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. When `torch.compile()` backend, `FuriosaBackend` is used or `furiosa-llm` is used, the Furiosa Compiler is transparently used to generate NPU executable programs for Runtime.\\nFuriosa Runtime [#](#furiosa-runtime \\\"Link to this heading\\\") ------------------------------------------------------------\\nRuntime loads multiple executable NPU programs generated by Furiosa compiler, and run them on the NPU. A single model can be compiled into multiple executable programs according to model architectures and applications. Runtime is responsible for scheduling NPU programs and managing computation and memory resource on NPUs and CPUs. Also, Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs.\\nFuriosa Model Compressor (Quantizer) [#](#furiosa-model-compressor-quantizer \\\"Link to this heading\\\") ----------------------------------------------------------------------------------------------------\\nFuriosa Model Compressor is a library as well as toolkit for model calibration and quantization. Model quantization is a powerful technique to reduce memory footprint, computation cost, inference latency and power consumption. Furiosa Model Compressor provides post-training quantization methods, such as\\n* BF16 (W16A16) * INT8 Weight-Only (W8A16) * FP8 (W8A8) * INT8 SmoothQuant (W8A8) * INT4 Weight-Only (W4A16 AWQ \/ GPTQ) (Planned in release 2024.2)\\nFuriosa LLM [#](#furiosa-llm \\\"Link to this heading\\\") ----------------------------------------------------\\nFuriosa LLM provides a high-performance inference engine for LLM models, such as Llama 3.1 70B, 8B, GPT-J, and Bert. Furiosa LLM is designed to provide the state-of-the-art serving optimization for LLM models. The key features of Furiosa LLM include vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and OpenAI-compatible API server. You can find further information at [Furiosa LLM](..\/furiosa_llm\/intro.html#furiosallm) .\\nKubernetes Support [#](#kubernetes-support \\\"Link to this heading\\\") ------------------------------------------------------------------\\nKubernetes, an open-source platform designed to manage containerized applications and services, is extensively adopted by various companies due to its robust capabilities for deploying, scaling, and automating containerized workloads. FuriosaAI software stack also offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment.\\nFuriosaAI\\u2019s device plugin enables Kubernetes clusters to recognize FuriosaAI\\u2019s NPUs and allows NPUs to be scheduled for workloads and services that require them. This feature allows users to easily deploy AI workloads with FuriosaAI NPUs on Kubernetes, enabling efficient resource utilization and scaling.\\nYou can find more information about Kubernetes support in the [Cloud Native Toolkit](..\/cloud_native_toolkit\/intro.html#cloudnativetoolkit) .\\n[previous\\nFuriosaAI RNGD](rngd.html \\\"previous page\\\") [next\\nSupported Models](supported_models.html \\\"next page\\\")\\nContents\\n* [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/furiosa_llm\/references\/sampling_params.html","title":"sampling_params","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"da255daf-c8d3-430e-b976-ad096f3a9ad7\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/furiosa_llm\/references\/sampling_params.html\", \"title\": \"sampling_params\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/..\/_sources\/furiosa_llm\/references\/sampling_params.rst \\\"Download source file\\\") * .pdf\\nSamplingParams ==============\\nContents --------\\n* [`SamplingParams`](#furiosa_llm.SamplingParams)\\nSamplingParams [#](#samplingparams \\\"Link to this heading\\\") ==========================================================\\n*class* furiosa\\\\_llm.\\nSamplingParams\\n(\\n*\\\\** ,\\n*n\\n:\\nint\\n=\\n1* ,\\n*best\\\\_of\\n:\\nint\\n|\\nNone\\n=\\nNone* ,\\n*temperature\\n:\\nfloat\\n=\\n1.0* ,\\n*top\\\\_p\\n:\\nfloat\\n=\\n1.0* ,\\n*top\\\\_k\\n:\\nint\\n=\\n-1* ,\\n*use\\\\_beam\\\\_search\\n:\\nbool\\n=\\nFalse* ,\\n*length\\\\_penalty\\n:\\nfloat\\n=\\n1.0* ,\\n*early\\\\_stopping\\n:\\nbool\\n|\\nstr\\n=\\nFalse* ,\\n*max\\\\_tokens\\n:\\nint\\n=\\n16* ,\\n*min\\\\_tokens\\n:\\nint\\n=\\n0* )\\n[[source]](..\/..\/_modules\/furiosa_llm\/sampling_params.html#SamplingParams) [#](#furiosa_llm.SamplingParams \\\"Link to this definition\\\")\\nBases: `object`  Sampling parameters for text generation.\\nThe default parameters represents greedy search.\\nParameters :\\n* **n**   \\u2013 Number of output sequences to return for the given prompt. * **best\\\\_of**   \\u2013 Number of output sequences that are generated from the prompt.   From these   best\\\\_of      sequences, the top   n      sequences are returned.   best\\\\_of      must be greater than or equal to   n      . This is treated as   the beam width when   use\\\\_beam\\\\_search      is True. By default,   best\\\\_of      is set to   n      . * **temperature**   \\u2013 Float that controls the randomness of the sampling. Lower   values make the model more deterministic, while higher values make   the model more random. Zero means greedy sampling. * **top\\\\_p**   \\u2013 Float that controls the cumulative probability of the top tokens   to consider. Must be in (0, 1]. Set to 1 to consider all tokens. * **top\\\\_k**   \\u2013 Integer that controls the number of top tokens to consider. Set   to -1 to consider all tokens. * **use\\\\_beam\\\\_search**   \\u2013 Whether to use beam search instead of sampling. * **length\\\\_penalty**   \\u2013 Float that penalizes sequences based on their length.   Used in beam search. * **early\\\\_stopping**   \\u2013 Controls the stopping condition for beam search. It   accepts the following values:   True      , where the generation stops as   soon as there are   best\\\\_of      complete candidates;   False      , where an   heuristic is applied and the generation stops when is it very   unlikely to find better candidates;   \\u201cnever\\u201d      , where the beam search   procedure only stops when there cannot be better candidates   (canonical beam search algorithm). * **max\\\\_tokens**   \\u2013 Maximum number of tokens to generate per output sequence. * **min\\\\_tokens**   \\u2013 Minimum number of tokens to generate per output sequence   before EOS or stop\\\\_token\\\\_ids can be generated\\n[previous\\nLLM class](llm.html \\\"previous page\\\") [next\\nCloud Native Toolkit](..\/..\/cloud_native_toolkit\/intro.html \\\"next page\\\")\\nContents\\n* [`SamplingParams`](#furiosa_llm.SamplingParams)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/device_management\/furiosa_smi.html","title":"furiosa_smi","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"edfa6fb4-e88c-4fa8-a6aa-9986213aa6ab\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/device_management\/furiosa_smi.html\", \"title\": \"furiosa_smi\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/_sources\/device_management\/furiosa_smi.rst \\\"Download source file\\\") * .pdf\\nfuriosa-smi ===========\\nContents --------\\n* [Installing   `furiosa-smi`   command](#installing-furiosa-smi-command)   + [Synopsis](#synopsis)   + [`furiosa-smi          info`](#furiosa-smi-info)   + [`furiosa-smi          status`](#furiosa-smi-status)   + [`furiosa-smi          ps`](#furiosa-smi-ps)   + [`furiosa-smi          topo`](#furiosa-smi-topo)\\nfuriosa-smi [#](#furiosa-smi \\\"Link to this heading\\\") ====================================================\\nThe `furiosa-smi` command provides a variety of subcommands and has the ability to obtain information or control the device.\\nInstalling `furiosa-smi` command [#](#installing-furiosa-smi-command \\\"Link to this heading\\\") --------------------------------------------------------------------------------------------\\nTo install the `furiosa-smi` command, you need to install `furiosa-smi` as following:\\nThe minimum requirements for `furiosa-smi` are as follows:\\n* Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers (   [Setting up APT server](..\/getting_started\/prerequisites.html#aptsetup)   )\\nThen, please install the `furiosa-smi` package as follows:\\n``` sudo apt update sudo apt install -y furiosa-smi\\n```\\nThis command installs packages `furiosa-libsmi` and `furiosa-smi` .\\n### Synopsis [#](#synopsis \\\"Link to this heading\\\")\\n``` furiosa-smi <sub command> [option] ..\\n```\\n### `furiosa-smi info` [#](#furiosa-smi-info \\\"Link to this heading\\\")\\nAfter installing the kernel driver, you can use the `furiosa-smi` command to check whether the NPU device is recognized. Currently, this command provides the `furiosa-smi\\ninfo` command to output temperature, power consumption and PCI information of the NPU device. If the device is not visible with this command after mounting it on the machine, please install the driver. If you add the `--full` option to the `info` command, you can see the device\\u2019s UUID and serial number information together.\\n``` $ furiosa-smi info +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware       | Temp.   | Power   | PCI-BDF      | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0   | 0.0.15+af1daaa | 30.18\\u00b0C | 53.00 W | 0000:17:00.0 | +------+--------+----------------+---------+---------+--------------+ | rngd | npu1   | 0.0.15+af1daaa | 29.25\\u00b0C | 53.00 W | 0000:2a:00.0 | +------+--------+----------------+---------+---------+--------------+\\n$ furiosa-smi info --format full +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ | Arch | Device | UUID                                 | S\/N        | Firmware       | Temp.   | Power   | Clock | PCI-BDF      | PCI-DEV | +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ | rngd | npu0   | 3E93AE7C-E8EA-4C62-BED6-AD2EC0461AE8 | RNGDXXXXXX | 0.0.15+af1daaa | 30.18\\u00b0C | 53.00 W |   N\/A | 0000:17:00.0 | 508:0   | +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ | rngd | npu1   | 176DAD0F-1510-475C-91D8-5F79551CF718 | RNGDXXXXXY | 0.0.15+af1daaa | 29.44\\u00b0C | 53.00 W |   N\/A | 0000:2a:00.0 | 506:0   | +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+\\n```\\n### `furiosa-smi status` [#](#furiosa-smi-status \\\"Link to this heading\\\")\\nThe `status` subcommand provides information about the device files available on the NPU device. You can also check whether each core present in the NPU is in use or idle.\\n``` $ furiosa-smi status +------+--------+---------------+------------------+ | Arch | Device | Cores         | Core Utilization | +------+--------+---------------+------------------+ |      |        | 0 (occupied), | Core 0: 0.00%,   | |      |        | 1 (occupied), | Core 1: 0.00%,   | |      |        | 2 (occupied), | Core 2: 0.00%,   | | rngd | npu0   | 3 (occupied), | Core 3: 0.00%,   | |      |        | 4 (occupied), | Core 4: 0.00%,   | |      |        | 5 (occupied), | Core 5: 0.00%,   | |      |        | 6 (occupied), | Core 6: 0.00%,   | |      |        | 7 (occupied)  | Core 7: 0.00%    | +------+--------+---------------+------------------+ |      |        | 0 (occupied), | Core 0: 0.00%,   | |      |        | 1 (occupied), | Core 1: 0.00%,   | |      |        | 2 (occupied), | Core 2: 0.00%,   | | rngd | npu1   | 3 (occupied), | Core 3: 0.00%,   | |      |        | 4 (occupied), | Core 4: 0.00%,   | |      |        | 5 (occupied), | Core 5: 0.00%,   | |      |        | 6 (occupied), | Core 6: 0.00%,   | |      |        | 7 (occupied)  | Core 7: 0.00%    | +------+--------+---------------+------------------+\\n```\\n### `furiosa-smi ps` [#](#furiosa-smi-ps \\\"Link to this heading\\\")\\nThe `ps` subcommand prints information about the OS process currently occupying the NPU device.\\n``` $ furiosa-smi ps +-----------+--------+------------------------------------------------------------+ | NPU       | PID    | CMD                                                        | +-----------+--------+------------------------------------------------------------+ | npu0pe0-3 | 132529 | \/usr\/bin\/python3 \/usr\/local\/bin\/uvicorn gptj:app           | +-----------+--------+------------------------------------------------------------+\\n```\\n### `furiosa-smi topo` [#](#furiosa-smi-topo \\\"Link to this heading\\\")\\nThe `topo` subcommand shows the topology of the NPU device and its NUMA node.\\n``` $ furiosa-smi topo +--------+--------------+--------------+-----------+ | Device | npu0         | npu1         | NUMA node | +--------+--------------+--------------+-----------+ | npu0   | Noc          | Interconnect | 0         | +--------+--------------+--------------+-----------+ | npu1   | Interconnect | Noc          | 0         | +--------+--------------+--------------+-----------+\\nLegend:\\n  Noc          = Connection within the same npu chip   Bridge       = Devices communicating via one or more PCIe switches   Cpu          = Devices communicating exclusively within a single CPU socket   Interconnect = Devices communicating via inter-socket links (e.g., QPI, GMI)   Unknown      = Connection type is unidentified\\n```\\n[previous\\nCloud Native Toolkit](..\/cloud_native_toolkit\/intro.html \\\"previous page\\\")\\nContents\\n* [Installing   `furiosa-smi`   command](#installing-furiosa-smi-command)   + [Synopsis](#synopsis)   + [`furiosa-smi          info`](#furiosa-smi-info)   + [`furiosa-smi          status`](#furiosa-smi-status)   + [`furiosa-smi          ps`](#furiosa-smi-ps)   + [`furiosa-smi          topo`](#furiosa-smi-topo)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/furiosa_llm\/references\/llm.html","title":"llm","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"e08e3845-eb5a-4b6f-8530-fb63f81ef7d0\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/furiosa_llm\/references\/llm.html\", \"title\": \"llm\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/..\/_sources\/furiosa_llm\/references\/llm.rst \\\"Download source file\\\") * .pdf\\nLLM class =========\\nContents --------\\n* [`LLM`](#furiosa_llm.LLM)   + [`LLM.generate()`](#furiosa_llm.LLM.generate)   + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms)\\nLLM class [#](#llm-class \\\"Link to this heading\\\") ================================================\\n*class* furiosa\\\\_llm.\\nLLM\\n(\\n*pretrained\\\\_id\\n:\\nstr* ,\\n*task\\\\_type\\n:\\nstr\\n|\\nNone\\n=\\nNone* ,\\n*llm\\\\_config\\n:\\nLLMConfig\\n|\\nNone\\n=\\nNone* ,\\n*qformat\\\\_path\\n:\\nPathLike\\n|\\nNone\\n=\\nNone* ,\\n*qparam\\\\_path\\n:\\nPathLike\\n|\\nNone\\n=\\nNone* ,\\n*prefill\\\\_quant\\\\_bin\\\\_path\\n:\\nPathLike\\n|\\nNone\\n=\\nNone* ,\\n*decode\\\\_quant\\\\_bin\\\\_path\\n:\\nPathLike\\n|\\nNone\\n=\\nNone* ,\\n*config\\n:\\nDict\\n[\\nstr\\n,\\nAny\\n]\\n=\\n{}* ,\\n*bucket\\\\_config\\n:\\nBucketConfig\\n|\\nNone\\n=\\nNone* ,\\n*max\\\\_seq\\\\_len\\\\_to\\\\_capture\\n:\\nint\\n=\\n2048* ,\\n*tensor\\\\_parallel\\\\_size\\n:\\nint\\n=\\n4* ,\\n*pipeline\\\\_parallel\\\\_size\\n:\\nint\\n=\\n1* ,\\n*data\\\\_parallel\\\\_size\\n:\\nint\\n|\\nNone\\n=\\nNone* ,\\n*tokenizer\\n:\\nPreTrainedTokenizer\\n|\\nPreTrainedTokenizerFast\\n|\\nNone\\n=\\nNone* ,\\n*tokenizer\\\\_mode\\n:\\nLiteral\\n[\\n'auto'\\n,\\n'slow'\\n]\\n=\\n'auto'* ,\\n*seed\\n:\\nint\\n|\\nNone\\n=\\nNone* ,\\n*devices\\n:\\nstr\\n|\\nSequence\\n[\\nDevice\\n]\\n|\\nNone\\n=\\nNone* ,\\n*param\\\\_file\\\\_path\\n:\\nPathLike\\n|\\nNone\\n=\\nNone* ,\\n*param\\\\_saved\\\\_format\\n:\\nLiteral\\n[\\n'safetensors'\\n,\\n'pt'\\n]\\n=\\n'safetensors'* ,\\n*do\\\\_decompositions\\\\_for\\\\_model\\\\_rewrite\\n:\\nbool\\n=\\nFalse* ,\\n*comp\\\\_supertask\\\\_kind\\n:\\nLiteral\\n[\\n'edf'\\n,\\n'dfg'\\n,\\n'fx'\\n]\\n|\\nNone\\n=\\nNone* ,\\n*cache\\\\_dir\\n:\\nPathLike\\n|\\nNone\\n=\\nPosixPath('\/home\/hyunsik\/.cache\/furiosa\/llm')* ,\\n*backend\\n:\\nLLMBackend\\n|\\nNone\\n=\\nNone* ,\\n*use\\\\_blockwise\\\\_compile\\n:\\nbool\\n=\\nTrue* ,\\n*num\\\\_blocks\\\\_per\\\\_supertask\\n:\\nint\\n=\\n1* ,\\n*embed\\\\_all\\\\_constants\\\\_into\\\\_graph\\n:\\nbool\\n=\\nFalse* ,\\n*paged\\\\_attention\\\\_num\\\\_blocks\\n:\\nint\\n|\\nNone\\n=\\nNone* ,\\n*paged\\\\_attention\\\\_block\\\\_size\\n:\\nint\\n=\\n1* ,\\n*kv\\\\_cache\\\\_sharing\\\\_across\\\\_beams\\\\_config\\n:\\nKvCacheSharingAcrossBeamsConfig\\n|\\nNone\\n=\\nNone* ,\\n*scheduler\\\\_config\\n:\\nSchedulerConfig\\n=\\nSchedulerConfig(npu\\\\_queue\\\\_limit=2,\\nmax\\\\_processing\\\\_samples=65536,\\nspare\\\\_blocks\\\\_ratio=0.2,\\nis\\\\_offline=False)* ,\\n*packing\\\\_type\\n:\\nLiteral\\n[\\n'IDENTITY'\\n]\\n=\\n'IDENTITY'* ,\\n*compiler\\\\_config\\\\_overrides\\n:\\nMapping\\n|\\nNone\\n=\\nNone* ,\\n*use\\\\_random\\\\_weight\\n:\\nbool\\n=\\nFalse* ,\\n*num\\\\_pipeline\\\\_builder\\\\_workers\\n:\\nint\\n=\\n1* ,\\n*num\\\\_compile\\\\_workers\\n:\\nint\\n=\\n1* ,\\n*skip\\\\_engine\\n:\\nbool\\n=\\nFalse* ,\\n*\\\\** ,\\n*\\\\_cleanup\\n:\\nbool\\n=\\nTrue* ,\\n*\\\\*\\\\*\\nkwargs* )\\n[[source]](..\/..\/_modules\/furiosa_llm\/api.html#LLM) [#](#furiosa_llm.LLM \\\"Link to this definition\\\")\\nBases: `object`  An LLM for generating texts from given prompts and sampling parameters.\\nParameters :\\n* **pretrained\\\\_id**   \\u2013 The name of the pretrained model. This corresponds to   pretrained\\\\_model\\\\_name\\\\_or\\\\_path in HuggingFace Transformers. * **task\\\\_type**   \\u2013 The type of the task. This corresponds to task in HuggingFace Transformers.   See   <https:\/\/huggingface.co\/docs\/transformers\/main\/en\/quicktour#pipeline>   for more   details. * **llm\\\\_config**   \\u2013 The configuration for the LLM. This includes quantization and optimization   configurations. * **qformat\\\\_path**   \\u2013 The path to the quantization format file. * **qparam\\\\_path**   \\u2013 The path to the quantization parameter file. * **prefill\\\\_quant\\\\_bin\\\\_path**   \\u2013 The path to the quantziation prefill bin file. * **decode\\\\_quant\\\\_bin\\\\_path**   \\u2013 The path to the quantziation decode bin file. * **config**   \\u2013 The configuration for the HuggingFace Transformers model. This is a dictionary   that includes the configuration for the model. * **bucket\\\\_config**   \\u2013 Config for bucket generating policy. If not given, the model will use single one batch,   max\\\\_seq\\\\_len\\\\_to\\\\_capture      attention size bucket per   each phase. * **max\\\\_seq\\\\_len\\\\_to\\\\_capture**   \\u2013 Maximum sequence length covered by LLM engine. Sequence with larger context than this will not be covered.   The default is 2048. * **tensor\\\\_parallel\\\\_size**   \\u2013 The number of PEs for each tensor parallelism group. The default is 4. * **pipeline\\\\_parallel\\\\_size**   \\u2013 The number of pipeline stages for pipeline parallelism. The default is 1,   which means no pipeline parallelism. * **data\\\\_parallel\\\\_size**   \\u2013 The size of the data parallelism group. If not given, it will be inferred from   total avaialble PEs and other parallelism degrees. * **tokenizer**   \\u2013 The name or path of a HuggingFace Transformers tokenizer. * **tokenizer\\\\_mode**   \\u2013 The tokenizer mode. \\u201cauto\\u201d will use the fast tokenizer   if available, and \\u201cslow\\u201d will always use the slow tokenizer. * **seed**   \\u2013 The seed to initialize the random number generator for sampling. * **devices**   \\u2013 The devices to run the model. It can be a single device or a list of devices.   Each device can be either \\u201ccpu:X\\u201d or \\u201ccuda:X\\u201d where X is a specific device index.   The default is \\u201ccpu:0\\u201d. * **param\\\\_file\\\\_path**   \\u2013 The path to the parameter file to use for pipeline generation.   If not specified, the parameters will be saved in a temporary file which will be   used for pipeline generation. * **param\\\\_saved\\\\_format**   \\u2013 The format of the parameter file. Only possible value is \\u201csafetensors\\u201d now.   The default is \\u201csafetensors\\u201d. * **do\\\\_decompositions\\\\_for\\\\_model\\\\_rewrite**   \\u2013 Whether to decompose some ops to describe various parallelism strategies   with mppp config. When the value is True, mppp config that matches with the decomposed FX graph should be given. * **comp\\\\_supertask\\\\_kind**   \\u2013 The format that pipeline\\u2019s supertask will be represented as.   Possible values are \\u201cfx\\u201d,\\u201ddfg\\u201d, and \\u201cedf\\u201d, and the default is \\u201cfx\\u201d. * **cache\\\\_dir**   \\u2013 The cache directory for all generated files for this LLM instance.   When its value is   `None`   , caching is disabled. The default is \\u201c$HOME\/.cache\/furiosa\/llm\\u201d. * **backend**   \\u2013 The backend implementation to run forward() of a model for the LLM.   The default is LLMBackend.TORCH\\\\_PIPELINE\\\\_RUNNER. * **use\\\\_blockwise\\\\_compile**   \\u2013 If True, each task will be compiled in the unit of transformer block,   and compilation result for transformer block is generated once and reused. * **num\\\\_blocks\\\\_per\\\\_supertask**   \\u2013 The number of transformer blocks that will be merged into one supertask. This option is valid   only when   use\\\\_blockwise\\\\_compile=True      . The default is 1. * **embed\\\\_all\\\\_constants\\\\_into\\\\_graph**   \\u2013 Whether to embed constant tensors into graph or make them as input of the graph and save them as separate files. * **paged\\\\_attention\\\\_num\\\\_blocks**   \\u2013 The maximum number of blocks that each k\/v storage per layer can store. This argument must be given   if model uses paged attention. * **paged\\\\_attention\\\\_block\\\\_size**   \\u2013 The maximum number of tokens that can be stored in a single paged attention block. This argument must be given   if model uses paged attention. * **kv\\\\_cache\\\\_sharing\\\\_across\\\\_beams\\\\_config**   \\u2013 Configuration for sharing kv cache across beams. This argument must be given if and only if   the model is optimized to share kv cache across beams. If this argument is given, decode phase buckets with batch size of   `batch_size`   \\\\*   `kv_cache_sharing_across_beams_config.beam_width`   will be created. * **scheduler\\\\_config**   \\u2013 Configuration for the scheduler, allowing to maximum number of tasks which can be queued to HW, maximum number of samples   that can be processed by the scheduler, and ratio of spare blocks that are reserved by scheduler. * **packing\\\\_type**   \\u2013 Packing algorithm. Possible values are \\u201cIDENTITY\\u201d only for now * **compiler\\\\_config\\\\_overrides**   \\u2013 Overrides for the compiler config. This is a dictionary that includes the configuration for the compiler. * **use\\\\_random\\\\_weight**   \\u2013 If True, the model will be initialized with random weights. * **num\\\\_pipeline\\\\_builder\\\\_workers**   \\u2013 number of workers used for building pipelines (except for compilation). The default is 1 (no parallelism).   Setting this value larger than 1 reduces pipeline building time, especially for large models, but requires much more memory. * **num\\\\_compile\\\\_workers**   \\u2013 number of workers used for compilation. The default is 1 (no parallelism). * **skip\\\\_engine**   \\u2013 If True, the native runtime engine will not be initialized. This is useful when you need   the pipelines for other purposes than running them with the engine.\\ngenerate\\n(\\n*prompts:\\nstr\\n|\\n~typing.List[str],\\nsampling\\\\_params:\\n~furiosa\\\\_llm.sampling\\\\_params.SamplingParams\\n=\\nSamplingParams(n=1,\\nbest\\\\_of=1,\\ntemperature=1.0,\\ntop\\\\_p=1.0,\\ntop\\\\_k=-1,\\nuse\\\\_beam\\\\_search=False,\\nlength\\\\_penalty=1.0,\\nearly\\\\_stopping=False,\\nmax\\\\_tokens=16min\\\\_tokens=0,\\n,\\nprompt\\\\_token\\\\_ids:\\n~typing.List[int]\\n|\\n~typing.List[~typing.List[int]]\\n|\\nNone\\n=\\nNone* )\\n\\u2192\\nRequestOutput\\n|\\nList\\n[\\nRequestOutput\\n]\\n[[source]](..\/..\/_modules\/furiosa_llm\/api.html#LLM.generate) [#](#furiosa_llm.LLM.generate \\\"Link to this definition\\\")\\nGenerate texts from given prompts and sampling parameters.\\nParameters :\\n* **prompts**   \\u2013 The prompts to generate texts. * **sampling\\\\_params**   \\u2013 The sampling parameters for generating texts. * **prompt\\\\_token\\\\_ids**   \\u2013 The token ids of the prompts. If not given, the token ids are   generated from the prompts using the tokenizer.\\nReturns :\\nA list of RequestOutput\\nobjects containing the generated completions in the same order as the input prompts.\\nget\\\\_splitted\\\\_gms\\n(\\n*get\\\\_input\\\\_constants\\n:\\nbool\\n=\\nFalse* )\\n\\u2192\\nDict\\n[\\nstr\\n,\\nTuple\\n[\\nGraphModule\\n,\\n...\\n]\\n|\\nTuple\\n[\\nTuple\\n[\\nGraphModule\\n,\\nTuple\\n[\\nTensor\\n|\\nNone\\n,\\n...\\n]\\n]\\n,\\n...\\n]\\n]\\n[[source]](..\/..\/_modules\/furiosa_llm\/api.html#LLM.get_splitted_gms) [#](#furiosa_llm.LLM.get_splitted_gms \\\"Link to this definition\\\")\\nGet sub GraphModules for each pipeline.\\nReturns :\\nDictionary whose key is the pipeline name and value is the tuple containing `GraphModule``s\\n(computation\\nsupertasks)\\nand\\nsome\\nadditional\\ninformation\\nif\\nnecessary.\\nif\\n``get_input_constants==False` , each value is just a tuple of `GraphModule``s\\nin\\nthe\\npipeline.\\nOtherwise,\\neach\\nvalue\\nis\\na\\ntuple\\nwhose\\nelement\\nis\\n``GraphModule` in the pipeline and list of input constant tensors, which were originally constant tensors, but converted to input. The list of input constant tensors has same length as corresponding `GraphModule` \\u2019s number of inputs with each element exactly corresponding to the input of the `GraphModule` with same index, but elements with original input tensor indexes are `None` .\\nReturn type :\\nDict[str, Union[Tuple[GraphModule, \\u2026], Tuple[Tuple[GraphModule, Tuple[Optional[torch.Tensor], \\u2026]], \\u2026],],]\\n[previous\\nReferences](..\/references.html \\\"previous page\\\") [next\\nSamplingParams](sampling_params.html \\\"next page\\\")\\nContents\\n* [`LLM`](#furiosa_llm.LLM)   + [`LLM.generate()`](#furiosa_llm.LLM.generate)   + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/furiosa_llm\/references.html","title":"references","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"85efd72a-7fd9-4895-a0c7-090891c1e2cf\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/furiosa_llm\/references.html\", \"title\": \"references\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/_sources\/furiosa_llm\/references.rst \\\"Download source file\\\") * .pdf\\nReferences ==========\\nReferences [#](#references \\\"Link to this heading\\\") ==================================================\\nReferences\\n* [LLM class](references\/llm.html) * [SamplingParams](references\/sampling_params.html)\\n[previous\\nOpenAI Compatible Server](furiosa-llm-serve.html \\\"previous page\\\") [next\\nLLM class](references\/llm.html \\\"next page\\\")\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/cloud_native_toolkit\/kubernetes\/metrics_exporter.html","title":"metrics_exporter","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"fe9686e2-f00f-4e68-87fa-80abbaf03e2b\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/cloud_native_toolkit\/kubernetes\/metrics_exporter.html\", \"title\": \"metrics_exporter\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/..\/_sources\/cloud_native_toolkit\/kubernetes\/metrics_exporter.rst \\\"Download source file\\\") * .pdf\\nInstalling Furiosa Metrics Exporter ===================================\\nContents --------\\n* [Furiosa Metrics Exporter](#furiosa-metrics-exporter)   + [Metrics](#metrics)   + [Deploying Furiosa Metrics Exporter with Helm](#deploying-furiosa-metrics-exporter-with-helm)\\nInstalling Furiosa Metrics Exporter [#](#installing-furiosa-metrics-exporter \\\"Link to this heading\\\") ====================================================================================================\\nFuriosa Metrics Exporter [#](#furiosa-metrics-exporter \\\"Link to this heading\\\") ------------------------------------------------------------------------------\\nThe Furiosa metrics exporter exposes collection of metrics related to FuriosaAI NPU devices in [Prometheus](https:\/\/prometheus.io\/) format. In a Kubernetes cluster, you can scrape the metrics provided by furiosa-metrics-exporter using Prometheus and visualize them with a Grafana dashboard. This can be easily set up using the [Prometheus Chart](https:\/\/github.com\/prometheus-community\/helm-charts\/tree\/main\/charts\/prometheus) and [Grafana](https:\/\/github.com\/grafana\/helm-charts\/tree\/main\/charts\/grafana) Helm charts, along with the furiosa-metrics-exporter Helm chart.\\n### Metrics [#](#metrics \\\"Link to this heading\\\")\\nThe exporter is composed of chain of collectors, each collector is responsible for collecting specific metrics from the Furiosa NPU devices. The following table shows the available collectors and metrics:\\nNPU Metrics\\n[#](#id1 \\\"Link to this table\\\")\\n| Collector Name | Metric | Type | Metric Labels | Description | | --- | --- | --- | --- | --- | | Liveness | furiosa\\\\_npu\\\\_alive | guage | arch, core, device, uuid, kubernetes\\\\_node\\\\_name | The liveness of the Furiosa NPU device. | | Error | furiosa\\\\_npu\\\\_error | guage | arch, core, device, uuid, kubernetes\\\\_node\\\\_name, label | The error count of the Furiosa NPU device. | | Temperature | furiosa\\\\_npu\\\\_hw\\\\_temperature | guage | arch, core, device, uuid, kubernetes\\\\_node\\\\_name, label | The temperature of the Furiosa NPU device. | | Power | furiosa\\\\_npu\\\\_hw\\\\_power | guage | arch, core, device, uuid, kubernetes\\\\_node\\\\_name, label | The power consumption of the Furiosa NPU device. | | Core Utilization | furiosa\\\\_npu\\\\_core\\\\_utilization | guage | arch, core, device, uuid, kubernetes\\\\_node\\\\_name | The core utilization of the Furiosa NPU device. |\\nAll metrics share common metric labels such as arch, core, device, kubernetes\\\\_node\\\\_name, and uuid. The following table describes the common metric labels:\\nCommon NPU Metrics Label\\n[#](#id2 \\\"Link to this table\\\")\\n| Common Metric Label | Description | | --- | --- | | arch | The architecture of the Furiosa NPU device. e.g. warboy, rngd | | core | The core number of the Furiosa NPU device. e.g. 0, 1, 2, 3, 4, 5, 6, 7, 0-1, 2-3, 0-3, 4-5, 6-7, 4-7, 0-7 | | device | The device name of the Furiosa NPU device. e.g. npu0 | | kubernetes\\\\_node\\\\_name | The name of the Kubernetes node where the exporter is running, this attribute can be missing if the exporter is running on the host machine or in a naked container. | | uuid | The UUID of the Furiosa NPU device. |\\nThe metric label \\u201clabel\\u201d is used to describe additional attributes specific to each metric. This approach helps avoid having too many metric definitions and effectively aggregates metrics that share common characteristics.\\nNPU Metrics Type\\n[#](#id3 \\\"Link to this table\\\")\\n| Metric Type | Label Attribute | Description | | --- | --- | --- | | Error | axi\\\\_post\\\\_error | Indicates count of axi post error. | | Error | axi\\\\_fetch\\\\_error | Indicates count of axi fetch error. | | Error | axi\\\\_discard\\\\_error | Indicates count of axi discard error. | | Error | axi\\\\_doorbell\\\\_done | Indicates count of axi doorbell done error. | | Error | pcie\\\\_post\\\\_error | Indicates count of PCIe post error. | | Error | pcie\\\\_fetch\\\\_error | Indicates count of PCIe fetch error. | | Error | pcie\\\\_discard\\\\_error | Indicates count of PCIe discard error. | | Error | pcie\\\\_doorbell\\\\_done | Indicates count of PCIe doorbell done error. | | Error | device\\\\_error | Total count of device error. | | Temperature | peak | The highest temperature observed from SoC sensors | | Temperature | ambient | The temperature observed from sensors attached to the board | | Power | rms | Root Mean Square (RMS) value of the power consumed by the device, providing an average power consumption metric over a period of time. |\\nThe following shows real-world example of the metrics:\\n``` #liveness furiosa_npu_alive{arch=\\\"rngd\\\",core=\\\"0-7\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",uuid=\\\"uuid\\\"} 1\\n#error furiosa_npu_error{arch=\\\"rngd\\\",core=\\\"0-7\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",label=\\\"axi_post_error\\\",uuid=\\\"uuid\\\"} 0 furiosa_npu_error{arch=\\\"rngd\\\",core=\\\"0-7\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",label=\\\"axi_fetch_error\\\",uuid=\\\"uuid\\\"} 0 furiosa_npu_error{arch=\\\"rngd\\\",core=\\\"0-7\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",label=\\\"axi_discard_error\\\",uuid=\\\"uuid\\\"} 0 furiosa_npu_error{arch=\\\"rngd\\\",core=\\\"0-7\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",label=\\\"axi_doorbell_done\\\",uuid=\\\"uuid\\\"} 0 furiosa_npu_error{arch=\\\"rngd\\\",core=\\\"0-7\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",label=\\\"pcie_post_error\\\",uuid=\\\"uuid\\\"} 0 furiosa_npu_error{arch=\\\"rngd\\\",core=\\\"0-7\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",label=\\\"pcie_fetch_error\\\",uuid=\\\"uuid\\\"} 0 furiosa_npu_error{arch=\\\"rngd\\\",core=\\\"0-7\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",label=\\\"pcie_discard_error\\\",uuid=\\\"uuid\\\"} 0 furiosa_npu_error{arch=\\\"rngd\\\",core=\\\"0-7\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",label=\\\"pcie_doorbell_done\\\",uuid=\\\"uuid\\\"} 0 furiosa_npu_error{arch=\\\"rngd\\\",core=\\\"0-7\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",label=\\\"device_error\\\",uuid=\\\"uuid\\\"} 0\\n#temperature furiosa_npu_hw_temperature{arch=\\\"rngd\\\",core=\\\"0-7\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",label=\\\"peak\\\",uuid=\\\"uuid\\\"} 39 furiosa_npu_hw_temperature{arch=\\\"rngd\\\",core=\\\"0-7\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",label=\\\"ambient\\\",uuid=\\\"uuid\\\"} 35\\n#power furiosa_npu_hw_power{arch=\\\"rngd\\\",core=\\\"0-7\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",label=\\\"rms\\\",uuid=\\\"uuid\\\"} 4795000\\n#core utilization furiosa_npu_core_utilization{arch=\\\"rngd\\\",core=\\\"0\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",uuid=\\\"uuid\\\"} 90 furiosa_npu_core_utilization{arch=\\\"rngd\\\",core=\\\"1\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",uuid=\\\"uuid\\\"} 90 furiosa_npu_core_utilization{arch=\\\"rngd\\\",core=\\\"2\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",uuid=\\\"uuid\\\"} 90 furiosa_npu_core_utilization{arch=\\\"rngd\\\",core=\\\"3\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",uuid=\\\"uuid\\\"} 90 furiosa_npu_core_utilization{arch=\\\"rngd\\\",core=\\\"4\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",uuid=\\\"uuid\\\"} 90 furiosa_npu_core_utilization{arch=\\\"rngd\\\",core=\\\"5\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",uuid=\\\"uuid\\\"} 90 furiosa_npu_core_utilization{arch=\\\"rngd\\\",core=\\\"6\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",uuid=\\\"uuid\\\"} 90 furiosa_npu_core_utilization{arch=\\\"rngd\\\",core=\\\"7\\\",device=\\\"npu0\\\",kubernetes_node_name=\\\"node\\\",uuid=\\\"uuid\\\"} 90\\n```\\n### Deploying Furiosa Metrics Exporter with Helm [#](#deploying-furiosa-metrics-exporter-with-helm \\\"Link to this heading\\\")\\nThe Furiosa metrics exporter helm chart is available at [furiosa-ai\/helm-charts](https:\/\/github.com\/furiosa-ai\/helm-charts) . To configure deployment as you need, you can modify `charts\/furiosa-metrics-exporter\/values.yaml` . For example, the Furiosa metrics exporter Helm chart automatically creates a Service Object with Prometheus annotations to enable metric scraping automatically. You can modify the values.yaml to change the port or disable the Prometheus annotations if needed. You can deploy the Furiosa Metrics Exporter by running the following commands:\\n``` helm repo add furiosa https:\/\/furiosa-ai.github.io\/helm-charts helm repo update helm install furiosa-metrics-exporter furiosa\/furiosa-metrics-exporter -n kube-system\\n```\\n[previous\\nInstalling Furiosa Device Plugin](device_plugin.html \\\"previous page\\\") [next\\nScheduling NPUs](scheduling_npus.html \\\"next page\\\")\\nContents\\n* [Furiosa Metrics Exporter](#furiosa-metrics-exporter)   + [Metrics](#metrics)   + [Deploying Furiosa Metrics Exporter with Helm](#deploying-furiosa-metrics-exporter-with-helm)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/cloud_native_toolkit\/kubernetes\/feature_discovery.html","title":"feature_discovery","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"a1f21dc2-1473-410f-a3ae-e46d451a9c0e\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/cloud_native_toolkit\/kubernetes\/feature_discovery.html\", \"title\": \"feature_discovery\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/..\/_sources\/cloud_native_toolkit\/kubernetes\/feature_discovery.rst \\\"Download source file\\\") * .pdf\\nInstalling Furiosa Feature Discovery ====================================\\nContents --------\\n* [Furiosa Feature discovery and NFD](#furiosa-feature-discovery-and-nfd)   + [Labels](#labels)   + [Deploying Furiosa Feature Discovery with Helm](#deploying-furiosa-feature-discovery-with-helm)\\nInstalling Furiosa Feature Discovery [#](#installing-furiosa-feature-discovery \\\"Link to this heading\\\") ======================================================================================================\\nFuriosa Feature discovery and NFD [#](#furiosa-feature-discovery-and-nfd \\\"Link to this heading\\\") ------------------------------------------------------------------------------------------------\\nThe Furiosa feature discovery automatically labels Kubernetes nodes with information about FuriosaAI NPU properties, such as the NPU family, count, and driver versions. Using these labels, you can schedule your Kubernetes workloads based on specific NPU requirements.\\nThe Furiosa feature Discovery leverage NFD(Node Feature Discovery) which is a tool that detects hardware features and labels Kubernetes nodes. It is recommended to use NFD and Furiosa Feature Discovery to ensure that the Cloud Native Toolkit is deployed only on nodes equipped with FuriosaAI NPUs.\\n### Labels [#](#labels \\\"Link to this heading\\\")\\nThe followings are the labels that the Furiosa Feature Discovery attaches and what they mean.\\nLabels\\n[#](#id1 \\\"Link to this table\\\")\\n| Label | Value | Description | | --- | --- | --- | | furiosa.ai\/npu.count | n | # of NPU devices | | furiosa.ai\/npu.family | warboy, rngd | Chip family | | furiosa.ai\/npu.product | warboy, rngd, rngd-s, rngd-max | Chip product name | | furiosa.ai\/npu.driver.version | x.y.z | NPU device driver version | | furiosa.ai\/npu.driver.version.major | x | NPU device driver version major part | | furiosa.ai\/npu.driver.version.minor | y | NPU device driver version minor part | | furiosa.ai\/npu.driver.version.patch | z | NPU device driver version patch part | | furiosa.ai\/npu.driver.version.metadata | abcxyz | NPU device driver version metadata |\\n### Deploying Furiosa Feature Discovery with Helm [#](#deploying-furiosa-feature-discovery-with-helm \\\"Link to this heading\\\")\\nWith the helm chart you can easily install Furiosa feature discovery and NFD into your Kubernetes cluster. Following command shows how to install them. The Furiosa device plugin helm chart is available at [furiosa-ai\/helm-charts](https:\/\/github.com\/furiosa-ai\/helm-charts) . To configure deployment as you need, you can modify `charts\/furiosa-feature-discovery\/values.yaml` .\\n``` helm repo add furiosa https:\/\/furiosa-ai.github.io\/helm-charts helm repo update helm install furiosa-feature-discovery furiosa\/furiosa-feature-discovery -n kube-system\\n```\\n[previous\\nKubernetes Support](..\/kubernetes.html \\\"previous page\\\") [next\\nInstalling Furiosa Device Plugin](device_plugin.html \\\"next page\\\")\\nContents\\n* [Furiosa Feature discovery and NFD](#furiosa-feature-discovery-and-nfd)   + [Labels](#labels)   + [Deploying Furiosa Feature Discovery with Helm](#deploying-furiosa-feature-discovery-with-helm)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/whatsnew\/index.html","title":"index","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"a53038c2-0668-4963-875e-79abe9c99e2c\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/whatsnew\/index.html\", \"title\": \"index\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/_sources\/whatsnew\/index.rst \\\"Download source file\\\") * .pdf\\nWhat\\u2019s New ==========\\nContents --------\\n* [Furiosa SDK 2024.1.0 (2024-10-11)](#furiosa-sdk-2024-1-0-2024-10-11)   + [Highlights](#highlights)\\nWhat\\u2019s New [#](#what-s-new \\\"Link to this heading\\\") ==================================================\\nThis page describes the changes and functionality available in in the latest releases of Furiosa SDK 2024.1.0.\\nFuriosa SDK 2024.1.0 (2024-10-11) [#](#furiosa-sdk-2024-1-0-2024-10-11 \\\"Link to this heading\\\") ----------------------------------------------------------------------------------------------\\n2024.1.0 is the first SDK release for RNGD. This release is alpha release, and the features and APIs described in this document may change in the future.\\n### Highlights [#](#highlights \\\"Link to this heading\\\")\\n* Model Support: LLaMA 3.1 8B\/70B, BERT Large, GPT-J 6B * Furiosa Quantizer supports the following quantization methods:      + BF16 (W16A16)   + INT8 Weight-Only (W8A16)   + FP8 (W8A8)   + INT8 SmoothQuant (W8A8) * Furiosa LLM      + Efficient KV cache management with PagedAttention   + Continuous batching support in serving   + OpenAI-compatible API server   + Greedy search and beam search   + Pipeline Parallelism and Data Parallelism across multiple NPUs * `furiosa-mlperf`   command      + Server and Offline scenarios   + BERT, GPT-J, LLaMA 3.1 benchmarks * System Management Interface      + System Management Interface Library and CLI for Furiosa NPU family * Cloud Native Toolkit      + Kubernetes integration for managing and monitoring the Furiosa NPU family\\nComponent version\\n[#](#id1 \\\"Link to this table\\\")\\n| Package name | Version | | --- | --- | | furiosa-compiler | 2024.1.0 | | furiosa-device-plugin | 2024.1.0 | | furiosa-driver-rngd | 2024.1.0 | | furiosa-feature-discovery | 2024.1.0 | | furiosa-firmware-image-tools | 2024.1.0 | | furiosa-firmware-image-rngd | 0.0.19 | | furiosa-libsmi | 2024.1.0 | | furiosa-llm | 2024.1.0 | | furiosa-llm-models | 2024.1.0 | | furiosa-mlperf | 2024.1.0 | | furiosa-mlperf-resources | 2024.1.0 | | furiosa-model-compressor | 2024.1.0 | | furiosa-model-compressor-impl | 2024.1.0 | | furiosa-native-compiler | 2024.1.0 | | furiosa-native-runtime | 2024.1.0 | | furiosa-smi | 2024.1.0 | | furiosa-torch-ext | 2024.1.0 |\\n[previous\\nSupported Models](..\/overview\/supported_models.html \\\"previous page\\\") [next\\nRoadmap](..\/overview\/roadmap.html \\\"next page\\\")\\nContents\\n* [Furiosa SDK 2024.1.0 (2024-10-11)](#furiosa-sdk-2024-1-0-2024-10-11)   + [Highlights](#highlights)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/getting_started\/furiosa_llm.html","title":"furiosa_llm","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"56ae0c48-fbd4-4d43-b6f7-395542108fa7\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/getting_started\/furiosa_llm.html\", \"title\": \"furiosa_llm\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/_sources\/getting_started\/furiosa_llm.rst \\\"Download source file\\\") * .pdf\\nQuick Start with Furiosa LLM ============================\\nContents --------\\n* [Installing Furiosa LLM](#installing-furiosa-llm)   + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment)\\nQuick Start with Furiosa LLM [#](#quick-start-with-furiosa-llm \\\"Link to this heading\\\") ======================================================================================\\nFuriosa LLM is a serving framework for LLM models that utilizes FuriosaAI\\u2019s NPU. It provides a Python API compatible with vLLM and a server compatible with the OpenAI API. This document explains how to install and use Furiosa LLM.\\nWarning\\nThis document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future.\\nInstalling Furiosa LLM [#](#installing-furiosa-llm \\\"Link to this heading\\\") --------------------------------------------------------------------------\\nThe minimum requirements for Furiosa LLM are as follows:\\n* Ubuntu 20.04 LTS (Debian bullseye) or later * Administrator privileges on system (root) * [Setting up APT server](prerequisites.html#aptsetup)   and   [Installing Prerequisites](prerequisites.html#installingprerequisites) * Python 3.8, 3.9, or 3.10 * Enough storage space for model weights; e.g., about 100GB for Llama 3.1 70B model\\nThen, please install the `furiosa-compiler` package as follows:\\n``` sudo apt install -y furiosa-compiler\\n```\\nAlso, you need to create a Python virtual environment depending on your environment.\\nNote\\nNote that some models, such as meta-llama\/Meta-Llama-3.1-8B require you to accept their license, hence, you need to create a HuggingFace account, accept the model\\u2019s license, and generate a token. Usually, you can create your token at <https:\/\/huggingface.co\/settings\/tokens> .\\nOnce you get a token, you can authenticate on the HuggingFace Hub as following:\\n``` huggingface-cli login --token $HF_TOKEN\\n```\\nThen, you can install the Furiosa LLM with the following command:\\n``` pip install furiosa-llm\\n```\\n### Offline Batch Inference with Furiosa LLM [#](#offline-batch-inference-with-furiosa-llm \\\"Link to this heading\\\")\\nIn this section, we will explain how to perform offline LLM inference using the Python API of Furiosa LLM. First, import the `LLM` class and `SamplingParams` from the furiosa\\\\_llm module. `LLM` class is used to load LLM models and provides the core API for LLM inference. `SamplingParams` allows to specify various parameters for text generation.\\n``` from furiosa_llm import LLM, SamplingParams\\n```\\nNext, we will load an LLM model using the LLM class. The following example loads the meta-llama\/Meta-Llama-3.1-8B model from the HuggingFace Hub and performs quantization. The model will be loaded into the memory and ready for inference.\\n``` llm = LLM(model=\\\"meta-llama\/Llama-3.1-8B-Instruct\\\")\\n```\\nAfter loading the model, you can perform LLM inference by calling the `generate` method.\\n``` prompts = [   \\\"\\\" ]\\nsampling_params = SamplingParams(temperature=0.0)\\n```\\nLaunching the OpenAI-Compatible Server [#](#launching-the-openai-compatible-server \\\"Link to this heading\\\") ----------------------------------------------------------------------------------------------------------\\nYou can find more details at [OpenAI Compatible Server](..\/furiosa_llm\/furiosa-llm-serve.html#openaiserver) .\\nRunning Furiosa LLM in container environment [#](#running-furiosa-llm-in-container-environment \\\"Link to this heading\\\") ----------------------------------------------------------------------------------------------------------------------\\nFuriosaAI provides an image for running `furiosa-llm` in a containerized environment. By using the containerized version, you can run environments that utilize `furiosa-llm` without installing the FuriosaAI Software Stack on your host system, or you can execute it within a Kubernetes environment.\\nTo run the `furiosa-llm` container, you can use the following command:\\n``` $ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash\\n(container) # python\\n```\\nWarning\\nThe example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](..\/cloud_native_toolkit\/intro.html#cloudnativetoolkit)\\n[previous\\nInstalling Prerequisites](prerequisites.html \\\"previous page\\\") [next\\nRunning MLPerf\\u2122 Inference Benchmark](furiosa_mlperf.html \\\"next page\\\")\\nContents\\n* [Installing Furiosa LLM](#installing-furiosa-llm)   + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/cloud_native_toolkit\/intro.html","title":"intro","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"79971985-74d0-445e-8c78-a80a93ef215b\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/cloud_native_toolkit\/intro.html\", \"title\": \"intro\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/_sources\/cloud_native_toolkit\/intro.rst \\\"Download source file\\\") * .pdf\\nCloud Native Toolkit ====================\\nCloud Native Toolkit [#](#cloud-native-toolkit \\\"Link to this heading\\\") ======================================================================\\nFuriosaAI Cloud Native Toolkit is a software stack to enable FuriosaAI\\u2019s NPU product in Kubernetes and Container ecosystem.\\n[previous\\nSamplingParams](..\/furiosa_llm\/references\/sampling_params.html \\\"previous page\\\") [next\\nKubernetes Support](kubernetes.html \\\"next page\\\")\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/overview\/rngd.html","title":"rngd","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"ccf0c765-aa95-4b97-bf6b-fe5650ff8858\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/overview\/rngd.html\", \"title\": \"rngd\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/_sources\/overview\/rngd.rst \\\"Download source file\\\") * .pdf\\nFuriosaAI RNGD ==============\\nFuriosaAI RNGD [#](#furiosaai-rngd \\\"Link to this heading\\\") ==========================================================\\nFuriosaAI\\u2019s second-generation Neural Processing Unit (NPU), RNGD, is a chip designed for deep learning inference, supporting high-performance Large Language Models (LLM), Multi-Modal LLM, Vision models, and other deep learning models.\\nRNGD is based the Tensor Contraction Processor (TCP) architecture which utilizes TSMC\\u2019s 5nm process node, and operates at 1.0 GHz. It offers 512 TOPS and 1024 TOPS of INT8 and INT4 performance respectively. RNGD is configured with two HBM3 modules providing a memory bandwidth of 1.5 TB\/s, and supports PCIe Gen5 x16. For multi-tenant environments like Kubernetes and virtual environment, a single RNGD chip can work as 2, 4, 8 individual NPUs, each fully isolated with its own cores and memory bandwidth. RNGD supports Single Root IO Virtualization (SR-IOV) and virtualization for multi-instance NPUs.\\nPlease refer to the followings to learn more about TCP architecture and RNGD:\\n* [TCP: A Tensor Contraction Processor for AI Workloads, ACM\/IEEE ISCA 2024](https:\/\/ieeexplore.ieee.org\/document\/10609575)   (   [PDF](https:\/\/furiosa.ai\/download\/FuriosaAI-tensor-contraction-processor-isca24)   ) * [FuriosaAI RNGD: A Tensor Contraction Processor for Sustainable AI Computing, Hotchips 2024](https:\/\/hc2024.hotchips.org\/#clip=8jnhm5vdlsow) * [Tensor Contraction Processor: The first future-proof AI chip architecture](https:\/\/furiosa.ai\/blog\/tensor-contraction-processor-ai-chip-architecture)\\nRNGD Hardware Specification\\n[#](#id1 \\\"Link to this table\\\")\\n| Architecture | Tensor Contraction Processor | | --- | --- | | Process Node | TSMC 5nm | | Frequency | 1.0GHz | | BF16 | 256TFLOPS | | FP8 | 512TFLOPS | | INT8 | 512TOPS | | INT4 | 1024TOPS | | Memory Bandwidth | HBM3 1.5TB\/s | | Memory Capacity | HBM3 48GB | | On-Chip SRAM | 256MB | | Interconnect Interface | PCIe Gen5 x16 | | Thermal Solution | Passive | | Thermal Design Power (TDP) | 150W | | Power Connector | 12VHPWR | | Form Factor | PCIe dual-slot full-height 3\/4 Length | | Multi-Instance Support | 8 | | Virtualization Support | Yes | | SR-IOV | 8 Virtual Functions | | ECC Memory Support | Yes | | Secure Boot with Root of Trust | Yes |\\n[previous\\nFuriosaAI Developer Center](..\/index.html \\\"previous page\\\") [next\\nFuriosaAI\\u2019s Software Stack](software_stack.html \\\"next page\\\")\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/getting_started\/prerequisites.html","title":"prerequisites","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"a214fb49-b797-4d38-b877-597b6bb059eb\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/getting_started\/prerequisites.html\", \"title\": \"prerequisites\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/_sources\/getting_started\/prerequisites.rst \\\"Download source file\\\") * .pdf\\nInstalling Prerequisites ========================\\nContents --------\\n* [Requirements](#requirements) * [Verifying if the system has devices](#verifying-if-the-system-has-devices) * [Setting up APT server](#setting-up-apt-server) * [Installing Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware)\\nInstalling Prerequisites [#](#installing-prerequisites \\\"Link to this heading\\\") ==============================================================================\\nWe will explain how to install the prerequisite packages necessary for FuriosaAI software stack. The prerequisite packages include device driver, firmware, and PE Runtime. These packages are available in package format for installation on Debian and Ubuntu systems.\\nRequirements [#](#requirements \\\"Link to this heading\\\") ------------------------------------------------------\\nThe minimum requirements are as follows:\\n* Ubuntu 20.04 LTS (or Debian bullseye) or later * Administrator privileges on system (root)\\nVerifying if the system has devices [#](#verifying-if-the-system-has-devices \\\"Link to this heading\\\") ----------------------------------------------------------------------------------------------------\\nYou can verify the proper installation of FuriosaAI\\u2019s devices on your machine by running the following commands:\\n``` lspci -nn | grep FuriosaAI\\n```\\nIf the device is properly installed, you should see the PCI information as shown below.\\n``` 4e:00.0 Processing accelerators [1200]: FuriosaAI, Inc. Device [1ed2:0001] (rev 01)\\n```\\nIf the `lspci` command is not available, please install the following packages and run the commands to update PCIe ID database:\\n``` sudo apt update sudo apt install -y pciutils sudo update-pciids\\n```\\nSetting up APT server [#](#setting-up-apt-server \\\"Link to this heading\\\") ------------------------------------------------------------------------\\nTo use the APT server provided by FuriosaAI, you must configure it on Ubuntu or Debian Linux as outlined below.\\n1. Install the required packages and register the signing key.\\n``` sudo apt update sudo apt install -y curl gnupg curl https:\/\/packages.cloud.google.com\/apt\/doc\/apt-key.gpg | sudo gpg --dearmor -o \/etc\/apt\/trusted.gpg.d\/cloud.google.gpg\\n```\\n2. Configure the APT server according to the instructions provided for the Linux distribution versions.\\n> ``` > echo \\\"deb [arch=$(dpkg --print-architecture)] http:\/\/asia-northeast3-apt.pkg.dev\/projects\/furiosa-ai $(. \/etc\/os-release && echo \\\"$VERSION_CODENAME\\\") main\\\" | sudo tee \/etc\/apt\/sources.list.d\/furiosa.list >  > ```\\nInstalling Pre-requisite Packages [#](#installing-pre-requisite-packages \\\"Link to this heading\\\") ------------------------------------------------------------------------------------------------\\nIf you have registered the APT server as described above, you will be able to install the required packages: the device driver and PE Runtime.\\n``` sudo apt update sudo apt install furiosa-pert-rngd furiosa-driver-rngd\\n```\\n[furiosa-smi](..\/device_management\/furiosa_smi.html#furiosasmi) is an useful CLI tool for listing and managing FuriosaAI NPUs.\\n``` sudo apt install furiosa-smi\\n```\\nChecking NPU devices [#](#checking-npu-devices \\\"Link to this heading\\\") ----------------------------------------------------------------------\\nOnce the device driver and [furiosa-smi](..\/device_management\/furiosa_smi.html#furiosasmi) are successfully installed, you can check the list of NPU devices as following command:\\n``` furiosa-smi info\\n```\\nOutput:\\n``` +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware       | Temp.   | Power   | PCI-BDF      | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0   | 0.0.16+b4a67ca | 28.88\\u00b0C | 38.00 W | 0000:4e:00.0 | +------+--------+----------------+---------+---------+--------------+\\n```\\nPlease refer to [furiosa-smi](..\/device_management\/furiosa_smi.html#furiosasmi) to learn more about `furiosa-smi` command.\\nUpgrading Device Firmware [#](#upgrading-device-firmware \\\"Link to this heading\\\") --------------------------------------------------------------------------------\\nUpgrading firmware versions can improve the performance and stability of the devices. If there is newer firmware in the latest release, you can upgrade them using the following methods:\\n``` sudo apt install furiosa-firmware-tools-rngd sudo apt install furiosa-firmware-image-rngd\\n```\\nInstalling the `furiosa-firmware-image-rngd` package will automatically upgrade the firmware. The process takes approximately 3 to 5 minutes per device to complete.\\n[previous\\nRoadmap](..\/overview\/roadmap.html \\\"previous page\\\") [next\\nQuick Start with Furiosa LLM](furiosa_llm.html \\\"next page\\\")\\nContents\\n* [Requirements](#requirements) * [Verifying if the system has devices](#verifying-if-the-system-has-devices) * [Setting up APT server](#setting-up-apt-server) * [Installing Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/cloud_native_toolkit\/kubernetes\/scheduling_npus.html","title":"scheduling_npus","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"1ba93fae-bf2e-42c1-a66d-dabbee880912\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs-dev\/2024.1\/en\/cloud_native_toolkit\/kubernetes\/scheduling_npus.html\", \"title\": \"scheduling_npus\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* [.rst](..\/..\/_sources\/cloud_native_toolkit\/kubernetes\/scheduling_npus.rst \\\"Download source file\\\") * .pdf\\nScheduling NPUs ===============\\nContents --------\\n* [Preparing Node](#preparing-node) * [Requesting NPUs](#requesting-npus) * [Scheduling NPUs With Specific Requirements](#scheduling-npus-with-specific-requirements)\\nScheduling NPUs [#](#scheduling-npus \\\"Link to this heading\\\") ============================================================\\nThis page describes how administrator prepares node and user can consume NPU in Kubernetes.\\nPreparing Node [#](#preparing-node \\\"Link to this heading\\\") ----------------------------------------------------------\\nAs an administrator, you have to install [prerequisites](..\/..\/getting_started\/prerequisites.html#installingprerequisites) such as driver, firmware on nodes and deploy [Furiosa Device Plugin](device_plugin.html#deviceplugin) .\\nOnce you have installed it, your cluster exposes Furiosa NPUs as schedulable resources, such as `furiosa.ai\/rngd` .\\nTo ensure your node is ready, you can examine Capacity and\/or Allocatable field of `v1.node` object. Here is an example of node that has 2 RNGD NPUs:\\n``` ... status:   ...   allocatable:     cpu: \\\"20\\\"     ephemeral-storage: \\\"1770585791219\\\"     furiosa.ai\/rngd: \\\"2\\\"     hugepages-1Gi: \\\"0\\\"     hugepages-2Mi: \\\"0\\\"     memory: 527727860Ki     pods: \\\"110\\\"   capacity:     cpu: \\\"20\\\"     ephemeral-storage: 1921208544Ki     furiosa.ai\/rngd: \\\"2\\\"     hugepages-1Gi: \\\"0\\\"     hugepages-2Mi: \\\"0\\\"     memory: 527830260Ki     pods: \\\"110\\\" ...\\n```\\nThe following command should show the `Capacity` field of each node in the Kubernetes cluster.\\n``` kubectl get nodes -o json | jq -r '.items[] | .metadata.name as $name | .status.capacity | to_entries | map(\\\"    \\\\(.key): \\\\(.value)\\\") | $name + \\\":\\\\n  capacity:\\\\n\\\" + join(\\\"\\\\n\\\")'\\n```\\nRequesting NPUs [#](#requesting-npus \\\"Link to this heading\\\") ------------------------------------------------------------\\nYou can consume NPUs from your containers in a Pod by requesting NPU resources, the same way you request CPU or memory.\\nHowever, since NPUs are exposed as a custom resource, there are some limitations you should be aware of when requesting NPU resources:\\n* You can specify NPU   `limits`   without specifying   `requests`   , because kubernetes will use limit as request if request is not specified. * You can specify NPU in both   `limits`   and   `requests`   but these two values must be equal. * You cannot specify NPU   `request`   without specifying   `limits`   .\\nHere is an example manifest for a Pod that requests 2 RNGD NPUs:\\n``` apiVersion: v1 kind: Pod metadata:   name: example-npu-request spec:   containers:   - name: furiosa     image: furiosaai\/furiosa-smi:latest     imagePullPolicy: IfNotPresent     command: [\\\"sleep\\\"]     args: [\\\"120\\\"]     resources:       limits:         furiosa.ai\/rngd: 2\\n```\\nScheduling NPUs With Specific Requirements [#](#scheduling-npus-with-specific-requirements \\\"Link to this heading\\\") ------------------------------------------------------------------------------------------------------------------\\nIn certain cases, user may need to schedule NPU workload on node that meet specific hardware or software requirements, such as particular driver versions. If the [Furiosa Feature Discovery](feature_discovery.html#featurediscovery) is deployed in your cluster, nodes are automatically labelled based on their hardware and software configurations including driver version. This allows user to schedule Pod on nodes that meet specific requirements.\\nFollowing example shows how to use affinity to schedule a Pod that request 2 RNGD NPUs with specific driver version:\\n``` apiVersion: v1 kind: Pod metadata:   name: example-npu-scheduling-with-affinity spec:   containers:   - name: furiosa     image: furiosaai\/furiosa-smi:latest     imagePullPolicy: IfNotPresent     command: [\\\"sleep\\\"]     args: [\\\"120\\\"]     resources:       limits:         furiosa.ai\/rngd: 2   affinity:     nodeAffinity:       requiredDuringSchedulingIgnoredDuringExecution:         nodeSelectorTerms:         - matchExpressions:           - key: furiosa.ai\/driver-version             operator: In             values:             - \\\"1.0.12\\\"\\n```\\n[previous\\nInstalling Furiosa Metrics Exporter](metrics_exporter.html \\\"previous page\\\") [next\\nfuriosa-smi](..\/..\/device_management\/furiosa_smi.html \\\"next page\\\")\\nContents\\n* [Preparing Node](#preparing-node) * [Requesting NPUs](#requesting-npus) * [Scheduling NPUs With Specific Requirements](#scheduling-npus-with-specific-requirements)\\nBy FuriosaAI, Inc.\\n\\u00a9 Copyright 2024, FuriosaAI, Inc..\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
