{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/vm_support.html","title":"vm_support","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"cf227685-cc4e-420e-b21a-e7da166093e5\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/vm_support.html\", \"title\": \"vm_support\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Configuring Warboy Pass-through for Virtual Machine * [View page source](..\/_sources\/software\/vm_support.rst.txt)\\n---\\nConfiguring Warboy Pass-through for Virtual Machine [\\uf0c1](#configuring-warboy-pass-through-for-virtual-machine \\\"Permalink to this heading\\\") =========================================================================================================================================\\nThis section describes how to enable Warboy pass-through for a virtual machine. The example of this section is based on a specific VM tool `QEMU-KVM` , but it also works in other VM tools. The environment used in the example is as follows:\\n* Host OS: CentOS 8 * Guest OS: Ubuntu 20.04 * Virtual Machine: QEMU-KVM\\nPrerequisites [\\uf0c1](#prerequisites \\\"Permalink to this heading\\\") -------------------------------------------------------------\\n* IOMMU and VT-x should be enabled in BIOS. * `qemu-kvm`   ,   `libvirt`   ,   `virt-install`   should be installed in a host machine.\\nSetup Instruction [\\uf0c1](#setup-instruction \\\"Permalink to this heading\\\") ---------------------------------------------------------------------\\n### 1. Enabling IOMMU in BIOS and Linux OS [\\uf0c1](#enabling-iommu-in-bios-and-linux-os \\\"Permalink to this heading\\\")\\nFirst of all, you need to enable IOMMU in BIOS and Linux OS. The following command shows if IOMMU is enabled.\\n``` dmesg | grep -e DMAR -e IOMMU\\n```\\nYou will be able to see some messages related to DMAR or IOMMU if IOMMU is enabled. If you cannot find any messages related to DMAR or IOMMU, you need to enable IOMMU in BIOS, Linux OS or both.\\nThe ways to enable IOMMU in BIOS may depend on server or motherboard models. Please refer to the manufacturer\\u2019s manual.\\nYou check if IOMMU is enabled in Linux OS as follows:\\n``` grep GRUB_CMDLINE_LINUX \/etc\/default\/grub | grep iommu\\n```\\nIf you cannot find any messages related to IOMMU, please add `intel_iommu=on` for Intel CPU or `amd_iommu=on` for AMD CPU to `GRUB_CMDLINE_LINUX` in `\/etc\/default\/grub` and apply the changes by rebooting the machine.\\nIf you use a legacy BIOS boot mode or UEFI boot mode, the way to enable IOMMU in Linux OS can be different.\\n* Legacy BIOS boot mode:   `grub2-mkconfig      -o      \/boot\/grub2\/grub.cfg` * UEFI boot mode,   `grub2-mkconfig      -o      \/boot\/efi\/EFI\/{linux_distrib}\/grub.cfg`   .\\nPlease replace `{linux_distrib}` with a Linux OS name, such as `centos` , `redhat` , or `ubuntu` .\\n### 2. Loading `vfio-pci` module [\\uf0c1](#loading-vfio-pci-module \\\"Permalink to this heading\\\")\\nPlease make sure if the kernel module `vfio-pci` is loaded.\\n> ``` > [root@localhost ~]# lsmod | grep vfio_pci > vfio_pci               61440  0 > vfio_virqfd            16384  1 vfio_pci > vfio_iommu_type1       36864  0 > vfio                   36864  2 vfio_iommu_type1,vfio_pci > irqbypass              16384  2 vfio_pci,kvm >  > ```\\nIf `vfio_pci` is not loaded yet, please run `modprobe\\nvfio-pci` to load the module. In some OS environments, you don\\u2019t have to load `vfio-pci` . To make sure, please refer to the OS manual.\\n### 3. Checking if a virtual machine tool is ready [\\uf0c1](#checking-if-a-virtual-machine-tool-is-ready \\\"Permalink to this heading\\\")\\nPlease check if a virtual machine tool is ready to run as follows. If `virt-host-validate` is not found, please install the prerequisite packages described in [Prerequisites](#vmsupport-prerequisites)\\n> ``` > [root@localhost ~]# virt-host-validate >   QEMU: Checking for hardware virtualization                                 : PASS >  >   QEMU: Checking for device assignment IOMMU support                         : PASS >   QEMU: Checking if IOMMU is enabled by kernel                               : PASS >  > ```\\nIf check items are PASSED, the virtual machine tool is ready.\\n### 4. Finding Warboy\\u2019s PCIe device name [\\uf0c1](#finding-warboy-s-pcie-device-name \\\"Permalink to this heading\\\")\\nPCI BDF (Bus, Device, Function) is a unique identifier assigned to every PCIe device connected to a machine. Please find a PCI BDF of a Warboy card that you want to pass through to a virtual machine.\\n> ``` > [root@localhost ~]# lspci -nD | grep 1ed2 > 0000:01:00.0 1200: 1ed2:0000 (rev 01) >  > ```  `1ed2` is the PCI vendor ID of FursioaAI Inc. `01:00.0` is the PCI BDF of a Warboy card in the above example. Your PCI BDF will be different according to motherboard model, server model, and PCIe slot.\\nAlternatively, you can use `lspci\\n-DD` command to show a PCI BDF list with vendor names and find a Warboy card from the list. The vendor names depend on PCIe ID database in OS. If the database is outdated in OS, the command will show `Device\\n1ed2:0000` instead of `FuriosaAI,\\nInc.\\nWarboy` .\\nYou can update outdated PCIe ID database by running `update-pciids` in shell.\\nOnce you find the PCIe BDB name, you can find a PCIe device name accepted by a virtual machine tool as follows:\\n> ``` > [root@localhost ~]# virsh nodedev-list | grep pci > ... >  > pci_0000_01_00_0 >  > ```\\nA PCIe device name consists of `pci_` and a PCI BDF concatnated with `_` . In the above example, `pci_0000_01_00_0` is the PCIe device name of a Warboy card.\\n### 5. Creating a virtual machine [\\uf0c1](#creating-a-virtual-machine \\\"Permalink to this heading\\\")\\nIf you reach here, you are ready to create a virtual machine with a Warboy passthrough device. Please create a virtual machine as follows.\\n> ``` > virt-install --name ubuntu-vm \\\\ >   --os-variant ubuntu20.04 \\\\ >   --vcpus 2 \\\\ >   --memory 4096 \\\\ >   --location \/var\/lib\/libvirt\/images\/ubuntu-20.04.5-live-server-amd64.iso,kernel=casper\/vmlinuz,initrd=casper\/initrd \\\\ >   --network bridge=br0,model=virtio \\\\ >   --disk size=50 \\\\ >   --graphics none \\\\ >   --host-device=pci_0000_01_00_0 >  > ```\\nPlease note the option `--host-device` with the PCIe device name that we found in the previous step. Also, you can add more options to the command for your use cases.\\nIn the above example, we set the guest OS image. So, it will start the guest OS installation step once the virtual machine starts. Ubuntu 20.04 or above is recommended for a guest OS. You can find recommended OS distributions for FuriosaAI SDK at [Minimum requirements for SDK installation](installation.html#minimumrequirements) .\\n### 6. Checking the availability of a Warboy device in VM [\\uf0c1](#checking-the-availability-of-a-warboy-device-in-vm \\\"Permalink to this heading\\\")\\nPlease make sure if the Warboy device is available on the virtual machine. `lspci` will shows all PCIe devices available on the virtual machine as follows.\\n> ``` > furiosa@ubuntu-vm:~$ lspci > ... > 05:00.0 Processing accelerators: Device 1ed2:0000 (rev 01) > ... >  > furiosa@ubuntu-vm:~$ sudo update-pciids >  > furiosa@ubuntu-vm:~$ lspci | grep Furiosa > 05:00.0 Processing accelerators: FuriosaAI, Inc. Warboy (rev 01) >  > ```\\n### 7. SDK installation [\\uf0c1](#sdk-installation \\\"Permalink to this heading\\\")\\nOnce you confirm that Warboy is available in a virtual machine, please install [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) to install SDK and move forward next steps.\\n[Previous](kubernetes_support.html \\\"Kubernetes Support\\\") [Next](tutorials.html \\\"Tutorial and Code Examples\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/performance.html","title":"performance","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"333851a4-2ea4-4903-87a2-0d50943faf1f\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/performance.html\", \"title\": \"performance\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Performance Optimization * [View page source](..\/_sources\/software\/performance.rst.txt)\\n---\\nPerformance Optimization [\\uf0c1](#performance-optimization \\\"Permalink to this heading\\\") ===================================================================================\\nTo ensure efficient inference serving in production, it\\u2019s essential to focus on throughput and latency as key metrics. Furiosa SDK offers two optimization methods for both throughput and latency:\\n* **Model Optimization**   : are ways to optimize models during the phases of model development,   quantization, and compilation. Some optimization techniques may modify the models, leading to   more efficient compiled programs. * **Runtime Optimization**   : are ways to optimize the runtime execution of compiled programs.   They are about how to optimize inference codes through Runtime library depending   on the characteristics of models workloads for higher throughput.\\nIn this section, we will discuss the performance metrics and how to optimize them in both above ways.\\nPerformance Metrics: Latency and Throughput [\\uf0c1](#performance-metrics-latency-and-throughput \\\"Permalink to this heading\\\") ------------------------------------------------------------------------------------------------------------------------\\n*Latency* is one of the major performance evaluation criteria for model inference. it\\u2019s a measure of how long a single inference takes from when the input data is passed to the model until the output value is received. With low latency, users can experience high responsiveness.\\nAnother performance evaluation criterion is throughput. Throughput means the number of inferences that can be processed within a unit of time. Throughput implies that how many requests a system handle simultaneously.\\nA single end-to-end inference consists of three kinds of operations: NPU execution, CPU computation and IO operation between host and NPU device. Three kinds of operations run independently without blocking one another. So, multiple inferences can run simultaneously while different operations run. When we continue to run multiple requests simultaneously, the longer operation among NPU, CPU, and IO operations is likely to determine the inference time. This is because the shorter operations will be hidden by other longer operations. This is a key characteristic to understand how to optimize the performance of a model. You can find more details at [Concurrency Optimization](#concurrencyoptimization) .\\nNPU utilization is not a performance metrics, but it\\u2019s one of the key metrics to indicate how much the model utilizes a NPU device for inferences. NPU utilization can be defined as the proportion of time the NPU is used during inference. With NPU utilziation, we can evaluate how well the model is well-optimized for NPU acceleration. Sometimes, it may also imply the room for further optimization opportunities. Please refer to [Toolkit](cli.html#toolkit) for how to measure NPU utilization.\\n### Performance Profiling [\\uf0c1](#performance-profiling \\\"Permalink to this heading\\\")\\nTo analyze the performance of a workload, we need to measure performance metrics as well as we need to have a closer look at the times of NPU executions, CPU computations, and I\/O operations.\\nFor them, there are two useful tools in Furiosa SDK.\\n* [furiosa-bench (Benchmark Tool)](cli.html#furiosabench)   is a tool to measure the performance metrics such as latencies and   throughput (i.e., QPS - queries per second). * [Performance Profiling](profiler.html#profiling)   provides a way to measure the durations of NPU executions and other operations.  `furiosa-bench` also provides `--trace-output` option to generate a trace file. This is another easy way to measure the durations of NPU executions and other operations from a running workload.\\nModel Optimization [\\uf0c1](#model-optimization \\\"Permalink to this heading\\\") -----------------------------------------------------------------------\\nIn this seciton, we introduce some model optimization techniques to improve the performance of models. They key idea of the model optimization is to identify the bottleneck parts (usually operators) of the model and to reduce the times of the bottleneck parts or remove them.\\nFor example, if some operators of a model are not accelerated by NPU, they can be a major bottleneck. If you remove the operators or replace them with other equivalents, the inference latency can be reduced significantly.\\n### Optimizing `Quantize` Operator [\\uf0c1](#optimizing-quantize-operator \\\"Permalink to this heading\\\")\\nFuriosaAI\\u2019s first-generation NPU, Warboy, supports only int8 type. As the majority of deep learning models are built upon floating point types like fp32 and fp16, to execute these models on Warboy, a quantization step is necessary to convert the fp32 weights to int8 model weights. In addition, the quantization step adds `quantize` , `dequantize` operators to the input and output parts of the model respectively. `quantize` and `dequantize` operators convert fp32 input values to int8 values and vice versa. Those operators are executed on the CPU and are time-consuming.\\nInputs of many CNN-based models are images. In particular, an image is represented as RGB channels. In other words, a single image is composed of three images for each channel of RGB, where each image is represented with 8-bit integer values, ranging from 0 to 255.\\nTo feed an image to a model, we need to convert the `int8` values of each RGB channel to `fp32` values, and `quantize` operator in the model converts `fp32` values to `int8` values. It\\u2019s unnecessary if we can feed RGB images in `int8` to a model directly.\\nTo support this optimization, `furiosa-quantizer` provides the `ModelEditor` API. `ModelEditor` takes the model optimized by `optimize_model()` .\\n``` model = onnx.load_model(\\\"yolox_l.onnx\\\") model = optimize_model(model)\\neditor = ModelEditor(model)\\n```  `convert_input_type()` method of `ModelEditor` takes a tensor name and a data type as arguments. It modifies the data type of the input tensor in the model to be the given arguments. The target type can be either `INT8` or `UINT8` . You can find the tensor name through `get_pure_input_names` method.\\n``` input_tensor_name = get_pure_input_names(model)[0]\\n# Convert this input tensor to uint8 editor.convert_input_type(input_tensor_name, TensorType.UINT8)\\n```\\nAs you can see in the above example, `convert_input_type` changes the data type of the input tensor to be `uint8` . The reason why we use `uint8` instead of `int8` is that the pixel values are represented as positive values.\\nBefore this model modification, `quantize` operator converts `float32` values to `int8` values. After this model modification, the quantize operator converts `uint8` values to `int8` values. This conversion from `uint8` to `int8` is much faster than the conversion from `float32` to `int8` . The followings are the the benchmark results of before and after the model modification. Also, the figure shows how the `quantize` operator is changed.\\nQuantization in YOLOX\\\\_L\\n[\\uf0c1](#id2 \\\"Permalink to this table\\\")\\n| Input type | `Quantize` execution time | | --- | --- | | float32 | 60.639 ms | | uint8 | 0.277 ms |\\nquantize without `ModelEditor`  [\\uf0c1](#id3 \\\"Permalink to this image\\\")\\nquantize with `convert_input_type`  [\\uf0c1](#id4 \\\"Permalink to this image\\\")\\nWarning\\nThis optmization may affect the accurarcy of the model. Since it depends on models and applications, it is recommended to validate the accuracy of the model.\\nThe following is a real example code to use `ModelEditor` API with `convert_input_type()` .\\n``` #!\/usr\/bin\/env python\\nimport time import numpy as np import onnx import torch import torchvision from torchvision import transforms import tqdm\\nfrom furiosa.optimizer import optimize_model from furiosa.quantizer import get_pure_input_names, quantize, Calibrator, CalibrationMethod, ModelEditor, TensorType from furiosa.runtime import session from furiosa.runtime.profiler import profile\\ntorch_model = torchvision.models.resnet50(weights='DEFAULT') torch_model = torch_model.eval()\\ndummy_input = (torch.randn(1, 3, 224, 224),)\\ntorch.onnx.export(     torch_model,  # PyTorch model to export     dummy_input,  # model input     \\\"resnet50.onnx\\\",  # where to save the exported ONNX model     opset_version=13,  # the ONNX OpSet version to export the model to     do_constant_folding=True,  # whether to execute constant folding for optimization     input_names=[\\\"input\\\"],  # the ONNX model's input names     output_names=[\\\"output\\\"],  # the ONNX model's output names )\\nonnx_model = onnx.load_model(\\\"resnet50.onnx\\\") onnx_model = optimize_model(onnx_model)\\ncalibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM) calibrator.collect_data([[torch.randn(1, 3, 224, 224).numpy()]]) ranges = calibrator.compute_range()\\neditor = ModelEditor(onnx_model) input_tensor_name = get_pure_input_names(onnx_model)[0]\\n# Convert the input type to uint8 editor.convert_input_type(input_tensor_name, TensorType.UINT8)\\ngraph = quantize(onnx_model, ranges)\\nwith open(\\\"trace.json\\\", \\\"w\\\") as trace:     with profile(file=trace) as profiler:         with session.create(graph) as session:             image = torch.randint(256, (1, 3, 224, 224), dtype=torch.uint8)             with profiler.record(\\\"pre\\\"):                 image = image.numpy()             with profiler.record(\\\"inf\\\"):                 outputs = session.run(image)             with profiler.record(\\\"post\\\"):                 prediction = np.argmax(outputs[0].numpy(), axis=1)\\n```\\n### Optimizing `Dequantize` Operator [\\uf0c1](#optimizing-dequantize-operator \\\"Permalink to this heading\\\")\\nSimilar to the above `Quantize` operator optimization, `Dequantize` operator also can be optimized in the similar way.\\nIf the model output tensor is `fp32` , the output of `int8` values must be converted to f32 values. `Dequantize` operator converts int8 values to fp32 values, and it\\u2019s executed on CPU. If the model output is an RGB image or something else which can be represented as `int8` or `uint8` values, we can skip converting `int8` or `uint8` to `fp32` . It will reduce the inference latency significantly.\\nWe can enable this optimization by using `convert_output_type()` method of `ModelEditor` . `convert_output_type()` method can modifies a model output by a given tensor name and a target data type. The target type can be either `INT8` or `UINT8` .\\nquantize with `convert_output_type`  [\\uf0c1](#id5 \\\"Permalink to this image\\\")\\nquantize with `convert_input_type` and `convert_output_type`  [\\uf0c1](#id6 \\\"Permalink to this image\\\")\\nNote\\nFuriosa Compiler may automatically apply this optimization to the model even if this optmization is not explicitly applied. In that case, the optimization by Furiosa Compiler may result in lower latency than the one manually applied by `ModelEditor` . It is recommended to do experiments to find the best option.\\nWarning\\nThis optmization may affect the accurarcy of the model. Since it depends on models and applications, it is recommended to validate the accuracy of the model.\\nThe following is an real example code to use `convert_output_type` option.\\n``` #!\/usr\/bin\/env python\\nimport time import numpy as np import onnx import torch import torchvision from torchvision import transforms import tqdm\\nfrom furiosa.optimizer import optimize_model from furiosa.quantizer import get_output_names, quantize, Calibrator, CalibrationMethod, ModelEditor, TensorType from furiosa.runtime import session from furiosa.runtime.profiler import profile\\ntorch_model = torchvision.models.resnet50(weights='DEFAULT') torch_model = torch_model.eval()\\ndummy_input = (torch.randn(1, 3, 224, 224),)\\ntorch.onnx.export(     torch_model,  # PyTorch model to export     dummy_input,  # model input     \\\"resnet50.onnx\\\",  # where to save the exported ONNX model     opset_version=13,  # the ONNX OpSet version to export the model to     do_constant_folding=True,  # whether to execute constant folding for optimization     input_names=[\\\"input\\\"],  # the ONNX model's input names     output_names=[\\\"output\\\"],  # the ONNX model's output names )\\nonnx_model = onnx.load_model(\\\"resnet50.onnx\\\") onnx_model = optimize_model(onnx_model)\\ncalibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM) calibrator.collect_data([[torch.randn(1, 3, 224, 224).numpy()]]) ranges = calibrator.compute_range()\\neditor = ModelEditor(onnx_model) output_tensor_name = get_output_names(onnx_model)[0]\\n# output \\ud150\\uc11c\\uc758 \\uc790\\ub8cc\\ud615\\uc744 int8\\ub85c \\ubcc0\\ud658 editor.convert_output_type(output_tensor_name, TensorType.INT8)\\ngraph = quantize(onnx_model, ranges)\\nwith open(\\\"trace.json\\\", \\\"w\\\") as trace:     with profile(file=trace) as profiler:         with session.create(graph) as session:             image = torch.randint(256, (1, 3, 224, 224), dtype=torch.uint8)             with profiler.record(\\\"pre\\\"):                 image = image.numpy()             with profiler.record(\\\"inf\\\"):                 outputs = session.run(image)             with profiler.record(\\\"post\\\"):                 prediction = np.argmax(outputs[0].numpy(), axis=1)\\n```\\n### Lower\/Unlower Acceleration [\\uf0c1](#lower-unlower-acceleration \\\"Permalink to this heading\\\")\\nWarboy internally uses its inherent memory layout to accelerate the computation by leveraging the NPU architecture. For the memory layout, `Lower` operator reshapes the input tensor to the NPU\\u2019s memory layout and `Unlower` operator reshapes the output tensor from the NPU\\u2019s memory layout to the original shape. For them, Furiosa Compiler automatically adds `Lower` and `Unlower` operators to the model.\\nIn many cases, `Lower` and `Unlower` are executed on CPU, causing some overhead of the inference latency. However, if the last axis of input or output tensor shape is `width` and the size of the last axis is a multiple of 32, `Lower` and `Unlower` operators can be accleerated on NPU. Then, the inference latency can be reduced significantly.\\nTherefore, if you are able to specify the shape of the input and output tensors, it\\u2019s more optimal to use `NxCxHxW` and specify the width as a multiple of 32. Also, this optimization can be applied independently to the input and output tensors respectively.\\n### Removal of Pad\/Slice [\\uf0c1](#removal-of-pad-slice \\\"Permalink to this heading\\\")\\nAs described above, the `Lower` \/ `Unlower` operations can be accelerated if the last axis of the tensor for either operator is width\\nand the size of the last axis is a multiple of 32.\\nIf the last tensor axis of `Lower` is width\\nbut not a multiple of 32, Furiosa Compiler may automatically add `Pad` operator before `Lower` operator to adjust the size of the last axis to a multiple of 32. In the similar way, Furiosa Compiler may automatically add `Slice` operator after `Unlower` operator to slice data contents from the tensor with the last axis of a multiple of 32 to the original tensor shape.\\nThis optimization gains some performance benefits by accelerating `Lower` \/ `Unlower` operations. However, `Pad` and `Slice` requires CPU computation. There\\u2019s futher optimization opportunity to remove even `Pad` and `Slice` operators too. If you can accept the constraints of the input and output tensor shapes, it is strongly recommended using the shape of the tensors `NxCxHxW` and a multiple of 32 of the width.\\n### Change the Order of Input Tensor Axes at Compiler Time [\\uf0c1](#change-the-order-of-input-tensor-axes-at-compiler-time \\\"Permalink to this heading\\\")\\nAs we discussed above, there are more optimization opportunities if the last axis of the input tensor is `width` . However, changing the order of axes requires to modify the models. It may require some effort to modify the original models in some cases.\\nSo, Furiosa Compiler provides a way to change the order of the input tensor axes at compile time. You can specify `permute_input` option in compiler config to specify the new order of the input tensor axes as follows:\\n* `compiler_config      =      {      \\\"permute_input\\\":      [[0,      3,      1,      2]]      }`      > + The parameter of   >   `permute_input`   >   is the same as   >   [torch.permute](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.permute.html)   >   .   > + For example, the above example code will change   >   `NxHxWxC`   >   to   >   `NxCxHxW`   >   .\\nThe following is a real example code to use `permute_input` option.\\n``` #!\/usr\/bin\/env python\\nimport time\\nimport numpy as np import onnx import torch import tqdm\\nfrom furiosa.optimizer import optimize_model from furiosa.quantizer import quantize, Calibrator, CalibrationMethod from furiosa.runtime import session from furiosa.runtime.profiler import profile\\nonnx_model = onnx.load_model(\\\"model_nhwc.onnx\\\") onnx_model = optimize_model(onnx_model)\\ncalibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM) calibrator.collect_data([[torch.randn(1, 512, 512, 3).numpy()]]) ranges = calibrator.compute_range()\\ngraph = quantize(onnx_model, ranges)\\ncompiler_config = { \\\"permute_input\\\": [[0, 3, 1, 2]] }\\nwith open(\\\"trace.json\\\", \\\"w\\\") as trace:     with profile(file=trace) as profiler:         with session.create(graph, compiler_config=compiler_config) as session:             image = torch.randint(256, (1, 3, 512, 512), dtype=torch.uint8)             with profiler.record(\\\"pre\\\"):                 image = image.numpy()             with profiler.record(\\\"inf\\\"):                 outputs = session.run(image)             with profiler.record(\\\"post\\\"):                 prediction = outputs[0].numpy()\\n```\\nThis is another case to use `permute_input` option. In some cases, it\\u2019s necessary to change the order of the input tensor axes from `NxCxHxW` to `NxHxWxC` . Python OpenCV is a popular computer vision library. `cv2.imread()` of OpenCV returns a 3D NumPy array with `HxWxC` order. If the axes of the input tensors of a model are `NxCxHxW` , it requires to transpose the tensor. The transpose is a time-consuming operation running in CPU. In this case, we can remove the transpose operation if we change the order of the input tensor axes of the model to the same as OpenCV\\u2019s output; e.g., `NxHxWxC` . It will reduce the inference latency significantly.\\n### Optimization of Large Input and Output Tensors [\\uf0c1](#optimization-of-large-input-and-output-tensors \\\"Permalink to this heading\\\")\\nSome models have large images and as inputs and outputs. For example, Denoising and super resolution models basically take large images as inputs and outputs. Depending on your implementation, those models may be slow in Furiosa SDK and Warboy. Furiosa Compiler optimizes the models with various techniques while preserving the semantics of the original models. Basically, Furiosa Compiler handles large tensors as defined by the model. However, if the size of tensors is too large, it may exceed SRAM memory of Warboy, causing more I\/O operations between DRAM and SRAM. It may result in poor performance.\\nWe can optimize this case by splitting a large tensor into a number of smaller tensors and then merging the results. Generally, we can apply this optimization to denosing and super resolution models because the small parts of images can be independently processed and merged to get the final results. The small parts of images are called patches, and the size of patches is called patch size.\\nTo understand the optimization mechanism, we need to understand how the Furiosa Compiler works. Furiosa Compiler tries to hide IO times between DRAM and SRAM by overlapping them with NPU executions. In other words, NPU can execute operators while I\/O operations are working. If we split a large tensor into a number of smaller tensors, the number of I\/O operations can be hidden by NPU executions.\\nOnce we decide to use this optimization, the next step is to determine the patch size. Here, one good metric to determine the patch size is the ratio of the time spent on NPU executions. The smaller the patch size, the more time is spent on NPU computation. In contrast, the larger the patch size, the more time is spent on I\/O operations.\\nAlso, this optimization can be combined with using multiple NPU devices. The multiple patches can run across multiple NPU devices in parallel.\\n### More Batch, More NPU Utilization [\\uf0c1](#more-batch-more-npu-utilization \\\"Permalink to this heading\\\")\\nFor some models with small weights or few layers, the NPU utilization may be low. In this case, we can increase the batch size to make the NPU utilization higher. With this optimization, the inference may still have the same latency, but its throughput can be increased significantly.\\nA batch size can be specified when compiling a model with `--batch-size` option as follows:\\n`furiosa-compiler\\n--batch-size\\n32\\n--target-npu\\nwarboy\\nmnist.dfg\\n-o\\nmnist.enf`\\nA batch size also can be specified when creating a session with `batch_size` option. You can learn more about the `batch_size` option from [Runner API](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#furiosa.runtime.furiosa.runtime.Runtime.create_runner) .\\n### Single PE vs Fusion PE [\\uf0c1](#single-pe-vs-fusion-pe \\\"Permalink to this heading\\\")\\nA single Warboy chip consists of two processing elements (PEs). Each PE of Warboy has its own control unit, and the two PEs can work independently. In this mode, each PE works with spatially-partitioned memory and processing units. In contrast, two PEs can also be fused as a single PE. In this fused mode, two PEs work as a single PE with an unified memory and processing units.\\nThese two modes allow applications to have more flexibility to optimize the performance. For example, if a model has large weights, we can use a 2PE-fused mode to load the weights for a lower latency. If a model fits in a single PE, we can use two single PEs separately to run the two model instances for higher throughput.\\nIf a workload is latency-oriented, using a 2PE-fused mode is generally recommended. If a workload is throughput-oriented, using two single PEs is generally recommended. It still depends on models and workloads. You need to find the optimal NPU configuration through experiments.\\nThe followings are example commands to compile a model with a single PE or a fused-PE respectively.\\n* Single PE:   `furiosa-compiler      --target-npu      warboy      resnet50.dfg      -o      resnet50.enf` * Fusion PE:   `furiosa-compiler      --target-npu      warboy-2pe      resnet50.dfg      -o      resnet50_2pe.enf`\\nThis NPU configuration can also be specified when creating a [Runtime](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#furiosa.runtime.Runtime) with `device` option which are specified by [Device Configuration](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#device-specification)\\nRuntime Optimization [\\uf0c1](#runtime-optimization \\\"Permalink to this heading\\\") ---------------------------------------------------------------------------\\nSo far, we have discussed the model optimization techniques to reduce the inference latency. After we apply the model optimization, we can futher optimize the performance in Runtime level.\\nAs we mentioned above, an end-to-end inference consists of three operations: NPU execution, CPU computation, and IO operation. Three kinds of operations can run independently without blocking one another. They can be overlapped if we run multiple inferences simultaneously. Leveraging this characteristic is a key idea of the runtime optimization.\\n### More inference concurrency (the number of workers) [\\uf0c1](#more-inference-concurrency-the-number-of-workers \\\"Permalink to this heading\\\")\\nWhen we create a session through [Runner API](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#furiosa.runtime.furiosa.runtime.Runtime.create_runner) ,\\nwe can specify the number of workers as an option. A single worker is a unit that can run inferences independently sharing NPUs. This concept is similar to a thread and CPUs.\\nIf there is only one worker, multiple inference requests are processed sequentially through a single worker. When one inference is completed, the next inference is processed by the owrker. In this case, the NPU can be idle while the CPU is working, causing low NPU utilization.\\nHowever, if there are multiple workers, the workers consume requests from the request queue in Runtime. The multiple inferences can be processed simultaneously. In this case, NPU executions are overlapped with CPU executions, possibly leading to higher NPU utilization.\\nEach worker requires more memory resources to maintain context information for its execution. If the number of workers is too large, the memory resources may be exhausted. If the number of workers is too small, the NPU utilization may be low. Finding the optimal number of workers is important to maximize the performance of the model. Usually, we can find the optimal number of workers through experimentation.\\n### Sync API vs Async APIs [\\uf0c1](#sync-api-vs-async-apis \\\"Permalink to this heading\\\")\\nThere are two types of runtime APIs: Sync API and Async API. Sync API is a blocking API that waits for the completion of the inference. Async APIs are non-blocking APIs that don\\u2019t wait for the completion of the inference. Async APIs allow to request multiple inferences simultaneously and wait for the results asynchronously.  `furiosa.session.create()` a creates a syncronous session. As the below example, `session.run()` is blocked until the result is returned. It generally is enough for batch workloads with large batch sizes, but it\\u2019s not sufficient for serving workloads that handle multiple current requests simultaneously.\\n``` from furiosa.runtime import session\\nwith session.create(model) as sess:     input = ...     outputs = sess.run(input) # Wait for completion     ...\\n```\\nTo overcome this limitation, Furiosa SDK provides two types of Async APIs: Queue API and Async\/Await API. They allow to request multiple inferences simultaneously and wait for the results asynchronously. They are also useful to hide I\/O and CPU computation by overlapping them with NPU executions.\\n#### Queue API [\\uf0c1](#queue-api \\\"Permalink to this heading\\\")  `create_async()` creates a pair of a submitter and a queue. With both, we can submit inference requests without waiting for completion and wait for the inference results asynchronously.\\n``` import numpy as np import random\\nfrom furiosa.runtime import session\\nsubmitter, queue = session.create_async(\\\"mnist.onnx\\\",                                         worker_num=2,                                         # Determine how many asynchronous requests you can submit                                         # without blocking.                                         input_queue_size=100,                                         output_queue_size=100)\\nfor i in range(0, 5):     idx = random.randint(0, 59999)     input = np.random.rand(1, 1, 28, 28).astype(np.float32)     submitter.submit(input, context=idx) # non blocking call\\nfor i in range(0, 5):     context, outputs = queue.recv(100) # 100 ms for timeout. If None, queue.recv() will be blocking.     print(outputs[0].numpy())\\nif queue:     queue.close() if submitter:     submitter.close()\\n```\\n#### Using Async\/Await syntax [\\uf0c1](#using-async-await-syntax \\\"Permalink to this heading\\\")\\nIn the the example below, `NPUModel` of furiosa-server provide an easier way to implement a serving application using async\/await API.\\n``` import asyncio import numpy as np\\nfrom furiosa.server.model import NPUModel, NPUModelConfig\\nclass SimpleApplication:     def __init__(self):         self.model = NPUModel(             NPUModelConfig(                 name=\\\"MNIST\\\",                 model=\\\"mnist.onnx\\\",             )         )\\n    async def load(self):         await self.model.load()\\n    async def process(self, image):         input = self.preprocess(image)         tensor = await self.model.predict(input)         output = self.postprocess(tensor)         return output\\n    def preprocess(self, image):         # do preprocess         return image\\n    def postprocess(self, tensor):         # do postprocess         return tensor\\nAPP = SimpleApplication()\\nasync def startup():     await APP.load()\\nasync def run(image):     result = await APP.process(image)     return result\\nif __name__ == \\\"__main__\\\":     asyncio.run(startup())\\n    image = np.random.rand(1, 1, 28, 28).astype(np.float32)     asyncio.run(run(image))\\n```\\n[Previous](quantization.html \\\"Model Quantization\\\") [Next](profiler.html \\\"Performance Profiling\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.9.0.html","title":"0","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"13973d88-9f7d-49e4-8019-68c7f29141b3\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.9.0.html\", \"title\": \"0\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Release Notes - 0.9.0 * [View page source](..\/_sources\/releases\/0.9.0.rst.txt)\\n---\\nRelease Notes - 0.9.0 [\\uf0c1](#release-notes-0-9-0 \\\"Permalink to this heading\\\") ===========================================================================\\nFuriosa SDK 0.9.0 is a major release, including many performance enhancements, additional functions, and bug fixes. In partcular, 0.9.0 release includes the significant improvements of the quantization tools.\\nComponent Version Information\\n[\\uf0c1](#id1 \\\"Permalink to this table\\\")\\n| Package Name | Version | | --- | --- | | NPU Driver | 1.7.0 | | NPU Firmware Tools | 1.4.0 | | NPU Firmware Image | 1.7.0 | | HAL (Hardware Abstraction Layer) | 0.11.0 | | Furiosa Compiler | 0.9.0 | | Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..) | 0.9.0 | | NPU Management CLI (furiosactl) | 0.11.0 | | NPU Device Plugin | 0.10.1 | | NPU Feature Discovery | 0.2.0 |\\nInstalling the latest SDK [\\uf0c1](#installing-the-latest-sdk \\\"Permalink to this heading\\\") -------------------------------------------------------------------------------------\\nIf you are using APT repository, the upgrade process is simpler.\\n``` apt-get update && apt-get upgrade\\n```\\nIf you wish to designate a specific package for upgrade, execute as below: You can find more details about APT repository setup at [Driver, Firmware, and Runtime Installation](..\/software\/installation.html#requiredpackages) .\\n``` apt-get update && \\\\ apt-get install -y furiosa-driver-pdma furiosa-libhal-warboy furiosa-libnux furiosactl\\n```\\nYou can upgrade firmware as follows:\\n``` apt-get update && \\\\ apt-get install -y furiosa-firmware-tools furiosa-firmware-image\\n```\\nYou can upgrade Python package as follows:\\n``` pip install --upgrade pip setuptools wheel pip install --upgrade furiosa-sdk\\n```\\nWarning\\nWhen installing or upgrading the furiosa-sdk without updating pip to the latest version, you may encounter the following errors.\\n``` ERROR: Could not find a version that satisfies the requirement furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk) (from versions: none) ERROR: No matching distribution found for furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk)\\n```\\nMajor changes [\\uf0c1](#major-changes \\\"Permalink to this heading\\\") -------------------------------------------------------------\\n### Quantization tool [\\uf0c1](#quantization-tool \\\"Permalink to this heading\\\")\\nQuantization tool is a library that converts a pre-trained model to a quantized model. You can refer to more details at [Model Quantization](..\/software\/quantization.html#modelquantization) 0.9.0 release includes the API improvement and new calibration methods, possibly leading to better accuracy.\\n* Added new quantization-related APIs that are more flexible and solid. (   `furiosa.quantizer`   ,   `furiosa.optimizer`   )\\n``` optimized_onnx_model = optimize_model(source_onnx_model) calibrator = Calibrator(optimized_onnx_model, CalibrationMethod.MIN_MAX_ASYM) for calibration_data, _ in tqdm.tqdm(calibration_dataloader, desc=\\\"Calibration\\\", unit=\\\"images\\\", mininterval=0.5):   calibrator.collect_data([[calibration_data.numpy()]]) ranges = calibrator.compute_range() quantizated_graph = quantize(optimized_onnx_model, ranges)\\n```\\n* Added an option to decide whether to perform quantize at the beginning of the model.      + Instead of     `without_quantize`     being removed from the compiler options, it can be specified via the argument     `with_quantize`     to the     `quantize`     function. * The   `normalized_pixel_outputs`   argument to the   `quantize`   function can be set to convert the model output to uint8 instead of dequantizing to fp32.      + A tensor with an element range of     `(0.          ,          1.)`     can be optimized to convert to pixel data in uint8. * Provides more calibration methods.\\nSupported Calibration Methods\\n[\\uf0c1](#id2 \\\"Permalink to this table\\\")\\n| Calibration Method | Asymmetric | QuasiSymmetric | | --- | --- | --- | | Min-Max | MIN\\\\_MAX\\\\_ASYM | MIN\\\\_MAX\\\\_SYM | | Entropy | ENTROPY\\\\_ASYM | ENTROPY\\\\_SYM | | Percentile | PERCENTILE\\\\_ASYM | PERCENTILE\\\\_SYM | | Mean squared error | MSE\\\\_ASYM | MSE\\\\_SYM | | Signal-to-quantization-noise ratio | SQNR\\\\_ASYM | SQNR\\\\_SYM |\\nTo ensure the effectiveness of new calibration methods, we measured the accuracy of 10 popular models with the new calibration methods. Among them, 8 models showed better accuracy than the existing calibration methods. For example, the accuracy of EfficientNet-B0 increased by 57.452%. With the min-max calibration method, EfficientNet-B0 had an accuracy of 16.104%. In contrast, with the percentile calibration method, the accuracy was 73.556%. The details of the experiment results can be found at [Quantization Accuracy](..\/software\/quantization.html#quantizationaccuracytable) .\\nFor more information on installing and using the new quantizer, you can refer to the following examples.\\n* [Tutorial: How to use Furiosa SDK from Start to Finish](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/v0.9.0\/examples\/notebooks\/HowToUseFuriosaSDKFromStartToFinish.ipynb)\\n### Compiler [\\uf0c1](#compiler \\\"Permalink to this heading\\\")\\n* Added acceleration support for operators Lower, Unlower * Added acceleration support for operator Dequantize * Support for executing binaries that are larger than the hardware\\u2019s instruction memory * Improved scheduler and memory allocator to eliminate unnecessary I\/O * Various improvements optimize compilation for better execution performance\\n### furiosa-toolkit [\\uf0c1](#furiosa-toolkit \\\"Permalink to this heading\\\")\\nThe `furiosactl` command-line tool included in the furiosa-toolkit 0.11.0 release includes improvements to the includes the following major improvements\\nThe newly added `furiosactl\\ntop` command is used to view utilization by NPU device over time.\\n``` $ furiosactl top --interval 200 NOTE: furiosa top is under development. Usage and output formats may change. Please enter Ctrl+C to stop. Datetime                        PID       Device        NPU(%)   Comp(%)   I\/O(%)   Command 2023-03-21T09:45:56.699483936Z  152616    npu1pe0-1      19.06    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:56.906443888Z  152616    npu1pe0-1      51.09     93.05     6.95   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.110489333Z  152616    npu1pe0-1      46.40     97.98     2.02   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.316060982Z  152616    npu1pe0-1      51.43    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.521140588Z  152616    npu1pe0-1      54.28     94.10     5.90   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.725910558Z  152616    npu1pe0-1      48.93     98.93     1.07   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.935041998Z  152616    npu1pe0-1      47.91    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:58.13929122Z   152616    npu1pe0-1      49.06     94.94     5.06   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n```\\nThe `furiosactl\\ninfo` command has been improved to display concise information about each device. As before, you can enter the `--full` option if you want to see more information about a device.\\n``` $ furiosactl info +------+--------+----------------+-------+--------+--------------+ | NPU  | Name   | Firmware       | Temp. | Power  | PCI-BDF      | +------+--------+----------------+-------+--------+--------------+ | npu1 | warboy | 1.6.0, 3c10fd3 |  54\\u00b0C | 0.99 W | 0000:44:00.0 | +------+--------+----------------+-------+--------+--------------+\\n$ furiosactl info --full +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+ | NPU  | Name   | UUID                                 | S\/N               | Firmware       | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+ | npu1 | warboy | 00000000-0000-0000-0000-000000000000 | WBYB0000000000000 | 1.6.0, 3c10fd3 |  54\\u00b0C | 0.99 W | 0000:44:00.0 | 511:0   | +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n```\\nMore information about installing and using `furiosactl` can be found in [furiosa-toolkit](..\/software\/cli.html#toolkit) .\\n[Previous](0.10.0.html \\\"Release Notes - 0.10.0\\\") [Next](0.8.0.html \\\"Release Notes - 0.8.0\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/profiler.html","title":"profiler","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"64f0ffea-6087-4f27-8889-83479e61e89e\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/profiler.html\", \"title\": \"profiler\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Performance Profiling * [View page source](..\/_sources\/software\/profiler.rst.txt)\\n---\\nPerformance Profiling [\\uf0c1](#performance-profiling \\\"Permalink to this heading\\\") =============================================================================\\nLow latency and high throughput performance are critical factors in many DNN applications. For performance optimization, model developers and ML engineers must understand the model performance and be able to analyze bottlenecks. To assist developers with this process, Furiosa SDK provides a profiling tool.\\nTrace Analysis [\\uf0c1](#trace-analysis \\\"Permalink to this heading\\\") ---------------------------------------------------------------\\nTrace analysis provides structured data on execution time by step, by actually executing model inference task. You can also visualize the data using the [Trace Event Profiling Tool](https:\/\/www.chromium.org\/developers\/how-tos\/trace-event-profiling-tool\/) function of the Chrome web browser.\\nThough small, trace generation generates temporal overheads as it measures time for each step and writes the results to a file. It is thus not enabled by default. You can create trace by using one of the following methods.\\nTracing via Environment Variable [\\uf0c1](#tracing-via-environment-variable \\\"Permalink to this heading\\\") ---------------------------------------------------------------------------------------------------\\nYou can enable trace generation by setting the path of the file to which the trace result will be written in `FURIOSA_PROFILER_OUTPUT_PATH` . The advantage of this method is that the code remains unchanged. The downside is that you cannot set a specific section or category for measurement.\\n``` git clone https:\/\/github.com\/furiosa-ai\/furiosa-sdk cd furiosa-sdk\/examples\/inferences export FURIOSA_PROFILER_OUTPUT_PATH=`pwd`\/tracing.json .\/image_classify.py ..\/assets\/images\/car.jpg\\nls -l .\/tracing.json -rw-r--r-- 1 furiosa furiosa 456493 Jul 27 17:56 .\/tracing.json\\n```\\nIf you enable trace generation through environment variables as described above, a JSON file will be written to the path specified by the environment variable `FURIOSA_PROFILER_OUTPUT_PATH` . If you enter `chrome:\/\/tracing` in Chrome\\u2019s address bar, the trace viewer will start. Click the `Load` button in the upper left corner of the trace viewer, and select the saved file ( `tracing.json` in the example above) to view the trace result.\\nTracing via Profiler Context [\\uf0c1](#tracing-via-profiler-context \\\"Permalink to this heading\\\") -------------------------------------------------------------------------------------------\\nYou can also trace a model inference performance by using a Profiler Context in your Python code. The advantages of this method, in comparison to the tracing by environment variable, are as follows:\\n* Allow to enable trace immediately even in interactive environments, such as Python Interpreter or Jupyter Notebook * Allow to specify labels to certain inference runs * Allow to measure specified operator categories selectively\\n``` #!\/usr\/bin\/env python\\nimport numpy as np from furiosa.runtime.profiler import profile from furiosa.runtime.sync import create_runner\\n# You can find 'examples' directory of the root of furiosa-sdk source tree model_path = \\\"examples\/assets\/quantized_models\/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\\\"\\nwith open(\\\"mobilenet_v1_trace.json\\\", \\\"w\\\") as output:     with profile(file=output) as profiler:         with create_runner(model_path) as runner:             input_shape = runner.model.input(0).shape\\n            with profiler.record(\\\"warm up\\\") as record:                 for _ in range(0, 2):                     runner.run([np.uint8(np.random.rand(*input_shape))])\\n            with profiler.record(\\\"trace\\\") as record:                 for _ in range(0, 2):                     runner.run([np.uint8(np.random.rand(*input_shape))])\\n```\\nThe above is a code example using a profiling context. Once the above Python code is executed, the mnist\\\\_trace.json\\nfile is created. The trace results are labelled \\u2018warm up\\u2019 and \\u2018trace\\u2019 as shown below.\\n### Pause\/Resume of Profiler Context [\\uf0c1](#pause-resume-of-profiler-context \\\"Permalink to this heading\\\")\\nTracing long-running jobs can cause following problems:\\n* Produce large trace files which take huge disk space and are difficult to be shared. * Make it hard to identify interesting section when the trace is visualized, without additional processing. * Take much time to produce trace files.\\nTo avoid the above issues, the profiler provides an additional API to temporarily pause or resume a profiler within the context. Users can exclude execution they do not want to profile, thereby reducing profiling overhead and trace file size.\\nThe below is an example of pausing profiler not to trace `warm\\nup` phase between `profile.pause` and `profile.resume` .\\n``` #!\/usr\/bin\/env python\\nimport numpy as np from furiosa.runtime.profiler import RecordFormat, profile from furiosa.runtime.sync import create_runner\\n# You can find 'examples' directory of the root of furiosa-sdk source tree model_path = \\\"examples\/assets\/quantized_models\/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\\\"\\nwith profile(format=RecordFormat.PandasDataFrame) as profiler:     with create_runner(model_path) as runner:         input_shape = runner.model.input(0).shape\\n        # pause profiling during warmup         profiler.pause()\\n        for _ in range(0, 10):             with profiler.record(\\\"warm up\\\") as record:                 runner.run([np.uint8(np.random.rand(*input_shape))])\\n        # resume profiling         profiler.resume()\\n        with profiler.record(\\\"trace\\\") as record:             runner.run([np.uint8(np.random.rand(*input_shape))])\\ndf = profiler.get_pandas_dataframe()\\nassert len(df[df[\\\"name\\\"] == \\\"trace\\\"]) == 1 assert len(df[df[\\\"name\\\"] == \\\"warm up\\\"]) == 0\\n```\\n### Trace analysis using Pandas DataFrame [\\uf0c1](#trace-analysis-using-pandas-dataframe \\\"Permalink to this heading\\\")\\nWith the measured tracing data, in addition to visualizing it with Chrome Trace Format, it can also be expressed and used in Pandas DataFrame, commonly used for data analysis. These are the advantages in comparison to Chrome Trace Format.\\n* Can be used directly in Python Interpreter or Jupyter Notebook interactive shell * Users can directly access DataFrame for analysis, on top of the reporting function which is provided as default\\n``` #!\/usr\/bin\/env python\\nimport numpy as np from furiosa.runtime.profiler import RecordFormat, profile from furiosa.runtime.sync import create_runner\\n# You can find 'examples' directory of the root of furiosa-sdk source tree model_path = \\\"examples\/assets\/quantized_models\/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\\\"\\nwith profile(format=RecordFormat.PandasDataFrame) as profiler:     with create_runner(model_path) as runner:         input_shape = runner.model.input(0).shape\\n        with profiler.record(\\\"warm up\\\") as record:             for _ in range(0, 2):                 runner.run([np.uint8(np.random.rand(*input_shape))])\\n        with profiler.record(\\\"trace\\\") as record:             for _ in range(0, 2):                 runner.run([np.uint8(np.random.rand(*input_shape))])\\nprofiler.print_summary()  # (1)\\nprofiler.print_inferences()  # (2)\\nprofiler.print_npu_executions()  # (3)\\nprofiler.print_npu_operators()  # (4)\\nprofiler.print_external_operators()  # (5)\\ndf = profiler.get_pandas_dataframe()  # (6) print(df[df[\\\"name\\\"] == \\\"trace\\\"][[\\\"trace_id\\\", \\\"name\\\", \\\"thread.id\\\", \\\"dur\\\"]])\\n```\\nAbove is a code example that designates a profiling context format into PandasDataFrame.\\nWhen `(1)` line is executed, the following summary of the results is produced.\\n``` ================================================   Inference Results Summary ================================================ Inference counts                : 4 Min latency (ns)                : 1584494 Max latency (ns)                : 3027309 Mean latency (ns)               : 2136984 Median latency (ns)             : 1968066 90.0 percentile Latency (ns)    : 2752525 95.0 percentile Latency (ns)    : 2889917 97.0 percentile Latency (ns)    : 2944874 99.0 percentile Latency (ns)    : 2999831 99.9 percentile Latency (ns)    : 3024561\\n```\\nWhen `(2)` line is executed, duration of one inference query is shown.\\n``` \\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510 \\u2502 trace_id                         \\u2506 span_id          \\u2506 thread.id \\u2506 dur     \\u2502 \\u255e\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2561 \\u2502 7cf3d3b7439cf4c3fac1a47998783102 \\u2506 403ada67f1d8220e \\u2506 1         \\u2506 3027309 \\u2502 \\u2502 16d65f6f8f1db256d0f39953855dea72 \\u2506 78b065c19c3675ef \\u2506 1         \\u2506 2111363 \\u2502 \\u2502 d0534e3a9f19edadab81954ad28ab44f \\u2506 9a7addaf0f28c9fe \\u2506 1         \\u2506 1824769 \\u2502 \\u2502 70512188522f45b87cfe4f545de3cf2c \\u2506 c75f697f8e72d333 \\u2506 1         \\u2506 1584494 \\u2502 \\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\\n```\\nWhen `(3)` line is executed, elapsed times of NPU executions will be shown:\\n``` \\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510 \\u2502 trace_id                         \\u2506 span_id          \\u2506 pe_index \\u2506 execution_index \\u2506 NPU Total \\u2506 NPU Run \\u2506 NPU IoWait           \\u2502 \\u255e\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2561 \\u2502 8f6fce6c0e52b4735cae3379732a0943 \\u2506 3e1e4a76523cbf89 \\u2506 0        \\u2506 0               \\u2506 119145    \\u2506 108134  \\u2506 18446744073709540605 \\u2502 \\u2502 195366613b1da9b0350c0a3c2a608f42 \\u2506 07dff2e92172fabd \\u2506 0        \\u2506 0               \\u2506 119363    \\u2506 108134  \\u2506 18446744073709540387 \\u2502 \\u2502 3b65b8fa3eabfaf8f815ec9f41fcc7d9 \\u2506 639a366a7f932a23 \\u2506 0        \\u2506 0               \\u2506 119157    \\u2506 108134  \\u2506 18446744073709540593 \\u2502 \\u2502 e48825df32a07e5559f7f50048c08e1f \\u2506 ecaab4915bfda725 \\u2506 0        \\u2506 0               \\u2506 119219    \\u2506 108134  \\u2506 18446744073709540531 \\u2502 \\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\\n```\\nWhen `(4)` line is executed, elapsed times of operators will be shown:\\n``` \\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510 \\u2502 name                    \\u2506 average_elapsed (ns) \\u2506 count \\u2502 \\u255e\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2561 \\u2502 LowLevelConv2d          \\u2506 5327.8               \\u2506 60    \\u2502 \\u2502 LowLevelDepthwiseConv2d \\u2506 1412.285714          \\u2506 56    \\u2502 \\u2502 LowLevelPad             \\u2506 575.785714           \\u2506 56    \\u2502 \\u2502 LowLevelTranspose       \\u2506 250.0                \\u2506 4     \\u2502 \\u2502 LowLevelReshape         \\u2506 2.0                  \\u2506 240   \\u2502 \\u2502 LowLevelSlice           \\u2506 2.0                  \\u2506 12    \\u2502 \\u2502 LowLevelExpand          \\u2506 2.0                  \\u2506 16    \\u2502 \\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\\n```\\nWhen `(5)` line is executed, the time data for operators in the CPU is shown as below.\\n``` \\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510 \\u2502 trace_id                         \\u2506 span_id          \\u2506 thread.id \\u2506 name       \\u2506 operator_index \\u2506 dur    \\u2502 \\u255e\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u256a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2561 \\u2502 e7ab6656cc090a8d05992a9e4683b8b7 \\u2506 206a1d6f351ca4b1 \\u2506 40        \\u2506 Quantize   \\u2506 0              \\u2506 136285 \\u2502 \\u2502 03636fd6c7dbc42f0a9dd29a7283d3fc \\u2506 f636740983e095a6 \\u2506 40        \\u2506 Lower      \\u2506 1              \\u2506 133350 \\u2502 \\u2502 c9a0858f7e0885a976f51c6cb57d3e0f \\u2506 bb6c84f88e453055 \\u2506 40        \\u2506 Unlower    \\u2506 2              \\u2506 44775  \\u2502 \\u2502 8777c67ad9fe597139bbd6970362c2fc \\u2506 63bac982c7b98aba \\u2506 40        \\u2506 Dequantize \\u2506 3              \\u2506 14682  \\u2502 \\u2502 98aeba2a25b0525166b6a4065ab01774 \\u2506 34ccd560571d733f \\u2506 40        \\u2506 Quantize   \\u2506 0              \\u2506 45465  \\u2502 \\u2502 420525dc13ba9624083e0a276f7ee718 \\u2506 9f6d342da5eb86bc \\u2506 40        \\u2506 Lower      \\u2506 1              \\u2506 152748 \\u2502 \\u2502 cb67393f6949bbbb396053c1e00931ff \\u2506 2d724fa6ab8ca024 \\u2506 40        \\u2506 Unlower    \\u2506 2              \\u2506 67140  \\u2502 \\u2502 00424b4f02039ae0ca98388a964062b0 \\u2506 a5fb9fbd5bffe6a6 \\u2506 40        \\u2506 Dequantize \\u2506 3              \\u2506 32388  \\u2502 \\u2502 d7412c59d360067e8b7a2508a30d1079 \\u2506 8e426d778fa95722 \\u2506 40        \\u2506 Quantize   \\u2506 0              \\u2506 71736  \\u2502 \\u2502 6820acf9345c5b373c512f6cd5edcbc7 \\u2506 2d787c2df381f010 \\u2506 40        \\u2506 Lower      \\u2506 1              \\u2506 311310 \\u2502 \\u2502 84d24b02a95c63c3e40f7682384749e4 \\u2506 1236a974a619ff1a \\u2506 40        \\u2506 Unlower    \\u2506 2              \\u2506 51930  \\u2502 \\u2502 8d25dff1cfd6624509cbf95503e93382 \\u2506 673efb3bfb8deac6 \\u2506 40        \\u2506 Dequantize \\u2506 3              \\u2506 12362  \\u2502 \\u2502 4cc60ec1eee7d9f3cdd290d07b303a18 \\u2506 e7903b0a584d6388 \\u2506 40        \\u2506 Quantize   \\u2506 0              \\u2506 56736  \\u2502 \\u2502 c5f04d9fea26e5b52c6ec5e5406775fc \\u2506 701118dabd065e6f \\u2506 40        \\u2506 Lower      \\u2506 1              \\u2506 265447 \\u2502 \\u2502 c5fdfb9cf454da130148e8e364eeee93 \\u2506 5cf3750def19c6e8 \\u2506 40        \\u2506 Unlower    \\u2506 2              \\u2506 35869  \\u2502 \\u2502 e1e650d23061140404915f1df36daf9c \\u2506 ddd76ff19b5cd713 \\u2506 40        \\u2506 Dequantize \\u2506 3              \\u2506 14688  \\u2502 \\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2534\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\\n```\\nWith line `(6)` , you can access DataFrame from the code and perform direct analysis.\\n```                             trace_id   name  thread.id       dur 487  f3b158734e3684f2e043ed41309c4c2d  trace          1  11204385\\n```\\n[Previous](performance.html \\\"Performance Optimization\\\") [Next](serving.html \\\"Model Server (Serving Framework)\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.5.0.html","title":"0","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"65a65779-8c0e-48f0-890a-f6ecc20a9f41\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.5.0.html\", \"title\": \"0\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Release Notes - 0.5.0 * [View page source](..\/_sources\/releases\/0.5.0.rst.txt)\\n---\\nRelease Notes - 0.5.0 [\\uf0c1](#release-notes-0-5-0 \\\"Permalink to this heading\\\") ===========================================================================\\nFuriosaAI SDK 0.5.0 release includes approximately 87 bug fixes, added functionalities, and improvements. The following are some of the key changes:\\nCompiler Improvement [\\uf0c1](#compiler-improvement \\\"Permalink to this heading\\\") ---------------------------------------------------------------------------\\n0.5.0 release adds NPU acceleration support for the following operators. You can find the entire list of acceleration-supported operators at [List of Supported Operators for Warboy Acceleration](..\/npu\/warboy.html#supportedoperators) .\\n* BatchNormalization * ConvTranspose * LeakyRelu * Pow * Sqrt * Sub\\nAdditionally, we have improved the compiler with this release by adding support for [Opset 13](https:\/\/github.com\/onnx\/onnx\/releases\/tag\/v1.8.0) operator of Onnx.\\nSession API Improvement [\\uf0c1](#session-api-improvement \\\"Permalink to this heading\\\") ---------------------------------------------------------------------------------\\nAPI Improvement 1 ( [commit b1d2b74](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/commit\/b1d2b742f9f1ed43dbe7aa5f2ed822ae38d636e4) ): you can now designate NPU device in Session API. With 0.5.0, you can designate NPU device when generating a session, whereas previously you could only designate device to be used through the environment variable `NPU_DEVNAME` . If not explicitly stated, designated device in the environment variable `NPU_DEVNAME` will be used, as was done before.\\n``` from furiosa.runtime import session\\nsess1 = session.create(\\\"model1.onnx\\\", device=\\\"npu0pe0\\\") sess2 = session.create(\\\"model2.onnx\\\", device=\\\"npu0pe1\\\")\\n# Asynchronous API async_sess, queue = session.create_async(\\\"model2.onnx\\\", device=\\\"npu1pe2\\\")\\n```\\nAPI Improvement 2 ( [commit 4f1f114](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/commit\/4f1f1149d137a58ada31df57de6e1234881ccf5b) ) is support for tensor name. The existing API was limited in that it could only identify the order of the tensor built into the model, and pass it as an input parameter to session.run()\\n. From 0.5.0, you can explicitly designate name of input and output tensors as shown below.\\n``` np1 = np.random.randint(0, 255, session_input.shape, dtype=np.uint8) np2 = np.random.randint(0, 255, session_input.shape, dtype=np.uint8) sess1.run_with(outputs=[\\\"output1\\\"], inputs={   \\\"input2\\\": np2,   \\\"input1\\\": np1, }\\n```\\nError Diagnosis Message & Error Handling Improvements [\\uf0c1](#error-diagnosis-message-error-handling-improvements \\\"Permalink to this heading\\\") -------------------------------------------------------------------------------------------------------------------------------------------\\nIn case of an error, version information for debugging and compiler log are output independently. This enables easier bug reporting. You can find more information about reporting issues and customer service at [Bug Report](..\/customer-support\/bugs.html#bugreport) .\\n``` >>> from furiosa.runtime import session >>> session.create(\\\"mnist-8.onnx\\\") Saving the compilation log into \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log ... 2021-11-22T06:30:28.423371Z ERROR fail to compile the model: the static shape of tensor 'input' contains an unsupported dimension value: Some(DimParam(\\\"batch_size\\\")) ================================================================================ Information Dump ================================================================================ - Python version: 3.8.10 (default, Sep 28 2021, 16:10:42)  [GCC 9.3.0] - furiosa-libnux path: libnux.so.0.5.0 - furiosa-libnux version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34) - furiosa-compiler version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34) - furiosa-sdk-runtime version: Furiosa SDK Runtime  (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\\nPlease check the compiler log at \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log. If you have a problem, please report the log file to https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals with the information dumped above. ================================================================================\\n```\\nImprovements such as error handling for the following issues are also included.\\n* Error fix when using duplicate devices (   [commit 01aaa40](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/commit\/01aaa40fd31573dc578fa1c805e1ed36decc9088)   ) * Added timeout of CompletionQueue & error handling fix for session connection termination (   [commit 21cba85](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/commit\/21cba85737840546357f2dd709d33d9bc2b00390)   ) * Hanging issue fix for interruption during compiling   [(commit a0f4bd7](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/commit\/a0f4bd7ab8d199e3e46f9820fe4dc75bfa845e61)   )\\nIntroducing Furiosa Server (serving framework) [\\uf0c1](#introducing-furiosa-server-serving-framework \\\"Permalink to this heading\\\") -----------------------------------------------------------------------------------------------------------------------------\\n0.5.0 includes Furiosa Server\\n, a serving framework that supports GRPC and REST API. You can easily install it by running `pip\\ninstall\\nfuriosa-sdk[server]` . By running it with the command below, the model can be served immediately with the NPU. You can find more detailed instructions and functions at [Model Server (Serving Framework)](..\/software\/serving.html#modelserving) .\\n``` furiosa server \\\\ --model-path MNISTnet_uint8_quant_without_softmax.tflite \\\\ --model-name mnist\\n```\\nIntroducing Furiosa Model package [\\uf0c1](#introducing-furiosa-model-package \\\"Permalink to this heading\\\") -----------------------------------------------------------------------------------------------------\\nFrom 0.5.0, the optimized model for the FuriosaAI NPU can be used directly as a Python package. You can easily install it with the command `pip\\ninstall\\nfuriosa-sdk[models]` ,\\nand can immediately be used in Session API as shown in the following example.\\n``` import asyncio\\nfrom furiosa.registry import Model from furiosa.models.vision import MLCommonsResNet50 from furiosa.runtime import session\\nresnet50: Model = asyncio.run(MLCommonsResNet50()) sess = session.create(resnet50.model, device='npu0pe0')\\n```\\nCommand line NPU management tool: furiosactl [\\uf0c1](#command-line-npu-management-tool-furiosactl \\\"Permalink to this heading\\\") --------------------------------------------------------------------------------------------------------------------------\\n0.5.0 includes furiosactl, a command line NPU management tool. You can install it with `apt\\ninstall\\nfuriosa-toolkit` . You can use this tool to check NPU device status, as well as identify idle NPUs. You can find `apt` server configuration instructions at [APT server configuration](..\/software\/installation.html#setupaptrepository) .\\n``` $ furiosactl info\\n+------+------------------+-------+---------+--------------+---------+ | NPU  | Name             | Temp. | Power   | PCI-BDF      | PCI-DEV | +------+------------------+-------+---------+--------------+---------+ | npu0 | FuriosaAI Warboy |  34\\u00b0C | 12.92 W | 0000:01:00.0 | 510:0   | +------+------------------+-------+---------+--------------+---------+\\n$ furiosactl list +------+-----------+-----------+--------+ | NPU  | DEVNAME   | Type      | Status | +------+-----------+-----------+--------+ | npu0 | npu0      | All PE(s) | Ready  | |      | npu0pe0   | Single PE | Ready  | |      | npu0pe1   | Single PE | Ready  | |      | npu0pe0-1 | PE Fusion | Ready  | +------+-----------+-----------+--------+\\n```\\nKubernetes support [\\uf0c1](#kubernetes-support \\\"Permalink to this heading\\\") -----------------------------------------------------------------------\\n0.5.0 includes NPU support for Kubernetes. You can install the NPU device plugin and node labeller with the command below, and have the NPU be scheduled together when deploying pods. More details can be found at [Kubernetes Support](..\/software\/kubernetes_support.html#kubernetesintegration) .\\n``` kubectl apply -f https:\/\/raw.githubusercontent.com\/furiosa-ai\/furiosa-sdk\/main\/kubernetes\/deployments\/device-plugin.yaml kubectl apply -f https:\/\/raw.githubusercontent.com\/furiosa-ai\/furiosa-sdk\/main\/kubernetes\/deployments\/node-labeller.yaml\\n```\\n[Previous](0.6.0.html \\\"Release Notes - 0.6.0\\\") [Next](..\/customer-support\/bugs.html \\\"Bug Report\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/compiler.html","title":"compiler","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"b6c45005-84e1-46e6-a185-42be1be00b6e\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/compiler.html\", \"title\": \"compiler\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Compiler * [View page source](..\/_sources\/software\/compiler.rst.txt)\\n---\\nCompiler [\\uf0c1](#compiler \\\"Permalink to this heading\\\") ===================================================\\nThe FuriosaAI compiler compiles models of formats [TFLite](https:\/\/www.tensorflow.org\/lite) and [Onnx](https:\/\/onnx.ai\/) model (( [OpSet 13](https:\/\/github.com\/onnx\/onnx\/blob\/main\/docs\/Changelog.md#version-13-of-the-default-onnx-operator-set) or lower version), thereby generating programs that execute inference using FuriosaAI NPU and resources (CPU, memory, etc) of the host machine. In this process, the compiler analyses the model at the operator level, optimizes it, and generates a program so as to maximize NPU acceleration and host resources utilization. Even for models that are not well known, so long as supported operators are utilized well, you can design models that are optimized for the NPU .\\nYou can find the list of NPU acceleration supported operators at [List of Supported Operators for Warboy Acceleration](..\/npu\/warboy.html#supportedoperators) .\\n `furiosa-compiler` [\\uf0c1](#furiosa-compiler \\\"Permalink to this heading\\\") ---------------------------------------------------------------------\\nThe most common ways to use a compiler would be to automatically call it during the process of resetting the inference API or the NPU.\\nBut you can directly compile a model and generate a program by using the command line tool `furiosa-compiler` in shell. You can install `furiosa-compiler` command via APT package manager.\\n``` $ apt install furiosa-compiler\\n```\\nThe usage of `furiosa-compiler` is as follows:\\n``` $ furiosa-compiler --help\\nFuriosa SDK Compiler v0.10.0 (f8f05c8ea 2023-07-31T19:30:30Z)\\nUsage: furiosa-compiler [OPTIONS] <SOURCE>\\nArguments:   <SOURCE>           Path to source file (tflite, onnx, and other IR formats, such as dfg, cdfg, gir, lir)\\nOptions:   -o, --output <OUTPUT>           Writes output to <OUTPUT>\\n          [default: output.<TARGET_IR>]\\n  -b, --batch-size <BATCH_SIZE>           Specifies the batch size which is effective when SOURCE is TFLite, ONNX, or DFG\\n      --target-ir <TARGET_IR>           (experimental) Target IR - possible values: [enf]\\n          [default: enf]\\n      --target-npu <TARGET_NPU>           Target NPU family - possible values: [warboy, warboy-2pe]\\n          [default: warboy-2pe]\\n      --dot-graph <DOT_GRAPH>           Filename to write DOT-formatted graph to\\n      --analyze-memory <ANALYZE_MEMORY>           Analyzes the memory allocation and save the report to <ANALYZE_MEMORY>\\n  -v, --verbose           Shows details about the compilation process\\n      --no-cache           Disables the compiler result cache\\n  -h, --help           Print help (see a summary with '-h')\\n  -V, --version           Print version\\n```  `SOURCE` is the file path of [TFLite](https:\/\/www.tensorflow.org\/lite) or [ONNX](https:\/\/onnx.ai\/) .\\nYou have to use quantized models through [Model Quantization](quantization.html#modelquantization) for NPU accleration.\\nYou can omit the option -o OUTPUT\\n, and you can also choose to designate the output file name. When omitted, the default output file name is `output.enf` . Here, enf stands for Executable NPU Format. So if you run as shown below, it will generate a `output.enf` file.\\n``` furiosa-compiler foo.onnx\\n```\\nIf you designate the output file name as below, it will generate a `foo.enf` file.\\n``` furiosa-compiler foo.onnx -o foo.enf\\n```  `--target-npu` lets the generated binary to designate target NPU\\ub294.\\nTarget NPUs\\n[\\uf0c1](#id4 \\\"Permalink to this table\\\")\\n| NPU Family | Number of PEs | Value | | --- | --- | --- | | Warboy | 1 | warboy | | Warboy | 2 | warboy-2pe |\\nIf generated program\\u2019s target NPU is Warboy that uses one PE independently, you can run the following command.\\n``` furiosa-compiler foo.onnx --target-npu warboy\\n```\\nWhen 2 PEs are fused, execute as follows.\\n``` furiosa-compiler foo.onnx --target-npu warboy-2pe\\n```\\nThe `--batch-size` option lets you specify batch size\\n, the number of samples to be passed as input when executing inference through the inference API. The larger the batch size, the higher the NPU utilization, since more data is given as input and executed at once. This allows the inference process to be shared across the batch, increasing efficiency. However, if the larger batch size results in the necessary memory size exceeding NPU DRAM size, the memory I\/O cost between the host and the NPU may increase and lead to significant performance degradation. The default value of batch size is one. Appropriate value can usually be found through trial and error. For reference, the optimal batch sizes for some models included in the [MLPerf\\u2122 Inference Edge v2.0](https:\/\/mlcommons.org\/en\/inference-edge-20\/) benchmark are as follows.\\nOptimal Batch Size for Well-known Models\\n[\\uf0c1](#id5 \\\"Permalink to this table\\\")\\n| Model | Optimal Batch | | --- | --- | | SSD-MobileNets-v1 | 2 | | Resnet50-v1.5 | 1 | | SSD-ResNet34 | 1 |\\nIf your desired batch size is two, you can run the following command.\\n``` furiosa-compiler foo.onnx --batch-size 2\\n```\\nUsing ENF files [\\uf0c1](#using-enf-files \\\"Permalink to this heading\\\") -----------------------------------------------------------------\\nAfter the compilation process, the final output of the FuriosaAI compiler is ENF (Executable NPU Format) type data. In general, the compilation process takes from a few seconds to several minutes depending on the model. Once you have the ENF file, you can reuse it to omit this compilation process.\\nThis may be useful if you need to frequently create sessions or serve one model across several machines in an actual operation environment.\\nFor example, you can first create an ENF file by referring to [furiosa-compiler](#compilercli) .\\nThen, with [PythonSDK](python-sdk.html#pythonsdk) as shown below, you can instantly create a runner without the compilation process by passing the ENF file as an argument to the `create_runner()` function as follows:\\n``` from furiosa.runtime import sync\\nwith sync.create_runner(\\\"path\/to\/model.enf\\\") as runner:   outputs = runner.run(inputs)\\n```\\nCompiler Cache [\\uf0c1](#compiler-cache \\\"Permalink to this heading\\\") ---------------------------------------------------------------\\nCompiler cache allows to user applications to reuse once-compiled results. It\\u2019s very helpful especially when you are developing applications because the compilation usually takes at least a couple of minutes.\\nBy default, the compiler cache uses a local file system ( `$HOME\/.cache\/furiosa\/compiler` ) as a cache storage. If you specify a configuration, you can also use Redis as a remote and distributed cache storage.\\nThe compiler cache is enabled by default, but you can explicitly enable or disable the cache by setting `FC_CACHE_ENABLED` . This setting is effective in CLI tools, Python SDK, and serving frameworks.\\n``` # Enable Compiler Cache export FC_CACHE_ENABLED=1 # Disable Compiler Cache export FC_CACHE_ENABLED=0\\n```\\nThe default cache location is `$HOME\/.cache\/furiosa\/compiler` , but you can explicitly specify the cache storage by setting the shell environment variable `FC_CACHE_STORE_URL` . If you want to Redis as a cache storage, you can specify some URLs starting with `redis:\/\/` or `rediss:\/\/` (over SSL).\\n``` # When you want to specify a cache directory export FC_CACHE_STORE_URL=\/tmp\/cache\\n# When you want to specify a Redis cluster as the cache storage export FC_CACHE_STORE_URL=redis:\/\/:<PASSWORD>@127.0.0.1:6379 # When you want to specify a Redis cluster over SSL as the cache storage export FC_CACHE_STORE_URL=rediss:\/\/:<PASSWORD>@127.0.0.1:25945\\n```\\nThe cache will be valid for 30 days by default, but you can explicitly specify the cache lifetime by setting seconds to the environment variable `FC_CACHE_LIFETIME` .\\n``` # 2 hours cache lifetime export FC_CACHE_LIFETIME=7200\\n```\\nAlso, you can control more the cache behavior according to your purpose as follows:\\nCache behaviors according to `FC_CACHE_LIFETIME`  [\\uf0c1](#id6 \\\"Permalink to this table\\\")\\n| Value (secs) | Description | Example | | --- | --- | --- | | *N* > 0 | Cache will be alive for N secs | 7200 (2 hours) | | 0 | All previous cache will be invalidated. (When you want to compile the model without cache) | 0 | | *N* < 0 | Cache will be alive forever without expiration. (it can be useful when you want read-only cache) | -1 |\\n[Previous](cli.html \\\"Command Line Tools\\\") [Next](quantization.html \\\"Model Quantization\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/references.html","title":"references","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"a50ce0f1-25f2-4882-a3e5-8da4cb7215b7\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/references.html\", \"title\": \"references\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* References * [View page source](..\/_sources\/software\/references.rst.txt)\\n---\\nReferences [\\uf0c1](#references \\\"Permalink to this heading\\\") =======================================================\\n* [C Language SDK Reference](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/c\/index.html) * [Python SDK Reference](..\/api\/python\/modules.html) * [Model Zoo Reference](https:\/\/furiosa-ai.github.io\/furiosa-models\/v0.10.0\/)\\n[Previous](tutorials.html \\\"Tutorial and Code Examples\\\") [Next](..\/api\/python\/modules.html \\\"Furiosa SDK 0.10.1 API Documentation\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/serving.html","title":"serving","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"6c328d98-54c8-4c8e-bea7-5927c3921609\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/serving.html\", \"title\": \"serving\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Model Server (Serving Framework) * [View page source](..\/_sources\/software\/serving.rst.txt)\\n---\\nModel Server (Serving Framework) [\\uf0c1](#model-server-serving-framework \\\"Permalink to this heading\\\") =================================================================================================\\nTo serve DNN models through GRPC and REST API, you can use [Furiosa Model Server](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/main\/python\/furiosa-server) .\\nModel Server provides the endpoints compatible with [KServe Predict Protocol Version 2](https:\/\/github.com\/kserve\/kserve\/blob\/master\/docs\/predict-api\/v2\/required_api.md) .\\nIts major features are:\\n> * REST\/GRPC endpoints support > * Multiple model serving using multiple NPU devices\\nInstallation [\\uf0c1](#installation \\\"Permalink to this heading\\\") -----------------------------------------------------------\\nIts requirements are:\\n* Ubuntu 20.04 LTS (Debian bullseye) or higher * [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) * Python 3.8 or higher version\\nIf you need Python environment, please refer to [Python execution environment setup](python-sdk.html#setuppython) first.\\nInstallation using PIP\\nInstallation from source code\\nRun the following command\\n``` $ pip install 'furiosa-sdk[server]'\\n```\\nCheck out the source code and run the following command\\n``` $ git clone https:\/\/github.com\/furiosa-ai\/furiosa-sdk.git $ cd furiosa-sdk\/python\/furiosa-server $ pip install .\\n```\\nRunning a Model Server [\\uf0c1](#running-a-model-server \\\"Permalink to this heading\\\") -------------------------------------------------------------------------------\\nYou can run model sever command by running `furiosa\\nserver` in your shell.\\nTo run simply a model server with `tflite` or `onnx` , you need to specify just the model path and its name as follows:\\n``` $ cd furiosa-sdk $ furiosa server \\\\ --model-path examples\/assets\/quantized_models\/MNISTnet_uint8_quant_without_softmax.tflite \\\\ --model-name mnist\\n```  `--model-path` option allows to specify a path of a model file. If you want to use a specific binding address and port, you can use additionally `--host` , `--host-port` .\\nPlease run `furiosa\\nserver\\n--help` if you want to learn more about the command with various options.\\n``` $ furiosa server --help Usage: furiosa server [OPTIONS]\\n    Start serving models from FuriosaAI model server\\nOptions:     --log-level [ERROR|INFO|WARN|DEBUG|TRACE]                                     [default: LogLevel.INFO]     --model-path TEXT               Path to Model file (tflite, onnx are                                     supported)     --model-name TEXT               Model name used in URL path     --model-version TEXT            Model version used in URL path  [default:                                     default]     --host TEXT                     IP address to bind  [default: 0.0.0.0]     --http-port INTEGER             HTTP port to listen to requests  [default:                                     8080]     --model-config FILENAME         Path to a config file about models with                                     specific configurations     --server-config FILENAME        Path to Model file (tflite, onnx are                                     supported)     --install-completion [bash|zsh|fish|powershell|pwsh]                                     Install completion for the specified shell.     --show-completion [bash|zsh|fish|powershell|pwsh]                                     Show completion for the specified shell, to                                     copy it or customize the installation.     --help                          Show this message and exit.\\n```\\nRunning a Model Server with a Configuration File [\\uf0c1](#running-a-model-server-with-a-configuration-file \\\"Permalink to this heading\\\") -----------------------------------------------------------------------------------------------------------------------------------\\nIf you need more advanced configurations like compilation options and device options, you can use a configuration file based on Yaml.\\n``` model_config_list:   - name: mnist     model: \\\"samples\/data\/MNISTnet_uint8_quant.tflite\\\"     version: \\\"1\\\"     platform: npu     npu_device: warboy(1)*1     compiler_config:       keep_unsignedness: true       split_unit: 0   - name: ssd     model: \\\"samples\/data\/SSD512_MOBILENET_V2_BDD_int_without_reshape.tflite\\\"     version: \\\"1\\\"     platform: npu     npu_device: warboy(1)*1\\n```\\nWhen you run a model sever with a configuration file, you need to specify `--model-config` as follows. You can find the model files described in the above example from [furiosa-models\/samples](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/main\/python\/furiosa-server\/samples) .\\n``` $ cd furiosa-sdk\/python\/furiosa-server $ furiosa server --model-config samples\/model_config_example.yaml libfuriosa_hal.so --- v0.11.0, built @ 43c901f 2023-08-02T07:42:42.263133Z  INFO furiosa_rt_core::driver::event_driven::device: DeviceManager has detected 1 NPUs 2023-08-02T07:42:42.267247Z  INFO furiosa_rt_core::driver::event_driven::device: [1] npu:6:1 (warboy-b0, 64dpes) 2023-08-02T07:42:42.267264Z  INFO furiosa_rt_core::driver::event_driven::coord: furiosa-rt (v0.10.0-rc6, rev: d021ff71d, built_at: 2023-07-31T19:05:26Z) is being initialized 2023-08-02T07:42:42.267269Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:1-io-thread-0 thread has started 2023-08-02T07:42:42.267398Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:1-commit-thread thread has started 2023-08-02T07:42:42.267405Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:1-io-thread-1 thread has started 2023-08-02T07:42:42.270837Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libcompiler 0.10.0 (rev: f8f05c built: 2023-07-26T09:49:17Z) 2023-08-02T07:42:42.270851Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libhal-warboy 0.11.0 (rev: 43c901f built: 2023-04-19T14:04:55Z) 2023-08-02T07:42:42.271144Z  INFO furiosa_rt_core::driver::event_driven::coord: [NONAME] Runtime has started 2023-08-02T07:42:42.273772Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0001 is being loaded to npu:6:1 2023-08-02T07:42:42.283260Z  INFO furiosa_rt_core::driver::event_driven::coord: Compiling Model#0001 (target: warboy-b0, 64dpes, file: MNISTnet_uint8_quant.tflite, size: 18.2 kiB) 2023-08-02T07:42:42.299091Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0001 has been compiled successfully (took 0 secs) 2023-08-02T07:42:42.299293Z  INFO furiosa_rt_core::dag: Task Statistics: TaskStats { cpu: 5, npu: 1, alias: 0, coalesce: 0 } 2023-08-02T07:42:42.300701Z  INFO furiosa_rt_core::driver::event_driven::coord: NpuApi (AsyncNpuApiImpl) has started.. 2023-08-02T07:42:42.300721Z  INFO furiosa_rt_core::driver::event_driven::coord: Creating 1 Contexts on npu:6:1 (DRAM usage: 6.0 kiB \/ 16.0 GiB, SRAM usage: 124.0 kiB \/ 64.0 MiB) 2023-08-02T07:42:42.300789Z  INFO furiosa_rt_core::driver::event_driven::coord: npu:6:1 has scheduled to Model#0001 2023-08-02T07:42:42.304216Z  WARN furiosa_rt_core::consts::envs: NPU_DEVNAME will be deprecated. Use FURIOSA_DEVICES instead. 2023-08-02T07:42:42.313084Z  INFO furiosa_rt_core::driver::event_driven::device: DeviceManager has detected 1 NPUs 2023-08-02T07:42:42.315470Z  INFO furiosa_rt_core::driver::event_driven::device: [1] npu:6:0 (warboy-b0, 64dpes) 2023-08-02T07:42:42.315483Z  INFO furiosa_rt_core::driver::event_driven::coord: furiosa-rt (v0.10.0-rc6, rev: d021ff71d, built_at: 2023-07-31T19:05:26Z) is being initialized 2023-08-02T07:42:42.315560Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:0-io-thread-1 thread has started 2023-08-02T07:42:42.315610Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:0-io-thread-0 thread has started 2023-08-02T07:42:42.315657Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:0-commit-thread thread has started 2023-08-02T07:42:42.319127Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libcompiler 0.10.0 (rev: f8f05c built: 2023-07-26T09:49:17Z) 2023-08-02T07:42:42.319141Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libhal-warboy 0.11.0 (rev: 43c901f built: 2023-04-19T14:04:55Z) 2023-08-02T07:42:42.319364Z  INFO furiosa_rt_core::driver::event_driven::coord: [NONAME] Runtime has started 2023-08-02T07:42:42.324283Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0002 is being loaded to npu:6:0 2023-08-02T07:42:42.333521Z  INFO furiosa_rt_core::driver::event_driven::coord: Compiling Model#0002 (target: warboy-b0, 64dpes, file: SSD512_MOBILENET_V2_BDD_int_without_reshape.tflite, size: 5.2 MiB) 2023-08-02T07:42:42.814260Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0002 has been compiled successfully (took 0 secs) 2023-08-02T07:42:42.815406Z  INFO furiosa_rt_core::dag: Task Statistics: TaskStats { cpu: 26, npu: 1, alias: 0, coalesce: 0 } 2023-08-02T07:42:42.893745Z  INFO furiosa_rt_core::driver::event_driven::coord: NpuApi (AsyncNpuApiImpl) has started.. 2023-08-02T07:42:42.893772Z  INFO furiosa_rt_core::driver::event_driven::coord: Creating 1 Contexts on npu:6:0 (DRAM usage: 1.0 MiB \/ 16.0 GiB, SRAM usage: 14.8 MiB \/ 64.0 MiB) 2023-08-02T07:42:42.894265Z  INFO furiosa_rt_core::driver::event_driven::coord: npu:6:0 has scheduled to Model#0002 INFO:     Started server process [2448540] INFO:     Waiting for application startup. INFO:     Application startup complete. INFO:     Uvicorn running on http:\/\/0.0.0.0:8080 (Press CTRL+C to quit)\\n```\\nOnce a model server starts up, you can call the inference request through HTTP protocol. If the model name is `mnist` and its version `1` , the endpoint of the model will be `http:\/\/<host>:<port>\/v2\/models\/mnist\/version\/1\/infer` , accepting `POST` http request. The following is an example using `curl` to send the inference request and return the response.\\nThe following is a Python example, doing same as `curl` does in the above example.\\n``` import requests import mnist import numpy as np\\nmnist_images = mnist.train_images().reshape((60000, 1, 28, 28)).astype(np.uint8) url = 'http:\/\/localhost:8080\/v2\/models\/mnist\/versions\/1\/infer'\\ndata = mnist_images[0:1].flatten().tolist() request = {     \\\"inputs\\\": [{         \\\"name\\\":         \\\"mnist\\\",         \\\"datatype\\\": \\\"UINT8\\\",         \\\"shape\\\": (1, 1, 28, 28),         \\\"data\\\": data     }] }\\nresponse = requests.post(url, json=request) print(response.json())\\n```\\nEndpoints [\\uf0c1](#endpoints \\\"Permalink to this heading\\\") -----------------------------------------------------\\nThe following table shows REST API endpoints and its descriptions. The model server is following KServe Predict Protocol Version 2. So, you can find more details from [KServe Predict Protocol Version 2 - HTTP\/REST](https:\/\/github.com\/kserve\/kserve\/blob\/master\/docs\/predict-api\/v2\/required_api.md#httprest) .\\nEndpoints of KServe Predict Protocol Version 2\\n[\\uf0c1](#id1 \\\"Permalink to this table\\\")\\n| Method and Endpoint | Description | | --- | --- | | GET \/v2\/health\/live | Returns HTTP Ok (200) if the inference server is able to receive and respond to metadata and inference requests. This API can be directly used for the Kubernetes livenessProbe. | | GET \/v2\/health\/ready | Returns HTTP Ok (200) if all the models are ready for inferencing. This API can be directly used for the Kubernetes readinessProbe. | | GET \/v2\/models\/${MODEL\\\\_NAME}\/versions\/${MODEL\\\\_VERSION} | Returns a model metadata | | GET \/v2\/models\/${MODEL\\\\_NAME}\/versions\/${MODEL\\\\_VERSION}\/ready | Returns HTTP Ok (200) if a specific model is ready for inferencing. | | POST \/v2\/models\/${MODEL\\\\_NAME}[\/versions\/${MODEL\\\\_VERSION}]\/infer | Inference request |\\n[Previous](profiler.html \\\"Performance Profiling\\\") [Next](kubernetes_support.html \\\"Kubernetes Support\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/tutorials.html","title":"tutorials","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"4336ecbc-28cf-41e6-863f-25212b860e38\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/tutorials.html\", \"title\": \"tutorials\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Tutorial and Code Examples * [View page source](..\/_sources\/software\/tutorials.rst.txt)\\n---\\nTutorial and Code Examples [\\uf0c1](#tutorial-and-code-examples \\\"Permalink to this heading\\\") =======================================================================================\\nTutorial [\\uf0c1](#id1 \\\"Permalink to this heading\\\") ----------------------------------------------\\n* [How to use Furiosa SDK from Start to Finish](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/HowToUseFuriosaSDKFromStartToFinish.ipynb) * [Basic Inference API](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/GettingStartedWithPythonSDK.ipynb) * [Advanced Inference API](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/AdvancedTopicsInInferenceAPIs.ipynb)\\nCode Examples [\\uf0c1](#code-examples \\\"Permalink to this heading\\\") -------------------------------------------------------------\\n* [Comparing Accuracy with CPU-based inference](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/InferenceAccuracyCheck.ipynb) * [Image Classification Model Inference](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/Image_Classification.ipynb) * [SSD Object Detection Model Inference](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/SSD_Object_Detection.ipynb) * [Other Python Code Examples](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/branch-0.10.0\/examples\/inferences)\\n[Previous](vm_support.html \\\"Configuring Warboy Pass-through for Virtual Machine\\\") [Next](references.html \\\"References\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.8.0.html","title":"0","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"e527d72f-b0b3-4132-b935-b89558bc7add\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.8.0.html\", \"title\": \"0\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Release Notes - 0.8.0 * [View page source](..\/_sources\/releases\/0.8.0.rst.txt)\\n---\\nRelease Notes - 0.8.0 [\\uf0c1](#release-notes-0-8-0 \\\"Permalink to this heading\\\") ===========================================================================\\nFuriosa SDK 0.8.0 is a major release, including many performance enhancements, additional functions, and bug fixes. 0.8.0 also includes the serving framework, a core tool of user application development, as well as major improvements to the Model Zoo.\\nComponent Version Information\\n[\\uf0c1](#id3 \\\"Permalink to this table\\\")\\n| Package Name | Version | | --- | --- | | NPU Driver | 1.4.0 | | NPU Firmware Tools | 1.2.0 | | NPU Firmware Image | 1.2.0 | | HAL (Hardware Abstraction Layer) | 0.9.0 | | Furiosa Compiler | 0.8.0 | | Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..) | 0.8.0 | | NPU Device Plugin | 0.10.1 | | NPU Feature Discovery | 0.2.0 | | NPU Management CLI (furiosactl) | 0.10.0 |\\nInstalling the latest SDK [\\uf0c1](#installing-the-latest-sdk \\\"Permalink to this heading\\\") -------------------------------------------------------------------------------------\\nIf you are using APT repository, the upgrade process is simpler.\\n> ``` > apt-get update && apt-get upgrade >  > ```\\nIf you wish to designate a specific package for upgrade, execute as below: You can find more details about APT repository setup at [Driver, Firmware, and Runtime Installation](..\/software\/installation.html#requiredpackages) .\\n> ``` > apt-get update && \\\\ > apt-get install -y furiosa-driver-pdma furiosa-libhal-warboy furiosa-libnux furiosactl >  > ```\\nYou can upgrade firmware as follows:\\n> ``` > apt-get update && \\\\ > apt-get install -y furiosa-firmware-tools furiosa-firmware-image >  > ```\\nYou can upgrade Python package as follows:\\n> ``` > pip install --upgrade furiosa-sdk >  > ```\\nMajor changes [\\uf0c1](#major-changes \\\"Permalink to this heading\\\") -------------------------------------------------------------\\n### Improvements to serving framework API [\\uf0c1](#improvements-to-serving-framework-api \\\"Permalink to this heading\\\")\\nThe [furiosa-serving](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/branch-0.8.0\/python\/furiosa-serving) is a FastAPI-based serving framework. With the [furiosa-serving](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/branch-0.8.0\/python\/furiosa-serving) ,\\nusers can quickly develop Python-based high performance web service applications that utilize NPUs. The 0.8.0 release includes the following major updates.\\n**Session pool that enables model serving with multiple NPUs**\\nSession pools improve significantly throughput of model APIs by using multiple NPUs. If large inputs can be divided into a number of small inputs, this improvement can be used to reduce the latency of serving applications.\\n``` model: NPUServeModel = synchronous(serve.model(\\\"nux\\\"))(     \\\"MNIST\\\",     location=\\\".\/assets\/models\/MNISTnet_uint8_quant_without_softmax.tflite\\\",     # Specify multiple devices     npu_device=\\\"npu0pe0,npu0pe1,npu1pe0\\\"     worker_num=4, )\\n```\\n**Shift from thread-based to asyncio-based NPU query processing**\\nSmall and frequent NPU inference queries may now be processed with lower latency. As shown in the example below, applications requiring multiple NPU inferences in a single API query can be processed with better performance.\\n``` async def inference(self, tensors: List[np.ndarray]) -> List[np.ndarray]:     # The following code runs multiple inferences at the same time and wait until all requests are completed.     return await asyncio.gather(*(self.model.predict(tensor) for tensor in tensors))\\n```\\n**Added expanded support for external device & runtime**\\nIn complex serving scenarios, additional\/external device and runtime programs may be required, in addition to NPU-based Furiosa Runtime. In this release, the framework has been expanded such that external device and runtime may be used. The first external runtime added is OpenVINO.\\n``` imagenet: ServeModel = synchronous(serve.model(\\\"openvino\\\"))(     'imagenet',     location='.\/examples\/assets\/models\/image_classification.onnx' )\\n```\\n**Support for S3 cloud storage repository**\\nSet model `location` as S3 URL.\\n``` # Load model from S3 (Auth environment variable for aioboto library required) densenet: ServeModel = synchronous(serve.model(\\\"nux\\\"))(     'imagenet',  location='s3:\/\/furiosa\/models\/93d63f654f0f192cc4ff5691be60fb9379e9d7fd' )\\n```\\n**Support for OpenTelemetry compatible tracing**\\nWith the [OpenTelemetry Collector](https:\/\/opentelemetry.io\/docs\/collector\/) function, you can now track the execution time of specific code sections of the serving applications.\\nTo use this function, you can activate `trace.get_tracer()` , reset the tracer, activate the `tracer.start_as_current_span()` function, and designate the section.\\n``` from opentelemetry import trace\\ntracer = trace.get_tracer(__name__)\\nclass Application:\\n        async def process(self, image: Image.Image) -> int:             with tracer.start_as_current_span(\\\"preprocess\\\"):                 input_tensors = self.preprocess(image)             with tracer.start_as_current_span(\\\"inference\\\"):                 output_tensors = await self.inference(input_tensors)             with tracer.start_as_current_span(\\\"postprocess\\\"):                 return self.postprocess(output_tensors)\\n```\\nThe specification of [OpenTelemetry Collector](https:\/\/opentelemetry.io\/docs\/collector\/) can be done through the configuration of `FURIOSA_SERVING_OTLP_ENDPOINT` , as shown below. The following diagram is an example that visualizes the tracing result with Grafana.\\nOther major improvements are as follows:\\n* Several inference requests can be executed at once, with serving API now supporting compiler setting   `batch_size` * More threads can share the NPU, with serving API now supporting session option   `worker_num`  ### Profiler [\\uf0c1](#profiler \\\"Permalink to this heading\\\")\\nYou can now analyze the profiler tracing results with [Pandas](https:\/\/pandas.pydata.org\/) ,\\na data analysis framework. With this function, you can analyze the tracing result data, allowing you to quickly identify bottlenecks and reasons for model performance changes. More detailed instructions can be found at [Trace analysis using Pandas DataFrame](..\/software\/profiler.html#pandasprofilinganalysis) .\\n``` from furiosa.runtime import session, tensor from furiosa.runtime.profiler import RecordFormat, profile\\nwith profile(format=RecordFormat.PandasDataFrame) as profiler:     with session.create(\\\"MNISTnet_uint8_quant_without_softmax.tflite\\\") as sess:         input_shape = sess.input(0)\\n        with profiler.record(\\\"record\\\") as record:             for _ in range(0, 2):                 sess.run(tensor.rand(input_shape))\\ndf = profiler.get_pandas_dataframe() print(df[df[\\\"name\\\"] == \\\"trace\\\"][[\\\"trace_id\\\", \\\"name\\\", \\\"thread.id\\\", \\\"dur\\\"]])\\n```\\n### Quantization tool [\\uf0c1](#quantization-tool \\\"Permalink to this heading\\\")\\n[Model Quantization](..\/software\/quantization.html#modelquantization) is a tool that converts pre-trained models to quantized models. This release includes the following major updates.\\n* Accuracy improvement when processing SiLU operator * Improved usability of compiler setting   `without_quantize` * Accuracy improvement when processing MatMul\/Gemm operators * Accuracy improvement when processing Add\/Sub\/Mul\/Div operators * NPU acceleration now added for more auto\\\\_pad properties, when processing Conv\/ConvTranspose\/MaxPool operators * NPU acceleration support for PRelu operator\\n### furiosa-toolkit [\\uf0c1](#furiosa-toolkit \\\"Permalink to this heading\\\")\\nThe `furiosactl` command line tool, which has been added to the furiosa-toolkit 0.10.0 release, includes the following improvements.\\nThe newly added furiosactl ps\\ncommand allows you to print the OS processes which are occupying the NPU device.\\n``` # furiosactl ps +-----------+--------+------------------------------------------------------------+ | NPU       | PID    | CMD                                                        | +-----------+--------+------------------------------------------------------------+ | npu0pe0-1 | 132529 | \/usr\/bin\/python3 \/usr\/local\/bin\/uvicorn image_classify:app | +-----------+--------+------------------------------------------------------------+\\n```\\nThe furiosactl info\\ncommand now prints the unique UUID for each device.\\n``` $ furiosactl info +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+ | NPU  | Name   | UUID                                 | Firmware        | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+ | npu0 | warboy | 72212674-61BE-4FCA-A2C9-555E4EE67AB5 | v1.1.0, 12180b0 |  49\\u00b0C | 3.12 W | 0000:24:00.0 | 235:0   | +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+ | npu1 | warboy | DF80FB54-8190-44BC-B9FB-664FA36C754A | v1.1.0, 12180b0 |  54\\u00b0C | 2.53 W | 0000:6d:00.0 | 511:0   | +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+\\n```\\nDetailed instructions on installation and usage for furiosactl\\ncan be found in [furiosa-toolkit](..\/software\/cli.html#toolkit) .\\n### Model Zoo API improvements, added models, and added native post-processing code [\\uf0c1](#model-zoo-api-improvements-added-models-and-added-native-post-processing-code \\\"Permalink to this heading\\\")\\n[furioa-models](https:\/\/furiosa-ai.github.io\/furiosa-models) is a public Model Zoo project, providing FuriosaAI NPU-optimized models. The 0.8.0 release includes the following major updates.\\n**YOLOv5 Large\/Medium models added**\\nSupport for `YOLOv5l` , `YOLOv5m` , which are SOTA object detection models, have been added. The total list of available models can be found in [Model List](https:\/\/furiosa-ai.github.io\/furiosa-models\/v0.8.0\/#model_list) .\\n**Improvements to model class and loading API**\\nThe model class has been improved to include pre and post-processing code, while the model loading API has been improved as shown below.\\nMore explanation on model class and the API can be found at [Model Object](https:\/\/furiosa-ai.github.io\/furiosa-models\/latest\/model_object\/) .\\nBlocking API\\nNonblocking API\\nBefore update\\n``` from furiosa.models.vision import MLCommonsResNet50\\nresnet50 = MLCommonsResNet50()\\n```\\nUpdated code\\n``` from furiosa.models.vision import ResNet50\\nresnet50 = ResNet50.load()\\n```\\nBefore update\\n``` import asyncio\\nfrom furiosa.models.nonblocking.vision import MLCommonsResNet50\\nresnet50: Model = asyncio.run(MLCommonsResNet50())\\n```\\n0.8.0 improvements\\n``` import asyncio\\nfrom furiosa.models.vision import ResNet50\\nresnet50: Model = asyncio.run(ResNet50.load_async())\\n```\\nThe model post-processing process converts the inference ouput tensor into structural data, which is more accessible for the application. Depending on the model, this may require a longer execution time. The 0.8.0 release includes native post-processing code for ResNet50, SSD-MobileNet, and SSD-ResNet34. Based on internal benchmarks, native post-processing code can reduce latency by up to 70%, depending on the model.\\nThe following is a complete example of ResNet50, utilizing native post-processing code. More information can be found at [Pre\/Postprocessing](https:\/\/furiosa-ai.github.io\/furiosa-models\/v0.8.0\/model_object\/#prepostprocessing) .\\n> ``` > from furiosa.models.vision import ResNet50 > from furiosa.models.vision.resnet50 import NativePostProcessor, preprocess > from furiosa.runtime import session >  > model = ResNet50.load() >  > postprocessor = NativePostProcessor(model) > with session.create(model) as sess: >     image = preprocess(\\\"tests\/assets\/cat.jpg\\\") >     output = sess.run(image).numpy() >     postprocessor.eval(output) >  > ```\\nOther changes and updates can be found at [Furiosa Model - 0.8.0 Changelogs](https:\/\/furiosa-ai.github.io\/furiosa-models\/v0.8.0\/changelog\/) .\\n[Previous](0.9.0.html \\\"Release Notes - 0.9.0\\\") [Next](0.7.0.html \\\"Release Notes - 0.7.0\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/kubernetes_support.html","title":"kubernetes_support","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"a643c5bc-d22e-493d-bee8-90e6b58fc9dd\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/kubernetes_support.html\", \"title\": \"kubernetes_support\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Kubernetes Support * [View page source](..\/_sources\/software\/kubernetes_support.rst.txt)\\n---\\nKubernetes Support [\\uf0c1](#kubernetes-support \\\"Permalink to this heading\\\") =======================================================================\\n[Kuberentes](https:\/\/kubernetes.io\/) is an open source platform for managing containerized workloads and services. Furiosa SDK provides the following components to support the Kubernetes environment.\\n* FuriosaAI NPU Device Plugin (   [Introduction to Kubernetes Device Plugin](https:\/\/kubernetes.io\/docs\/concepts\/extend-kubernetes\/compute-storage-net\/device-plugins\/)   ) * FuriosaAI NPU Feature Discovery (   [Introduction to Node Feature Discovery](https:\/\/kubernetes-sigs.github.io\/node-feature-discovery\/stable\/get-started\/index.html)   )\\nThe two components above provide the following functions.\\n* Make the Kubernetes cluster aware of the NPUs available to the node. * Through Kubernetes   `spec.containers[].resources.limits`   , schedule the NPU simultaneously when distributing Pod workload. * Identify NPU information of NPU-equipped machine, and register it as node label (you can selectively schedule Pods with this information and   nodeSelector      )      + The node-feature-discovery needs to be installed to the cluster, and the     `nfd-worker`     Pod must be running in the nodes equipped with NPUs.\\nThe setup process for Kubernetes support is as follows.\\n1. Preparing NPU nodes [\\uf0c1](#preparing-npu-nodes \\\"Permalink to this heading\\\") ----------------------------------------------------------------------------\\nRequirements for Kubernetes nodes are as follows.\\n* Ubuntu 20.04 or higher * Intel compatible CPU\\nYou also need to install NPU driver and toolkit on each node of NPU-equipped Kubernetes. If the APT server is set up (see [APT server configuration](installation.html#setupaptrepository) ), you can easily install as follows.\\n``` apt-get update && apt install -y furiosa-driver-warboy furiosa-toolkit\\n```\\nOnce the required package is installed as above, you can check for NPU recognition as follows, with the `furiosactl` command included in furiosa-toolkit. If the NPU is not recognized with the command below, try again after rebooting - depending on the environment.\\n``` $ furiosactl info +------+------------------+-------+--------+--------------+---------+ | NPU  | Name             | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+------------------+-------+--------+--------------+---------+ | npu0 | FuriosaAI Warboy |  40\\u00b0C | 1.37 W | 0000:01:00.0 | 509:0   | +------+------------------+-------+--------+--------------+---------+\\n```\\n2. Installing Node Feature Discovery [\\uf0c1](#installing-node-feature-discovery \\\"Permalink to this heading\\\") --------------------------------------------------------------------------------------------------------\\nIn order to make Kubernetes to recognize NPUs, you need to install Node Feature Discovery. By running the command as shown in the example below, if there is a node label that begins with `feature.node.kubernetes.io\/...` , Node Feature Discovery\\u2019s DaemonSet has already been installed\\n``` $ kubectl get no -o json | jq '.items[].metadata.labels' {\\n  \\\"beta.kubernetes.io\/arch\\\": \\\"amd64\\\",   \\\"beta.kubernetes.io\/os\\\": \\\"linux\\\",   \\\"feature.node.kubernetes.io\/cpu-cpuid.ADX\\\": \\\"true\\\",   \\\"feature.node.kubernetes.io\/cpu-cpuid.AESNI\\\": \\\"true\\\",   ...\\n```\\n* If you do not have the Node Feature Discovery in your cluster, refer to the following document.      > + [Quick start \/ Installation](https:\/\/kubernetes-sigs.github.io\/node-feature-discovery\/v0.11\/get-started\/quick-start.html#installation) * The following options must be applied when executing Node Feature Discovery.      + `beta.furiosa.ai`     needs to be included in the     `--extra-label-ns`     option of     `nfd-master`   + In the config file of     `nfd-worker`     ,     \\\\* Only     `vendor`     in the     `sources.pci.deviceLabelFields`     value     \\\\*     `\\\"12\\\"`     must be included as a value in     `sources.pci.deviceClassWhitelist`  Note\\nInstalling Node Feature Discovery is not mandatory, but is recommended. The next step will explain the additional tasks that must be performed if you are not using Node Feature Discovery.\\n3. Installing Device Plugin and NPU Feature Discovery [\\uf0c1](#installing-device-plugin-and-npu-feature-discovery \\\"Permalink to this heading\\\") ------------------------------------------------------------------------------------------------------------------------------------------\\nWhen the NPU node is ready, install Device Plugin and NPU Feature Discovery\\u2019s DaemonSet as follows.\\n``` kubectl apply -f https:\/\/raw.githubusercontent.com\/furiosa-ai\/furiosa-sdk\/v0.7.0\/kubernetes\/deployments\/device-plugin.yaml kubectl apply -f https:\/\/raw.githubusercontent.com\/furiosa-ai\/furiosa-sdk\/v0.7.0\/kubernetes\/deployments\/npu-feature-discovery.yaml\\n```\\nAfter executing the above command, you can check whether the installed daemonset is functioning normally with the `kubectl\\nget\\ndaemonset\\n-n\\nkube-system` command. For reference, the DaemonSet is distributed only to nodes equipped with NPUs, and uses `alpha.furiosa.ai\/npu.family=warboy` information that the Node Feature Discovery ( `feature.node.kubernetes.io\/pci-1ed2.present=true` ) attaches to each node.\\n```  $ kubectl get daemonset -n kube-system NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                      AGE  furiosa-device-plugin          3         3         3       3            3           feature.node.kubernetes.io\/pci-1ed2.present=true   128m  furiosa-npu-feature-discovery  3         3         3       3            3           feature.node.kubernetes.io\/pci-1ed2.present=true   162m\\n```\\nThe metadata attached by the Node Feature Discovery is shown in the following table.\\nNPU Node Labels\\n[\\uf0c1](#id1 \\\"Permalink to this table\\\")\\n| Label | Value | Description | | --- | --- | --- | | beta.furiosa.ai\/npu.count | 1 | The number of NPUs e x b number of NPUs attached to node | | beta.furiosa.ai\/npu.product | warboy, warboyB0 | NPU Product Name (Code) | | beta.furiosa.ai\/npu.family | warboy, renegade | NPU Architecture (Family) | | beta.furiosa.ai\/machine.vendor | (depends on machine) | Machine Manufacturer | | beta.furiosa.ai\/machine.name | (depends on machine) | The Nmae of Machine (Code) | | beta.furiosa.ai\/driver.version | 1.3.0 | NPU Device Driver Version | | beta.furiosa.ai\/driver.version.major | 1 | Major Version Number of NPU Device Driver Version | | beta.furiosa.ai\/driver.version.minor | 3 | Minor Version Number of NPU Device Driver | | beta.furiosa.ai\/driver.version.patch | 0 | Patch Version Number of NPU Device Driver | | beta.furiosa.ai\/driver.reference | 57ac7b0 | Build Commit Hash of NPU Device Driver |\\nIf you want to check node labels, then execute the `kubectl\\nget\\nnodes\\n--show-labels` command. If you see labels which start with `beta.furiosa.ai` Node Feature Discovery is successfully installed.\\n``` kubectl get nodes --show-labels\\nwarboy-node01     Ready   <none>  65d   v1.20.10   beta.furiosa.ai\/npu.count=1,beta.furiosa.ai\/npu.product=warboy...,kubernetes.io\/os=linux warboy-node02     Ready   <none>  12d   v1.20.10   beta.furiosa.ai\/npu.count=1,beta.furiosa.ai\/npu.product=warboy...,kubernetes.io\/os=linux\\n```\\n### Device Plugin Configuration [\\uf0c1](#device-plugin-configuration \\\"Permalink to this heading\\\")\\nExecution options for Device Plugin can be set by the argument of command line or configuration file.\\n1. Command Line Arguments\\nThe option can be set by the `k8s-device-plugin` command as follows.\\n``` $ k8s-device-plugin --interval 10\\n```\\nFor the Pod or DaemonSet specification command line arguments can be set as follows.\\n``` apiVersion: v1 kind: Pod metadata:   name: furiosa-device-plugin   namespace: kube-system spec:   containers:     - name: device-plugin       image: ghcr.io\/furiosa-ai\/k8s-device-plugin:latest       command: [\\\"\/usr\/bin\/k8s-device-plugin\\\"]       args: [\\\"--interval\\\", \\\"10\\\"] # (the reset is omitted)\\n```\\narguments of k8s-device-plugin\\n[\\uf0c1](#id2 \\\"Permalink to this table\\\")\\n| Item | Explanation | Default Value | | --- | --- | --- | | default-pe | default core type when pod is allocated (Fusion\/Single) | Fusion | | interval | interval for searching device (seconds) | 10 | | disabled-devices | devices not for allocations (several devices can be designated using comma) |  | | plugin-dir | directory path of kubelet device-plugin | \/var\/lib\/kubelet\/device-plugins | | socket-name | file name of socket created under <plugin-dir> | furiosa-npu | | resource-name | name of NPU resource registered for k8s node | beta.furiosa.ai\/npu |\\n2. Setting Configuration File\\nYou may set configuration file by executing `k8s-device-plugin` command with argument `config-file` . If `config-file` is set then the other arguments are not permitted.\\n``` $ k8s-device-plugin --config-file \/etc\/furiosa\/device-plugin.conf\\n```\\n\/etc\/furiosa\/device-plugin.conf\\n[\\uf0c1](#id3 \\\"Permalink to this code\\\")\\n``` interval: 10 defaultPe: Fusion disabledDevices:             # device npu1 equipped in warboy-node01 will not be used   - devName: npu1     nodeName: warboy-node01 pluginDir: \/var\/lib\/kubelet\/device-plugins socketName: furiosa-npu resourceName: beta.furiosa.ai\/npu\\n```\\nConfiguration file is a text file with Yaml format. The modification of file contents is applied to Device Plugin immediately. Updated configuration is recorded on log of Device Plugin. (but, modifications on `pluginDir` , `socketName` , or `resourceName` require reboot.)\\n[3. Installing Device Plugin and NPU Feature Discovery](#installingdevicepluginandnfd) provides `device-plugin.yaml` which is default configuration file based on ConfigMap. If you want to modify execution options of Device Plugin, modify ConfigMap. Once modified ConfigMap is applied to Pod, Device Plugin reads the ConfigMap and then reflects modification.\\n``` $ kubectl edit configmap npu-device-plugin -n kube-system\\n```\\nconfigmap\/npu-device-plugin\\n[\\uf0c1](#id4 \\\"Permalink to this code\\\")\\n``` apiVersion: v1 data:   config.yaml: |     defaultPe: Fusion     interval: 15     disabledDevices:       - devName: npu2         nodeName: npu-001 kind: ConfigMap\\n```\\n4. Creating a Pod with NPUs [\\uf0c1](#creating-a-pod-with-npus \\\"Permalink to this heading\\\") --------------------------------------------------------------------------------------\\nTo allocate NPU to a Pod, add as shown below to `spec.containers[].resources.limits` .\\n``` resources:     limits:         beta.furiosa.ai\/npu: \\\"1\\\" # requesting 1 NPU\\n```\\n[Full example](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/v0.7.0\/kubernetes\/deployments\/pod-example.yaml) for Pod creation is as follows.\\n``` $ cat > npu-pod.yaml <<EOL apiVersion: v1 kind: Pod metadata:   name: npu-pod spec:   containers:     - name: npu-pod       image: ubuntu:focal       resources:         limits:           cpu: \\\"4\\\"           memory: \\\"8Gi\\\"           beta.furiosa.ai\/npu: \\\"1\\\"         requests:           cpu: \\\"4\\\"           memory: \\\"8Gi\\\"           beta.furiosa.ai\/npu: \\\"1\\\" EOL\\n$ kubectl apply -f npu-pod.yaml\\n```\\nAfter Pod creation, you can check NPU allocation as follows.\\n``` $ kubectl get pods npu-pod -o yaml | grep alpha.furiosa.ai\/npu     beta.furiosa.ai\/npu: \\\"1\\\"     beta.furiosa.ai\/npu: \\\"1\\\"\\n```\\nThe SDK application automatically recognizes the allocated NPU device. If there are multiple NPU devices on a node, you can check which device is allocated as follows:\\n``` $ kubectl exec npu-pod -it -- \/bin\/bash root@npu-pod:\/# echo $NPU_DEVNAME npu0pe0-1\\n```\\nIf furiosa-toolkit is installed in the Pod, you can check for more detailed device information using the furiosactl command as shown below.\\nSee [APT server configuration](installation.html#setupaptrepository) for installation guide using APT.\\n``` root@npu-pod:\/# furiosactl furiosactl controls the FURIOSA NPU.\\nFind more information at: https:\/\/furiosa.ai\/\\nBasic Commands:     version    Print the furiosactl version information     info       Show information one or many NPU(s)     config     Get\/Set configuration for NPU environment\\nUsage:     furiosactl COMMAND\\nroot@npu-pod:\/# furiosactl info +------+------------------+-------+--------+--------------+---------+ | NPU  | Name             | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+------------------+-------+--------+--------------+---------+ | npu0 | FuriosaAI Warboy |  40\\u00b0C | 1.37 W | 0000:01:00.0 | 509:0   | +------+------------------+-------+--------+--------------+---------+\\n```\\n5. NPU monitoring [\\uf0c1](#npu-monitoring \\\"Permalink to this heading\\\") ------------------------------------------------------------------\\nIf you install `npu-metrics-exporter` , its daemon set and service will be created in your kubernetes cluster. The Pod that is executed through DaemonSet outputs various NPU status information that may be useful for monitoring. The data is expressed in Prometheus format. If Prometheus is installed, and service discovery is active, Prometheus will automatically collect data through the Exporter.\\nThe collected data may be reviewed with visualization tools such as Grafana.\\nnpu-metrics-exporter collection category list\\n[\\uf0c1](#id5 \\\"Permalink to this table\\\")\\n| Name | Details | | --- | --- | | furiosa\\\\_npu\\\\_alive | NPU operation status (1:normal) | | furiosa\\\\_npu\\\\_uptime | NPU operation time (s) | | furiosa\\\\_npu\\\\_error | Number of detected NPU errors | | furiosa\\\\_npu\\\\_hw\\\\_temperature | Temperature of each NPU components (\\u00b0mC) | | furiosa\\\\_npu\\\\_hw\\\\_power | NPU instantaneous power usage (\\u00b5W) | | furiosa\\\\_npu\\\\_hw\\\\_voltage | NPU instantaenous voltage (mV) | | furiosa\\\\_npu\\\\_hw\\\\_current | NPU instantaneous current (mA) |\\n[Previous](serving.html \\\"Model Server (Serving Framework)\\\") [Next](vm_support.html \\\"Configuring Warboy Pass-through for Virtual Machine\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/quantization.html","title":"quantization","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"3e1275e1-f809-4d7a-beb7-ec8b8b5de8f5\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/quantization.html\", \"title\": \"quantization\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Model Quantization * [View page source](..\/_sources\/software\/quantization.rst.txt)\\n---\\nModel Quantization [\\uf0c1](#model-quantization \\\"Permalink to this heading\\\") =======================================================================\\nFuriosa SDK and first generation Warboy support INT8 models. To support floating point models, Furiosa SDK provides quantization tools to convert FP16 or FP32 floating point data type models into INT8 data type models. Quantization is a common technique used to increase model processing performance or accelerate hardware. Using the quantization tool provied by Furiosa SDK, a greater variety of models can be accelerated by deploying the NPU.\\nQuantization method supported by Furiosa SDK is based on *post-training 8-bit quantization* , and follows [Tensorflow Lite 8-bit quantization specification](https:\/\/www.tensorflow.org\/lite\/performance\/quantization_spec) .\\nHow It Works [\\uf0c1](#how-it-works \\\"Permalink to this heading\\\") -----------------------------------------------------------\\nAs shown in the diagram below, the quantization tool receives the ONNX model as input, performs quantization through the following three steps, and outputs the quantized ONNX model.\\n1. Graph Optimization 2. Calibration 3. Quantization\\nIn the graph optimization process, the topological structure of the graph is changed by adding or replacing operators in the model through analysis of the original model network structure, so that the model can process quantized data with a minimal drop in accuracy.\\nIn the calibration process, the data used to train the model is required in order to calibrate the weights of the model.\\nAccuracy of Quantized Models [\\uf0c1](#accuracy-of-quantized-models \\\"Permalink to this heading\\\") -------------------------------------------------------------------------------------------\\nThe table below compares the accuracy of the original floating-point models with that of the quantized models obtained using the quantizer and various calibration methods provided by Furiosa SDK:\\nQuantization Accuracy\\n[\\uf0c1](#id1 \\\"Permalink to this table\\\")\\n| Model | FP Accuracy | INT8 Accuracy (Calibration Method) | INT8 Accuracy \\u00f7 FP Accuracy | | --- | --- | --- | --- | | ConvNext-B | 85.8% | 80.376% (Asymmetric MSE) | 93.678% | | EfficientNet-B0 | 77.698% | 73.556% (Asymmetric 99.99%-Percentile) | 94.669% | | EfficientNetV2-S | 84.228% | 83.566% (Asymmetric 99.99%-Percentile) | 99.214% | | ResNet50 v1.5 | 76.456% | 76.228% (Asymmetric MSE) | 99.702% | | RetinaNet | mAP 0.3757 | mAP 0.37373 (Symmetric Entropy) | 99.476% | | YOLOX-l | mAP 0.497 | mAP 0.48524 (Asymmetric 99.99%-Percentile) | 97.634% | | YOLOv5-l | mAP 0.490 | mAP 0.47443 (Asymmetric MSE) | 96.822% | | YOLOv5-m | mAP 0.454 | mAP 0.43963 (Asymmetric SQNR) | 96.835% |\\nModel Quantization APIs [\\uf0c1](#model-quantization-apis \\\"Permalink to this heading\\\") ---------------------------------------------------------------------------------\\nYou can use the APU and command line tool provided in this SDK to convert an ONNX model into an 8bit quantized model.\\nRefer to the links below for further instructions.\\n* [Python SDK example: How to use Furiosa SDK from start to finish](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/HowToUseFuriosaSDKFromStartToFinish.ipynb) * [Python SDK Quantization example](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/main\/examples\/quantizers) * [Python reference - furiosa.quantizer](https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/api\/python\/furiosa.quantizer.html)\\n[Previous](compiler.html \\\"Compiler\\\") [Next](performance.html \\\"Performance Optimization\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/c-sdk.html","title":"c-sdk","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"2114c344-3971-4d26-8c09-a001327ce7bb\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/c-sdk.html\", \"title\": \"c-sdk\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* C SDK installation and user guide * [View page source](..\/_sources\/software\/c-sdk.rst.txt)\\n---\\nC SDK installation and user guide [\\uf0c1](#c-sdk-installation-and-user-guide \\\"Permalink to this heading\\\") =====================================================================================================\\nWe explain here how to write FuriosaAI NPU applications using C programming language. The C SDK provides a C ABI-based static library and C header file. Using these, you can write applications in C, C++, or other languages that support C ABI.\\nThe provided C SDK is relatively lower-level than [Python SDK](python-sdk.html#pythonsdk) . It can be used when lower latency and higher performance are required, or when Python runtime cannot be used.\\nWarning  `furiosa-libnux-dev` and the current C API are being deprecated in the future release.\\nAs substitute of the current API, new C API based on the next-generation runtime called FuriosaRT will be introduced with more features in the future release.\\nC SDK installation [\\uf0c1](#c-sdk-installation \\\"Permalink to this heading\\\") -----------------------------------------------------------------------\\nThe minimum requirements for C SDK are as follows.\\n* Ubuntu 20.04 LTS (Debian bullseye) or higher * System administrator privileges (root) * [FuriosaAI SDK required packages](installation.html#requiredpackages)\\nIn order to install and use C SDK, you must install the driver, firmware, and runtime library in accordance with the [Required Package Installation](installation.html#requiredpackages) guide.\\nOnce you have installed the required packages, follow the instructions below to install C SDK.\\nInstallation using APT server\\nTo use FuriosaAI APT, refer to [APT server configuration](installation.html#setupaptrepository) and complete the authentication setting for server connection.\\n``` apt-get update && apt-get install -y furiosa-libnux-dev\\n```\\nCompiling with C SDK [\\uf0c1](#compiling-with-c-sdk \\\"Permalink to this heading\\\") ---------------------------------------------------------------------------\\nOnce you install the package as above, you can compile using the C SDK.\\nC header files and static libraries are located in the `\/usr\/include\/furiosa` and `\/usr\/lib\/x86_64-linux-gnu` directories respectively. They are the system paths that gcc looks to find C headers and libraries by default, so you can simply compile C applications with only `-lnux` option as follows:\\n``` gcc example.c -lnux\\n```\\nAlso, you can find C SDK examples and C API reference at [C Language SDK Reference](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/c\/index.html) .\\n[Previous](python-sdk.html \\\"Python SDK installation and user guide\\\") [Next](cli.html \\\"Command Line Tools\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/installation.html","title":"installation","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"2a739e9e-c42e-4c07-82d5-41cf614dd6a9\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/installation.html\", \"title\": \"installation\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Driver, Firmware, and Runtime Installation * [View page source](..\/_sources\/software\/installation.rst.txt)\\n---\\nDriver, Firmware, and Runtime Installation [\\uf0c1](#driver-firmware-and-runtime-installation \\\"Permalink to this heading\\\") =====================================================================================================================\\nHere, we explain how to install the packages necessary to use the various SW components provided by FuriosaAI. The required packages are composed of kernel drivers, firmware, and runtime library, and they can be easily installed via the APT package manager.\\nNote\\nYou will be able to login [FuriosaAI IAM](https:\/\/iam.furiosa.ai) and create a new API key upon registration to the FuriosaAI evaluation program. Currently, the request for registration can be done through [contact @\\nfuriosa .\\nai](mailto:contact%40furiosa.ai) .\\nMinimum requirements for SDK installation [\\uf0c1](#minimum-requirements-for-sdk-installation \\\"Permalink to this heading\\\") ---------------------------------------------------------------------------------------------------------------------\\n* Ubuntu 20.04 LTS (Focal Fossa)\/Debian bullseye   or higher * Administrator privileges on system (root) * Internet-accessible network\\nAPT server configuration [\\uf0c1](#apt-server-configuration \\\"Permalink to this heading\\\") -----------------------------------------------------------------------------------\\nIn order to use the APT server as provided by FuriosaAI, the APT server must be configured on Ubuntu or Debian Linux as delineated below.\\n1. Install the necessary packages to access HTTPS-based APT server.\\n``` sudo apt update sudo apt install -y ca-certificates apt-transport-https gnupg wget\\n```\\n2. Register the FuriosaAI public Signing key.\\n``` mkdir -p \/etc\/apt\/keyrings && \\\\ wget -q -O- https:\/\/archive.furiosa.ai\/furiosa-apt-key.gpg \\\\ | gpg --dearmor \\\\ | sudo tee \/etc\/apt\/keyrings\/furiosa-apt-key.gpg > \/dev\/null\\n```\\n3. Generate a new API key from    [FuriosaAI IAM](https:\/\/iam.furiosa.ai)    , and configure the API key as follows:\\n``` sudo tee -a \/etc\/apt\/auth.conf.d\/furiosa.conf > \/dev\/null <<EOT   machine archive.furiosa.ai   login [KEY (ID)]   password [PASSWORD] EOT\\nsudo chmod 400 \/etc\/apt\/auth.conf.d\/furiosa.conf\\n```\\n4. Configure the APT server according to the explanation given in the Linux distribution version tab.\\nUbuntu 20.04 (Debian Bullseye)\\nUbuntu 22.04 (Debian Bookworm)\\nRegister the APT server through the command below.\\n``` sudo tee -a \/etc\/apt\/sources.list.d\/furiosa.list <<EOT deb [arch=amd64 signed-by=\/etc\/apt\/keyrings\/furiosa-apt-key.gpg] https:\/\/archive.furiosa.ai\/ubuntu focal restricted EOT\\n```\\nRegister the APT server through the command below.\\n``` sudo tee -a \/etc\/apt\/sources.list.d\/furiosa.list <<EOT deb [arch=amd64 signed-by=\/etc\/apt\/keyrings\/furiosa-apt-key.gpg] https:\/\/archive.furiosa.ai\/ubuntu jammy restricted EOT\\n```\\nInstalling required packages. [\\uf0c1](#installing-required-packages \\\"Permalink to this heading\\\") --------------------------------------------------------------------------------------------\\nIf you have registered the APT server as above, or registered on the download site, you will be able to install the required packages - NPU kernel driver, firmware, and runtime.\\nInstallation using APT server\\n``` sudo apt-get update && sudo apt-get install -y furiosa-driver-warboy furiosa-libnux\\n```\\n### Adding a user to the `furiosa` Group [\\uf0c1](#adding-a-user-to-the-furiosa-group \\\"Permalink to this heading\\\")\\nLinux is a multi-user operating system that enables file and device access for both the owner and users within a specific group. The NPU device driver creates a group called `furiosa` and restricts access to NPU devices exclusively to users who are members of the `furiosa` group. To add a user to a member of `furiosa` group, please run as follows:\\n``` sudo usermod -aG furiosa <username>\\n```\\nReplace <username> with the name of the user you want to add to the `furiosa` group. For example, in order to add the current user (i.e., `$USER` ) to the `furiosa` group, you can run as follows:\\n``` sudo usermod -aG furiosa $USER\\n```\\nUpon logging out and logging back in, the change to the group membership will take effect.\\n### Holding\/unholding installed version [\\uf0c1](#holding-unholding-installed-version \\\"Permalink to this heading\\\")\\nFollowing package installation, in order to maintain a stable operating environment, there may be a need to hold the installed packages versions. By using the command below, you will be able to hold the currently installed versions.\\n``` sudo apt-mark hold furiosa-driver-warboy furiosa-libhal-warboy furiosa-libnux libonnxruntime\\n```\\nIn order to unhold and update the current package versions, designate the package that you wish to unhold with the command `apt-mark\\nunhold` .\\nHere, you can state the name of the package, thereby unholding selectively a specific package. In order to show the properties of an already held package, use the command `apt-mark\\nshowhold` .\\n``` sudo apt-mark unhold furiosa-driver-warboy furiosa-libhal-warboy furiosa-libnux libonnxruntime\\n```\\n### Installing a specific version [\\uf0c1](#installing-a-specific-version \\\"Permalink to this heading\\\")\\nIf you need to install a specific version, you may designate the version that you want and install as follows.\\n1. Check available versions through    `apt        list`    .\\n``` sudo apt list -a furiosa-libnux\\n```\\n2. State the package name and version as options in the command    `apt-get        install`\\n``` sudo apt-get install -y furiosa-libnux=0.9.1-?\\n```\\nNPU Firmware Update [\\uf0c1](#npu-firmware-update \\\"Permalink to this heading\\\") -------------------------------------------------------------------------\\n[Previous](intro.html \\\"FuriosaAI SW Stack Introduction\\\") [Next](python-sdk.html \\\"Python SDK installation and user guide\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/cli.html","title":"cli","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"8bcf3c20-b1d0-44c9-bedc-9e24a3ce9586\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/cli.html\", \"title\": \"cli\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Command Line Tools * [View page source](..\/_sources\/software\/cli.rst.txt)\\n---\\nCommand Line Tools [\\uf0c1](#command-line-tools \\\"Permalink to this heading\\\") =======================================================================\\nThrough the command line tools, Furiosa SDK provides functions such as monitoring NPU device information, compiling models, and checking compatibility between models and SDKs. This section explains how to install and use each command line tool.\\nfuriosa-toolkit [\\uf0c1](#furiosa-toolkit \\\"Permalink to this heading\\\") -----------------------------------------------------------------  `furiosa-toolkit` provides a command line tool that enables users to manage and check the information of NPU devices.\\n### furiosa-toolkit installation [\\uf0c1](#furiosa-toolkit-installation \\\"Permalink to this heading\\\")\\nTo use this command line tool, you first need to install the kernel driver as shown in [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) .\\nSubsequently, follow the instructions below to install furiosa-toolkit.\\nInstallation using APT server\\n``` sudo apt-get install -y furiosa-toolkit\\n```\\n### furiosactl [\\uf0c1](#furiosactl \\\"Permalink to this heading\\\")\\nThe furiosactl command provides a variety of subcommands and has the ability to obtain information or control the device.\\n``` furiosactl <sub command> [option] ..\\n```\\n#### `furiosactl info` [\\uf0c1](#furiosactl-info \\\"Permalink to this heading\\\")\\nAfter installing the kernel driver, you can use the `furiosactl` command to check whether the NPU device is recognized. Currently, this command provides the `furiosactl\\ninfo` command to output temperature, power consumption and PCI information of the NPU device. If the device is not visible with this command after mounting it on the machine, [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) to install the driver. If you add the `--full` option to the `info` command, you can see the device\\u2019s UUID and serial number information together.\\n``` $ furiosactl info +------+--------+----------------+-------+--------+--------------+ | NPU  | Name   | Firmware       | Temp. | Power  | PCI-BDF      | +------+--------+----------------+-------+--------+--------------+ | npu1 | warboy | 1.6.0, 3c10fd3 |  54\\u00b0C | 0.99 W | 0000:44:00.0 | +------+--------+----------------+-------+--------+--------------+\\n$ furiosactl info --full +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+ | NPU  | Name   | UUID                                 | S\/N               | Firmware       | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+ | npu1 | warboy | 00000000-0000-0000-0000-000000000000 | WBYB0000000000000 | 1.6.0, 3c10fd3 |  54\\u00b0C | 0.99 W | 0000:44:00.0 | 511:0   | +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\\n```\\n#### `furiosactl list` [\\uf0c1](#furiosactl-list \\\"Permalink to this heading\\\")\\nThe `list` subcommand provides information about the device files available on the NPU device. You can also check whether each core present in the NPU is in use or idle.\\n``` furiosactl list +------+------------------------------+-----------------------------------+ | NPU  | Cores                        | DEVFILES                          | +------+------------------------------+-----------------------------------+ | npu1 | 0 (available), 1 (available) | npu1, npu1pe0, npu1pe1, npu1pe0-1 | +------+------------------------------+-----------------------------------+\\n```\\n#### `furiosactl ps` [\\uf0c1](#furiosactl-ps \\\"Permalink to this heading\\\")\\nThe `ps` subcommand prints information about the OS process currently occupying the NPU device.\\n``` $ furiosactl ps +-----------+--------+------------------------------------------------------------+ | NPU       | PID    | CMD                                                        | +-----------+--------+------------------------------------------------------------+ | npu0pe0-1 | 132529 | \/usr\/bin\/python3 \/usr\/local\/bin\/uvicorn image_classify:app | +-----------+--------+------------------------------------------------------------+\\n```\\n#### `furiosactl top` (experimental) [\\uf0c1](#furiosactl-top-experimental \\\"Permalink to this heading\\\")\\nThe `top` subcommand is used to view utilization by NPU unit over time. The output has the following meaning By default, utilization is calculated every 1 second, but you can set the calculation interval yourself with the `--interval` option. (unit: ms)\\nfuriosa top fields\\n[\\uf0c1](#id1 \\\"Permalink to this table\\\")\\n| Item | Description | | --- | --- | | Datetime | Observation time | | PID | Process ID that is using the NPU | | Device | NPU device in use | | NPU(%) | Percentage of time the NPU was used during the observation time. | | Comp(%) | Percentage of time the NPU was used for computation during the observation time | | I\/O (%) | Percentage of time the NPU was used for I\/O out of the time the NPU was used | | Command | Executed command line of the process |\\n``` $ furiosactl top --interval 200 NOTE: furiosa top is under development. Usage and output formats may change. Please enter Ctrl+C to stop. Datetime                        PID       Device        NPU(%)   Comp(%)   I\/O(%)   Command 2023-03-21T09:45:56.699483936Z  152616    npu1pe0-1      19.06    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:56.906443888Z  152616    npu1pe0-1      51.09     93.05     6.95   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.110489333Z  152616    npu1pe0-1      46.40     97.98     2.02   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.316060982Z  152616    npu1pe0-1      51.43    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.521140588Z  152616    npu1pe0-1      54.28     94.10     5.90   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.725910558Z  152616    npu1pe0-1      48.93     98.93     1.07   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.935041998Z  152616    npu1pe0-1      47.91    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:58.13929122Z   152616    npu1pe0-1      49.06     94.94     5.06   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\\n```\\nfuriosa-bench (Benchmark Tool) [\\uf0c1](#furiosa-bench-benchmark-tool \\\"Permalink to this heading\\\") ---------------------------------------------------------------------------------------------  `furiosa-bench` command carries out a benchmark with a ONNX or TFLite model and a workload using furiosa-runtime. A benchmark result includes tail latency and QPS.\\nThe arguments of the command are as follows:\\n``` $ furiosa-bench --help USAGE:   furiosa-bench [OPTIONS] <model-path>\\n  OPTIONS:       -b, --batch <number>                       Sets the number of batch size, which should be exponents of two [default: 1]       -o, --output <bench-result-path>           Create json file that has information about the benchmark       -C, --compiler-config <compiler-config>    Sets a file path for compiler configuration (YAML format)       -d, --devices <devices>                    Designates NPU devices to be used (e.g., \\\"warboy(2)*1\\\" or \\\"npu0pe0-1\\\")       -h, --help                                 Prints help information       -t, --io-threads <number>                  Sets the number of I\/O Threads [default: 1]           --duration <min-duration>              Sets the minimum test time in seconds. Both min_query_count and min_duration should be met to finish the test                                                 [default: 0]       -n, --queries <min-query-count>            Sets the minimum number of test queries. Both min_query_count and min_duration_ms should be met to finish the                                                 test [default: 1]       -T, --trace-output <trace-output>          Sets a file path for profiling result (Chrome Trace JSON format)       -V, --version                              Prints version information       -v, --verbose                              Print verbose log       -w, --workers <number>                     Sets the number of workers [default: 1]           --workload <workload>                  Sets the bench workload which can be either latency-oriented (L) or throughput-oriented (T) [default: L]\\n  ARGS:       <model-path>\\n```\\nMODEL\\\\_PATH is the file path of ONNX, TFLite or ENF (format produced by [furiosa-compiler](compiler.html#compilercli) ).\\nThe following is an example usage of furiosa-bench without an output path option (i.e., `--output` ):\\n``` $ furiosa-bench mnist-8.onnx --workload L -n 1000 -w 8 -t 2\\n  ======================================================================   This benchmark was executed with latency-workload which prioritizes latency of individual queries over throughput.   1000 queries executed with batch size 1   Latency stats are as follows   QPS(Throughput): 34.40\/s\\n  Per-query latency:   Min latency (us)    : 8399   Max latency (us)    : 307568   Mean latency (us)   : 29040   50th percentile (us): 19329   95th percentile (us): 62797   99th percentile (us): 79874   99th percentile (us): 307568\\n```\\nIf an output path is specified, furiosa-bench will save a JSON document as follows:\\n``` $ furiosa-bench mnist-8.onnx --workload L -n 1000 -w 8 -t 2 -o mnist.json $ cat mnist.json\\n  {       \\\"model_data\\\": {           \\\"path\\\": \\\".\/mnist-8.onnx\\\",           \\\"md5\\\": \\\"d7cd24a0a76cd492f31065301d468c3d  .\/mnist-8.onnx\\\"       },       \\\"compiler_version\\\": \\\"0.10.0-dev (rev: 2d862de8a built_at: 2023-07-13T20:05:04Z)\\\",       \\\"hal_version\\\": \\\"Version: 0.12.0-2+nightly-230716\\\",       \\\"git_revision\\\": \\\"fe6f77a\\\",       \\\"result\\\": {           \\\"mode\\\": \\\"Latency\\\",           \\\"total run time\\\": \\\"30025 us\\\",           \\\"total num queries\\\": 1000,           \\\"batch size\\\": 1,           \\\"qps\\\": \\\"33.31\/s\\\",           \\\"latency stats\\\": {               \\\"min\\\": \\\"8840 us\\\",               \\\"max\\\": \\\"113254 us\\\",               \\\"mean\\\": \\\"29989 us\\\",               \\\"50th percentile\\\": \\\"18861 us\\\",               \\\"95th percentile\\\": \\\"64927 us\\\",               \\\"99th percentile\\\": \\\"87052 us\\\",               \\\"99.9th percentile\\\": \\\"113254 us\\\"           }       }   }\\n```\\nfuriosa [\\uf0c1](#furiosa \\\"Permalink to this heading\\\") -------------------------------------------------\\nThe `furiosa` command is a meta-command line tool that can be used by installing the Python SDK <PythonSDK>\\n. Additional subcommands are also added when the extension package is installed.\\nIf the Python execution environment is not prepared, refer to [Python execution environment setup](python-sdk.html#setuppython) .\\nInstalling command line tool.\\n``` $ pip install furiosa-sdk\\n```\\nVerifying installation.\\n``` $ furiosa compile --version libnpu.so --- v2.0, built @ fe1fca3 0.5.0 (rev: 49b97492a built at 2021-12-07 04:07:08) (wrapper: None)\\n```\\n### furiosa compile [\\uf0c1](#furiosa-compile \\\"Permalink to this heading\\\")\\nThe `compile` command compiles models such as [ONNX](https:\/\/onnx.ai\/) and [TFLite](https:\/\/www.tensorflow.org\/lite) , generating programs that utilize FuriosaAI NPU.\\nDetailed explanations and options can be found in the [furiosa-compiler](compiler.html#compilercli) page.\\n### furiosa litmus (Model Compatibility Checker) [\\uf0c1](#furiosa-litmus-model-compatibility-checker \\\"Permalink to this heading\\\")\\nThe `litmus` is a tool to check quickly if an [ONNX](https:\/\/onnx.ai\/) model can work normally with Furiosa SDK using NPU. `litmus` goes through all usage steps of Furiosa SDK, including quantization, compilation, and inferences on FuriosaAI NPU. `litmus` is also a useful bug reporting tool. If you specify `--dump` option, `litmus` will collect logs and environment information and dump an archive file. The archive file can be used to report issues.\\nThe steps executed by `litmus` command are as follows.\\n> * Step1: Load an input model and check it is a valid model. > * Step2: Quantize the model with random calibration. > * Step3: Compile the quantized model. > * Step4: Inference the compiled model using >   `furiosa-bench` >   . This step is skipped if >   `furiosa-bench` >   was not installed.\\nUsage:\\n``` furiosa-litmus [-h] [--dump OUTPUT_PREFIX] [--skip-quantization] [--target-npu TARGET_NPU] [-v] model_path\\n```\\nA simple example using `litmus` command is as follows.\\n``` $ furiosa litmus model.onnx libfuriosa_hal.so --- v0.11.0, built @ 43c901f INFO:furiosa.common.native:loaded native library libfuriosa_compiler.so.0.10.0 (0.10.0-dev d7548b7f6) furiosa-quantizer 0.10.0 (rev. 9ecebb6) furiosa-litmus 0.10.0 (rev. 9ecebb6) [Step 1] Checking if the model can be loaded and optimized ... [Step 1] Passed [Step 2] Checking if the model can be quantized ... [Step 2] Passed [Step 3] Checking if the model can be compiled for the NPU family [warboy-2pe] ... [1\/6] \\ud83d\\udd0d   Compiling from onnx to dfg Done in 0.09272794s [2\/6] \\ud83d\\udd0d   Compiling from dfg to ldfg \\u25aa\\u25aa\\u25aa\\u25aa\\u25aa [1\/3] Splitting graph(LAS)...Done in 9.034934s \\u25aa\\u25aa\\u25aa\\u25aa\\u25aa [2\/3] Lowering graph(LAS)...Done in 20.140083s \\u25aa\\u25aa\\u25aa\\u25aa\\u25aa [3\/3] Optimizing graph...Done in 0.019548794s Done in 29.196825s [3\/6] \\ud83d\\udd0d   Compiling from ldfg to cdfg Done in 0.001701888s [4\/6] \\ud83d\\udd0d   Compiling from cdfg to gir Done in 0.015205072s [5\/6] \\ud83d\\udd0d   Compiling from gir to lir Done in 0.0038304s [6\/6] \\ud83d\\udd0d   Compiling from lir to enf Done in 0.020943863s \\u2728  Finished in 29.331545s [Step 3] Passed [Step 4] Perform inference once for data collection... (Optional) \\u2728  Finished in 0.000001198s ====================================================================== This benchmark was executed with latency-workload which prioritizes latency of individual queries over throughput. 1 queries executed with batch size 1 Latency stats are as follows QPS(Throughput): 125.00\/s\\nPer-query latency: Min latency (us)    : 7448 Max latency (us)    : 7448 Mean latency (us)   : 7448 50th percentile (us): 7448 95th percentile (us): 7448 99th percentile (us): 7448 99th percentile (us): 7448 [Step 4] Finished\\n```\\nIf you have quantized model already, you can skip Step1 and Step2 with `--skip-quantization` option.\\n``` $ furiosa litmus --skip-quantization quantized-model.onnx libfuriosa_hal.so --- v0.11.0, built @ 43c901f INFO:furiosa.common.native:loaded native library libfuriosa_compiler.so.0.10.0 (0.10.0-dev d7548b7f6) furiosa-quantizer 0.10.0 (rev. 9ecebb6) furiosa-litmus 0.10.0 (rev. 9ecebb6) [Step 1] Skip model loading and optimization [Step 2] Skip model quantization [Step 1 & Step 2] Load quantized model ... [Step 3] Checking if the model can be compiled for the NPU family [warboy-2pe] ... ...\\n```\\nYou can use the `--dump\\n<path>` option to create a <path>-<unix\\\\_epoch>.zip\\nfile that contains metadata necessary for analysis, such as compilation logs, runtime logs, software versions, and execution environments. If you have any problems, you can get support through [FuriosaAI customer service center](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals) with this zip file.\\n``` $ furiosa litmus --dump archive model.onnx libfuriosa_hal.so --- v0.11.0, built @ 43c901f INFO:furiosa.common.native:loaded native library libfuriosa_compiler.so.0.10.0 (0.10.0-dev d7548b7f6) furiosa-quantizer 0.10.0 (rev. 9ecebb6) furiosa-litmus 0.10.0 (rev. 9ecebb6) [Step 1] Checking if the model can be loaded and optimized ... [Step 1] Passed ...\\n$ zipinfo -1 archive-1690438803.zip archive-16904388032l4hoi3h\/meta.yaml archive-16904388032l4hoi3h\/compiler\/compiler.log archive-16904388032l4hoi3h\/compiler\/memory-analysis.html archive-16904388032l4hoi3h\/compiler\/model.dot archive-16904388032l4hoi3h\/runtime\/trace.json\\n```\\n[Previous](c-sdk.html \\\"C SDK installation and user guide\\\") [Next](compiler.html \\\"Compiler\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.6.0.html","title":"0","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"aa27e152-d0c0-4c00-99e9-54d1516e3a5b\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.6.0.html\", \"title\": \"0\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Release Notes - 0.6.0 * [View page source](..\/_sources\/releases\/0.6.0.rst.txt)\\n---\\nRelease Notes - 0.6.0 [\\uf0c1](#release-notes-0-6-0 \\\"Permalink to this heading\\\") ===========================================================================\\nFuriosaAI SDK 0.6.0 is a major release. It includes 234 PRs on performance improvements, added functionalities, and bug fixes, as well as approximately 900 commits.\\nHow to upgrade [\\uf0c1](#how-to-upgrade \\\"Permalink to this heading\\\") ---------------------------------------------------------------\\nIf you are using the APT repositories, you easily upgrade with the instructions below: More detailed instructions can be found at [Driver, Firmware, and Runtime Installation](..\/software\/installation.html#requiredpackages) .\\n> ``` > apt-get update && \\\\ > apt-get install furiosa-driver-pdma furiosa-libnpu-warboy furiosa-libnux >  > pip uninstall furiosa-sdk-quantizer furiosa-sdk-runtime furiosa-sdk-validator && \\\\ > pip install --upgrade furiosa-sdk >  > ```\\nMajor changes [\\uf0c1](#major-changes \\\"Permalink to this heading\\\") -------------------------------------------------------------\\nThe kernel driver (furiosa-driver-pdma) has been upgraded to 1.2.2, and the user-level driver (furiosa-libnpu-warboy) has been upgraded to 0.5.2, thereby providing more stable and higher NPU performance. Other major changes include the following:\\n### Compiler [\\uf0c1](#compiler \\\"Permalink to this heading\\\")\\n* Addition of NPU accelerated operators (see   [List of Supported Operators for Warboy Acceleration](..\/npu\/warboy.html#supportedoperators)   for full list of accelerated operators)      > + Space-to-depth (CRD mode)   > + Transpose   > + Slice (height axis only)   > + Concat (height axis only)   > + Grouped Convolution (if groups <= 128) * Improvements to significantly reduce frequency of CPU tasks in models with   operators that require large memory usage (reduced execution time)\\n### Quantizer [\\uf0c1](#quantizer \\\"Permalink to this heading\\\")\\n* Improve model quantization process to ensure idempotency * Remove PyTorch reliance * Improve code quality by removing multiple Pylint warnings * Upgrade multiple library dependencies (e.g. Numpy -> 1.21.5, Pyyaml -> 6.0.0)\\n### Python SDK [\\uf0c1](#python-sdk \\\"Permalink to this heading\\\")\\n* Python SDK project structure change      + furiosa-sdk-runtime -> furiosa-sdk   + furiosa-sdk-quantizer -> furiosa-quantizer   + furiosa-sdk-validator -> furiosa-litmus * Validator, a package that checks for model compatibility with Furiosa SDK, is renamed to litmus. Installation instruction has also been updated accordingly.\\nSee [furiosa litmus (Model Compatibility Checker)](..\/software\/cli.html#litmus) for more detailed usage instructions.\\n> ``` > $ pip install 'furiosa-sdk[litmus]' >  > ```\\n#### Furiosa Serving: Addition of FastAPI-based advanced serving library [\\uf0c1](#furiosa-serving-addition-of-fastapi-based-advanced-serving-library \\\"Permalink to this heading\\\")\\nfuriosa-serving is based on FastAPI. It allows you to easily add Python-based business logic or image pre\/postprocessing code, before or after executing model inference API.\\nYou can install using the following instructions.\\n> ``` > $ pip install 'furiosa-sdk[serving]' >  > ```\\nThe usage example is shown below. You can find more detailed instructions at [furiosa-serving Github](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/main\/python\/furiosa-serving) .\\n> ``` > from typing import Dict >  > from fastapi import File, UploadFile > from furiosa.server.utils.thread import synchronous > from furiosa.serving import ServeAPI, ServeModel > import numpy as np >  >  > serve = ServeAPI() >  >  > model: ServeModel = synchronous(serve.model)( >     'imagenet', >     location='.\/examples\/assets\/models\/image_classification.onnx' > ) >  > @model.post(\\\"\/models\/imagenet\/infer\\\") > async def infer(image: UploadFile = File(...)) -> Dict: >     # Convert image to Numpy array with your preprocess() function >     tensors: List[np.ndarray] = preprocess(image) >  >     # Infer from ServeModel >     result: List[np.ndarray] = await model.predict(tensors) >  >     # Classify model from numpy array with your postprocess() function >     response: Dict = postprocess(result) >  >     return response >  > ```\\n[Previous](0.7.0.html \\\"Release Notes - 0.7.0\\\") [Next](0.5.0.html \\\"Release Notes - 0.5.0\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/python-sdk.html","title":"python-sdk","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"ab2af890-296a-47f5-89d9-ffc5cf4a924f\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/python-sdk.html\", \"title\": \"python-sdk\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Python SDK installation and user guide * [View page source](..\/_sources\/software\/python-sdk.rst.txt)\\n---\\nPython SDK installation and user guide [\\uf0c1](#python-sdk-installation-and-user-guide \\\"Permalink to this heading\\\") ===============================================================================================================\\nFuriosaAI Python SDK is a software development kit for writing Python applications that use the NPU. With the Python SDK, you can utilize various tools, libraries, and frameworks of the Python ecosystem that are most widely used in the AI\/ML field for developing NPU applications. Python SDK consists of various modules and provides an inference API, a quantization API, a command line tool, and a server program for serving.\\nRequirements [\\uf0c1](#requirements \\\"Permalink to this heading\\\") -----------------------------------------------------------\\n* Ubuntu 20.04 LTS (Debian bullseye) or higher * [FuriosaAI SDK required packages](installation.html#requiredpackages) * Python 3.8 or higher (See   [Python execution environment setup](#setuppython)   for setup Python environment) * Latest version of pip\\nTo install and use Python SDK, follow the [Installing required packages](installation.html#requiredpackages) guide. You need to install the required kernel driver, firmware, and runtime library.\\nPython execution environment setup [\\uf0c1](#python-execution-environment-setup \\\"Permalink to this heading\\\") -------------------------------------------------------------------------------------------------------\\nPython SDK requires Python 3.8 or above. Here, we describe Python execution environment configuration.\\nNote\\nIf you are not using the FuriosaAI Python SDK, or if you are familiar with configuring a Python execution environment, you can skip this section.\\nYou can check the Python version currently installed in your system with the command below.\\n``` python --version Python 3.8.10\\n```\\nIf the Python command does not exist, or if your Python version is below 3.8, configure the Python environment by selecting one of the methods below.\\n* [Python environment configuration with Conda](#condainstall)   (recommended):   [Conda](https:\/\/docs.conda.io\/projects\/conda\/en\/latest\/index.html)   allows you to   configure a separate, isolated Python environment for specific Python applications.   Conda therefore prevents the package dependency issues or Python version issues that users   often encounter when installing Python applications. * Configure the Python execution environment directly on the   [Configuring Python environment using Linux packages](#setuppythononlinux)   : Linux system.   You can select this option if you are not concerned about conflicts with other Python execution environments.\\n### Python environment configuration with Conda [\\uf0c1](#python-environment-configuration-with-conda \\\"Permalink to this heading\\\")\\nConda makes it easy to configure a isolated Python environment for a specific Python application. To find out more about Conda, refer to readings available in [Conda](https:\/\/docs.conda.io\/projects\/conda\/en\/latest\/index.html) .\\nYou can get started by downloading the installer as shown below. Select yes\\nto all questions when running `sh\\n.\/Miniconda3-latest-Linux-x86_64.sh` .\\n``` wget https:\/\/repo.anaconda.com\/miniconda\/Miniconda3-latest-Linux-x86_64.sh sh .\/Miniconda3-latest-Linux-x86_64.sh source ~\/.bashrc conda --version\\n```\\n#### Creating and activating isolated Python execution environment [\\uf0c1](#creating-and-activating-isolated-python-execution-environment \\\"Permalink to this heading\\\")\\nAfter installing Anaconda, you can create an isolated Python execution environment and activate it as needed.\\n1. If you want to use Python 3.8, create an execution environment with the name    `furiosa-3.8`    , by using the following command.\\n``` conda create -n furiosa-3.8 python=3.8\\n```\\n2. The created Python 3.8 environment is activated with the    `activate`    command.\\n``` conda activate furiosa-3.8 # version check python --version\\n```\\n3. Once the Python execution environment is activated, install the Python SDK as explained in    [Installing Python SDK package](#installpippackages)    . 4. If you wish to terminate the Python execution environment, use the    `deactivate`    command.\\n``` $ conda deactivate\\n```\\nAn environment created once can be used again at any time with the `activate` command. Packages that have been installed do not need to be reinstalled after activation.\\n### Configuring Python environment using Linux packages [\\uf0c1](#configuring-python-environment-using-linux-packages \\\"Permalink to this heading\\\")\\n1. If you can configure the Python environment directly on the system, install the necessary packages as shown below.\\n``` sudo apt install -y python3 python3-pip python-is-python3\\n```\\n2. Check the Python version to ensure proper installation.\\n``` python --version Python 3.8.10\\n```\\nInstalling Python SDK package [\\uf0c1](#installing-python-sdk-package \\\"Permalink to this heading\\\") ---------------------------------------------------------------------------------------------\\nBefore installing the furiosa-sdk, you need to update Python\\u2019s package installer to the latest version.\\n``` pip install --upgrade pip setuptools wheel\\n```\\nWarning\\nIf you install the furiosa-sdk without updating to the latest version, you may encounter the following error.\\n``` ERROR: Could not find a version that satisfies the requirement furiosa-quantizer-impl==0.10.* (from furiosa-quantizer==0.10.*->furiosa-sdk) (from versions: none) ERROR: No matching distribution found for furiosa-quantizer-impl==0.10.* (from furiosa-quantizer==0.10.*->furiosa-sdk)\\n```\\nInstalling with PIP\\nInstalling with the source code\\nFuriosaAI Python SDK package is uploaded on the [pypi](https:\/\/pypi.org\/) repository, so you can easily install it as shown by using the `pip` command.\\n``` pip install furiosa-sdk\\n```\\nThe package contains a compiler command line interface and an inference API. Refer to [furiosa-compiler](compiler.html#compilercli) and [Tutorial and Code Examples](tutorials.html#tutorial) for detailed usage guides.\\nAdditional functions are provided in the form of Python extra packages, and you can select and install packages as you require from [Extra packages](#pythonextrapackages) .\\nFor example, if you need to install server`\\nfor model serving and `litmus` to check the compatibility between model and SDK, specify the extension package as follows.\\n``` pip install 'furiosa-sdk[server, litmus]'\\n```\\nDownload the source code from [FuriosaAI Github repository](https:\/\/github.com\/furiosa-ai\/furiosa-sdk) and install the packages in the following order.\\n``` git clone https:\/\/github.com\/furiosa-ai\/furiosa-sdk cd furiosa-sdk\/python pip install furiosa-runtime pip install furiosa-tools pip install furiosa-sdk\\n```\\nIf you wish to install extra packages, install the Python module in the subdirectory of furiosa-sdk\/python. For example, if you want to install a model server, install it according to the order of dependencies as follows.\\n``` cd furiosa-sdk\/python pip install furiosa-server\\n```\\nExtra packages [\\uf0c1](#extra-packages \\\"Permalink to this heading\\\") ---------------------------------------------------------------\\n### Legacy Runtime\/API [\\uf0c1](#legacy-runtime-api \\\"Permalink to this heading\\\")\\nRather than the next-generation runtime and its API newly adoted since 0.10.0, you can install furiosa-sdk with the legacy runtime and API as follows:\\n``` pip install 'furiosa-sdk[legacy]'\\n```\\n### FuriosaAI Models [\\uf0c1](#furiosaai-models \\\"Permalink to this heading\\\")\\nIt can be executed directly on the NPU and provides optimized DNN model architecture, pre-trained model image, among others, in the form of a Python module. You can install them with the following command.\\n``` pip install 'furiosa-sdk[models]'\\n```\\n### Quantizer [\\uf0c1](#quantizer \\\"Permalink to this heading\\\")\\nThe quantizer package provides a set of APIs for converting a model into a quantized model. You can find more information about the quantization function provided by the Furiosa SDK and the NPU at [Model Quantization](quantization.html#modelquantization) .\\n``` pip install 'furiosa-sdk[quantizer]'\\n```\\n### Model Server [\\uf0c1](#model-server \\\"Permalink to this heading\\\")\\nProvides the function of accelerating DNN model with the NPU, and serving it with GRPC or Restful API.\\n``` pip install 'furiosa-sdk[server]'\\n```\\n### Litmus [\\uf0c1](#litmus \\\"Permalink to this heading\\\")\\nA tool to check whether the specified model is compatible with the Furiosa SDK. Here, we simulate execution of processes such as model quantization and compilation.\\n``` pip install 'furiosa-sdk[litmus]'\\n```\\n[Previous](installation.html \\\"Driver, Firmware, and Runtime Installation\\\") [Next](c-sdk.html \\\"C SDK installation and user guide\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/intro.html","title":"intro","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"bff490c4-d18d-40e2-8cdc-089ac8f529ea\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/intro.html\", \"title\": \"intro\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* FuriosaAI SW Stack Introduction * [View page source](..\/_sources\/software\/intro.rst.txt)\\n---\\nFuriosaAI SW Stack Introduction [\\uf0c1](#furiosaai-sw-stack-introduction \\\"Permalink to this heading\\\") =================================================================================================\\nFuriosaAI provides various SW components to allow the NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials.\\nThe above diagram demonstrates the SW stack provided by FuriosaAI, by layers. At the lowest level is the IntroToWarboy\\n, FuriosaAI\\u2019s first generation NPU.\\nThe following outlines the key components.\\nKernel Driver and Firmware [\\uf0c1](#kernel-driver-and-firmware \\\"Permalink to this heading\\\") ---------------------------------------------------------------------------------------\\nThe kernel driver allows the Linux operating system to recognize the NPU device and acknowledge it as a Linux device file. If the NPU is not recognized by the operating system, try reinstalling the driver. The firmware provides a low-level API for the NPU device based on the NPU device file recognized by the Linux operating system. The runtime and compiler control the NPU using the low-level API provided by the firmware, thereby executing and scheduling tasks for inference on the NPU using the compiled binary.\\nThere is no need for the user to utilize kernel driver and firmware directly, but they must be installed for Furiosa SDK to work. Installation guide can be found in [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) .\\nCompiler [\\uf0c1](#compiler \\\"Permalink to this heading\\\") ---------------------------------------------------\\nThe compiler plays a key role in optimizing DNN models and generating executable code in the NPU. Currently, the compiler supports [TFLite](https:\/\/www.tensorflow.org\/lite) and [ONNX](https:\/\/onnx.ai\/) models, and optimizes the models by introducing various latest research work and methods.\\nThe compiler provided with Warboy\\nsupports NPU acceleration of various operators in the vision area. For operators that are not supported with acceleration on NPU, the compiler compiles them such that the CPU will be utilized.\\nAdditionally, the compiler not only accelerates major vision models such as ResNet50, SSD-MobileNet, and EfficientNet, but also models designed by the users so long as supported operators are utilized - to generate code optimized for NPU.\\nFor reference, operators supported by NPU acceleration can be found in [List of Supported Operators for Warboy Acceleration](..\/npu\/warboy.html#supportedoperators) .\\nThe compiler is embedded within runtime, so users do not need to install it separately. It can be used automatically in the process of creating a session through the Python\/C SDK, or through [furiosa-compiler](compiler.html#compilercli) .\\nRuntime [\\uf0c1](#runtime \\\"Permalink to this heading\\\") -------------------------------------------------\\nRuntime analyzes the executable program generated by the compiler, and actually executes the DNN model inference task as described in the program. During compilation, the DNN model inference is optimized, and split into a number of smaller tasks running on NPU and CPU. Runtime is responsible for balancing the available resources, scheduling these tasks in accordance with the workload, and controlling the NPU via firmware for tasks being executed on the NPU.\\nRuntime functions are provided as APIs through the Python\/C SDK, which will be described in the section below, and installation instructions can be found in [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) .\\nPython SDK and C SDK [\\uf0c1](#python-sdk-and-c-sdk \\\"Permalink to this heading\\\") ---------------------------------------------------------------------------\\nPython and C SDK are packages that provide runtime functions as Python and C libraries as APIs, respectively. They provide an APIs that create objects called \\u2018session\\u2019, that allows the specified model to infer using the designated device, and enables high-performance inference in a blocking and asynchronous manner. If you need to write an application or service that utilizes the NPU, you can select and install one of the SDKs according to the programming language of the application you are using. Installation and usage of each SDK can be found in [Python SDK installation and user guide](python-sdk.html#pythonsdk) and [C SDK installation and user guide](c-sdk.html#csdk) .\\nModel quantizer API [\\uf0c1](#model-quantizer-api \\\"Permalink to this heading\\\") -------------------------------------------------------------------------\\nFuriosaAI SDK and Warboy\\nsupport INT8 models, while models with floating point data as weights undergo quantization, and can be used in Warboy\\n. To facilitate this quantization process, Furiosa SDK provides a Model quantizer API. More information about the Model quantizer API provided by the Furiosa SDK can be found in [Model Quantization](quantization.html#modelquantization) .\\nModel Server [\\uf0c1](#model-server \\\"Permalink to this heading\\\") -----------------------------------------------------------\\nThe model server exposes the DNN model as a GRPC or REST API. Model formats such as [TFLite](https:\/\/www.tensorflow.org\/lite) and [ONNX](https:\/\/onnx.ai\/) contain within them the data type and tensor shape or the input\/output tensors. Using this information, the models are exposed through the commonly used [Predict Protocol - Version 2](https:\/\/github.com\/kserve\/kserve\/blob\/master\/docs\/predict-api\/v2\/required_api.md) .\\nWith the model server, users do not need to directly access the NPU through the library and Python\/C SDK, but can access it through a remote API. In addition, horizontal scaling of services can be easily implemented by using multiple model servers serving the same model and using a load balancer.\\nThe model server requires low latency and high throughput. Here, the scheduling function of the runtime is utilized. Installation and utilization of the model server can be found in [Model Server (Serving Framework)](serving.html#modelserving) .\\nKubernetes Support [\\uf0c1](#kubernetes-support \\\"Permalink to this heading\\\") -----------------------------------------------------------------------\\nKubernetes, a platform for managing containerized workloads and services, is popular with many enterprises. FuriosaAI SW stack also provides native Kubernetes support. Kubernetes Device Plugin enables the Kubernetes cluster to recognize FuriosaAI\\u2019s NPUs and schedule them for workloads\/services that require the NPU. This feature helps the allocation of resources when multiple workloads require NPU in a multi-tenant environment such as Kubernetes, and enables efficient utilization of limited NPU resources.\\nKubernetes Node Labeller adds the information of the physical NPU mounted on the node, participating in Kubernetes, as metadata to the Kubernetes node object.\\nThis function allows the user to identify information of the NPU mounted on the node using Kubernetes API or command line tool, and to distribute workload to nodes that satisfy certain conditions by utilizing the Pod\\u2019s `spec.nodeSelector` or `spec.nodeAffinity` .\\nInstallation and usage instructions for NPU support in the Kubernetes environment can be found in the [Kubernetes Support](kubernetes_support.html#kubernetesintegration) page.\\n[Previous](..\/npu\/warboy.html \\\"FuriosaAI Warboy\\\") [Next](installation.html \\\"Driver, Firmware, and Runtime Installation\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/npu\/warboy.html","title":"warboy","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"c40d4e1b-c960-4ffb-883f-e35d53def7a1\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/npu\/warboy.html\", \"title\": \"warboy\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* FuriosaAI Warboy * [View page source](..\/_sources\/npu\/warboy.rst.txt)\\n---\\nFuriosaAI Warboy [\\uf0c1](#furiosaai-warboy \\\"Permalink to this heading\\\") ===================================================================\\nFuriosaAI\\u2019s first generation NPU Warboy is a chip with an architecture optimized for deep learning inference. It demonstrates high performance for deep learning inference while maintaining cost-efficiency. FuriosaAI Warboy is optimized for inferences with low batch sizes; for inference requests with low batch sizes, all of the chip\\u2019s resources are maximally utilized to achieve low latency. The large on-chip memory is also able to retain most major CNN models, thereby eliminating memory bottlenecks, and achieving high energy efficiency.\\nWarboy supports key CNN models used in various vision tasks, including Image Classification, Object Detection, OCR, Super Resolution, and Pose Estimation. In particular, the chip demonstrates superior performance in computations such as depthwise\/group convolution, that drive high accuracy and computational efficiency in state-of-the-art CNN models.\\nWarboy delivers 64 TOPS performance and includes 32MB of SRAM. Warboy consists of two processing elements (PE), which each delivers 32 TOPS performance and can be deployed independently. With a total performance of 64 TOPS, should there be a need to maximize response speed to models, the two PEs may undergo fusion, to aggregate as a larger, single PE. Depending on the users\\u2019 model size or performance requirements the PEs may be 1) fused so as to minimize response time, or 2) utilized independently to maximize throughput.\\nFuriosaAI SDK provides the compiler, runtime software, and profiling tools for the FuriosaAI Warboy. It also supports the INT8 quantization scheme, used as a standard in TensorFLow and PyTorch, while providing tools to convert Floating Point models using Post Training Quantization. With the FuriosaAI SDK, users can compile trained or exported models in formats commonly used for inference (TFLite or ONNX), and accelerate them on FuriosaAI Warboy.\\nHW Specifications [\\uf0c1](#hw-specifications \\\"Permalink to this heading\\\") ---------------------------------------------------------------------\\nThe chip is built with 5 billion transistors, dimensions of 180mm^2, clock speed of 2GHz, and delivers peak performance of 64 TOPS of INT8. It also supports a maximum of 4266 for LPDDR4x. Warboy has a DRAM bandwidth of 66GB\/s, and supports PCIe Gen4 8x.\\nWarboy Hardware Specification\\n[\\uf0c1](#id1 \\\"Permalink to this table\\\")\\n| Peak Performance | 64 TOPS | | --- | --- | | On-chip SRAM | 32 MB | | Host Interface | PCIe Gen4 8-lane | | Form Factor | Full-Height Half-Length (FHHL)  Half-Height Half-Length (HHHL) | | Thermal Solution | Passive Fan  Active Fan | | TDP | 40 - 60W (Configurable) | | Operating Temperature | 0 ~ 50\\u2103 | | Clock Speed | 2.0 GHz | | DDR Speed | 4266 Mbps | | Memory Type | LPDDR4X | | Memory Size | 16 GB (max. 32 GB) | | Peak Memory Bandwidth | 66 GB\/s |\\nList of Supported Operators for Warboy Acceleration [\\uf0c1](#list-of-supported-operators-for-warboy-acceleration \\\"Permalink to this heading\\\") -----------------------------------------------------------------------------------------------------------------------------------------\\nFuriosaAI Warboy and SDK can accelerate the following operators, as supported by [Tensorflow Lite](https:\/\/www.tensorflow.org\/lite) model and [ONNX](https:\/\/onnx.ai\/) .\\nThe names of the operators use [ONNX](https:\/\/onnx.ai\/) as a reference.\\nNote\\nAny operators cannot be accelerated on Warboy, the operators will run on the CPU. For some operators, even if Warboy acceleration is supported, if certain conditions are not met, they may be split into several operators or may run on the CPU. Some examples of this would be when the weight of the model is larger than Warboy memory, or if the Warboy memory is not sufficient to process a certain computation.\\nOperators Accelerated on Warboy\\n[\\uf0c1](#id2 \\\"Permalink to this table\\\")\\n| Name of operator | Additional details | | --- | --- | | [Add](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Add) |  | | [AveragePool](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#AveragePool) |  | | [BatchNormalization](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#batchnormalization) | Acceleration supported, only if after Conv | | [Clip](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#clip) |  | | [Concat](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#concat) | Acceleration supported, only for height axis | | [Conv](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#conv) | Acceleration supported, only for group  <= 128 and dilation <= 12 | | [ConvTranspose](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#convtranspose) |  | | [DepthToSpace](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#depthtospace) |  | | [Exp](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#exp) |  | | [Expand](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#expand) |  | | [Flatten](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Flatten) |  | | [Gemm](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#gemm) |  | | [LeakyRelu](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#leakyrelu) |  | | [LpNormalization](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#lpnormalization) | Acceleration supported, only for p = 2 and batch <= 2 | | [MatMul](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#matmul) |  | | [MaxPool](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#maxpool) |  | | [Mean](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#mean) |  | | [Mul](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#mul) |  | | [Pad](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Pad) |  | | [ReduceL2](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#ReduceL2) |  | | [ReduceSum](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#ReduceSum) |  | | [Relu](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Relu) |  | | [Reshape](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#reshape) |  | | [Pow](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Pow) |  | | [SpaceToDepth](https:\/\/github.com\/onnx\/onnx\/blob\/main\/docs\/Operators.md#SpaceToDepth) | Acceleration supported, only for mode=\\u201dCRD\\u201d and Furiosa SDK version 0.6.0 or higher | | [Sigmoid](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Sigmoid) |  | | [Slice](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#slice) | Acceleration supported, only for height axis | | [Softmax](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Softmax) | Acceleration supported, only for batch <= 2 | | [Softplus](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Softplus) |  | | [Sub](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#sub) |  | | [Split](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Split) |  | | [Sqrt](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Sqrt) |  | | [Transpose](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Transpose) |  | | [Unsqueeze](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#unsqueeze) |  |\\nMLPerf [\\uf0c1](#mlperf \\\"Permalink to this heading\\\") -----------------------------------------------\\nResults submitted to MLPerf can be found at [MLPerf\\u2122 Inference Edge v2.0 Results](https:\/\/mlcommons.org\/en\/inference-edge-20\/)\\n### See Also [\\uf0c1](#see-also \\\"Permalink to this heading\\\")\\n* [MLPerf\\u2122 Inference Edge v1.1 Results](https:\/\/mlcommons.org\/en\/inference-edge-11\/) * [MLPerf\\u2122 Inference Edge v0.5 Results](https:\/\/mlcommons.org\/en\/inference-edge-05\/)\\n[Previous](..\/index.html \\\"FuriosaAI NPU & SDK 0.10.1 Documents\\\") [Next](..\/software\/intro.html \\\"FuriosaAI SW Stack Introduction\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.10.0.html","title":"0","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"9197c2b9-4a70-427e-910e-e57d9f467929\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.10.0.html\", \"title\": \"0\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Release Notes - 0.10.0 * [View page source](..\/_sources\/releases\/0.10.0.rst.txt)\\n---\\nRelease Notes - 0.10.0 [\\uf0c1](#release-notes-0-10-0 \\\"Permalink to this heading\\\") =============================================================================\\nFuriosa SDK 0.10.0 is a major release which includes the followings:\\n* Adds the next generation runtime engine (FuriosaRT) with higher performance and multi-device features * Improves usability of optimization for vision models by removing quantization operators from models * Supports OpenMetrics format in Metrics Exporter and provide more metrics such as NPU utilization * Improves furiosa-litmus to collect and dump from the diagnosis steps for reporting * Removes Python dependencies from   `furiosa-compiler`   command * Adds the new benchmark tool   `furiosa-bench`  This release also includes a number of other feature additions, bug fixes, and performance improvements.\\nComponent versions\\n[\\uf0c1](#id2 \\\"Permalink to this table\\\")\\n| Package Name | Version | | --- | --- | | NPU Driver | 1.9.2 | | NPU Firmware Tools | 1.5.1 | | NPU Firmware Image | 1.7.3 | | HAL (Hardware Abstraction Layer) | 0.11.0 | | Furiosa Compiler | 0.10.0 | | Furiosa Quantizer | 0.10.0 | | Furiosa Runtime | 0.10.0 | | Python SDK (furiosa-server, furiosa-serving, ..) | 0.10.0 | | NPU Toolkit (furiosactl) | 0.11.0 | | NPU Device Plugin | 0.10.1 | | NPU Feature Discovery | 0.2.0 |\\nInstalling the latest SDK or Upgrading [\\uf0c1](#installing-the-latest-sdk-or-upgrading \\\"Permalink to this heading\\\") ---------------------------------------------------------------------------------------------------------------\\nIf you are using APT repository, the upgrade process is simple. Please run as follows. If you are not familiar with how to use FurioaAI\\u2019s APT repository, please find more detais from [Driver, Firmware, and Runtime Installation](..\/software\/installation.html#requiredpackages) .\\n``` apt-get update && apt-get upgrade\\n```\\nYou can also upgrade specific packages as follows:\\n``` apt-get update && \\\\ apt-get install -y furiosa-driver-warboy furiosa-libnux\\n```\\nYou can upgrade firmware as follows:\\n``` apt-get update && \\\\ apt-get install -y furiosa-firmware-tools furiosa-firmware-image\\n```\\nYou can upgrade Python package as follows:\\n``` pip install --upgrade pip setuptools wheel pip install --upgrade furiosa-sdk\\n```\\nWarning\\nWhen installing or upgrading the furiosa-sdk without updating pip to the latest version, you may encounter the following errors.\\n``` ERROR: Could not find a version that satisfies the requirement furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk) (from versions: none) ERROR: No matching distribution found for furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk)\\n```\\nMajor changes [\\uf0c1](#major-changes \\\"Permalink to this heading\\\") -------------------------------------------------------------\\n### Next Generation Runtime Engine, FuriosaRT [\\uf0c1](#next-generation-runtime-engine-furiosart \\\"Permalink to this heading\\\")\\nSDK 0.10.0 includes the next-generation runtime engine called FuriosaRT\\n. FuriosaRT is a newly designed runtime library that offers more advanced features and high performance in various workloads. Many components, such as furiosa-litmus, furiosa-bench, and furiosa-seving, are based on FuriosaRT, and the benefits of the new runtime engine are reflected in these components. FuriosaRT provides the backward compatibility with the previous runtime and includes the following new features:\\n#### New Runtime API [\\uf0c1](#new-runtime-api \\\"Permalink to this heading\\\")\\nFuriosaRT introduces a native asynchronous API based on Python\\u2019s asyncio < <https:\/\/docs.python.org\/3\/library\/asyncio.html> >. The existing APIs were sufficient for batch applications, but it requires extra code to implement high-performance serving applications, handling many concurrent individual requests. The new API natively supports asynchronous execution. With the new API, users can easily write their applications running on existing web frameworks such as [FastAPI](https:\/\/fastapi.tiangolo.com\/)\\nThe new API introduced many advanced features, and you can learn more about the details at [Furiosa SDK API Reference - furiosa.runtime](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html)\\n#### Multi-device Support and Improvement on Device Configuration [\\uf0c1](#multi-device-support-and-improvement-on-device-configuration \\\"Permalink to this heading\\\")\\nFuriosaRT natively supports multiple devices with a single session. This feature leads to high-performance inference using multiple devices without extra implementations. Furthermore, FuriosaRT adopts more abstracted way to specify NPU devices. Before 0.9.0 release, users used to set device file names (e.g., `npu0pe0-1` ) explicitly in the environment variable `NPU_DEVNAME` or `session.create(..,\\ndevice=\\u201d..\\u201d)` .\\nThis way was inconvinient in many cases because users need to find all available device files and specify them manually.\\nFuriosaRT allows users to specify NPU arch, count of NPUs in a textutal representation. This representation is allowed in the new environment variable `FURIOSA_DEVICES` as follows:\\n``` export FURIOSA_DEVICES=\\\"warboy(2)*8\\\"\\n```\\nThe above example lets FuriosaRT to find 8 Warboys, each of which is configured as two PEs fusion in the system.\\n``` export FURIOSA_DEVICES=\\\"npu:0:0-1,npu:1:0-1\\\"\\n```\\nFor backward compatibility, FuriosaRT still supports `NPU_DEVNAME` environment variable. However, `NPU_DEVNAME` will be deprecated in a future release.\\nYou can find more details about the device configuration at [Furiosa SDK API Reference - Device Specification](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#device-specification) .\\n#### Higher Throughput [\\uf0c1](#higher-throughput \\\"Permalink to this heading\\\")\\nAccording to our benchmark, FuriosaRT shows significantly improved throughput compared to the previous runtime. In particular, [worker\\\\_num](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#runner-api) configuration became more effective in FuriosaRT. For example, in the previous runtime, higher than 2 `worker_num` did not show significant performance improvement in most cases. However, in FuriosaRT, we observed that performance improvement is still significant even with `worker_num\\n>=\\n10` .\\nWe carried out benchmarking with Resnet50, YOLOv5m, YOLOv5L, SSD ResNet34, and SSD MobileNet models through `furiosa-bench` command introduced in this release. We observed that performance improvement is significantly up to tens of percent depending on the model with `worker_num\\n>=\\n4` .\\n### Model Server and Serving Framework [\\uf0c1](#model-server-and-serving-framework \\\"Permalink to this heading\\\")  `furiosa-server` and `furioa-serving` are a web server and a web framework respectively for serving models. The improvements of FuriosaRT are also reflected to the model server and serving framework.\\n* [Multi-device Support and Improvement on Device Configuration](#release-0-10-0-deviceselector)   can be used to configure multiple NPU devices in   `furiosa-server`   and   `furioa-serving` * New   [asyncio](https:\/\/docs.python.org\/3\/library\/asyncio.html)   -based API that FuriosaRT offers is introduced to handle more concurrent requests with less resources. * The model server and serving framework inherit the performance characteristics of FuriosaRT. Also, more   `worker_num`   can be used to improve the performance of the model server.\\nPlease refer to [Model Server (Serving Framework)](..\/software\/serving.html#modelserving) to learn more about the model server and serving framework.\\n### Model Quantization Tool [\\uf0c1](#model-quantization-tool \\\"Permalink to this heading\\\")\\nThe furiosa-quantizer is a library that transforms trained models into quantized models through the post-training quantization. In 0.10.0 release, the usability of the quantization tool has been improved, so some parameters of the `furiosa.quantizer.quantize()` API have a few breaking changes.\\n#### Motivation for Change [\\uf0c1](#motivation-for-change \\\"Permalink to this heading\\\")  `furiosa.quantizer.quantize()` function is a core function of the model quantization tool. `furiosa.quantizer.quantize()` transforms an ONNX model into a quantized model and returns it. The function has the parameter `with_quantize` that allows the model to accept directly the `uint8` type instead of `float32` , also enabling skipping the quantization process for inferences when the original data type (e.g., pixel values) is uint8. This option can result in significant performance improvements. For instance, YOLOv5 Large with this option can dramatically reduce the execution time from 60.639 ms to 0.277 ms.\\nSimilarly, `normalized_pixel_outputs` option allows to directly use `unt8` type for outputs instead of `float32` . This option can be useful when the model output is an image in RGB format or when it can be directly used as an integer value. This option shows significant performance boosts.\\nIn certain applications, two options can reduce execution time by several times to hundreds of times. However, there were the following limitations and feedback:\\n* The parameter   `normalized_pixel_outputs`   was ambiguous in expressing the purpose clearly. * `normalized_pixel_outputs`   assumes the output tensor value ranged from 0 to 1 in floating-point, and it had limited in real application. * `with_quantize`   and   `normalized_pixel_outputs`   options only supported   `uint8`   type, and didn\\u2019t support   `int8`   type.\\n#### What Changed [\\uf0c1](#what-changed \\\"Permalink to this heading\\\")\\n* Removed the parameters   `with_quantize`   and   `normalized_pixel_outputs`   from   `furiosa.quantizer.quantize()` * Instead, added the class   [ModelEditor](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.quantizer.html#furiosa.quantizer.ModelEditor)   , allowing more options for model input\/output types that offers following optimizations:      + `convert_input_type(tensor_name,          TensorType)`     method takes a tensor name, removes the corresponding quantize operator, and changes the input type to a given     `TensorType`     .   + `convert_output_type(tensor_name,          TensorType,          tensor_range)`     method takes a tensor name, removes the corresponding dequantize operator, and changes the output type to     `TensorType`     , then modifies the scale of the model output to a given     `tensor_range`     . * Since the   `convert_{output,input}_type`   methods are based on tensor names, users should be able to find tensor names from an original ONNX model.\\nFor that, `furiosa.quantizer` module provides `get_pure_input_names(ModelProto)` and `get_output_names(ModelProto)` functions to retrieve tensor names from the original ONNX model.\\nNote\\nThe removal of `with_quantize` , `normalized_pixel_outputs` parameters from `furiosa.quantizer.quantize()` is a breaking change that requires modifying existing code.\\nPlease refer to [ModelEditor](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.quantizer.html#furiosa.quantizer.ModelEditor) to learn more about the ModelEditor API and find examples from [Tutorial and Code Examples](..\/software\/tutorials.html#tutorial) .\\n### Compiler [\\uf0c1](#compiler \\\"Permalink to this heading\\\")\\nSince this release, the compiler supports NPU acceleration for the Dequantize\\noperator. So, the latency or throughput of models that include Dequantize\\noperators can be enhanced. More details of this performance optimization can be found from [Performance Optimization](..\/software\/performance.html#performanceoptimization) .\\nSince 0.10.0, the default lifetime of compiler cache has increased from 2 days to 30 days. Please refer to [Compiler Cache](..\/software\/compiler.html#compilercache) to learn the details of compiler cache feature.  `furiosa-compiler` command in 0.10.0 release also has the following improvements:\\n* Add   `furiosa-compiler`   command in addition to   `furiosa-compile`   command * `furiosa-compiler`   and   `furiosa-compile`   commands as a native executable   and do not require any Python runtime environment. * `furiosa-compiler`   is now available as an APT package, you can install via   `apt      install      furiosa-compiler`   . * `furiosa      compile`   is kept for backward compatibility, and it will be removed in a future release.\\nPlease visit [furiosa-compiler](..\/software\/compiler.html#compilercli) to learn more about furiosa-compiler\\ncommand.\\n### Performance Profiler [\\uf0c1](#performance-profiler \\\"Permalink to this heading\\\")\\nThe performance profiler is a tool that helps users to analyze performance by measuring the actual execution time of inferences. Since 0.10.0, [Tracing via Profiler Context](..\/software\/profiler.html#profilerenabledbycontext) API provides the pause\/resume features.\\nThis feature allows users to skip unnecessary steps like pre\/post processing or warming up times, leading to the reduction of the profiling overhead and the size of the profile result files. Literally, calling `profile.pause()` method immediately stops the profliling, and `profile.resume()` resumes the profiling again. The profiler will not collect any profiling information between both method calls. Please refer to [Pause\/Resume of Profiler Context](..\/software\/profiler.html#temporarilydisablingprofiler) to learn more about the profiling API.\\n### furiosa-litmus [\\uf0c1](#furiosa-litmus \\\"Permalink to this heading\\\")  `furiosa-litmus` is a command-line tool that checks the compatibility of models with the NPU and Furiosa SDK. Since 0.10.0, `furiosa-litmus` has a new feature to collect logs, profiling information, and an environment information for error reporting. This feature is enabled if `--dump\\n<OUTPUT_PREFIX>` option is specified. The collected data is saved into a zip file named `<OUTPUT_PREFIX>-<unix_epoch>.zip` .\\n``` $ furiosa-litmus <MODEL_PATH> --dump <OUTPUT_PREFIX>\\n```\\nThe collected information does not include the model itself but does contain only metadata of the model, memory usage, and environmental information (e.g., Python version, SDK, compiler version, and dependency library versions). You can directly unzip the zip file to check the contents. When reporting bugs, attaching this file will be very helpful for error dianosis and analysis.\\n### New Benchmark Tool \\u2018furiosa-bench\\u2019 [\\uf0c1](#new-benchmark-tool-furiosa-bench \\\"Permalink to this heading\\\")\\nThe new benchmark tool, `furiosa-bench` , has been added since 0.10.0. `furiosa-bench` command offers various options to run a diverse workloads with certain runtime settings. Users can choose either latency-oriented or throughput-oriented workload, and can specify the number of devices, how long time to run, and runtime settings. `furiosa-bench` accepts both ONNX and Tflite models as well as an ENF file compiled by the furiosa-compiler. More details about the command can be found at [furiosa-bench (Benchmark Tool)](..\/software\/cli.html#furiosabench) .\\nAn example of a throughput benchmark\\n``` $ furiosa-bench .\/model.onnx --workload throughput -n 10000 --devices \\\"warboy(1)*2\\\" --workers 8 --batch 8\\n```\\nAn example of a latency benchmark\\n``` $ furiosa-bench .\/model.onnx --workload latency -n 10000 --devices \\\"warboy(2)*1\\\"\\n```  `furiosa-bench` can be installed through apt package manager as follows:\\n``` $ apt install furiosa-bench\\n```\\n### furiosa-toolkit [\\uf0c1](#furiosa-toolkit \\\"Permalink to this heading\\\")\\nfuriosa-toolkit is a collection of command line tools that provide NPU management and NPU device monitoring. Since 0.10.0, `furiosa-toolkit` includes the following improvements:\\n**Improvements of furiosactl**\\nBefore 0.10.0, the sub-commands like `list` , `info` print out a tabular text. Since 0.10.0, `furiosactl` newly provides `--format` option, allowing to print out the result in a structured format like `json` or `yaml` . It will be useful when a users implements a shell pipeline or a script to process the output of `furiosactl` .\\n``` $ furiosactl info --format json [{\\\"dev_name\\\":\\\"npu7\\\",\\\"product_name\\\":\\\"warboy\\\",\\\"device_uuid\\\":\\\"<device_uuid>\\\",\\\"device_sn\\\":\\\"<device_sn>\\\",\\\"firmware\\\":\\\"1.6.0, 7a3b908\\\",\\\"temperature\\\":\\\"47\\u00b0C\\\",\\\"power\\\":\\\"0.99 W\\\",\\\"pci_bdf\\\":\\\"0000:d6:00.0\\\",\\\"pci_dev\\\":\\\"492:0\\\"}]\\n$ furiosactl info --format yaml - dev_name: npu7   product_name: warboy   device_uuid: <device_uuid>   device_sn: <device_sn>   firmware: 1.6.0, 7a3b908   temperature: 47\\u00b0C   power: 0.98 W   pci_bdf: 0000:d6:00.0   pci_dev: 492:0\\n```\\nAlso, the subcommand `info` results in two more metrics:\\n* NPU Clock Frequency * Entire power consumption of card\\n**Improvements of furiosa-npu-metrics-exporter**  `furiosa-npu-metrics-exporter` is a HTTP server to export NPU metrics and status in [OpenMetrics](https:\/\/github.com\/OpenObservability\/OpenMetrics\/blob\/main\/specification\/OpenMetrics.md) format. The metrics that `furiosa-npu-metrics-exporter` exports can be collected by Prometheus and other OpenMetrics compatible collectors.\\nSince 0.10.0, `furiosa-npu-metrics-exporter` includes NPU clock frequency and NPU utilization as metrics. NPU utilziation is still an experimental feature, and it is disabled by default. To enable this feature, you need to specify `--enable-npu-utilization` option as follows:\\n``` furiosa-npu-metrics-exporter --enable-npu-utilization\\n```\\nAdditionally, `furiosa-npu-metrics-exporter` is now available as an APT package in addition to the docker image. You can install it as follows:\\n``` apt install furiosa-toolkit\\n```\\n[Previous](..\/api\/python\/furiosa.serving.processors.html \\\"furiosa.serving.processors package\\\") [Next](0.9.0.html \\\"Release Notes - 0.9.0\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/","title":"","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"180d17f3-caa8-46a6-87f5-74a1c03de3a5\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/\", \"title\": \"\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* FuriosaAI NPU & SDK 0.10.1 Documents * [View page source](_sources\/index.rst.txt)\\n---\\nFuriosaAI NPU & SDK 0.10.1 Documents [\\uf0c1](#furiosaai-npu-sdk-release-documents \\\"Permalink to this heading\\\") ==========================================================================================================\\nThis document explains FuriosaAI NPU and its SDKs.\\nNote\\nFuriosaAI software components include kernel driver, firmware, runtime, C SDK, Python SDK, and command lines tools. Currently, we offer them for only users who register *Early Access Program (EAP)* and agree to *End User Licence Agreement (EULA)* .\\nPlease contact [contact @\\nfuriosa .\\nai](mailto:contact%40furiosa.ai) to learn how to start the EAP.\\nFuriosaAI NPU [\\uf0c1](#furiosaai-npu \\\"Permalink to this heading\\\") -------------------------------------------------------------\\n* [Introduction to FuriosaAI Warboy](npu\/warboy.html)   : HW specification, performance, and supported operators\\nFuriosaAI Software [\\uf0c1](#furiosaai-software \\\"Permalink to this heading\\\") -----------------------------------------------------------------------\\n* [FuriosaAI SW Stack Introduction](software\/intro.html) * [Driver, Firmware, and Runtime Installation](software\/installation.html) * [Python SDK installation and user guide](software\/python-sdk.html) * [C SDK installation and user guide](software\/c-sdk.html) * [Command Line Tools](software\/cli.html) * [Compiler](software\/compiler.html) * [Model Quantization](software\/quantization.html) * [FuriosaAI Model Zoo](https:\/\/furiosa-ai.github.io\/furiosa-models\/latest\/) * [Kubernetes Support](software\/kubernetes_support.html) * [Configuring Warboy Pass-through for Virtual Machine](software\/vm_support.html)\\n### FuriosaAI SDK Tutorial and Examples [\\uf0c1](#furiosaai-sdk-tutorial-and-examples \\\"Permalink to this heading\\\")\\n* [Tutorial: How to use Furiosa SDK from Start to Finish](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/HowToUseFuriosaSDKFromStartToFinish.ipynb) * [Tutorial: Basic Inference API](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/GettingStartedWithPythonSDK.ipynb) * [Tutorial: Advanced Inference API](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/AdvancedTopicsInInferenceAPIs.ipynb) * [Example: Comparing Accuracy with CPU-based Inference](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/InferenceAccuracyCheck.ipynb) * [Example: Image Classification Inference](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/Image_Classification.ipynb) * [Example: SSD Object Detection Inference](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/SSD_Object_Detection.ipynb) * [Other Python SDK Examples](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/main\/examples\/inferences)\\n### Serving, Model Deployment, MLOps [\\uf0c1](#serving-model-deployment-mlops \\\"Permalink to this heading\\\")\\n* [Model Server (Serving Framework)](software\/serving.html) * [Kubernetes Support](software\/kubernetes_support.html)\\nReferences [\\uf0c1](#references \\\"Permalink to this heading\\\") -------------------------------------------------------\\n* [C Language SDK Reference](https:\/\/furiosa-ai.github.io\/docs\/v0.9.0\/en\/api\/c\/index.html) * [Python SDK Reference](api\/python\/modules.html)\\nOther Links [\\uf0c1](#other-links \\\"Permalink to this heading\\\") ---------------------------------------------------------\\n* [FuriosaAI Home](https:\/\/furiosa.ai) * [FuriosaAI Customer Support Center](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals\/) * [Bug Report](customer-support\/bugs.html#bugreport)\\n[Next](npu\/warboy.html \\\"FuriosaAI Warboy\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/customer-support\/bugs.html","title":"bugs","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"f6a8c152-6f4c-4ac0-98ae-fbe3093522c6\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/customer-support\/bugs.html\", \"title\": \"bugs\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Bug Report * [View page source](..\/_sources\/customer-support\/bugs.rst.txt)\\n---\\nBug Report [\\uf0c1](#bug-report \\\"Permalink to this heading\\\") =======================================================\\nIf you encounter an unresolvable issue, you can file a bug report at [FuriosaAI customer service center](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals) .\\nThe following information should be included in a bug report.\\n1. How to reproduce the bug 2. Log or screenshot of the bug 3. SDK version information 4. Compilation log, if model compilation failed\\nBy default, when an error happens furiosa-sdk outputs the following message. If you see the following message, file a report to the Bug Report\\nsection of [FuriosaAI customer service center](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals) with:\\n1. The information given below the    `Information        Dump`    , and 2. The compilation log file (this would be    `\/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log`    in the following example) outputted in the message.\\n``` Saving the compilation log into \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log Using furiosa-compiler 0.5.0 (rev: 407c0c51f-modified built at 2021-11-18 22:32:34) 2021-11-22T06:30:28.392114Z  INFO Npu (npu0pe0) is being initialized 2021-11-22T06:30:28.397757Z  INFO NuxInner create with pes: [PeId(0)] [1\/6] \\ud83d\\udd0d   Compiling from onnx to dfg 2021-11-22T06:30:28.423026Z  INFO [Profiler] Received a termination signal. 2021-11-22T06:30:28.423371Z ERROR fail to compile the model: the static shape of tensor 'input' contains an unsupported dimension value: Some(DimParam(\\\"batch_size\\\")) ================================================================================ Information Dump ================================================================================ - Python version: 3.8.10 (default, Sep 28 2021, 16:10:42)  [GCC 9.3.0] - furiosa-libnux path: libnux.so.0.5.0 - furiosa-libnux version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34) - furiosa-compiler version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34) - furiosa-sdk-runtime version: Furiosa SDK Runtime  (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\\nPlease check the compiler log at \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log. If you have a problem, please report the log file to https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals with the information dumped above. ================================================================================\\n```\\nIf you do not see a message as shown above, refer to the instructions below to collect the necessary information yourself to file a bug report at [FuriosaAI customer service center](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals) .\\nYou can find the Python runtime version information as shown.\\n``` $ python --version Python 3.8.6\\n```\\nYou can find the SDK version information as shown.\\n``` $ python -c \\\"from furiosa import runtime;print(runtime.__full_version__)\\\" loaded native library \/usr\/lib64\/libnux.so (0.5.0 407c0c51f) Furiosa SDK Runtime 0.5.0 (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\\n```\\n[Previous](..\/releases\/0.5.0.html \\\"Release Notes - 0.5.0\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
{"source":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.7.0.html","title":"0","parent_doc_id":"","child_doc_ids":"[]","_node_content":"{\"id_\": \"ca538c67-ce79-412f-bd1b-c0f73dd818e7\", \"embedding\": null, \"metadata\": {\"source\": \"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.7.0.html\", \"title\": \"0\", \"parent_doc_id\": \"\", \"child_doc_ids\": \"[]\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {}, \"text\": \"* Release Notes - 0.7.0 * [View page source](..\/_sources\/releases\/0.7.0.rst.txt)\\n---\\nRelease Notes - 0.7.0 [\\uf0c1](#release-notes-0-7-0 \\\"Permalink to this heading\\\") ===========================================================================\\nFuriosa SDK 0.7.0 is a major release, and includes approximately 1,400 commits towards performance enhancement, added functions, and bug fixes.\\ncomponent version information\\n[\\uf0c1](#id1 \\\"Permalink to this table\\\")\\n| Package name | Version | | --- | --- | | NPU Driver | 1.3.0 | | HAL (Hardware Abstraction Layer) | 0.8.0 | | Furiosa Compiler | 0.7.0 | | Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..) | 0.7.0 | | NPU Device Plugin | 0.10.0 | | NPU Feature Discovery | 0.1.0 | | NPU Management CLI (furiosactl) | 0.9.1 |\\nHow to upgrade [\\uf0c1](#how-to-upgrade \\\"Permalink to this heading\\\") ---------------------------------------------------------------\\nThe upgrade is a simple process if you are using an APT repository. Detailed information on APT repository setting and installation can be found in [Driver, Firmware, and Runtime Installation](..\/software\/installation.html#requiredpackages) .\\n> ``` > apt-get update && \\\\ > apt-get install -y furiosa-driver-pdma furiosa-libnux >  > pip install --upgrade furiosa-sdk >  > ```\\nKey changes [\\uf0c1](#key-changes \\\"Permalink to this heading\\\") ---------------------------------------------------------\\n### Compiler - More NPU acceleration supports [\\uf0c1](#compiler-more-npu-acceleration-supports \\\"Permalink to this heading\\\")\\nThrough improvements in the compiler, more operators can be accelerated in various use cases. Accelerated operators with its condition adopted by 0.7.0 release are following. You can find the entire list of accelerated operators at [List of Supported Operators for Warboy Acceleration](..\/npu\/warboy.html#supportedoperators) .\\n> * Added Linear and Nearest mode support for the Resize operator > * Added DCR mode support for the SpaceToDepth operator > * Added DCR mode support for the DepthToSpace operator > * Added CHW axis support for the Pad operator > * Added C axis support for the Slice operator > * Added acceleration support for operators Tanh, Exp, and Log > * Added C axis support for the Concat operator > * Increased Dilation support to up to x12 > * Added acceleration support for operators Gelu, Erf, and Elu\\n### Compiler - Compiler Cache [\\uf0c1](#compiler-compiler-cache \\\"Permalink to this heading\\\")\\nCompiler cache stores the compiled binary into a cache directory, and reuses the cache when the same model is compiled. Also, you can also use Redis as the compiler cache storage. More detailed instructions can be found in [Compiler Cache](..\/software\/compiler.html#compilercache) .\\n### Compiler - Compiler Hint [\\uf0c1](#compiler-compiler-hint \\\"Permalink to this heading\\\")\\nWhen running a function that includes compilation, such as `session.create()` , a path that includes the compilation log is printed as follows.\\n> ``` > Saving the compilation log into \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log >  > ```\\nSince 0.7.0, compilation logs contain compilation hints more helpful to understand the compilation process and give some optimization opportunities.\\nThe `cat\\n<log\\nfile>\\n|\\ngrep\\nHint` command will show you only hint from the log. The hint informs why certain operators are not accelerated as shown in the below example.\\n> ``` > cat \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log | grep Hint > 2022-05-24T02:44:11.399402Z  WARN nux::session: Hint [19]: 'LogSoftmax' cannot be accelerated yet > 2022-05-24T02:44:11.399407Z  WARN nux::session: Hint [12]: groups should be bigger than 1 > 2022-05-24T02:44:11.399408Z  WARN nux::session: Hint [17]: Softmax with large batch (36 > 2) cannot be accelerated by Warboy >  > ```\\n### Performance Profiling Tools [\\uf0c1](#performance-profiling-tools \\\"Permalink to this heading\\\")\\nThe profiler had been an experimental and closed-beta feature. The release 0.7.0 includes the performance profiler by default. It allow users to view the time taken in each step in the model inference process. You can activate the profiler through a shell environment variable or a profiler context in your Python code.\\nMore details can be found in [Performance Profiling](..\/software\/profiler.html#profiling) .\\n### Improvements\/Bug fixes of Python SDK [\\uf0c1](#improvements-bug-fixes-of-python-sdk \\\"Permalink to this heading\\\")\\n* Since 0.7.0,   `session.create()`   and   `session.create_async()`   can take the batch size. * Fixed a bug that compiler options passed to   `session.create()`   and   `session.create_async()`   wasn\\u2019t effective.\\nBelow is an example that uses batch size and compiler option.\\n> ``` > config = { >   \\\"without_quantize\\\": { >       \\\"parameters\\\": [{\\\"input_min\\\": 0.0, \\\"input_max\\\": 255.0, \\\"permute\\\": [0, 2, 3, 1]}] >   } > } >  > with session.create(\\\"model.onnx\\\", batch_size=2, compile_config=config) as sess: >   outputs = sess.run(inputs) >  > ```\\n### Improvements\/Bug fixes of Quantization tools [\\uf0c1](#improvements-bug-fixes-of-quantization-tools \\\"Permalink to this heading\\\")\\n* You can now infer published tensor shapes even if   axes      property is not designated in ONNX Squeeze operators below version OpSet 12 * Added support not just for Conv receiving tensors with NxCxHxW shapes as input, but also for Conv receiving tensors with NxCxD shapes * Modified \\u201cConv - BatchNormalization\\u201d subgraph to be fused to Conv even when Conv does not receive bias as input * Modified to always quantize Sub, Concat, and Pow operators in QDQ format, regardless of whether operands have initial values, so that the model can be processed in a consistent way in the post-quantization process * Modified to prevent ONNX Runtime related warnings in the quantization process and the result model * Reinforced the inspection condition to not miss any cases where tensor shape information cannot be inferred * Modified to allow random calibration not only for models that receive float32 data as inputs, but also for models that receive other decimal or integer types as inputs * Modified to find and terminate in a stable manner when given an already quantized model * Modified to adjust scale of weight appropriately if Conv data input or scale of weight is too small, such that scale of bias becomes 0 * Reinforced conditions for \\u201cGather - MatMul\\u201d subgraph to be fused into Gather * Dependent libraries updated to latest version\\n### Device Plugin - Configuration file support [\\uf0c1](#device-plugin-configuration-file-support \\\"Permalink to this heading\\\")\\nA function to set the execution option of the NPU Device Plugin used in Kubernetes with a file has been added. As before, option items can be entered as command-line arguments, or options can be specified by selecting a configuration file. Detailed instructions can be found in [Kubernetes Support](..\/software\/kubernetes_support.html#kubernetesintegration) .\\n[Previous](0.8.0.html \\\"Release Notes - 0.8.0\\\") [Next](0.6.0.html \\\"Release Notes - 0.6.0\\\")\\n---\\n\\u00a9 Copyright 2023 FuriosaAI, Inc..\\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .\", \"mimetype\": \"text\/plain\", \"start_char_idx\": null, \"end_char_idx\": null, \"text_template\": \"{metadata_str}\\n\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\n\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"None","doc_id":"None","ref_doc_id":"None"}
