[{"page_id":"cf227685-cc4e-420e-b21a-e7da166093e5","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/vm_support.html","original_content":"* Configuring Warboy Pass-through for Virtual Machine * [View page source](..\/_sources\/software\/vm_support.rst.txt)\n---\nConfiguring Warboy Pass-through for Virtual Machine [](#configuring-warboy-pass-through-for-virtual-machine \"Permalink to this heading\") =========================================================================================================================================\nThis section describes how to enable Warboy pass-through for a virtual machine. The example of this section is based on a specific VM tool `QEMU-KVM` , but it also works in other VM tools. The environment used in the example is as follows:\n* Host OS: CentOS 8 * Guest OS: Ubuntu 20.04 * Virtual Machine: QEMU-KVM\nPrerequisites [](#prerequisites \"Permalink to this heading\") -------------------------------------------------------------\n* IOMMU and VT-x should be enabled in BIOS. * `qemu-kvm`   ,   `libvirt`   ,   `virt-install`   should be installed in a host machine.\nSetup Instruction [](#setup-instruction \"Permalink to this heading\") ---------------------------------------------------------------------\n### 1. Enabling IOMMU in BIOS and Linux OS [](#enabling-iommu-in-bios-and-linux-os \"Permalink to this heading\")\nFirst of all, you need to enable IOMMU in BIOS and Linux OS. The following command shows if IOMMU is enabled.\n``` dmesg | grep -e DMAR -e IOMMU\n```\nYou will be able to see some messages related to DMAR or IOMMU if IOMMU is enabled. If you cannot find any messages related to DMAR or IOMMU, you need to enable IOMMU in BIOS, Linux OS or both.\nThe ways to enable IOMMU in BIOS may depend on server or motherboard models. Please refer to the manufacturer’s manual.\nYou check if IOMMU is enabled in Linux OS as follows:\n``` grep GRUB_CMDLINE_LINUX \/etc\/default\/grub | grep iommu\n```\nIf you cannot find any messages related to IOMMU, please add `intel_iommu=on` for Intel CPU or `amd_iommu=on` for AMD CPU to `GRUB_CMDLINE_LINUX` in `\/etc\/default\/grub` and apply the changes by rebooting the machine.\nIf you use a legacy BIOS boot mode or UEFI boot mode, the way to enable IOMMU in Linux OS can be different.\n* Legacy BIOS boot mode:   `grub2-mkconfig      -o      \/boot\/grub2\/grub.cfg` * UEFI boot mode,   `grub2-mkconfig      -o      \/boot\/efi\/EFI\/{linux_distrib}\/grub.cfg`   .\nPlease replace `{linux_distrib}` with a Linux OS name, such as `centos` , `redhat` , or `ubuntu` .\n### 2. Loading `vfio-pci` module [](#loading-vfio-pci-module \"Permalink to this heading\")\nPlease make sure if the kernel module `vfio-pci` is loaded.\n> ``` > [root@localhost ~]# lsmod | grep vfio_pci > vfio_pci               61440  0 > vfio_virqfd            16384  1 vfio_pci > vfio_iommu_type1       36864  0 > vfio                   36864  2 vfio_iommu_type1,vfio_pci > irqbypass              16384  2 vfio_pci,kvm >  > ```\nIf `vfio_pci` is not loaded yet, please run `modprobe\nvfio-pci` to load the module. In some OS environments, you don’t have to load `vfio-pci` . To make sure, please refer to the OS manual.\n### 3. Checking if a virtual machine tool is ready [](#checking-if-a-virtual-machine-tool-is-ready \"Permalink to this heading\")\nPlease check if a virtual machine tool is ready to run as follows. If `virt-host-validate` is not found, please install the prerequisite packages described in [Prerequisites](#vmsupport-prerequisites)\n> ``` > [root@localhost ~]# virt-host-validate >   QEMU: Checking for hardware virtualization                                 : PASS >  >   QEMU: Checking for device assignment IOMMU support                         : PASS >   QEMU: Checking if IOMMU is enabled by kernel                               : PASS >  > ```\nIf check items are PASSED, the virtual machine tool is ready.\n### 4. Finding Warboy’s PCIe device name [](#finding-warboy-s-pcie-device-name \"Permalink to this heading\")\nPCI BDF (Bus, Device, Function) is a unique identifier assigned to every PCIe device connected to a machine. Please find a PCI BDF of a Warboy card that you want to pass through to a virtual machine.\n> ``` > [root@localhost ~]# lspci -nD | grep 1ed2 > 0000:01:00.0 1200: 1ed2:0000 (rev 01) >  > ```  `1ed2` is the PCI vendor ID of FursioaAI Inc. `01:00.0` is the PCI BDF of a Warboy card in the above example. Your PCI BDF will be different according to motherboard model, server model, and PCIe slot.\nAlternatively, you can use `lspci\n-DD` command to show a PCI BDF list with vendor names and find a Warboy card from the list. The vendor names depend on PCIe ID database in OS. If the database is outdated in OS, the command will show `Device\n1ed2:0000` instead of `FuriosaAI,\nInc.\nWarboy` .\nYou can update outdated PCIe ID database by running `update-pciids` in shell.\nOnce you find the PCIe BDB name, you can find a PCIe device name accepted by a virtual machine tool as follows:\n> ``` > [root@localhost ~]# virsh nodedev-list | grep pci > ... >  > pci_0000_01_00_0 >  > ```\nA PCIe device name consists of `pci_` and a PCI BDF concatnated with `_` . In the above example, `pci_0000_01_00_0` is the PCIe device name of a Warboy card.\n### 5. Creating a virtual machine [](#creating-a-virtual-machine \"Permalink to this heading\")\nIf you reach here, you are ready to create a virtual machine with a Warboy passthrough device. Please create a virtual machine as follows.\n> ``` > virt-install --name ubuntu-vm \\ >   --os-variant ubuntu20.04 \\ >   --vcpus 2 \\ >   --memory 4096 \\ >   --location \/var\/lib\/libvirt\/images\/ubuntu-20.04.5-live-server-amd64.iso,kernel=casper\/vmlinuz,initrd=casper\/initrd \\ >   --network bridge=br0,model=virtio \\ >   --disk size=50 \\ >   --graphics none \\ >   --host-device=pci_0000_01_00_0 >  > ```\nPlease note the option `--host-device` with the PCIe device name that we found in the previous step. Also, you can add more options to the command for your use cases.\nIn the above example, we set the guest OS image. So, it will start the guest OS installation step once the virtual machine starts. Ubuntu 20.04 or above is recommended for a guest OS. You can find recommended OS distributions for FuriosaAI SDK at [Minimum requirements for SDK installation](installation.html#minimumrequirements) .\n### 6. Checking the availability of a Warboy device in VM [](#checking-the-availability-of-a-warboy-device-in-vm \"Permalink to this heading\")\nPlease make sure if the Warboy device is available on the virtual machine. `lspci` will shows all PCIe devices available on the virtual machine as follows.\n> ``` > furiosa@ubuntu-vm:~$ lspci > ... > 05:00.0 Processing accelerators: Device 1ed2:0000 (rev 01) > ... >  > furiosa@ubuntu-vm:~$ sudo update-pciids >  > furiosa@ubuntu-vm:~$ lspci | grep Furiosa > 05:00.0 Processing accelerators: FuriosaAI, Inc. Warboy (rev 01) >  > ```\n### 7. SDK installation [](#sdk-installation \"Permalink to this heading\")\nOnce you confirm that Warboy is available in a virtual machine, please install [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) to install SDK and move forward next steps.\n[Previous](kubernetes_support.html \"Kubernetes Support\") [Next](tutorials.html \"Tutorial and Code Examples\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 가상 머신을 위한 Warboy 패스스루 구성 * [페이지 소스 보기](..\/_sources\/software\/vm_support.rst.txt)\n---\n가상 머신을 위한 Warboy 패스스루 구성 [](#configuring-warboy-pass-through-for-virtual-machine \"Permalink to this heading\") =========================================================================================================================================\n이 섹션에서는 가상 머신에 Warboy 패스스루를 활성화하는 방법을 설명합니다. 이 섹션의 예제는 특정 VM 도구 `QEMU-KVM`을 기반으로 하지만, 다른 VM 도구에서도 작동합니다. 예제에서 사용된 환경은 다음과 같습니다:\n* 호스트 OS: CentOS 8 * 게스트 OS: Ubuntu 20.04 * 가상 머신: QEMU-KVM\n사전 준비 사항 [](#prerequisites \"Permalink to this heading\") -------------------------------------------------------------\n* IOMMU와 VT-x가 BIOS에서 활성화되어 있어야 합니다. * 호스트 머신에 `qemu-kvm`, `libvirt`, `virt-install`이 설치되어 있어야 합니다.\n설정 지침 [](#setup-instruction \"Permalink to this heading\") ---------------------------------------------------------------------\n### 1. BIOS 및 Linux OS에서 IOMMU 활성화 [](#enabling-iommu-in-bios-and-linux-os \"Permalink to this heading\")\n먼저, BIOS와 Linux OS에서 IOMMU를 활성화해야 합니다. 다음 명령어는 IOMMU가 활성화되었는지 보여줍니다.\n``` \ndmesg | grep -e DMAR -e IOMMU\n```\nIOMMU가 활성화된 경우 DMAR 또는 IOMMU와 관련된 메시지를 볼 수 있습니다. DMAR 또는 IOMMU와 관련된 메시지를 찾을 수 없다면, BIOS, Linux OS 또는 둘 다에서 IOMMU를 활성화해야 합니다. BIOS에서 IOMMU를 활성화하는 방법은 서버나 메인보드 모델에 따라 다를 수 있습니다. 제조업체의 매뉴얼을 참조하십시오. Linux OS에서 IOMMU가 활성화되었는지 확인하려면 다음과 같이 합니다:\n``` \ngrep GRUB_CMDLINE_LINUX \/etc\/default\/grub | grep iommu\n```\nIOMMU와 관련된 메시지를 찾을 수 없다면, Intel CPU의 경우 `intel_iommu=on`, AMD CPU의 경우 `amd_iommu=on`을 `\/etc\/default\/grub`의 `GRUB_CMDLINE_LINUX`에 추가하고, 머신을 재부팅하여 변경 사항을 적용하십시오. 레거시 BIOS 부팅 모드 또는 UEFI 부팅 모드를 사용하는 경우, Linux OS에서 IOMMU를 활성화하는 방법이 다를 수 있습니다.\n* 레거시 BIOS 부팅 모드: `grub2-mkconfig -o \/boot\/grub2\/grub.cfg` * UEFI 부팅 모드: `grub2-mkconfig -o \/boot\/efi\/EFI\/{linux_distrib}\/grub.cfg`.\n`{linux_distrib}`를 `centos`, `redhat`, `ubuntu` 등과 같은 Linux OS 이름으로 교체하십시오.\n### 2. `vfio-pci` 모듈 로드 [](#loading-vfio-pci-module \"Permalink to this heading\")\n커널 모듈 `vfio-pci`가 로드되었는지 확인하십시오.\n> ``` \n> [root@localhost ~]# lsmod | grep vfio_pci \n> vfio_pci               61440  0 \n> vfio_virqfd            16384  1 vfio_pci \n> vfio_iommu_type1       36864  0 \n> vfio                   36864  2 vfio_iommu_type1,vfio_pci \n> irqbypass              16384  2 vfio_pci,kvm \n> ```\n`vfio_pci`가 아직 로드되지 않았다면, `modprobe vfio-pci`를 실행하여 모듈을 로드하십시오. 일부 OS 환경에서는 `vfio-pci`를 로드할 필요가 없습니다. 확실히 하려면 OS 매뉴얼을 참조하십시오.\n### 3. 가상 머신 도구가 준비되었는지 확인 [](#checking-if-a-virtual-machine-tool-is-ready \"Permalink to this heading\")\n다음과 같이 가상 머신 도구가 실행 준비가 되었는지 확인하십시오. `virt-host-validate`가 발견되지 않으면, [사전 준비 사항](#vmsupport-prerequisites)에 설명된 필수 패키지를 설치하십시오.\n> ``` \n> [root@localhost ~]# virt-host-validate \n>   QEMU: Checking for hardware virtualization                                 : PASS \n>   \n>   QEMU: Checking for device assignment IOMMU support                         : PASS \n>   QEMU: Checking if IOMMU is enabled by kernel                               : PASS \n> ```\n검사 항목이 PASSED이면, 가상 머신 도구가 준비된 것입니다.\n### 4. Warboy의 PCIe 장치 이름 찾기 [](#finding-warboy-s-pcie-device-name \"Permalink to this heading\")\nPCI BDF (버스, 장치, 기능)는 머신에 연결된 모든 PCIe 장치에 할당된 고유 식별자입니다. 가상 머신에 패스스루하려는 Warboy 카드의 PCI BDF를 찾으십시오.\n> ``` \n> [root@localhost ~]# lspci -nD | grep 1ed2 \n> 0000:01:00.0 1200: 1ed2:0000 (rev 01) \n> ```\n`1ed2`는 FursioaAI Inc.의 PCI 공급업체 ID입니다. 위 예제에서 `01:00.0`은 Warboy 카드의 PCI BDF입니다. PCI BDF는 메인보드 모델, 서버 모델 및 PCIe 슬롯에 따라 다를 수 있습니다. 또는 `lspci -DD` 명령어를 사용하여 공급업체 이름과 함께 PCI BDF 목록을 표시하고 목록에서 Warboy 카드를 찾을 수 있습니다. 공급업체 이름은 OS의 PCIe ID 데이터베이스에 따라 다릅니다. OS의 데이터베이스가 오래된 경우, 명령어는 `FuriosaAI, Inc. Warboy` 대신 `Device 1ed2:0000`을 표시할 것입니다. 오래된 PCIe ID 데이터베이스는 셸에서 `update-pciids`를 실행하여 업데이트할 수 있습니다. PCIe BDB 이름을 찾으면, 가상 머신 도구에서 허용하는 PCIe 장치 이름을 다음과 같이 찾을 수 있습니다:\n> ``` \n> [root@localhost ~]# virsh nodedev-list | grep pci \n> ... \n> \n> pci_0000_01_00_0 \n> ```\nPCIe 장치 이름은 `pci_`와 PCI BDF가 `_`로 연결된 형태로 구성됩니다. 위 예제에서 `pci_0000_01_00_0`은 Warboy 카드의 PCIe 장치 이름입니다.\n### 5. 가상 머신 생성 [](#creating-a-virtual-machine \"Permalink to this heading\")\n여기까지 도달했다면, Warboy 패스스루 장치가 있는 가상 머신을 생성할 준비가 된 것입니다. 다음과 같이 가상 머신을 생성하십시오.\n> ``` \n> virt-install --name ubuntu-vm \\ \n>   --os-variant ubuntu20.04 \\ \n>   --vcpus 2 \\ \n>   --memory 4096 \\ \n>   --location \/var\/lib\/libvirt\/images\/ubuntu-20.04.5-live-server-amd64.iso,kernel=casper\/vmlinuz,initrd=casper\/initrd \\ \n>   --network bridge=br0,model=virtio \\ \n>   --disk size=50 \\ \n>   --graphics none \\ \n>   --host-device=pci_0000_01_00_0 \n> ```\n이전 단계에서 찾은 PCIe 장치 이름과 함께 `--host-device` 옵션을 주의하십시오. 또한, 사용 사례에 맞게 명령어에 더 많은 옵션을 추가할 수 있습니다. 위 예제에서는 게스트 OS 이미지를 설정했습니다. 따라서 가상 머신이 시작되면 게스트 OS 설치 단계가 시작됩니다. 게스트 OS로는 Ubuntu 20.04 이상을 권장합니다. FuriosaAI SDK에 대한 권장 OS 배포판은 [SDK 설치를 위한 최소 요구 사항](installation.html#minimumrequirements)에서 확인할 수 있습니다.\n### 6. VM에서 Warboy 장치의 가용성 확인 [](#checking-the-availability-of-a-warboy-device-in-vm \"Permalink to this heading\")\n가상 머신에서 Warboy 장치가 사용 가능한지 확인하십시오. `lspci`는 가상 머신에서 사용 가능한 모든 PCIe 장치를 다음과 같이 보여줍니다.\n> ``` \n> furiosa@ubuntu-vm:~$ lspci \n> ... \n> 05:00.0 Processing accelerators: Device 1ed2:0000 (rev 01) \n> ... \n> \n> furiosa@ubuntu-vm:~$ sudo update-pciids \n> \n> furiosa@ubuntu-vm:~$ lspci | grep Furiosa \n> 05:00.0 Processing accelerators: FuriosaAI, Inc. Warboy (rev 01) \n> ```\n### 7. SDK 설치 [](#sdk-installation \"Permalink to this heading\")\n가상 머신에서 Warboy가 사용 가능하다는 것을 확인한 후, [드라이버, 펌웨어 및 런타임 설치](installation.html#requiredpackages)를 통해 SDK를 설치하고 다음 단계로 진행하십시오.\n[이전](kubernetes_support.html \"Kubernetes Support\") [다음](tutorials.html \"Tutorial and Code Examples\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용합니다.\n```"},{"page_id":"333851a4-2ea4-4903-87a2-0d50943faf1f","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/performance.html","original_content":"* Performance Optimization * [View page source](..\/_sources\/software\/performance.rst.txt)\n---\nPerformance Optimization [](#performance-optimization \"Permalink to this heading\") ===================================================================================\nTo ensure efficient inference serving in production, it’s essential to focus on throughput and latency as key metrics. Furiosa SDK offers two optimization methods for both throughput and latency:\n* **Model Optimization**   : are ways to optimize models during the phases of model development,   quantization, and compilation. Some optimization techniques may modify the models, leading to   more efficient compiled programs. * **Runtime Optimization**   : are ways to optimize the runtime execution of compiled programs.   They are about how to optimize inference codes through Runtime library depending   on the characteristics of models workloads for higher throughput.\nIn this section, we will discuss the performance metrics and how to optimize them in both above ways.\nPerformance Metrics: Latency and Throughput [](#performance-metrics-latency-and-throughput \"Permalink to this heading\") ------------------------------------------------------------------------------------------------------------------------\n*Latency* is one of the major performance evaluation criteria for model inference. it’s a measure of how long a single inference takes from when the input data is passed to the model until the output value is received. With low latency, users can experience high responsiveness.\nAnother performance evaluation criterion is throughput. Throughput means the number of inferences that can be processed within a unit of time. Throughput implies that how many requests a system handle simultaneously.\nA single end-to-end inference consists of three kinds of operations: NPU execution, CPU computation and IO operation between host and NPU device. Three kinds of operations run independently without blocking one another. So, multiple inferences can run simultaneously while different operations run. When we continue to run multiple requests simultaneously, the longer operation among NPU, CPU, and IO operations is likely to determine the inference time. This is because the shorter operations will be hidden by other longer operations. This is a key characteristic to understand how to optimize the performance of a model. You can find more details at [Concurrency Optimization](#concurrencyoptimization) .\nNPU utilization is not a performance metrics, but it’s one of the key metrics to indicate how much the model utilizes a NPU device for inferences. NPU utilization can be defined as the proportion of time the NPU is used during inference. With NPU utilziation, we can evaluate how well the model is well-optimized for NPU acceleration. Sometimes, it may also imply the room for further optimization opportunities. Please refer to [Toolkit](cli.html#toolkit) for how to measure NPU utilization.\n### Performance Profiling [](#performance-profiling \"Permalink to this heading\")\nTo analyze the performance of a workload, we need to measure performance metrics as well as we need to have a closer look at the times of NPU executions, CPU computations, and I\/O operations.\nFor them, there are two useful tools in Furiosa SDK.\n* [furiosa-bench (Benchmark Tool)](cli.html#furiosabench)   is a tool to measure the performance metrics such as latencies and   throughput (i.e., QPS - queries per second). * [Performance Profiling](profiler.html#profiling)   provides a way to measure the durations of NPU executions and other operations.  `furiosa-bench` also provides `--trace-output` option to generate a trace file. This is another easy way to measure the durations of NPU executions and other operations from a running workload.\nModel Optimization [](#model-optimization \"Permalink to this heading\") -----------------------------------------------------------------------\nIn this seciton, we introduce some model optimization techniques to improve the performance of models. They key idea of the model optimization is to identify the bottleneck parts (usually operators) of the model and to reduce the times of the bottleneck parts or remove them.\nFor example, if some operators of a model are not accelerated by NPU, they can be a major bottleneck. If you remove the operators or replace them with other equivalents, the inference latency can be reduced significantly.\n### Optimizing `Quantize` Operator [](#optimizing-quantize-operator \"Permalink to this heading\")\nFuriosaAI’s first-generation NPU, Warboy, supports only int8 type. As the majority of deep learning models are built upon floating point types like fp32 and fp16, to execute these models on Warboy, a quantization step is necessary to convert the fp32 weights to int8 model weights. In addition, the quantization step adds `quantize` , `dequantize` operators to the input and output parts of the model respectively. `quantize` and `dequantize` operators convert fp32 input values to int8 values and vice versa. Those operators are executed on the CPU and are time-consuming.\nInputs of many CNN-based models are images. In particular, an image is represented as RGB channels. In other words, a single image is composed of three images for each channel of RGB, where each image is represented with 8-bit integer values, ranging from 0 to 255.\nTo feed an image to a model, we need to convert the `int8` values of each RGB channel to `fp32` values, and `quantize` operator in the model converts `fp32` values to `int8` values. It’s unnecessary if we can feed RGB images in `int8` to a model directly.\nTo support this optimization, `furiosa-quantizer` provides the `ModelEditor` API. `ModelEditor` takes the model optimized by `optimize_model()` .\n``` model = onnx.load_model(\"yolox_l.onnx\") model = optimize_model(model)\neditor = ModelEditor(model)\n```  `convert_input_type()` method of `ModelEditor` takes a tensor name and a data type as arguments. It modifies the data type of the input tensor in the model to be the given arguments. The target type can be either `INT8` or `UINT8` . You can find the tensor name through `get_pure_input_names` method.\n``` input_tensor_name = get_pure_input_names(model)[0]\n# Convert this input tensor to uint8 editor.convert_input_type(input_tensor_name, TensorType.UINT8)\n```\nAs you can see in the above example, `convert_input_type` changes the data type of the input tensor to be `uint8` . The reason why we use `uint8` instead of `int8` is that the pixel values are represented as positive values.\nBefore this model modification, `quantize` operator converts `float32` values to `int8` values. After this model modification, the quantize operator converts `uint8` values to `int8` values. This conversion from `uint8` to `int8` is much faster than the conversion from `float32` to `int8` . The followings are the the benchmark results of before and after the model modification. Also, the figure shows how the `quantize` operator is changed.\nQuantization in YOLOX\\_L\n[](#id2 \"Permalink to this table\")\n| Input type | `Quantize` execution time | | --- | --- | | float32 | 60.639 ms | | uint8 | 0.277 ms |\nquantize without `ModelEditor`  [](#id3 \"Permalink to this image\")\nquantize with `convert_input_type`  [](#id4 \"Permalink to this image\")\nWarning\nThis optmization may affect the accurarcy of the model. Since it depends on models and applications, it is recommended to validate the accuracy of the model.\nThe following is a real example code to use `ModelEditor` API with `convert_input_type()` .\n``` #!\/usr\/bin\/env python\nimport time import numpy as np import onnx import torch import torchvision from torchvision import transforms import tqdm\nfrom furiosa.optimizer import optimize_model from furiosa.quantizer import get_pure_input_names, quantize, Calibrator, CalibrationMethod, ModelEditor, TensorType from furiosa.runtime import session from furiosa.runtime.profiler import profile\ntorch_model = torchvision.models.resnet50(weights='DEFAULT') torch_model = torch_model.eval()\ndummy_input = (torch.randn(1, 3, 224, 224),)\ntorch.onnx.export(     torch_model,  # PyTorch model to export     dummy_input,  # model input     \"resnet50.onnx\",  # where to save the exported ONNX model     opset_version=13,  # the ONNX OpSet version to export the model to     do_constant_folding=True,  # whether to execute constant folding for optimization     input_names=[\"input\"],  # the ONNX model's input names     output_names=[\"output\"],  # the ONNX model's output names )\nonnx_model = onnx.load_model(\"resnet50.onnx\") onnx_model = optimize_model(onnx_model)\ncalibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM) calibrator.collect_data([[torch.randn(1, 3, 224, 224).numpy()]]) ranges = calibrator.compute_range()\neditor = ModelEditor(onnx_model) input_tensor_name = get_pure_input_names(onnx_model)[0]\n# Convert the input type to uint8 editor.convert_input_type(input_tensor_name, TensorType.UINT8)\ngraph = quantize(onnx_model, ranges)\nwith open(\"trace.json\", \"w\") as trace:     with profile(file=trace) as profiler:         with session.create(graph) as session:             image = torch.randint(256, (1, 3, 224, 224), dtype=torch.uint8)             with profiler.record(\"pre\"):                 image = image.numpy()             with profiler.record(\"inf\"):                 outputs = session.run(image)             with profiler.record(\"post\"):                 prediction = np.argmax(outputs[0].numpy(), axis=1)\n```\n### Optimizing `Dequantize` Operator [](#optimizing-dequantize-operator \"Permalink to this heading\")\nSimilar to the above `Quantize` operator optimization, `Dequantize` operator also can be optimized in the similar way.\nIf the model output tensor is `fp32` , the output of `int8` values must be converted to f32 values. `Dequantize` operator converts int8 values to fp32 values, and it’s executed on CPU. If the model output is an RGB image or something else which can be represented as `int8` or `uint8` values, we can skip converting `int8` or `uint8` to `fp32` . It will reduce the inference latency significantly.\nWe can enable this optimization by using `convert_output_type()` method of `ModelEditor` . `convert_output_type()` method can modifies a model output by a given tensor name and a target data type. The target type can be either `INT8` or `UINT8` .\nquantize with `convert_output_type`  [](#id5 \"Permalink to this image\")\nquantize with `convert_input_type` and `convert_output_type`  [](#id6 \"Permalink to this image\")\nNote\nFuriosa Compiler may automatically apply this optimization to the model even if this optmization is not explicitly applied. In that case, the optimization by Furiosa Compiler may result in lower latency than the one manually applied by `ModelEditor` . It is recommended to do experiments to find the best option.\nWarning\nThis optmization may affect the accurarcy of the model. Since it depends on models and applications, it is recommended to validate the accuracy of the model.\nThe following is an real example code to use `convert_output_type` option.\n``` #!\/usr\/bin\/env python\nimport time import numpy as np import onnx import torch import torchvision from torchvision import transforms import tqdm\nfrom furiosa.optimizer import optimize_model from furiosa.quantizer import get_output_names, quantize, Calibrator, CalibrationMethod, ModelEditor, TensorType from furiosa.runtime import session from furiosa.runtime.profiler import profile\ntorch_model = torchvision.models.resnet50(weights='DEFAULT') torch_model = torch_model.eval()\ndummy_input = (torch.randn(1, 3, 224, 224),)\ntorch.onnx.export(     torch_model,  # PyTorch model to export     dummy_input,  # model input     \"resnet50.onnx\",  # where to save the exported ONNX model     opset_version=13,  # the ONNX OpSet version to export the model to     do_constant_folding=True,  # whether to execute constant folding for optimization     input_names=[\"input\"],  # the ONNX model's input names     output_names=[\"output\"],  # the ONNX model's output names )\nonnx_model = onnx.load_model(\"resnet50.onnx\") onnx_model = optimize_model(onnx_model)\ncalibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM) calibrator.collect_data([[torch.randn(1, 3, 224, 224).numpy()]]) ranges = calibrator.compute_range()\neditor = ModelEditor(onnx_model) output_tensor_name = get_output_names(onnx_model)[0]\n# output 텐서의 자료형을 int8로 변환 editor.convert_output_type(output_tensor_name, TensorType.INT8)\ngraph = quantize(onnx_model, ranges)\nwith open(\"trace.json\", \"w\") as trace:     with profile(file=trace) as profiler:         with session.create(graph) as session:             image = torch.randint(256, (1, 3, 224, 224), dtype=torch.uint8)             with profiler.record(\"pre\"):                 image = image.numpy()             with profiler.record(\"inf\"):                 outputs = session.run(image)             with profiler.record(\"post\"):                 prediction = np.argmax(outputs[0].numpy(), axis=1)\n```\n### Lower\/Unlower Acceleration [](#lower-unlower-acceleration \"Permalink to this heading\")\nWarboy internally uses its inherent memory layout to accelerate the computation by leveraging the NPU architecture. For the memory layout, `Lower` operator reshapes the input tensor to the NPU’s memory layout and `Unlower` operator reshapes the output tensor from the NPU’s memory layout to the original shape. For them, Furiosa Compiler automatically adds `Lower` and `Unlower` operators to the model.\nIn many cases, `Lower` and `Unlower` are executed on CPU, causing some overhead of the inference latency. However, if the last axis of input or output tensor shape is `width` and the size of the last axis is a multiple of 32, `Lower` and `Unlower` operators can be accleerated on NPU. Then, the inference latency can be reduced significantly.\nTherefore, if you are able to specify the shape of the input and output tensors, it’s more optimal to use `NxCxHxW` and specify the width as a multiple of 32. Also, this optimization can be applied independently to the input and output tensors respectively.\n### Removal of Pad\/Slice [](#removal-of-pad-slice \"Permalink to this heading\")\nAs described above, the `Lower` \/ `Unlower` operations can be accelerated if the last axis of the tensor for either operator is width\nand the size of the last axis is a multiple of 32.\nIf the last tensor axis of `Lower` is width\nbut not a multiple of 32, Furiosa Compiler may automatically add `Pad` operator before `Lower` operator to adjust the size of the last axis to a multiple of 32. In the similar way, Furiosa Compiler may automatically add `Slice` operator after `Unlower` operator to slice data contents from the tensor with the last axis of a multiple of 32 to the original tensor shape.\nThis optimization gains some performance benefits by accelerating `Lower` \/ `Unlower` operations. However, `Pad` and `Slice` requires CPU computation. There’s futher optimization opportunity to remove even `Pad` and `Slice` operators too. If you can accept the constraints of the input and output tensor shapes, it is strongly recommended using the shape of the tensors `NxCxHxW` and a multiple of 32 of the width.\n### Change the Order of Input Tensor Axes at Compiler Time [](#change-the-order-of-input-tensor-axes-at-compiler-time \"Permalink to this heading\")\nAs we discussed above, there are more optimization opportunities if the last axis of the input tensor is `width` . However, changing the order of axes requires to modify the models. It may require some effort to modify the original models in some cases.\nSo, Furiosa Compiler provides a way to change the order of the input tensor axes at compile time. You can specify `permute_input` option in compiler config to specify the new order of the input tensor axes as follows:\n* `compiler_config      =      {      \"permute_input\":      [[0,      3,      1,      2]]      }`      > + The parameter of   >   `permute_input`   >   is the same as   >   [torch.permute](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.permute.html)   >   .   > + For example, the above example code will change   >   `NxHxWxC`   >   to   >   `NxCxHxW`   >   .\nThe following is a real example code to use `permute_input` option.\n``` #!\/usr\/bin\/env python\nimport time\nimport numpy as np import onnx import torch import tqdm\nfrom furiosa.optimizer import optimize_model from furiosa.quantizer import quantize, Calibrator, CalibrationMethod from furiosa.runtime import session from furiosa.runtime.profiler import profile\nonnx_model = onnx.load_model(\"model_nhwc.onnx\") onnx_model = optimize_model(onnx_model)\ncalibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM) calibrator.collect_data([[torch.randn(1, 512, 512, 3).numpy()]]) ranges = calibrator.compute_range()\ngraph = quantize(onnx_model, ranges)\ncompiler_config = { \"permute_input\": [[0, 3, 1, 2]] }\nwith open(\"trace.json\", \"w\") as trace:     with profile(file=trace) as profiler:         with session.create(graph, compiler_config=compiler_config) as session:             image = torch.randint(256, (1, 3, 512, 512), dtype=torch.uint8)             with profiler.record(\"pre\"):                 image = image.numpy()             with profiler.record(\"inf\"):                 outputs = session.run(image)             with profiler.record(\"post\"):                 prediction = outputs[0].numpy()\n```\nThis is another case to use `permute_input` option. In some cases, it’s necessary to change the order of the input tensor axes from `NxCxHxW` to `NxHxWxC` . Python OpenCV is a popular computer vision library. `cv2.imread()` of OpenCV returns a 3D NumPy array with `HxWxC` order. If the axes of the input tensors of a model are `NxCxHxW` , it requires to transpose the tensor. The transpose is a time-consuming operation running in CPU. In this case, we can remove the transpose operation if we change the order of the input tensor axes of the model to the same as OpenCV’s output; e.g., `NxHxWxC` . It will reduce the inference latency significantly.\n### Optimization of Large Input and Output Tensors [](#optimization-of-large-input-and-output-tensors \"Permalink to this heading\")\nSome models have large images and as inputs and outputs. For example, Denoising and super resolution models basically take large images as inputs and outputs. Depending on your implementation, those models may be slow in Furiosa SDK and Warboy. Furiosa Compiler optimizes the models with various techniques while preserving the semantics of the original models. Basically, Furiosa Compiler handles large tensors as defined by the model. However, if the size of tensors is too large, it may exceed SRAM memory of Warboy, causing more I\/O operations between DRAM and SRAM. It may result in poor performance.\nWe can optimize this case by splitting a large tensor into a number of smaller tensors and then merging the results. Generally, we can apply this optimization to denosing and super resolution models because the small parts of images can be independently processed and merged to get the final results. The small parts of images are called patches, and the size of patches is called patch size.\nTo understand the optimization mechanism, we need to understand how the Furiosa Compiler works. Furiosa Compiler tries to hide IO times between DRAM and SRAM by overlapping them with NPU executions. In other words, NPU can execute operators while I\/O operations are working. If we split a large tensor into a number of smaller tensors, the number of I\/O operations can be hidden by NPU executions.\nOnce we decide to use this optimization, the next step is to determine the patch size. Here, one good metric to determine the patch size is the ratio of the time spent on NPU executions. The smaller the patch size, the more time is spent on NPU computation. In contrast, the larger the patch size, the more time is spent on I\/O operations.\nAlso, this optimization can be combined with using multiple NPU devices. The multiple patches can run across multiple NPU devices in parallel.\n### More Batch, More NPU Utilization [](#more-batch-more-npu-utilization \"Permalink to this heading\")\nFor some models with small weights or few layers, the NPU utilization may be low. In this case, we can increase the batch size to make the NPU utilization higher. With this optimization, the inference may still have the same latency, but its throughput can be increased significantly.\nA batch size can be specified when compiling a model with `--batch-size` option as follows:\n`furiosa-compiler\n--batch-size\n32\n--target-npu\nwarboy\nmnist.dfg\n-o\nmnist.enf`\nA batch size also can be specified when creating a session with `batch_size` option. You can learn more about the `batch_size` option from [Runner API](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#furiosa.runtime.furiosa.runtime.Runtime.create_runner) .\n### Single PE vs Fusion PE [](#single-pe-vs-fusion-pe \"Permalink to this heading\")\nA single Warboy chip consists of two processing elements (PEs). Each PE of Warboy has its own control unit, and the two PEs can work independently. In this mode, each PE works with spatially-partitioned memory and processing units. In contrast, two PEs can also be fused as a single PE. In this fused mode, two PEs work as a single PE with an unified memory and processing units.\nThese two modes allow applications to have more flexibility to optimize the performance. For example, if a model has large weights, we can use a 2PE-fused mode to load the weights for a lower latency. If a model fits in a single PE, we can use two single PEs separately to run the two model instances for higher throughput.\nIf a workload is latency-oriented, using a 2PE-fused mode is generally recommended. If a workload is throughput-oriented, using two single PEs is generally recommended. It still depends on models and workloads. You need to find the optimal NPU configuration through experiments.\nThe followings are example commands to compile a model with a single PE or a fused-PE respectively.\n* Single PE:   `furiosa-compiler      --target-npu      warboy      resnet50.dfg      -o      resnet50.enf` * Fusion PE:   `furiosa-compiler      --target-npu      warboy-2pe      resnet50.dfg      -o      resnet50_2pe.enf`\nThis NPU configuration can also be specified when creating a [Runtime](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#furiosa.runtime.Runtime) with `device` option which are specified by [Device Configuration](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#device-specification)\nRuntime Optimization [](#runtime-optimization \"Permalink to this heading\") ---------------------------------------------------------------------------\nSo far, we have discussed the model optimization techniques to reduce the inference latency. After we apply the model optimization, we can futher optimize the performance in Runtime level.\nAs we mentioned above, an end-to-end inference consists of three operations: NPU execution, CPU computation, and IO operation. Three kinds of operations can run independently without blocking one another. They can be overlapped if we run multiple inferences simultaneously. Leveraging this characteristic is a key idea of the runtime optimization.\n### More inference concurrency (the number of workers) [](#more-inference-concurrency-the-number-of-workers \"Permalink to this heading\")\nWhen we create a session through [Runner API](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#furiosa.runtime.furiosa.runtime.Runtime.create_runner) ,\nwe can specify the number of workers as an option. A single worker is a unit that can run inferences independently sharing NPUs. This concept is similar to a thread and CPUs.\nIf there is only one worker, multiple inference requests are processed sequentially through a single worker. When one inference is completed, the next inference is processed by the owrker. In this case, the NPU can be idle while the CPU is working, causing low NPU utilization.\nHowever, if there are multiple workers, the workers consume requests from the request queue in Runtime. The multiple inferences can be processed simultaneously. In this case, NPU executions are overlapped with CPU executions, possibly leading to higher NPU utilization.\nEach worker requires more memory resources to maintain context information for its execution. If the number of workers is too large, the memory resources may be exhausted. If the number of workers is too small, the NPU utilization may be low. Finding the optimal number of workers is important to maximize the performance of the model. Usually, we can find the optimal number of workers through experimentation.\n### Sync API vs Async APIs [](#sync-api-vs-async-apis \"Permalink to this heading\")\nThere are two types of runtime APIs: Sync API and Async API. Sync API is a blocking API that waits for the completion of the inference. Async APIs are non-blocking APIs that don’t wait for the completion of the inference. Async APIs allow to request multiple inferences simultaneously and wait for the results asynchronously.  `furiosa.session.create()` a creates a syncronous session. As the below example, `session.run()` is blocked until the result is returned. It generally is enough for batch workloads with large batch sizes, but it’s not sufficient for serving workloads that handle multiple current requests simultaneously.\n``` from furiosa.runtime import session\nwith session.create(model) as sess:     input = ...     outputs = sess.run(input) # Wait for completion     ...\n```\nTo overcome this limitation, Furiosa SDK provides two types of Async APIs: Queue API and Async\/Await API. They allow to request multiple inferences simultaneously and wait for the results asynchronously. They are also useful to hide I\/O and CPU computation by overlapping them with NPU executions.\n#### Queue API [](#queue-api \"Permalink to this heading\")  `create_async()` creates a pair of a submitter and a queue. With both, we can submit inference requests without waiting for completion and wait for the inference results asynchronously.\n``` import numpy as np import random\nfrom furiosa.runtime import session\nsubmitter, queue = session.create_async(\"mnist.onnx\",                                         worker_num=2,                                         # Determine how many asynchronous requests you can submit                                         # without blocking.                                         input_queue_size=100,                                         output_queue_size=100)\nfor i in range(0, 5):     idx = random.randint(0, 59999)     input = np.random.rand(1, 1, 28, 28).astype(np.float32)     submitter.submit(input, context=idx) # non blocking call\nfor i in range(0, 5):     context, outputs = queue.recv(100) # 100 ms for timeout. If None, queue.recv() will be blocking.     print(outputs[0].numpy())\nif queue:     queue.close() if submitter:     submitter.close()\n```\n#### Using Async\/Await syntax [](#using-async-await-syntax \"Permalink to this heading\")\nIn the the example below, `NPUModel` of furiosa-server provide an easier way to implement a serving application using async\/await API.\n``` import asyncio import numpy as np\nfrom furiosa.server.model import NPUModel, NPUModelConfig\nclass SimpleApplication:     def __init__(self):         self.model = NPUModel(             NPUModelConfig(                 name=\"MNIST\",                 model=\"mnist.onnx\",             )         )\n    async def load(self):         await self.model.load()\n    async def process(self, image):         input = self.preprocess(image)         tensor = await self.model.predict(input)         output = self.postprocess(tensor)         return output\n    def preprocess(self, image):         # do preprocess         return image\n    def postprocess(self, tensor):         # do postprocess         return tensor\nAPP = SimpleApplication()\nasync def startup():     await APP.load()\nasync def run(image):     result = await APP.process(image)     return result\nif __name__ == \"__main__\":     asyncio.run(startup())\n    image = np.random.rand(1, 1, 28, 28).astype(np.float32)     asyncio.run(run(image))\n```\n[Previous](quantization.html \"Model Quantization\") [Next](profiler.html \"Performance Profiling\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 성능 최적화 * [페이지 소스 보기](..\/_sources\/software\/performance.rst.txt)\n---\n성능 최적화 [](#performance-optimization \"이 제목으로의 영구 링크\") ===================================================================================\n프로덕션 환경에서 효율적인 추론 서비스를 보장하기 위해서는 처리량과 지연 시간을 주요 지표로 삼는 것이 중요합니다. Furiosa SDK는 처리량과 지연 시간을 최적화하기 위한 두 가지 방법을 제공합니다:\n* **모델 최적화**: 모델 개발, 양자화 및 컴파일 단계에서 모델을 최적화하는 방법입니다. 일부 최적화 기술은 모델을 수정하여 더 효율적인 컴파일 프로그램을 생성할 수 있습니다.\n* **런타임 최적화**: 컴파일된 프로그램의 런타임 실행을 최적화하는 방법입니다. 이는 모델 워크로드의 특성에 따라 런타임 라이브러리를 통해 추론 코드를 최적화하는 방법입니다.\n\n이 섹션에서는 성능 지표와 위의 두 가지 방법으로 이를 최적화하는 방법에 대해 논의합니다.\n\n성능 지표: 지연 시간과 처리량 [](#performance-metrics-latency-and-throughput \"이 제목으로의 영구 링크\") ------------------------------------------------------------------------------------------------------------------------\n*지연 시간*은 모델 추론의 주요 성능 평가 기준 중 하나입니다. 이는 입력 데이터가 모델에 전달된 시점부터 출력 값을 받을 때까지의 시간을 측정한 것입니다. 지연 시간이 낮으면 사용자는 높은 응답성을 경험할 수 있습니다.\n\n또 다른 성능 평가 기준은 처리량입니다. 처리량은 단위 시간 내에 처리할 수 있는 추론의 수를 의미합니다. 처리량은 시스템이 동시에 처리할 수 있는 요청의 수를 의미합니다.\n\n단일 엔드 투 엔드 추론은 NPU 실행, CPU 계산 및 호스트와 NPU 장치 간의 IO 작업의 세 가지 종류의 작업으로 구성됩니다. 세 가지 종류의 작업은 서로 차단하지 않고 독립적으로 실행됩니다. 따라서 여러 추론이 동시에 실행될 수 있습니다. 여러 요청을 동시에 실행할 때, NPU, CPU 및 IO 작업 중 더 긴 작업이 추론 시간을 결정할 가능성이 큽니다. 이는 더 짧은 작업이 다른 더 긴 작업에 의해 숨겨지기 때문입니다. 이는 모델의 성능을 최적화하는 방법을 이해하는 데 중요한 특성입니다. 자세한 내용은 [동시성 최적화](#concurrencyoptimization)에서 확인할 수 있습니다.\n\nNPU 활용도는 성능 지표는 아니지만, 모델이 추론을 위해 NPU 장치를 얼마나 활용하는지를 나타내는 주요 지표 중 하나입니다. NPU 활용도는 추론 중 NPU가 사용된 시간의 비율로 정의할 수 있습니다. NPU 활용도를 통해 모델이 NPU 가속에 얼마나 잘 최적화되었는지를 평가할 수 있습니다. 때로는 추가 최적화 기회의 여지를 나타낼 수도 있습니다. NPU 활용도를 측정하는 방법은 [툴킷](cli.html#toolkit)을 참조하십시오.\n\n### 성능 프로파일링 [](#performance-profiling \"이 제목으로의 영구 링크\")\n워크로드의 성능을 분석하려면 성능 지표를 측정하고 NPU 실행, CPU 계산 및 I\/O 작업의 시간을 자세히 살펴봐야 합니다.\n\n이를 위해 Furiosa SDK에는 두 가지 유용한 도구가 있습니다.\n* [furiosa-bench (벤치마크 도구)](cli.html#furiosabench)는 지연 시간 및 처리량(QPS - 초당 쿼리 수)과 같은 성능 지표를 측정하는 도구입니다.\n* [성능 프로파일링](profiler.html#profiling)은 NPU 실행 및 기타 작업의 지속 시간을 측정하는 방법을 제공합니다. `furiosa-bench`는 또한 `--trace-output` 옵션을 제공하여 실행 중인 워크로드에서 NPU 실행 및 기타 작업의 지속 시간을 측정하는 또 다른 쉬운 방법을 제공합니다.\n\n모델 최적화 [](#model-optimization \"이 제목으로의 영구 링크\") -----------------------------------------------------------------------\n이 섹션에서는 모델의 성능을 향상시키기 위한 몇 가지 모델 최적화 기술을 소개합니다. 모델 최적화의 핵심 아이디어는 모델의 병목 부분(보통 연산자)을 식별하고 병목 부분의 시간을 줄이거나 제거하는 것입니다.\n\n예를 들어, 모델의 일부 연산자가 NPU에 의해 가속되지 않는 경우, 이는 주요 병목이 될 수 있습니다. 연산자를 제거하거나 다른 동등한 것으로 대체하면 추론 지연 시간을 크게 줄일 수 있습니다.\n\n### `Quantize` 연산자 최적화 [](#optimizing-quantize-operator \"이 제목으로의 영구 링크\")\nFuriosaAI의 1세대 NPU인 Warboy는 int8 타입만 지원합니다. 대부분의 딥러닝 모델은 fp32 및 fp16과 같은 부동 소수점 타입을 기반으로 구축되므로, 이러한 모델을 Warboy에서 실행하려면 fp32 가중치를 int8 모델 가중치로 변환하는 양자화 단계가 필요합니다. 추가로, 양자화 단계는 모델의 입력 및 출력 부분에 `quantize`, `dequantize` 연산자를 추가합니다. `quantize` 및 `dequantize` 연산자는 fp32 입력 값을 int8 값으로 변환하고 그 반대로 변환합니다. 이러한 연산자는 CPU에서 실행되며 시간이 많이 소요됩니다.\n\n많은 CNN 기반 모델의 입력은 이미지입니다. 특히, 이미지는 RGB 채널로 표현됩니다. 즉, 단일 이미지는 RGB 각 채널에 대해 세 개의 이미지로 구성되며, 각 이미지는 0에서 255 사이의 8비트 정수 값으로 표현됩니다.\n\n모델에 이미지를 입력하려면 각 RGB 채널의 `int8` 값을 `fp32` 값으로 변환해야 하며, 모델의 `quantize` 연산자는 `fp32` 값을 `int8` 값으로 변환합니다. 만약 `int8`로 RGB 이미지를 직접 모델에 입력할 수 있다면 이는 불필요합니다.\n\n이 최적화를 지원하기 위해 `furiosa-quantizer`는 `ModelEditor` API를 제공합니다. `ModelEditor`는 `optimize_model()`로 최적화된 모델을 받습니다.\n```python\nmodel = onnx.load_model(\"yolox_l.onnx\")\nmodel = optimize_model(model)\neditor = ModelEditor(model)\n```\n`ModelEditor`의 `convert_input_type()` 메서드는 텐서 이름과 데이터 타입을 인수로 받습니다. 이는 모델의 입력 텐서의 데이터 타입을 주어진 인수로 수정합니다. 대상 타입은 `INT8` 또는 `UINT8`일 수 있습니다. 텐서 이름은 `get_pure_input_names` 메서드를 통해 찾을 수 있습니다.\n```python\ninput_tensor_name = get_pure_input_names(model)[0]\n# 이 입력 텐서를 uint8로 변환\neditor.convert_input_type(input_tensor_name, TensorType.UINT8)\n```\n위의 예에서 볼 수 있듯이, `convert_input_type`은 입력 텐서의 데이터 타입을 `uint8`로 변경합니다. `int8` 대신 `uint8`을 사용하는 이유는 픽셀 값이 양수로 표현되기 때문입니다.\n\n이 모델 수정 전에는 `quantize` 연산자가 `float32` 값을 `int8` 값으로 변환합니다. 이 모델 수정 후에는 `quantize` 연산자가 `uint8` 값을 `int8` 값으로 변환합니다. `uint8`에서 `int8`로의 변환은 `float32`에서 `int8`로의 변환보다 훨씬 빠릅니다. 다음은 모델 수정 전후의 벤치마크 결과입니다. 또한, 그림은 `quantize` 연산자가 어떻게 변경되었는지를 보여줍니다.\n\nYOLOX\\_L에서의 양자화\n[](#id2 \"이 표로의 영구 링크\")\n| 입력 타입 | `Quantize` 실행 시간 |\n| --- | --- |\n| float32 | 60.639 ms |\n| uint8 | 0.277 ms |\n\n`ModelEditor` 없이 양자화 [](#id3 \"이 이미지로의 영구 링크\")\n`convert_input_type`으로 양자화 [](#id4 \"이 이미지로의 영구 링크\")\n\n경고\n이 최적화는 모델의 정확도에 영향을 미칠 수 있습니다. 모델과 응용 프로그램에 따라 다르므로 모델의 정확성을 검증하는 것이 권장됩니다.\n\n다음은 `convert_input_type()`과 함께 `ModelEditor` API를 사용하는 실제 예제 코드입니다.\n```python\n#!\/usr\/bin\/env python\nimport time\nimport numpy as np\nimport onnx\nimport torch\nimport torchvision\nfrom torchvision import transforms\nimport tqdm\nfrom furiosa.optimizer import optimize_model\nfrom furiosa.quantizer import get_pure_input_names, quantize, Calibrator, CalibrationMethod, ModelEditor, TensorType\nfrom furiosa.runtime import session\nfrom furiosa.runtime.profiler import profile\n\ntorch_model = torchvision.models.resnet50(weights='DEFAULT')\ntorch_model = torch_model.eval()\n\ndummy_input = (torch.randn(1, 3, 224, 224),)\ntorch.onnx.export(\n    torch_model,  # 내보낼 PyTorch 모델\n    dummy_input,  # 모델 입력\n    \"resnet50.onnx\",  # 내보낼 ONNX 모델을 저장할 위치\n    opset_version=13,  # 모델을 내보낼 ONNX OpSet 버전\n    do_constant_folding=True,  # 최적화를 위한 상수 폴딩 실행 여부\n    input_names=[\"input\"],  # ONNX 모델의 입력 이름\n    output_names=[\"output\"],  # ONNX 모델의 출력 이름\n)\n\nonnx_model = onnx.load_model(\"resnet50.onnx\")\nonnx_model = optimize_model(onnx_model)\n\ncalibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM)\ncalibrator.collect_data([[torch.randn(1, 3, 224, 224).numpy()]])\nranges = calibrator.compute_range()\n\neditor = ModelEditor(onnx_model)\ninput_tensor_name = get_pure_input_names(onnx_model)[0]\n# 입력 타입을 uint8로 변환\neditor.convert_input_type(input_tensor_name, TensorType.UINT8)\n\ngraph = quantize(onnx_model, ranges)\n\nwith open(\"trace.json\", \"w\") as trace:\n    with profile(file=trace) as profiler:\n        with session.create(graph) as session:\n            image = torch.randint(256, (1, 3, 224, 224), dtype=torch.uint8)\n            with profiler.record(\"pre\"):\n                image = image.numpy()\n            with profiler.record(\"inf\"):\n                outputs = session.run(image)\n            with profiler.record(\"post\"):\n                prediction = np.argmax(outputs[0].numpy(), axis=1)\n```\n\n### `Dequantize` 연산자 최적화 [](#optimizing-dequantize-operator \"이 제목으로의 영구 링크\")\n위의 `Quantize` 연산자 최적화와 유사하게, `Dequantize` 연산자도 유사한 방식으로 최적화할 수 있습니다.\n\n모델 출력 텐서가 `fp32`인 경우, `int8` 값의 출력은 `fp32` 값으로 변환되어야 합니다. `Dequantize` 연산자는 `int8` 값을 `fp32` 값으로 변환하며, 이는 CPU에서 실행됩니다. 모델 출력이 RGB 이미지 또는 `int8` 또는 `uint8` 값으로 표현될 수 있는 다른 것이라면, `int8` 또는 `uint8`을 `fp32`로 변환하는 과정을 생략할 수 있습니다. 이는 추론 지연 시간을 크게 줄일 것입니다.\n\n이 최적화를 활성화하려면 `ModelEditor`의 `convert_output_type()` 메서드를 사용해야 합니다. `convert_output_type()` 메서드는 주어진 텐서 이름과 대상 데이터 타입으로 모델 출력을 수정할 수 있습니다. 대상 타입은 `INT8` 또는 `UINT8`일 수 있습니다.\n\n`convert_output_type`으로 양자화 [](#id5 \"이 이미지로의 영구 링크\")\n`convert_input_type` 및 `convert_output_type`으로 양자화 [](#id6 \"이 이미지로의 영구 링크\")\n\n참고\nFuriosa Compiler는 이 최적화를 명시적으로 적용하지 않더라도 모델에 자동으로 적용할 수 있습니다. 이 경우, Furiosa Compiler에 의한 최적화는 `ModelEditor`에 의해 수동으로 적용된 것보다 더 낮은 지연 시간을 초래할 수 있습니다. 최적의 옵션을 찾기 위해 실험을 수행하는 것이 권장됩니다.\n\n경고\n이 최적화는 모델의 정확도에 영향을 미칠 수 있습니다. 모델과 응용 프로그램에 따라 다르므로 모델의 정확성을 검증하는 것이 권장됩니다.\n\n다음은 `convert_output_type` 옵션을 사용하는 실제 예제 코드입니다.\n```python\n#!\/usr\/bin\/env python\nimport time\nimport numpy as np\nimport onnx\nimport torch\nimport torchvision\nfrom torchvision import transforms\nimport tqdm\nfrom furiosa.optimizer import optimize_model\nfrom furiosa.quantizer import get_output_names, quantize, Calibrator, CalibrationMethod, ModelEditor, TensorType\nfrom furiosa.runtime import session\nfrom furiosa.runtime.profiler import profile\n\ntorch_model = torchvision.models.resnet50(weights='DEFAULT')\ntorch_model = torch_model.eval()\n\ndummy_input = (torch.randn(1, 3, 224, 224),)\ntorch.onnx.export(\n    torch_model,  # 내보낼 PyTorch 모델\n    dummy_input,  # 모델 입력\n    \"resnet50.onnx\",  # 내보낼 ONNX 모델을 저장할 위치\n    opset_version=13,  # 모델을 내보낼 ONNX OpSet 버전\n    do_constant_folding=True,  # 최적화를 위한 상수 폴딩 실행 여부\n    input_names=[\"input\"],  # ONNX 모델의 입력 이름\n    output_names=[\"output\"],  # ONNX 모델의 출력 이름\n)\n\nonnx_model = onnx.load_model(\"resnet50.onnx\")\nonnx_model = optimize_model(onnx_model)\n\ncalibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM)\ncalibrator.collect_data([[torch.randn(1, 3, 224, 224).numpy()]])\nranges = calibrator.compute_range()\n\neditor = ModelEditor(onnx_model)\noutput_tensor_name = get_output_names(onnx_model)[0]\n# 출력 텐서의 자료형을 int8로 변환\neditor.convert_output_type(output_tensor_name, TensorType.INT8)\n\ngraph = quantize(onnx_model, ranges)\n\nwith open(\"trace.json\", \"w\") as trace:\n    with profile(file=trace) as profiler:\n        with session.create(graph) as session:\n            image = torch.randint(256, (1, 3, 224, 224), dtype=torch.uint8)\n            with profiler.record(\"pre\"):\n                image = image.numpy()\n            with profiler.record(\"inf\"):\n                outputs = session.run(image)\n            with profiler.record(\"post\"):\n                prediction = np.argmax(outputs[0].numpy(), axis=1)\n```\n\n### Lower\/Unlower 가속 [](#lower-unlower-acceleration \"이 제목으로의 영구 링크\")\nWarboy는 NPU 아키텍처를 활용하여 계산을 가속화하기 위해 고유의 메모리 레이아웃을 내부적으로 사용합니다. 메모리 레이아웃을 위해 `Lower` 연산자는 입력 텐서를 NPU의 메모리 레이아웃으로 재구성하고, `Unlower` 연산자는 출력 텐서를 NPU의 메모리 레이아웃에서 원래 모양으로 재구성합니다. 이를 위해 Furiosa Compiler는 모델에 `Lower` 및 `Unlower` 연산자를 자동으로 추가합니다.\n\n많은 경우, `Lower` 및 `Unlower`는 CPU에서 실행되어 추론 지연 시간에 일부 오버헤드를 발생시킵니다. 그러나 입력 또는 출력 텐서 모양의 마지막 축이 `width`이고 마지막 축의 크기가 32의 배수인 경우, `Lower` 및 `Unlower` 연산자는 NPU에서 가속될 수 있습니다. 그러면 추론 지연 시간이 크게 줄어들 수 있습니다.\n\n따라서 입력 및 출력 텐서의 모양을 지정할 수 있다면, `NxCxHxW`를 사용하고 너비를 32의 배수로 지정하는 것이 더 최적입니다. 또한, 이 최적화는 입력 및 출력 텐서 각각에 독립적으로 적용될 수 있습니다.\n\n### Pad\/Slice 제거 [](#removal-of-pad-slice \"이 제목으로의 영구 링크\")\n위에서 설명한 바와 같이, `Lower` \/ `Unlower` 연산은 텐서의 마지막 축이 `width`이고 마지막 축의 크기가 32의 배수인 경우 가속될 수 있습니다.\n\n`Lower`의 마지막 텐서 축이 `width`이지만 32의 배수가 아닌 경우, Furiosa Compiler는 `Lower` 연산자 앞에 `Pad` 연산자를 자동으로 추가하여 마지막 축의 크기를 32의 배수로 조정할 수 있습니다. 유사하게, Furiosa Compiler는 `Unlower` 연산자 뒤에 `Slice` 연산자를 자동으로 추가하여 32의 배수인 마지막 축을 가진 텐서에서 원래 텐서 모양으로 데이터 내용을 슬라이스할 수 있습니다.\n\n이 최적화는 `Lower` \/ `Unlower` 연산을 가속화하여 성능 이점을 얻습니다. 그러나 `Pad` 및 `Slice`는 CPU 계산을 요구합니다. `Pad` 및 `Slice` 연산자도 제거할 수 있는 추가 최적화 기회가 있습니다. 입력 및 출력 텐서 모양의 제약을 수용할 수 있다면, `NxCxHxW` 모양과 너비의 32 배수를 사용하는 것이 강력히 권장됩니다.\n\n### 컴파일러 시간에 입력 텐서 축 순서 변경 [](#change-the-order-of-input-tensor-axes-at-compiler-time \"이 제목으로의 영구 링크\")\n위에서 논의한 바와 같이, 입력 텐서의 마지막 축이 `width`인 경우 더 많은 최적화 기회가 있습니다. 그러나 축의 순서를 변경하려면 모델을 수정해야 합니다. 경우에 따라 원래 모델을 수정하는 데 약간의 노력이 필요할 수 있습니다.\n\n따라서 Furiosa Compiler는 컴파일 시간에 입력 텐서 축의 순서를 변경하는 방법을 제공합니다. 컴파일러 구성에서 `permute_input` 옵션을 지정하여 입력 텐서 축의 새로운 순서를 다음과 같이 지정할 수 있습니다:\n* `compiler_config = { \"permute_input\": [[0, 3, 1, 2]] }`\n  > + `permute_input`의 매개변수는 [torch.permute](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.permute.html)와 동일합니다.\n  > + 예를 들어, 위의 예제 코드는 `NxHxWxC`를 `NxCxHxW`로 변경합니다.\n\n다음은 `permute_input` 옵션을 사용하는 실제 예제 코드입니다.\n```python\n#!\/usr\/bin\/env python\nimport time\nimport numpy as np\nimport onnx\nimport torch\nimport tqdm\nfrom furiosa.optimizer import optimize_model\nfrom furiosa.quantizer import quantize, Calibrator, CalibrationMethod\nfrom furiosa.runtime import session\nfrom furiosa.runtime.profiler import profile\n\nonnx_model = onnx.load_model(\"model_nhwc.onnx\")\nonnx_model = optimize_model(onnx_model)\n\ncalibrator = Calibrator(onnx_model, CalibrationMethod.MIN_MAX_ASYM)\ncalibrator.collect_data([[torch.randn(1, 512, 512, 3).numpy()]])\nranges = calibrator.compute_range()\n\ngraph = quantize(onnx_model, ranges)\ncompiler_config = { \"permute_input\": [[0, 3, 1, 2]] }\n\nwith open(\"trace.json\", \"w\") as trace:\n    with profile(file=trace) as profiler:\n        with session.create(graph, compiler_config=compiler_config) as session:\n            image = torch.randint(256, (1, 3, 512, 512), dtype=torch.uint8)\n            with profiler.record(\"pre\"):\n                image = image.numpy()\n            with profiler.record(\"inf\"):\n                outputs = session.run(image)\n            with profiler.record(\"post\"):\n                prediction = outputs[0].numpy()\n```\n\n이것은 `permute_input` 옵션을 사용하는 또 다른 경우입니다. 경우에 따라 입력 텐서 축의 순서를 `NxCxHxW`에서 `NxHxWxC`로 변경해야 할 필요가 있습니다. Python OpenCV는 인기 있는 컴퓨터 비전 라이브러리입니다. OpenCV의 `cv2.imread()`는 `HxWxC` 순서의 3D NumPy 배열을 반환합니다. 모델의 입력 텐서 축이 `NxCxHxW`인 경우, 텐서를 전치해야 합니다. 전치는 CPU에서 실행되는 시간 소모적인 작업입니다. 이 경우, 모델의 입력 텐서 축의 순서를 OpenCV의 출력과 동일하게 변경하면 전치 작업을 제거할 수 있습니다. 이는 추론 지연 시간을 크게 줄일 것입니다.\n\n### 대형 입력 및 출력 텐서 최적화 [](#optimization-of-large-input-and-output-tensors \"이 제목으로의 영구 링크\")\n일부 모델은 대형 이미지와 같은 입력 및 출력을 가집니다. 예를 들어, 노이즈 제거 및 초해상도 모델은 기본적으로 대형 이미지를 입력 및 출력으로 사용합니다. 구현에 따라 이러한 모델은 Furiosa SDK와 Warboy에서 느릴 수 있습니다. Furiosa Compiler는 원래 모델의 의미를 유지하면서 다양한 기술로 모델을 최적화합니다. 기본적으로 Furiosa Compiler는 모델에 정의된 대형 텐서를 처리합니다. 그러나 텐서의 크기가 너무 크면 Warboy의 SRAM 메모리를 초과하여 DRAM과 SRAM 간의 I\/O 작업이 증가할 수 있습니다. 이는 성능 저하를 초래할 수 있습니다.\n\n이 경우 대형 텐서를 여러 개의 작은 텐서로 분할한 다음 결과를 병합하여 최적화할 수 있습니다. 일반적으로 이 최적화는 노이즈 제거 및 초해상도 모델에 적용할 수 있습니다. 이미지의 작은 부분은 독립적으로 처리되고 병합되어 최종 결과를 얻을 수 있습니다. 이미지의 작은 부분을 패치라고 하며, 패치의 크기를 패치 크기라고 합니다.\n\n최적화 메커니즘을 이해하려면 Furiosa Compiler가 어떻게 작동하는지 이해해야 합니다. Furiosa Compiler는 NPU 실행과 겹치게 하여 DRAM과 SRAM 간의 IO 시간을 숨기려고 합니다. 즉, NPU는 I\/O 작업이 진행 중일 때 연산자를 실행할 수 있습니다. 대형 텐서를 여러 개의 작은 텐서로 분할하면 I\/O 작업의 수가 NPU 실행에 의해 숨겨질 수 있습니다.\n\n이 최적화를 사용하기로 결정한 후 다음 단계는 패치 크기를 결정하는 것입니다. 여기서 패치 크기를 결정하는 좋은 지표는 NPU 실행에 소요된 시간의 비율입니다. 패치 크기가 작을수록 NPU 계산에 더 많은 시간이 소요됩니다. 반대로 패치 크기가 클수록 I\/O 작업에 더 많은 시간이 소요됩니다.\n\n또한, 이 최적화는 여러 NPU 장치를 사용하는 것과 결합할 수 있습니다. 여러 패치는 여러 NPU 장치에서 병렬로 실행될 수 있습니다.\n\n### 더 많은 배치, 더 많은 NPU 활용 [](#more-batch-more-npu-utilization \"이 제목으로의 영구 링크\")\n작은 가중치나 적은 레이어를 가진 일부 모델의 경우 NPU 활용도가 낮을 수 있습니다. 이 경우 배치 크기를 늘려 NPU 활용도를 높일 수 있습니다. 이 최적화를 통해 추론은 여전히 동일한 지연 시간을 가질 수 있지만, 처리량은 크게 증가할 수 있습니다.\n\n배치 크기는 모델을 컴파일할 때 `--batch-size` 옵션을 사용하여 다음과 같이 지정할 수 있습니다:\n```bash\nfuriosa-compiler --batch-size 32 --target-npu warboy mnist.dfg -o mnist.enf\n```\n배치 크기는 세션을 생성할 때 `batch_size` 옵션으로도 지정할 수 있습니다. `batch_size` 옵션에 대한 자세한 내용은 [Runner API](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#furiosa.runtime.furiosa.runtime.Runtime.create_runner)에서 확인할 수 있습니다.\n\n### 단일 PE 대 Fusion PE [](#single-pe-vs-fusion-pe \"이 제목으로의 영구 링크\")\n단일 Warboy 칩은 두 개의 처리 요소(PE)로 구성됩니다. Warboy의 각 PE는 자체 제어 장치를 가지고 있으며, 두 개의 PE는 독립적으로 작동할 수 있습니다. 이 모드에서는 각 PE가 공간적으로 분할된 메모리 및 처리 장치와 함께 작동합니다. 반대로, 두 개의 PE는 단일 PE로 융합될 수도 있습니다. 이 융합 모드에서는 두 개의 PE가 통합된 메모리 및 처리 장치와 함께 단일 PE로 작동합니다.\n\n이 두 가지 모드는 응용 프로그램이 성능을 최적화할 수 있는 더 많은 유연성을 제공합니다. 예를 들어, 모델에 큰 가중치가 있는 경우, 2PE 융합 모드를 사용하여 가중치를 로드하여 지연 시간을 줄일 수 있습니다. 모델이 단일 PE에 맞는 경우, 두 개의 단일 PE를 별도로 사용하여 두 개의 모델 인스턴스를 실행하여 처리량을 높일 수 있습니다.\n\n워크로드가 지연 시간 지향적인 경우, 일반적으로 2PE 융합 모드를 사용하는 것이 권장됩니다. 워크로드가 처리량 지향적인 경우, 일반적으로 두 개의 단일 PE를 사용하는 것이 권장됩니다. 여전히 모델과 워크로드에 따라 다릅니다. 실험을 통해 최적의 NPU 구성을 찾아야 합니다.\n\n다음은 각각 단일 PE 또는 융합 PE로 모델을 컴파일하는 예제 명령입니다.\n* 단일 PE: `furiosa-compiler --target-npu warboy resnet50.dfg -o resnet50.enf`\n* Fusion PE: `furiosa-compiler --target-npu warboy-2pe resnet50.dfg -o resnet50_2pe.enf`\n\n이 NPU 구성은 [Device Configuration](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#device-specification)으로 지정된 `device` 옵션을 사용하여 [Runtime](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#furiosa.runtime.Runtime)을 생성할 때도 지정할 수 있습니다.\n\n런타임 최적화 [](#runtime-optimization \"이 제목으로의 영구 링크\") ---------------------------------------------------------------------------\n지금까지 우리는 추론 지연 시간을 줄이기 위한 모델 최적화 기술에 대해 논의했습니다. 모델 최적화를 적용한 후, 런타임 수준에서 성능을 추가로 최적화할 수 있습니다.\n\n위에서 언급했듯이, 엔드 투 엔드 추론은 NPU 실행, CPU 계산 및 IO 작업의 세 가지 작업으로 구성됩니다. 세 가지 종류의 작업은 서로 차단하지 않고 독립적으로 실행될 수 있습니다. 여러 추론을 동시에 실행하면 이들을 겹칠 수 있습니다. 이 특성을 활용하는 것이 런타임 최적화의 핵심 아이디어입니다.\n\n### 더 많은 추론 동시성 (작업자 수) [](#more-inference-concurrency-the-number-of-workers \"이 제목으로의 영구 링크\")\n[Runner API](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#furiosa.runtime.furiosa.runtime.Runtime.create_runner)를 통해 세션을 생성할 때, 작업자 수를 옵션으로 지정할 수 있습니다. 단일 작업자는 NPU를 공유하여 독립적으로 추론을 실행할 수 있는 단위입니다. 이 개념은 스레드와 CPU와 유사합니다.\n\n작업자가 하나만 있는 경우, 여러 추론 요청은 단일 작업자를 통해 순차적으로 처리됩니다. 하나의 추론이 완료되면 다음 추론이 작업자에 의해 처리됩니다. 이 경우, CPU가 작동하는 동안 NPU가 유휴 상태가 되어 NPU 활용도가 낮아질 수 있습니다.\n\n그러나 여러 작업자가 있는 경우, 작업자는 런타임의 요청 큐에서 요청을 소비합니다. 여러 추론이 동시에 처리될 수 있습니다. 이 경우, NPU 실행은 CPU 실행과 겹쳐져 NPU 활용도가 높아질 수 있습니다.\n\n각 작업자는 실행을 위한 컨텍스트 정보를 유지하기 위해 더 많은 메모리 리소스를 요구합니다. 작업자 수가 너무 많으면 메모리 리소스가 고갈될 수 있습니다. 작업자 수가 너무 적으면 NPU 활용도가 낮아질 수 있습니다. 모델의 성능을 극대화하기 위해 최적의 작업자 수를 찾는 것이 중요합니다. 일반적으로 실험을 통해 최적의 작업자 수를 찾을 수 있습니다.\n\n### 동기 API 대 비동기 API [](#sync-api-vs-async-apis \"이 제목으로의 영구 링크\")\n런타임 API에는 두 가지 유형이 있습니다: 동기 API와 비동기 API. 동기 API는 추론이 완료될 때까지 기다리는 블로킹 API입니다. 비동기 API는 추론이 완료될 때까지 기다리지 않는 비블로킹 API입니다. 비동기 API는 여러 추론을 동시에 요청하고 비동기적으로 결과를 기다릴 수 있습니다. `furiosa.session.create()`는 동기 세션을 생성합니다. 아래 예제와 같이, `session.run()`은 결과가 반환될 때까지 차단됩니다. 이는 일반적으로 큰 배치 크기를 가진 배치 워크로드에 충분하지만, 동시에 여러 현재 요청을 처리하는 서비스를 위해서는 충분하지 않습니다.\n```python\nfrom furiosa.runtime import session\n\nwith session.create(model) as sess:\n    input = ...\n    outputs = sess.run(input)  # 완료될 때까지 대기\n    ...\n```\n이 제한을 극복하기 위해, Furiosa SDK는 두 가지 유형의 비동기 API를 제공합니다: 큐 API와 Async\/Await API. 이들은 여러 추론을 동시에 요청하고 비동기적으로 결과를 기다릴 수 있게 해줍니다. 또한, I\/O 및 CPU 계산을 NPU 실행과 겹치게 하여 숨기는 데 유용합니다.\n\n#### 큐 API [](#queue-api \"이 제목으로의 영구 링크\")\n`create_async()`는 제출자와 큐의 쌍을 생성합니다. 이를 통해 완료를 기다리지 않고 추론 요청을 제출하고 비동기적으로 추론 결과를 기다릴 수 있습니다.\n```python\nimport numpy as np\nimport random\nfrom furiosa.runtime import session\n\nsubmitter, queue = session.create_async(\"mnist.onnx\",\n                                        worker_num=2,  # 비동기 요청을 제출할 수 있는 수를 결정\n                                        input_queue_size=100,\n                                        output_queue_size=100)\n\nfor i in range(0, 5):\n    idx = random.randint(0, 59999)\n    input = np.random.rand(1, 1, 28, 28).astype(np.float32)\n    submitter.submit(input, context=idx)  # 비블로킹 호출\n\nfor i in range(0, 5):\n    context, outputs = queue.recv(100)  # 100ms 타임아웃. None이면 queue.recv()는 차단됩니다.\n    print(outputs[0].numpy())\n\nif queue:\n    queue.close()\nif submitter:\n    submitter.close()\n```\n\n#### Async\/Await 문법 사용 [](#using-async-await-syntax \"이 제목으로의 영구 링크\")\n아래 예제에서는, furiosa-server의 `NPUModel`이 async\/await API를 사용하여 서비스 응용 프로그램을 구현하는 더 쉬운 방법을 제공합니다.\n```python\nimport asyncio\nimport numpy as np\nfrom furiosa.server.model import NPUModel, NPUModelConfig\n\nclass SimpleApplication:\n    def __init__(self):\n        self.model = NPUModel(\n            NPUModelConfig(\n                name=\"MNIST\",\n                model=\"mnist.onnx\",\n            )\n        )\n\n    async def load(self):\n        await self.model.load()\n\n    async def process(self, image):\n        input = self.preprocess(image)\n        tensor = await self.model.predict(input)\n        output = self.postprocess(tensor)\n        return output\n\n    def preprocess(self, image):\n        # 전처리 수행\n        return image\n\n    def postprocess(self, tensor):\n        # 후처리 수행\n        return tensor\n\nAPP = SimpleApplication()\n\nasync def startup():\n    await APP.load()\n\nasync def run(image):\n    result = await APP.process(image)\n    return result\n\nif __name__ == \"__main__\":\n    asyncio.run(startup())\n    image = np.random.rand(1, 1, 28, 28).astype(np.float32)\n    asyncio.run(run(image))\n```\n\n[이전](quantization.html \"모델 양자화\") [다음](profiler.html \"성능 프로파일링\")\n---\n© Copyright 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용했습니다.\n```\n"},{"page_id":"13973d88-9f7d-49e4-8019-68c7f29141b3","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.9.0.html","original_content":"* Release Notes - 0.9.0 * [View page source](..\/_sources\/releases\/0.9.0.rst.txt)\n---\nRelease Notes - 0.9.0 [](#release-notes-0-9-0 \"Permalink to this heading\") ===========================================================================\nFuriosa SDK 0.9.0 is a major release, including many performance enhancements, additional functions, and bug fixes. In partcular, 0.9.0 release includes the significant improvements of the quantization tools.\nComponent Version Information\n[](#id1 \"Permalink to this table\")\n| Package Name | Version | | --- | --- | | NPU Driver | 1.7.0 | | NPU Firmware Tools | 1.4.0 | | NPU Firmware Image | 1.7.0 | | HAL (Hardware Abstraction Layer) | 0.11.0 | | Furiosa Compiler | 0.9.0 | | Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..) | 0.9.0 | | NPU Management CLI (furiosactl) | 0.11.0 | | NPU Device Plugin | 0.10.1 | | NPU Feature Discovery | 0.2.0 |\nInstalling the latest SDK [](#installing-the-latest-sdk \"Permalink to this heading\") -------------------------------------------------------------------------------------\nIf you are using APT repository, the upgrade process is simpler.\n``` apt-get update && apt-get upgrade\n```\nIf you wish to designate a specific package for upgrade, execute as below: You can find more details about APT repository setup at [Driver, Firmware, and Runtime Installation](..\/software\/installation.html#requiredpackages) .\n``` apt-get update && \\ apt-get install -y furiosa-driver-pdma furiosa-libhal-warboy furiosa-libnux furiosactl\n```\nYou can upgrade firmware as follows:\n``` apt-get update && \\ apt-get install -y furiosa-firmware-tools furiosa-firmware-image\n```\nYou can upgrade Python package as follows:\n``` pip install --upgrade pip setuptools wheel pip install --upgrade furiosa-sdk\n```\nWarning\nWhen installing or upgrading the furiosa-sdk without updating pip to the latest version, you may encounter the following errors.\n``` ERROR: Could not find a version that satisfies the requirement furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk) (from versions: none) ERROR: No matching distribution found for furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk)\n```\nMajor changes [](#major-changes \"Permalink to this heading\") -------------------------------------------------------------\n### Quantization tool [](#quantization-tool \"Permalink to this heading\")\nQuantization tool is a library that converts a pre-trained model to a quantized model. You can refer to more details at [Model Quantization](..\/software\/quantization.html#modelquantization) 0.9.0 release includes the API improvement and new calibration methods, possibly leading to better accuracy.\n* Added new quantization-related APIs that are more flexible and solid. (   `furiosa.quantizer`   ,   `furiosa.optimizer`   )\n``` optimized_onnx_model = optimize_model(source_onnx_model) calibrator = Calibrator(optimized_onnx_model, CalibrationMethod.MIN_MAX_ASYM) for calibration_data, _ in tqdm.tqdm(calibration_dataloader, desc=\"Calibration\", unit=\"images\", mininterval=0.5):   calibrator.collect_data([[calibration_data.numpy()]]) ranges = calibrator.compute_range() quantizated_graph = quantize(optimized_onnx_model, ranges)\n```\n* Added an option to decide whether to perform quantize at the beginning of the model.      + Instead of     `without_quantize`     being removed from the compiler options, it can be specified via the argument     `with_quantize`     to the     `quantize`     function. * The   `normalized_pixel_outputs`   argument to the   `quantize`   function can be set to convert the model output to uint8 instead of dequantizing to fp32.      + A tensor with an element range of     `(0.          ,          1.)`     can be optimized to convert to pixel data in uint8. * Provides more calibration methods.\nSupported Calibration Methods\n[](#id2 \"Permalink to this table\")\n| Calibration Method | Asymmetric | QuasiSymmetric | | --- | --- | --- | | Min-Max | MIN\\_MAX\\_ASYM | MIN\\_MAX\\_SYM | | Entropy | ENTROPY\\_ASYM | ENTROPY\\_SYM | | Percentile | PERCENTILE\\_ASYM | PERCENTILE\\_SYM | | Mean squared error | MSE\\_ASYM | MSE\\_SYM | | Signal-to-quantization-noise ratio | SQNR\\_ASYM | SQNR\\_SYM |\nTo ensure the effectiveness of new calibration methods, we measured the accuracy of 10 popular models with the new calibration methods. Among them, 8 models showed better accuracy than the existing calibration methods. For example, the accuracy of EfficientNet-B0 increased by 57.452%. With the min-max calibration method, EfficientNet-B0 had an accuracy of 16.104%. In contrast, with the percentile calibration method, the accuracy was 73.556%. The details of the experiment results can be found at [Quantization Accuracy](..\/software\/quantization.html#quantizationaccuracytable) .\nFor more information on installing and using the new quantizer, you can refer to the following examples.\n* [Tutorial: How to use Furiosa SDK from Start to Finish](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/v0.9.0\/examples\/notebooks\/HowToUseFuriosaSDKFromStartToFinish.ipynb)\n### Compiler [](#compiler \"Permalink to this heading\")\n* Added acceleration support for operators Lower, Unlower * Added acceleration support for operator Dequantize * Support for executing binaries that are larger than the hardware’s instruction memory * Improved scheduler and memory allocator to eliminate unnecessary I\/O * Various improvements optimize compilation for better execution performance\n### furiosa-toolkit [](#furiosa-toolkit \"Permalink to this heading\")\nThe `furiosactl` command-line tool included in the furiosa-toolkit 0.11.0 release includes improvements to the includes the following major improvements\nThe newly added `furiosactl\ntop` command is used to view utilization by NPU device over time.\n``` $ furiosactl top --interval 200 NOTE: furiosa top is under development. Usage and output formats may change. Please enter Ctrl+C to stop. Datetime                        PID       Device        NPU(%)   Comp(%)   I\/O(%)   Command 2023-03-21T09:45:56.699483936Z  152616    npu1pe0-1      19.06    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:56.906443888Z  152616    npu1pe0-1      51.09     93.05     6.95   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.110489333Z  152616    npu1pe0-1      46.40     97.98     2.02   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.316060982Z  152616    npu1pe0-1      51.43    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.521140588Z  152616    npu1pe0-1      54.28     94.10     5.90   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.725910558Z  152616    npu1pe0-1      48.93     98.93     1.07   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.935041998Z  152616    npu1pe0-1      47.91    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:58.13929122Z   152616    npu1pe0-1      49.06     94.94     5.06   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n```\nThe `furiosactl\ninfo` command has been improved to display concise information about each device. As before, you can enter the `--full` option if you want to see more information about a device.\n``` $ furiosactl info +------+--------+----------------+-------+--------+--------------+ | NPU  | Name   | Firmware       | Temp. | Power  | PCI-BDF      | +------+--------+----------------+-------+--------+--------------+ | npu1 | warboy | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 | +------+--------+----------------+-------+--------+--------------+\n$ furiosactl info --full +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+ | NPU  | Name   | UUID                                 | S\/N               | Firmware       | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+ | npu1 | warboy | 00000000-0000-0000-0000-000000000000 | WBYB0000000000000 | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 | 511:0   | +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\n```\nMore information about installing and using `furiosactl` can be found in [furiosa-toolkit](..\/software\/cli.html#toolkit) .\n[Previous](0.10.0.html \"Release Notes - 0.10.0\") [Next](0.8.0.html \"Release Notes - 0.8.0\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 릴리스 노트 - 0.9.0 * [페이지 소스 보기](..\/_sources\/releases\/0.9.0.rst.txt)\n---\n릴리스 노트 - 0.9.0 [](#release-notes-0-9-0 \"이 제목의 영구 링크\") ===========================================================================\nFuriosa SDK 0.9.0은 주요 릴리스로, 많은 성능 향상, 추가 기능 및 버그 수정이 포함되어 있습니다. 특히, 0.9.0 릴리스는 양자화 도구의 중요한 개선 사항을 포함하고 있습니다.\n구성 요소 버전 정보 [](#id1 \"이 표의 영구 링크\")\n| 패키지 이름 | 버전 | | --- | --- | | NPU 드라이버 | 1.7.0 | | NPU 펌웨어 도구 | 1.4.0 | | NPU 펌웨어 이미지 | 1.7.0 | | HAL (하드웨어 추상화 계층) | 0.11.0 | | Furiosa 컴파일러 | 0.9.0 | | Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..) | 0.9.0 | | NPU 관리 CLI (furiosactl) | 0.11.0 | | NPU 장치 플러그인 | 0.10.1 | | NPU 기능 탐지 | 0.2.0 |\n\n최신 SDK 설치 [](#installing-the-latest-sdk \"이 제목의 영구 링크\") -------------------------------------------------------------------------------------\nAPT 저장소를 사용하는 경우 업그레이드 프로세스가 더 간단합니다.\n```bash\napt-get update && apt-get upgrade\n```\n특정 패키지를 지정하여 업그레이드하려면 아래와 같이 실행하십시오: APT 저장소 설정에 대한 자세한 내용은 [드라이버, 펌웨어 및 런타임 설치](..\/software\/installation.html#requiredpackages)에서 확인할 수 있습니다.\n```bash\napt-get update && \\\napt-get install -y furiosa-driver-pdma furiosa-libhal-warboy furiosa-libnux furiosactl\n```\n펌웨어를 다음과 같이 업그레이드할 수 있습니다:\n```bash\napt-get update && \\\napt-get install -y furiosa-firmware-tools furiosa-firmware-image\n```\nPython 패키지를 다음과 같이 업그레이드할 수 있습니다:\n```bash\npip install --upgrade pip setuptools wheel\npip install --upgrade furiosa-sdk\n```\n경고\npip을 최신 버전으로 업데이트하지 않고 furiosa-sdk를 설치하거나 업그레이드할 때 다음과 같은 오류가 발생할 수 있습니다.\n```plaintext\nERROR: Could not find a version that satisfies the requirement furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk) (from versions: none)\nERROR: No matching distribution found for furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk)\n```\n\n주요 변경 사항 [](#major-changes \"이 제목의 영구 링크\") -------------------------------------------------------------\n### 양자화 도구 [](#quantization-tool \"이 제목의 영구 링크\")\n양자화 도구는 사전 학습된 모델을 양자화된 모델로 변환하는 라이브러리입니다. 자세한 내용은 [모델 양자화](..\/software\/quantization.html#modelquantization)를 참조하십시오. 0.9.0 릴리스에는 API 개선 및 새로운 보정 방법이 포함되어 있어 더 나은 정확도를 제공할 수 있습니다.\n* 더 유연하고 견고한 새로운 양자화 관련 API가 추가되었습니다. (`furiosa.quantizer`, `furiosa.optimizer`)\n```python\noptimized_onnx_model = optimize_model(source_onnx_model)\ncalibrator = Calibrator(optimized_onnx_model, CalibrationMethod.MIN_MAX_ASYM)\nfor calibration_data, _ in tqdm.tqdm(calibration_dataloader, desc=\"Calibration\", unit=\"images\", mininterval=0.5):\n    calibrator.collect_data([[calibration_data.numpy()]])\nranges = calibrator.compute_range()\nquantizated_graph = quantize(optimized_onnx_model, ranges)\n```\n* 모델의 시작 부분에서 양자화를 수행할지 여부를 결정하는 옵션이 추가되었습니다.\n  + `without_quantize`가 컴파일러 옵션에서 제거되는 대신, `quantize` 함수의 인수 `with_quantize`를 통해 지정할 수 있습니다.\n* `quantize` 함수의 `normalized_pixel_outputs` 인수를 설정하여 모델 출력을 fp32로 디양자화하는 대신 uint8로 변환할 수 있습니다.\n  + `(0., 1.)` 범위의 요소를 가진 텐서는 uint8로 픽셀 데이터를 변환하도록 최적화될 수 있습니다.\n* 더 많은 보정 방법을 제공합니다.\n\n지원되는 보정 방법 [](#id2 \"이 표의 영구 링크\")\n| 보정 방법 | 비대칭 | 준대칭 | | --- | --- | --- | | Min-Max | MIN\\_MAX\\_ASYM | MIN\\_MAX\\_SYM | | 엔트로피 | ENTROPY\\_ASYM | ENTROPY\\_SYM | | 백분위수 | PERCENTILE\\_ASYM | PERCENTILE\\_SYM | | 평균 제곱 오차 | MSE\\_ASYM | MSE\\_SYM | | 신호 대 양자화 잡음 비율 | SQNR\\_ASYM | SQNR\\_SYM |\n\n새로운 보정 방법의 효과를 보장하기 위해, 우리는 10개의 인기 있는 모델의 정확도를 측정했습니다. 그중 8개의 모델이 기존 보정 방법보다 더 나은 정확도를 보였습니다. 예를 들어, EfficientNet-B0의 정확도는 57.452% 증가했습니다. Min-Max 보정 방법으로는 EfficientNet-B0의 정확도가 16.104%였으나, 백분위수 보정 방법으로는 73.556%의 정확도를 보였습니다. 실험 결과의 세부 사항은 [양자화 정확도](..\/software\/quantization.html#quantizationaccuracytable)에서 확인할 수 있습니다.\n\n새로운 양자화 도구 설치 및 사용에 대한 자세한 정보는 다음 예제를 참조하십시오.\n* [튜토리얼: 처음부터 끝까지 Furiosa SDK 사용 방법](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/v0.9.0\/examples\/notebooks\/HowToUseFuriosaSDKFromStartToFinish.ipynb)\n\n### 컴파일러 [](#compiler \"이 제목의 영구 링크\")\n* 연산자 Lower, Unlower에 대한 가속 지원 추가\n* 연산자 Dequantize에 대한 가속 지원 추가\n* 하드웨어의 명령어 메모리보다 큰 바이너리 실행 지원\n* 불필요한 I\/O를 제거하기 위해 스케줄러 및 메모리 할당자 개선\n* 다양한 개선 사항이 컴파일 최적화를 통해 실행 성능을 향상시킴\n\n### furiosa-toolkit [](#furiosa-toolkit \"이 제목의 영구 링크\")\nfuriosa-toolkit 0.11.0 릴리스에 포함된 `furiosactl` 명령줄 도구에는 다음과 같은 주요 개선 사항이 포함되어 있습니다.\n새로 추가된 `furiosactl top` 명령은 NPU 장치의 시간 경과에 따른 활용도를 확인하는 데 사용됩니다.\n```bash\n$ furiosactl top --interval 200\nNOTE: furiosa top은 개발 중입니다. 사용법 및 출력 형식이 변경될 수 있습니다. 종료하려면 Ctrl+C를 입력하십시오.\nDatetime                        PID       Device        NPU(%)   Comp(%)   I\/O(%)   Command\n2023-03-21T09:45:56.699483936Z  152616    npu1pe0-1      19.06    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n2023-03-21T09:45:56.906443888Z  152616    npu1pe0-1      51.09     93.05     6.95   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n2023-03-21T09:45:57.110489333Z  152616    npu1pe0-1      46.40     97.98     2.02   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n2023-03-21T09:45:57.316060982Z  152616    npu1pe0-1      51.43    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n2023-03-21T09:45:57.521140588Z  152616    npu1pe0-1      54.28     94.10     5.90   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n2023-03-21T09:45:57.725910558Z  152616    npu1pe0-1      48.93     98.93     1.07   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n2023-03-21T09:45:57.935041998Z  152616    npu1pe0-1      47.91    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n2023-03-21T09:45:58.13929122Z   152616    npu1pe0-1      49.06     94.94     5.06   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n```\n`furiosactl info` 명령은 각 장치에 대한 간결한 정보를 표시하도록 개선되었습니다. 이전과 마찬가지로 장치에 대한 더 많은 정보를 보려면 `--full` 옵션을 입력할 수 있습니다.\n```bash\n$ furiosactl info\n+------+--------+----------------+-------+--------+--------------+\n| NPU  | Name   | Firmware       | Temp. | Power  | PCI-BDF      |\n+------+--------+----------------+-------+--------+--------------+\n| npu1 | warboy | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 |\n+------+--------+----------------+-------+--------+--------------+\n\n$ furiosactl info --full\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\n| NPU  | Name   | UUID                                 | S\/N               | Firmware       | Temp. | Power  | PCI-BDF      | PCI-DEV |\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\n| npu1 | warboy | 00000000-0000-0000-0000-000000000000 | WBYB0000000000000 | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 | 511:0   |\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\n```\n`furiosactl` 설치 및 사용에 대한 자세한 정보는 [furiosa-toolkit](..\/software\/cli.html#toolkit)에서 확인할 수 있습니다.\n[이전](0.10.0.html \"릴리스 노트 - 0.10.0\") [다음](0.8.0.html \"릴리스 노트 - 0.8.0\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공한 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용합니다.\n```"},{"page_id":"64f0ffea-6087-4f27-8889-83479e61e89e","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/profiler.html","original_content":"* Performance Profiling * [View page source](..\/_sources\/software\/profiler.rst.txt)\n---\nPerformance Profiling [](#performance-profiling \"Permalink to this heading\") =============================================================================\nLow latency and high throughput performance are critical factors in many DNN applications. For performance optimization, model developers and ML engineers must understand the model performance and be able to analyze bottlenecks. To assist developers with this process, Furiosa SDK provides a profiling tool.\nTrace Analysis [](#trace-analysis \"Permalink to this heading\") ---------------------------------------------------------------\nTrace analysis provides structured data on execution time by step, by actually executing model inference task. You can also visualize the data using the [Trace Event Profiling Tool](https:\/\/www.chromium.org\/developers\/how-tos\/trace-event-profiling-tool\/) function of the Chrome web browser.\nThough small, trace generation generates temporal overheads as it measures time for each step and writes the results to a file. It is thus not enabled by default. You can create trace by using one of the following methods.\nTracing via Environment Variable [](#tracing-via-environment-variable \"Permalink to this heading\") ---------------------------------------------------------------------------------------------------\nYou can enable trace generation by setting the path of the file to which the trace result will be written in `FURIOSA_PROFILER_OUTPUT_PATH` . The advantage of this method is that the code remains unchanged. The downside is that you cannot set a specific section or category for measurement.\n``` git clone https:\/\/github.com\/furiosa-ai\/furiosa-sdk cd furiosa-sdk\/examples\/inferences export FURIOSA_PROFILER_OUTPUT_PATH=`pwd`\/tracing.json .\/image_classify.py ..\/assets\/images\/car.jpg\nls -l .\/tracing.json -rw-r--r-- 1 furiosa furiosa 456493 Jul 27 17:56 .\/tracing.json\n```\nIf you enable trace generation through environment variables as described above, a JSON file will be written to the path specified by the environment variable `FURIOSA_PROFILER_OUTPUT_PATH` . If you enter `chrome:\/\/tracing` in Chrome’s address bar, the trace viewer will start. Click the `Load` button in the upper left corner of the trace viewer, and select the saved file ( `tracing.json` in the example above) to view the trace result.\nTracing via Profiler Context [](#tracing-via-profiler-context \"Permalink to this heading\") -------------------------------------------------------------------------------------------\nYou can also trace a model inference performance by using a Profiler Context in your Python code. The advantages of this method, in comparison to the tracing by environment variable, are as follows:\n* Allow to enable trace immediately even in interactive environments, such as Python Interpreter or Jupyter Notebook * Allow to specify labels to certain inference runs * Allow to measure specified operator categories selectively\n``` #!\/usr\/bin\/env python\nimport numpy as np from furiosa.runtime.profiler import profile from furiosa.runtime.sync import create_runner\n# You can find 'examples' directory of the root of furiosa-sdk source tree model_path = \"examples\/assets\/quantized_models\/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\"\nwith open(\"mobilenet_v1_trace.json\", \"w\") as output:     with profile(file=output) as profiler:         with create_runner(model_path) as runner:             input_shape = runner.model.input(0).shape\n            with profiler.record(\"warm up\") as record:                 for _ in range(0, 2):                     runner.run([np.uint8(np.random.rand(*input_shape))])\n            with profiler.record(\"trace\") as record:                 for _ in range(0, 2):                     runner.run([np.uint8(np.random.rand(*input_shape))])\n```\nThe above is a code example using a profiling context. Once the above Python code is executed, the mnist\\_trace.json\nfile is created. The trace results are labelled ‘warm up’ and ‘trace’ as shown below.\n### Pause\/Resume of Profiler Context [](#pause-resume-of-profiler-context \"Permalink to this heading\")\nTracing long-running jobs can cause following problems:\n* Produce large trace files which take huge disk space and are difficult to be shared. * Make it hard to identify interesting section when the trace is visualized, without additional processing. * Take much time to produce trace files.\nTo avoid the above issues, the profiler provides an additional API to temporarily pause or resume a profiler within the context. Users can exclude execution they do not want to profile, thereby reducing profiling overhead and trace file size.\nThe below is an example of pausing profiler not to trace `warm\nup` phase between `profile.pause` and `profile.resume` .\n``` #!\/usr\/bin\/env python\nimport numpy as np from furiosa.runtime.profiler import RecordFormat, profile from furiosa.runtime.sync import create_runner\n# You can find 'examples' directory of the root of furiosa-sdk source tree model_path = \"examples\/assets\/quantized_models\/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\"\nwith profile(format=RecordFormat.PandasDataFrame) as profiler:     with create_runner(model_path) as runner:         input_shape = runner.model.input(0).shape\n        # pause profiling during warmup         profiler.pause()\n        for _ in range(0, 10):             with profiler.record(\"warm up\") as record:                 runner.run([np.uint8(np.random.rand(*input_shape))])\n        # resume profiling         profiler.resume()\n        with profiler.record(\"trace\") as record:             runner.run([np.uint8(np.random.rand(*input_shape))])\ndf = profiler.get_pandas_dataframe()\nassert len(df[df[\"name\"] == \"trace\"]) == 1 assert len(df[df[\"name\"] == \"warm up\"]) == 0\n```\n### Trace analysis using Pandas DataFrame [](#trace-analysis-using-pandas-dataframe \"Permalink to this heading\")\nWith the measured tracing data, in addition to visualizing it with Chrome Trace Format, it can also be expressed and used in Pandas DataFrame, commonly used for data analysis. These are the advantages in comparison to Chrome Trace Format.\n* Can be used directly in Python Interpreter or Jupyter Notebook interactive shell * Users can directly access DataFrame for analysis, on top of the reporting function which is provided as default\n``` #!\/usr\/bin\/env python\nimport numpy as np from furiosa.runtime.profiler import RecordFormat, profile from furiosa.runtime.sync import create_runner\n# You can find 'examples' directory of the root of furiosa-sdk source tree model_path = \"examples\/assets\/quantized_models\/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\"\nwith profile(format=RecordFormat.PandasDataFrame) as profiler:     with create_runner(model_path) as runner:         input_shape = runner.model.input(0).shape\n        with profiler.record(\"warm up\") as record:             for _ in range(0, 2):                 runner.run([np.uint8(np.random.rand(*input_shape))])\n        with profiler.record(\"trace\") as record:             for _ in range(0, 2):                 runner.run([np.uint8(np.random.rand(*input_shape))])\nprofiler.print_summary()  # (1)\nprofiler.print_inferences()  # (2)\nprofiler.print_npu_executions()  # (3)\nprofiler.print_npu_operators()  # (4)\nprofiler.print_external_operators()  # (5)\ndf = profiler.get_pandas_dataframe()  # (6) print(df[df[\"name\"] == \"trace\"][[\"trace_id\", \"name\", \"thread.id\", \"dur\"]])\n```\nAbove is a code example that designates a profiling context format into PandasDataFrame.\nWhen `(1)` line is executed, the following summary of the results is produced.\n``` ================================================   Inference Results Summary ================================================ Inference counts                : 4 Min latency (ns)                : 1584494 Max latency (ns)                : 3027309 Mean latency (ns)               : 2136984 Median latency (ns)             : 1968066 90.0 percentile Latency (ns)    : 2752525 95.0 percentile Latency (ns)    : 2889917 97.0 percentile Latency (ns)    : 2944874 99.0 percentile Latency (ns)    : 2999831 99.9 percentile Latency (ns)    : 3024561\n```\nWhen `(2)` line is executed, duration of one inference query is shown.\n``` ┌──────────────────────────────────┬──────────────────┬───────────┬─────────┐ │ trace_id                         ┆ span_id          ┆ thread.id ┆ dur     │ ╞══════════════════════════════════╪══════════════════╪═══════════╪═════════╡ │ 7cf3d3b7439cf4c3fac1a47998783102 ┆ 403ada67f1d8220e ┆ 1         ┆ 3027309 │ │ 16d65f6f8f1db256d0f39953855dea72 ┆ 78b065c19c3675ef ┆ 1         ┆ 2111363 │ │ d0534e3a9f19edadab81954ad28ab44f ┆ 9a7addaf0f28c9fe ┆ 1         ┆ 1824769 │ │ 70512188522f45b87cfe4f545de3cf2c ┆ c75f697f8e72d333 ┆ 1         ┆ 1584494 │ └──────────────────────────────────┴──────────────────┴───────────┴─────────┘\n```\nWhen `(3)` line is executed, elapsed times of NPU executions will be shown:\n``` ┌──────────────────────────────────┬──────────────────┬──────────┬─────────────────┬───────────┬─────────┬──────────────────────┐ │ trace_id                         ┆ span_id          ┆ pe_index ┆ execution_index ┆ NPU Total ┆ NPU Run ┆ NPU IoWait           │ ╞══════════════════════════════════╪══════════════════╪══════════╪═════════════════╪═══════════╪═════════╪══════════════════════╡ │ 8f6fce6c0e52b4735cae3379732a0943 ┆ 3e1e4a76523cbf89 ┆ 0        ┆ 0               ┆ 119145    ┆ 108134  ┆ 18446744073709540605 │ │ 195366613b1da9b0350c0a3c2a608f42 ┆ 07dff2e92172fabd ┆ 0        ┆ 0               ┆ 119363    ┆ 108134  ┆ 18446744073709540387 │ │ 3b65b8fa3eabfaf8f815ec9f41fcc7d9 ┆ 639a366a7f932a23 ┆ 0        ┆ 0               ┆ 119157    ┆ 108134  ┆ 18446744073709540593 │ │ e48825df32a07e5559f7f50048c08e1f ┆ ecaab4915bfda725 ┆ 0        ┆ 0               ┆ 119219    ┆ 108134  ┆ 18446744073709540531 │ └──────────────────────────────────┴──────────────────┴──────────┴─────────────────┴───────────┴─────────┴──────────────────────┘\n```\nWhen `(4)` line is executed, elapsed times of operators will be shown:\n``` ┌─────────────────────────┬──────────────────────┬───────┐ │ name                    ┆ average_elapsed (ns) ┆ count │ ╞═════════════════════════╪══════════════════════╪═══════╡ │ LowLevelConv2d          ┆ 5327.8               ┆ 60    │ │ LowLevelDepthwiseConv2d ┆ 1412.285714          ┆ 56    │ │ LowLevelPad             ┆ 575.785714           ┆ 56    │ │ LowLevelTranspose       ┆ 250.0                ┆ 4     │ │ LowLevelReshape         ┆ 2.0                  ┆ 240   │ │ LowLevelSlice           ┆ 2.0                  ┆ 12    │ │ LowLevelExpand          ┆ 2.0                  ┆ 16    │ └─────────────────────────┴──────────────────────┴───────┘\n```\nWhen `(5)` line is executed, the time data for operators in the CPU is shown as below.\n``` ┌──────────────────────────────────┬──────────────────┬───────────┬────────────┬────────────────┬────────┐ │ trace_id                         ┆ span_id          ┆ thread.id ┆ name       ┆ operator_index ┆ dur    │ ╞══════════════════════════════════╪══════════════════╪═══════════╪════════════╪════════════════╪════════╡ │ e7ab6656cc090a8d05992a9e4683b8b7 ┆ 206a1d6f351ca4b1 ┆ 40        ┆ Quantize   ┆ 0              ┆ 136285 │ │ 03636fd6c7dbc42f0a9dd29a7283d3fc ┆ f636740983e095a6 ┆ 40        ┆ Lower      ┆ 1              ┆ 133350 │ │ c9a0858f7e0885a976f51c6cb57d3e0f ┆ bb6c84f88e453055 ┆ 40        ┆ Unlower    ┆ 2              ┆ 44775  │ │ 8777c67ad9fe597139bbd6970362c2fc ┆ 63bac982c7b98aba ┆ 40        ┆ Dequantize ┆ 3              ┆ 14682  │ │ 98aeba2a25b0525166b6a4065ab01774 ┆ 34ccd560571d733f ┆ 40        ┆ Quantize   ┆ 0              ┆ 45465  │ │ 420525dc13ba9624083e0a276f7ee718 ┆ 9f6d342da5eb86bc ┆ 40        ┆ Lower      ┆ 1              ┆ 152748 │ │ cb67393f6949bbbb396053c1e00931ff ┆ 2d724fa6ab8ca024 ┆ 40        ┆ Unlower    ┆ 2              ┆ 67140  │ │ 00424b4f02039ae0ca98388a964062b0 ┆ a5fb9fbd5bffe6a6 ┆ 40        ┆ Dequantize ┆ 3              ┆ 32388  │ │ d7412c59d360067e8b7a2508a30d1079 ┆ 8e426d778fa95722 ┆ 40        ┆ Quantize   ┆ 0              ┆ 71736  │ │ 6820acf9345c5b373c512f6cd5edcbc7 ┆ 2d787c2df381f010 ┆ 40        ┆ Lower      ┆ 1              ┆ 311310 │ │ 84d24b02a95c63c3e40f7682384749e4 ┆ 1236a974a619ff1a ┆ 40        ┆ Unlower    ┆ 2              ┆ 51930  │ │ 8d25dff1cfd6624509cbf95503e93382 ┆ 673efb3bfb8deac6 ┆ 40        ┆ Dequantize ┆ 3              ┆ 12362  │ │ 4cc60ec1eee7d9f3cdd290d07b303a18 ┆ e7903b0a584d6388 ┆ 40        ┆ Quantize   ┆ 0              ┆ 56736  │ │ c5f04d9fea26e5b52c6ec5e5406775fc ┆ 701118dabd065e6f ┆ 40        ┆ Lower      ┆ 1              ┆ 265447 │ │ c5fdfb9cf454da130148e8e364eeee93 ┆ 5cf3750def19c6e8 ┆ 40        ┆ Unlower    ┆ 2              ┆ 35869  │ │ e1e650d23061140404915f1df36daf9c ┆ ddd76ff19b5cd713 ┆ 40        ┆ Dequantize ┆ 3              ┆ 14688  │ └──────────────────────────────────┴──────────────────┴───────────┴────────────┴────────────────┴────────┘\n```\nWith line `(6)` , you can access DataFrame from the code and perform direct analysis.\n```                             trace_id   name  thread.id       dur 487  f3b158734e3684f2e043ed41309c4c2d  trace          1  11204385\n```\n[Previous](performance.html \"Performance Optimization\") [Next](serving.html \"Model Server (Serving Framework)\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 성능 프로파일링 * [페이지 소스 보기](..\/_sources\/software\/profiler.rst.txt)\n---\n성능 프로파일링 [](#performance-profiling \"Permalink to this heading\") =============================================================================\n낮은 지연 시간과 높은 처리량 성능은 많은 DNN 애플리케이션에서 중요한 요소입니다. 성능 최적화를 위해, 모델 개발자와 ML 엔지니어는 모델 성능을 이해하고 병목 현상을 분석할 수 있어야 합니다. 개발자를 돕기 위해, Furiosa SDK는 프로파일링 도구를 제공합니다.\n\n트레이스 분석 [](#trace-analysis \"Permalink to this heading\") ---------------------------------------------------------------\n트레이스 분석은 모델 추론 작업을 실제로 실행하여 단계별 실행 시간을 구조화된 데이터로 제공합니다. 또한 Chrome 웹 브라우저의 [Trace Event Profiling Tool](https:\/\/www.chromium.org\/developers\/how-tos\/trace-event-profiling-tool\/) 기능을 사용하여 데이터를 시각화할 수 있습니다.\n\n트레이스 생성은 각 단계의 시간을 측정하고 결과를 파일에 기록하기 때문에 작지만 일시적인 오버헤드를 발생시킵니다. 따라서 기본적으로 활성화되어 있지 않습니다. 다음 방법 중 하나를 사용하여 트레이스를 생성할 수 있습니다.\n\n환경 변수를 통한 트레이싱 [](#tracing-via-environment-variable \"Permalink to this heading\") ---------------------------------------------------------------------------------------------------\n`FURIOSA_PROFILER_OUTPUT_PATH`에 트레이스 결과가 기록될 파일의 경로를 설정하여 트레이스 생성을 활성화할 수 있습니다. 이 방법의 장점은 코드가 변경되지 않는다는 것입니다. 단점은 특정 섹션이나 카테고리를 측정할 수 없다는 것입니다.\n\n```bash\ngit clone https:\/\/github.com\/furiosa-ai\/furiosa-sdk\ncd furiosa-sdk\/examples\/inferences\nexport FURIOSA_PROFILER_OUTPUT_PATH=`pwd`\/tracing.json\n.\/image_classify.py ..\/assets\/images\/car.jpg\nls -l .\/tracing.json\n-rw-r--r-- 1 furiosa furiosa 456493 Jul 27 17:56 .\/tracing.json\n```\n\n위에서 설명한 대로 환경 변수를 통해 트레이스 생성을 활성화하면, JSON 파일이 환경 변수 `FURIOSA_PROFILER_OUTPUT_PATH`에 지정된 경로에 기록됩니다. Chrome의 주소창에 `chrome:\/\/tracing`을 입력하면 트레이스 뷰어가 시작됩니다. 트레이스 뷰어의 왼쪽 상단에 있는 `Load` 버튼을 클릭하고 저장된 파일(위 예제에서는 `tracing.json`)을 선택하여 트레이스 결과를 볼 수 있습니다.\n\n프로파일러 컨텍스트를 통한 트레이싱 [](#tracing-via-profiler-context \"Permalink to this heading\") -------------------------------------------------------------------------------------------\nPython 코드에서 프로파일러 컨텍스트를 사용하여 모델 추론 성능을 추적할 수도 있습니다. 환경 변수를 통한 트레이싱과 비교했을 때 이 방법의 장점은 다음과 같습니다:\n* Python 인터프리터나 Jupyter Notebook과 같은 대화형 환경에서도 즉시 트레이스를 활성화할 수 있습니다.\n* 특정 추론 실행에 레이블을 지정할 수 있습니다.\n* 특정 연산자 카테고리를 선택적으로 측정할 수 있습니다.\n\n```python\n#!\/usr\/bin\/env python\nimport numpy as np\nfrom furiosa.runtime.profiler import profile\nfrom furiosa.runtime.sync import create_runner\n\n# furiosa-sdk 소스 트리의 루트에서 'examples' 디렉토리를 찾을 수 있습니다.\nmodel_path = \"examples\/assets\/quantized_models\/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\"\n\nwith open(\"mobilenet_v1_trace.json\", \"w\") as output:\n    with profile(file=output) as profiler:\n        with create_runner(model_path) as runner:\n            input_shape = runner.model.input(0).shape\n            with profiler.record(\"warm up\") as record:\n                for _ in range(0, 2):\n                    runner.run([np.uint8(np.random.rand(*input_shape))])\n            with profiler.record(\"trace\") as record:\n                for _ in range(0, 2):\n                    runner.run([np.uint8(np.random.rand(*input_shape))])\n```\n\n위는 프로파일링 컨텍스트를 사용하는 코드 예제입니다. 위의 Python 코드를 실행하면 mnist_trace.json 파일이 생성됩니다. 트레이스 결과는 아래와 같이 'warm up'과 'trace'로 레이블이 지정됩니다.\n\n### 프로파일러 컨텍스트의 일시 중지\/재개 [](#pause-resume-of-profiler-context \"Permalink to this heading\")\n장시간 실행되는 작업을 추적하면 다음과 같은 문제가 발생할 수 있습니다:\n* 큰 디스크 공간을 차지하는 대용량 트레이스 파일이 생성되어 공유하기 어렵습니다.\n* 추가 처리 없이 트레이스를 시각화할 때 흥미로운 섹션을 식별하기 어렵습니다.\n* 트레이스 파일을 생성하는 데 많은 시간이 소요됩니다.\n\n위의 문제를 피하기 위해, 프로파일러는 컨텍스트 내에서 프로파일러를 일시 중지하거나 재개할 수 있는 추가 API를 제공합니다. 사용자는 프로파일링을 원하지 않는 실행을 제외하여 프로파일링 오버헤드와 트레이스 파일 크기를 줄일 수 있습니다.\n\n아래는 `profile.pause`와 `profile.resume` 사이에서 `warm up` 단계를 추적하지 않도록 프로파일러를 일시 중지하는 예제입니다.\n\n```python\n#!\/usr\/bin\/env python\nimport numpy as np\nfrom furiosa.runtime.profiler import RecordFormat, profile\nfrom furiosa.runtime.sync import create_runner\n\n# furiosa-sdk 소스 트리의 루트에서 'examples' 디렉토리를 찾을 수 있습니다.\nmodel_path = \"examples\/assets\/quantized_models\/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\"\n\nwith profile(format=RecordFormat.PandasDataFrame) as profiler:\n    with create_runner(model_path) as runner:\n        input_shape = runner.model.input(0).shape\n        # 워밍업 동안 프로파일링 일시 중지\n        profiler.pause()\n        for _ in range(0, 10):\n            with profiler.record(\"warm up\") as record:\n                runner.run([np.uint8(np.random.rand(*input_shape))])\n        # 프로파일링 재개\n        profiler.resume()\n        with profiler.record(\"trace\") as record:\n            runner.run([np.uint8(np.random.rand(*input_shape))])\ndf = profiler.get_pandas_dataframe()\nassert len(df[df[\"name\"] == \"trace\"]) == 1\nassert len(df[df[\"name\"] == \"warm up\"]) == 0\n```\n\n### Pandas DataFrame을 사용한 트레이스 분석 [](#trace-analysis-using-pandas-dataframe \"Permalink to this heading\")\n측정된 트레이싱 데이터를 사용하여 Chrome Trace Format으로 시각화하는 것 외에도, 데이터 분석에 일반적으로 사용되는 Pandas DataFrame으로 표현하고 사용할 수 있습니다. Chrome Trace Format과 비교했을 때의 장점은 다음과 같습니다:\n* Python 인터프리터나 Jupyter Notebook 대화형 셸에서 직접 사용할 수 있습니다.\n* 기본 제공되는 보고 기능 외에도 사용자가 직접 DataFrame에 접근하여 분석할 수 있습니다.\n\n```python\n#!\/usr\/bin\/env python\nimport numpy as np\nfrom furiosa.runtime.profiler import RecordFormat, profile\nfrom furiosa.runtime.sync import create_runner\n\n# furiosa-sdk 소스 트리의 루트에서 'examples' 디렉토리를 찾을 수 있습니다.\nmodel_path = \"examples\/assets\/quantized_models\/imagenet_224x224_mobilenet_v1_uint8_quantization-aware-trained_dm_1.0_without_softmax.tflite\"\n\nwith profile(format=RecordFormat.PandasDataFrame) as profiler:\n    with create_runner(model_path) as runner:\n        input_shape = runner.model.input(0).shape\n        with profiler.record(\"warm up\") as record:\n            for _ in range(0, 2):\n                runner.run([np.uint8(np.random.rand(*input_shape))])\n        with profiler.record(\"trace\") as record:\n            for _ in range(0, 2):\n                runner.run([np.uint8(np.random.rand(*input_shape))])\nprofiler.print_summary()  # (1)\nprofiler.print_inferences()  # (2)\nprofiler.print_npu_executions()  # (3)\nprofiler.print_npu_operators()  # (4)\nprofiler.print_external_operators()  # (5)\ndf = profiler.get_pandas_dataframe()  # (6)\nprint(df[df[\"name\"] == \"trace\"][[\"trace_id\", \"name\", \"thread.id\", \"dur\"]])\n```\n\n위는 프로파일링 컨텍스트 형식을 PandasDataFrame으로 지정한 코드 예제입니다. `(1)` 라인이 실행되면 다음과 같은 결과 요약이 생성됩니다.\n\n```plaintext\n================================================\n  Inference Results Summary\n================================================\nInference counts                : 4\nMin latency (ns)                : 1584494\nMax latency (ns)                : 3027309\nMean latency (ns)               : 2136984\nMedian latency (ns)             : 1968066\n90.0 percentile Latency (ns)    : 2752525\n95.0 percentile Latency (ns)    : 2889917\n97.0 percentile Latency (ns)    : 2944874\n99.0 percentile Latency (ns)    : 2999831\n99.9 percentile Latency (ns)    : 3024561\n```\n\n`(2)` 라인이 실행되면, 하나의 추론 쿼리의 지속 시간이 표시됩니다.\n\n```plaintext\n┌──────────────────────────────────┬──────────────────┬───────────┬─────────┐\n│ trace_id                         ┆ span_id          ┆ thread.id ┆ dur     │\n╞══════════════════════════════════╪══════════════════╪═══════════╪═════════╡\n│ 7cf3d3b7439cf4c3fac1a47998783102 ┆ 403ada67f1d8220e ┆ 1         ┆ 3027309 │\n│ 16d65f6f8f1db256d0f39953855dea72 ┆ 78b065c19c3675ef ┆ 1         ┆ 2111363 │\n│ d0534e3a9f19edadab81954ad28ab44f ┆ 9a7addaf0f28c9fe ┆ 1         ┆ 1824769 │\n│ 70512188522f45b87cfe4f545de3cf2c ┆ c75f697f8e72d333 ┆ 1         ┆ 1584494 │\n└──────────────────────────────────┴──────────────────┴───────────┴─────────┘\n```\n\n`(3)` 라인이 실행되면, NPU 실행의 경과 시간이 표시됩니다:\n\n```plaintext\n┌──────────────────────────────────┬──────────────────┬──────────┬─────────────────┬───────────┬─────────┬──────────────────────┐\n│ trace_id                         ┆ span_id          ┆ pe_index ┆ execution_index ┆ NPU Total ┆ NPU Run ┆ NPU IoWait           │\n╞══════════════════════════════════╪══════════════════╪══════════╪═════════════════╪═══════════╪═════════╪══════════════════════╡\n│ 8f6fce6c0e52b4735cae3379732a0943 ┆ 3e1e4a76523cbf89 ┆ 0        ┆ 0               ┆ 119145    ┆ 108134  ┆ 18446744073709540605 │\n│ 195366613b1da9b0350c0a3c2a608f42 ┆ 07dff2e92172fabd ┆ 0        ┆ 0               ┆ 119363    ┆ 108134  ┆ 18446744073709540387 │\n│ 3b65b8fa3eabfaf8f815ec9f41fcc7d9 ┆ 639a366a7f932a23 ┆ 0        ┆ 0               ┆ 119157    ┆ 108134  ┆ 18446744073709540593 │\n│ e48825df32a07e5559f7f50048c08e1f ┆ ecaab4915bfda725 ┆ 0        ┆ 0               ┆ 119219    ┆ 108134  ┆ 18446744073709540531 │\n└──────────────────────────────────┴──────────────────┴──────────┴─────────────────┴───────────┴─────────┴──────────────────────┘\n```\n\n`(4)` 라인이 실행되면, 연산자의 경과 시간이 표시됩니다:\n\n```plaintext\n┌─────────────────────────┬──────────────────────┬───────┐\n│ name                    ┆ average_elapsed (ns) ┆ count │\n╞═════════════════════════╪══════════════════════╪═══════╡\n│ LowLevelConv2d          ┆ 5327.8               ┆ 60    │\n│ LowLevelDepthwiseConv2d ┆ 1412.285714          ┆ 56    │\n│ LowLevelPad             ┆ 575.785714           ┆ 56    │\n│ LowLevelTranspose       ┆ 250.0                ┆ 4     │\n│ LowLevelReshape         ┆ 2.0                  ┆ 240   │\n│ LowLevelSlice           ┆ 2.0                  ┆ 12    │\n│ LowLevelExpand          ┆ 2.0                  ┆ 16    │\n└─────────────────────────┴──────────────────────┴───────┘\n```\n\n`(5)` 라인이 실행되면, CPU에서의 연산자에 대한 시간 데이터가 아래와 같이 표시됩니다.\n\n```plaintext\n┌──────────────────────────────────┬──────────────────┬───────────┬────────────┬────────────────┬────────┐\n│ trace_id                         ┆ span_id          ┆ thread.id ┆ name       ┆ operator_index ┆ dur    │\n╞══════════════════════════════════╪══════════════════╪═══════════╪════════════╪════════════════╪════════╡\n│ e7ab6656cc090a8d05992a9e4683b8b7 ┆ 206a1d6f351ca4b1 ┆ 40        ┆ Quantize   ┆ 0              ┆ 136285 │\n│ 03636fd6c7dbc42f0a9dd29a7283d3fc ┆ f636740983e095a6 ┆ 40        ┆ Lower      ┆ 1              ┆ 133350 │\n│ c9a0858f7e0885a976f51c6cb57d3e0f ┆ bb6c84f88e453055 ┆ 40        ┆ Unlower    ┆ 2              ┆ 44775  │\n│ 8777c67ad9fe597139bbd6970362c2fc ┆ 63bac982c7b98aba ┆ 40        ┆ Dequantize ┆ 3              ┆ 14682  │\n│ 98aeba2a25b0525166b6a4065ab01774 ┆ 34ccd560571d733f ┆ 40        ┆ Quantize   ┆ 0              ┆ 45465  │\n│ 420525dc13ba9624083e0a276f7ee718 ┆ 9f6d342da5eb86bc ┆ 40        ┆ Lower      ┆ 1              ┆ 152748 │\n│ cb67393f6949bbbb396053c1e00931ff ┆ 2d724fa6ab8ca024 ┆ 40        ┆ Unlower    ┆ 2              ┆ 67140  │\n│ 00424b4f02039ae0ca98388a964062b0 ┆ a5fb9fbd5bffe6a6 ┆ 40        ┆ Dequantize ┆ 3              ┆ 32388  │\n│ d7412c59d360067e8b7a2508a30d1079 ┆ 8e426d778fa95722 ┆ 40        ┆ Quantize   ┆ 0              ┆ 71736  │\n│ 6820acf9345c5b373c512f6cd5edcbc7 ┆ 2d787c2df381f010 ┆ 40        ┆ Lower      ┆ 1              ┆ 311310 │\n│ 84d24b02a95c63c3e40f7682384749e4 ┆ 1236a974a619ff1a ┆ 40        ┆ Unlower    ┆ 2              ┆ 51930  │\n│ 8d25dff1cfd6624509cbf95503e93382 ┆ 673efb3bfb8deac6 ┆ 40        ┆ Dequantize ┆ 3              ┆ 12362  │\n│ 4cc60ec1eee7d9f3cdd290d07b303a18 ┆ e7903b0a584d6388 ┆ 40        ┆ Quantize   ┆ 0              ┆ 56736  │\n│ c5f04d9fea26e5b52c6ec5e5406775fc ┆ 701118dabd065e6f ┆ 40        ┆ Lower      ┆ 1              ┆ 265447 │\n│ c5fdfb9cf454da130148e8e364eeee93 ┆ 5cf3750def19c6e8 ┆ 40        ┆ Unlower    ┆ 2              ┆ 35869  │\n│ e1e650d23061140404915f1df36daf9c ┆ ddd76ff19b5cd713 ┆ 40        ┆ Dequantize ┆ 3              ┆ 14688  │\n└──────────────────────────────────┴──────────────────┴───────────┴────────────┴────────────────┴────────┘\n```\n\n`(6)` 라인을 통해 코드에서 DataFrame에 접근하여 직접 분석을 수행할 수 있습니다.\n\n```plaintext\n                            trace_id   name  thread.id       dur\n487  f3b158734e3684f2e043ed41309c4c2d  trace          1  11204385\n```\n\n[이전](performance.html \"성능 최적화\") [다음](serving.html \"모델 서버 (서빙 프레임워크)\")\n\n---\n© 저작권 2023 FuriosaAI, Inc.\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용하였습니다.\n```"},{"page_id":"65a65779-8c0e-48f0-890a-f6ecc20a9f41","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.5.0.html","original_content":"* Release Notes - 0.5.0 * [View page source](..\/_sources\/releases\/0.5.0.rst.txt)\n---\nRelease Notes - 0.5.0 [](#release-notes-0-5-0 \"Permalink to this heading\") ===========================================================================\nFuriosaAI SDK 0.5.0 release includes approximately 87 bug fixes, added functionalities, and improvements. The following are some of the key changes:\nCompiler Improvement [](#compiler-improvement \"Permalink to this heading\") ---------------------------------------------------------------------------\n0.5.0 release adds NPU acceleration support for the following operators. You can find the entire list of acceleration-supported operators at [List of Supported Operators for Warboy Acceleration](..\/npu\/warboy.html#supportedoperators) .\n* BatchNormalization * ConvTranspose * LeakyRelu * Pow * Sqrt * Sub\nAdditionally, we have improved the compiler with this release by adding support for [Opset 13](https:\/\/github.com\/onnx\/onnx\/releases\/tag\/v1.8.0) operator of Onnx.\nSession API Improvement [](#session-api-improvement \"Permalink to this heading\") ---------------------------------------------------------------------------------\nAPI Improvement 1 ( [commit b1d2b74](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/commit\/b1d2b742f9f1ed43dbe7aa5f2ed822ae38d636e4) ): you can now designate NPU device in Session API. With 0.5.0, you can designate NPU device when generating a session, whereas previously you could only designate device to be used through the environment variable `NPU_DEVNAME` . If not explicitly stated, designated device in the environment variable `NPU_DEVNAME` will be used, as was done before.\n``` from furiosa.runtime import session\nsess1 = session.create(\"model1.onnx\", device=\"npu0pe0\") sess2 = session.create(\"model2.onnx\", device=\"npu0pe1\")\n# Asynchronous API async_sess, queue = session.create_async(\"model2.onnx\", device=\"npu1pe2\")\n```\nAPI Improvement 2 ( [commit 4f1f114](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/commit\/4f1f1149d137a58ada31df57de6e1234881ccf5b) ) is support for tensor name. The existing API was limited in that it could only identify the order of the tensor built into the model, and pass it as an input parameter to session.run()\n. From 0.5.0, you can explicitly designate name of input and output tensors as shown below.\n``` np1 = np.random.randint(0, 255, session_input.shape, dtype=np.uint8) np2 = np.random.randint(0, 255, session_input.shape, dtype=np.uint8) sess1.run_with(outputs=[\"output1\"], inputs={   \"input2\": np2,   \"input1\": np1, }\n```\nError Diagnosis Message & Error Handling Improvements [](#error-diagnosis-message-error-handling-improvements \"Permalink to this heading\") -------------------------------------------------------------------------------------------------------------------------------------------\nIn case of an error, version information for debugging and compiler log are output independently. This enables easier bug reporting. You can find more information about reporting issues and customer service at [Bug Report](..\/customer-support\/bugs.html#bugreport) .\n``` >>> from furiosa.runtime import session >>> session.create(\"mnist-8.onnx\") Saving the compilation log into \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log ... 2021-11-22T06:30:28.423371Z ERROR fail to compile the model: the static shape of tensor 'input' contains an unsupported dimension value: Some(DimParam(\"batch_size\")) ================================================================================ Information Dump ================================================================================ - Python version: 3.8.10 (default, Sep 28 2021, 16:10:42)  [GCC 9.3.0] - furiosa-libnux path: libnux.so.0.5.0 - furiosa-libnux version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34) - furiosa-compiler version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34) - furiosa-sdk-runtime version: Furiosa SDK Runtime  (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\nPlease check the compiler log at \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log. If you have a problem, please report the log file to https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals with the information dumped above. ================================================================================\n```\nImprovements such as error handling for the following issues are also included.\n* Error fix when using duplicate devices (   [commit 01aaa40](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/commit\/01aaa40fd31573dc578fa1c805e1ed36decc9088)   ) * Added timeout of CompletionQueue & error handling fix for session connection termination (   [commit 21cba85](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/commit\/21cba85737840546357f2dd709d33d9bc2b00390)   ) * Hanging issue fix for interruption during compiling   [(commit a0f4bd7](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/commit\/a0f4bd7ab8d199e3e46f9820fe4dc75bfa845e61)   )\nIntroducing Furiosa Server (serving framework) [](#introducing-furiosa-server-serving-framework \"Permalink to this heading\") -----------------------------------------------------------------------------------------------------------------------------\n0.5.0 includes Furiosa Server\n, a serving framework that supports GRPC and REST API. You can easily install it by running `pip\ninstall\nfuriosa-sdk[server]` . By running it with the command below, the model can be served immediately with the NPU. You can find more detailed instructions and functions at [Model Server (Serving Framework)](..\/software\/serving.html#modelserving) .\n``` furiosa server \\ --model-path MNISTnet_uint8_quant_without_softmax.tflite \\ --model-name mnist\n```\nIntroducing Furiosa Model package [](#introducing-furiosa-model-package \"Permalink to this heading\") -----------------------------------------------------------------------------------------------------\nFrom 0.5.0, the optimized model for the FuriosaAI NPU can be used directly as a Python package. You can easily install it with the command `pip\ninstall\nfuriosa-sdk[models]` ,\nand can immediately be used in Session API as shown in the following example.\n``` import asyncio\nfrom furiosa.registry import Model from furiosa.models.vision import MLCommonsResNet50 from furiosa.runtime import session\nresnet50: Model = asyncio.run(MLCommonsResNet50()) sess = session.create(resnet50.model, device='npu0pe0')\n```\nCommand line NPU management tool: furiosactl [](#command-line-npu-management-tool-furiosactl \"Permalink to this heading\") --------------------------------------------------------------------------------------------------------------------------\n0.5.0 includes furiosactl, a command line NPU management tool. You can install it with `apt\ninstall\nfuriosa-toolkit` . You can use this tool to check NPU device status, as well as identify idle NPUs. You can find `apt` server configuration instructions at [APT server configuration](..\/software\/installation.html#setupaptrepository) .\n``` $ furiosactl info\n+------+------------------+-------+---------+--------------+---------+ | NPU  | Name             | Temp. | Power   | PCI-BDF      | PCI-DEV | +------+------------------+-------+---------+--------------+---------+ | npu0 | FuriosaAI Warboy |  34°C | 12.92 W | 0000:01:00.0 | 510:0   | +------+------------------+-------+---------+--------------+---------+\n$ furiosactl list +------+-----------+-----------+--------+ | NPU  | DEVNAME   | Type      | Status | +------+-----------+-----------+--------+ | npu0 | npu0      | All PE(s) | Ready  | |      | npu0pe0   | Single PE | Ready  | |      | npu0pe1   | Single PE | Ready  | |      | npu0pe0-1 | PE Fusion | Ready  | +------+-----------+-----------+--------+\n```\nKubernetes support [](#kubernetes-support \"Permalink to this heading\") -----------------------------------------------------------------------\n0.5.0 includes NPU support for Kubernetes. You can install the NPU device plugin and node labeller with the command below, and have the NPU be scheduled together when deploying pods. More details can be found at [Kubernetes Support](..\/software\/kubernetes_support.html#kubernetesintegration) .\n``` kubectl apply -f https:\/\/raw.githubusercontent.com\/furiosa-ai\/furiosa-sdk\/main\/kubernetes\/deployments\/device-plugin.yaml kubectl apply -f https:\/\/raw.githubusercontent.com\/furiosa-ai\/furiosa-sdk\/main\/kubernetes\/deployments\/node-labeller.yaml\n```\n[Previous](0.6.0.html \"Release Notes - 0.6.0\") [Next](..\/customer-support\/bugs.html \"Bug Report\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 릴리스 노트 - 0.5.0 * [페이지 소스 보기](..\/_sources\/releases\/0.5.0.rst.txt)\n---\n릴리스 노트 - 0.5.0 [](#release-notes-0-5-0 \"이 제목의 고유 링크\") ===========================================================================\nFuriosaAI SDK 0.5.0 릴리스에는 약 87개의 버그 수정, 기능 추가 및 개선 사항이 포함되어 있습니다. 다음은 주요 변경 사항입니다:\n컴파일러 개선 [](#compiler-improvement \"이 제목의 고유 링크\") ---------------------------------------------------------------------------\n0.5.0 릴리스에서는 다음 연산자에 대한 NPU 가속 지원이 추가되었습니다. 가속 지원 연산자의 전체 목록은 [Warboy 가속 지원 연산자 목록](..\/npu\/warboy.html#supportedoperators)에서 확인할 수 있습니다.\n* BatchNormalization * ConvTranspose * LeakyRelu * Pow * Sqrt * Sub\n또한, 이번 릴리스에서는 Onnx의 [Opset 13](https:\/\/github.com\/onnx\/onnx\/releases\/tag\/v1.8.0) 연산자에 대한 지원을 추가하여 컴파일러를 개선했습니다.\n세션 API 개선 [](#session-api-improvement \"이 제목의 고유 링크\") ---------------------------------------------------------------------------------\nAPI 개선 1 ( [커밋 b1d2b74](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/commit\/b1d2b742f9f1ed43dbe7aa5f2ed822ae38d636e4) ): 이제 세션 API에서 NPU 장치를 지정할 수 있습니다. 0.5.0에서는 세션을 생성할 때 NPU 장치를 지정할 수 있으며, 이전에는 환경 변수 `NPU_DEVNAME`을 통해서만 사용할 장치를 지정할 수 있었습니다. 명시적으로 지정하지 않으면, 환경 변수 `NPU_DEVNAME`에 지정된 장치가 사용됩니다.\n```python\nfrom furiosa.runtime import session\nsess1 = session.create(\"model1.onnx\", device=\"npu0pe0\")\nsess2 = session.create(\"model2.onnx\", device=\"npu0pe1\")\n# 비동기 API\nasync_sess, queue = session.create_async(\"model2.onnx\", device=\"npu1pe2\")\n```\nAPI 개선 2 ( [커밋 4f1f114](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/commit\/4f1f1149d137a58ada31df57de6e1234881ccf5b) )는 텐서 이름 지원입니다. 기존 API는 모델에 내장된 텐서의 순서만 식별할 수 있었고, 이를 입력 매개변수로 session.run()에 전달할 수 있었습니다. 0.5.0부터는 아래와 같이 입력 및 출력 텐서의 이름을 명시적으로 지정할 수 있습니다.\n```python\nnp1 = np.random.randint(0, 255, session_input.shape, dtype=np.uint8)\nnp2 = np.random.randint(0, 255, session_input.shape, dtype=np.uint8)\nsess1.run_with(outputs=[\"output1\"], inputs={\n  \"input2\": np2,\n  \"input1\": np1,\n})\n```\n오류 진단 메시지 및 오류 처리 개선 [](#error-diagnosis-message-error-handling-improvements \"이 제목의 고유 링크\") -------------------------------------------------------------------------------------------------------------------------------------------\n오류 발생 시, 디버깅을 위한 버전 정보와 컴파일러 로그가 독립적으로 출력됩니다. 이를 통해 버그 보고가 더 쉬워집니다. 문제 보고 및 고객 서비스에 대한 자세한 정보는 [버그 보고](..\/customer-support\/bugs.html#bugreport)에서 확인할 수 있습니다.\n```python\n>>> from furiosa.runtime import session\n>>> session.create(\"mnist-8.onnx\")\nSaving the compilation log into \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log ...\n2021-11-22T06:30:28.423371Z ERROR fail to compile the model: the static shape of tensor 'input' contains an unsupported dimension value: Some(DimParam(\"batch_size\"))\n================================================================================\n정보 덤프\n================================================================================\n- Python 버전: 3.8.10 (default, Sep 28 2021, 16:10:42)  [GCC 9.3.0]\n- furiosa-libnux 경로: libnux.so.0.5.0\n- furiosa-libnux 버전: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34)\n- furiosa-compiler 버전: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34)\n- furiosa-sdk-runtime 버전: Furiosa SDK Runtime  (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\n컴파일러 로그는 \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log에서 확인하세요. 문제가 있을 경우, 위의 정보와 함께 로그 파일을 https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals에 보고해 주세요.\n================================================================================\n```\n다음 문제에 대한 오류 처리 개선도 포함되어 있습니다.\n* 중복 장치 사용 시 오류 수정 (   [커밋 01aaa40](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/commit\/01aaa40fd31573dc578fa1c805e1ed36decc9088)   )\n* CompletionQueue의 타임아웃 추가 및 세션 연결 종료에 대한 오류 처리 수정 (   [커밋 21cba85](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/commit\/21cba85737840546357f2dd709d33d9bc2b00390)   )\n* 컴파일 중단 시 발생하는 중단 문제 수정   [(커밋 a0f4bd7](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/commit\/a0f4bd7ab8d199e3e46f9820fe4dc75bfa845e61)   )\nFuriosa 서버 소개 (서빙 프레임워크) [](#introducing-furiosa-server-serving-framework \"이 제목의 고유 링크\") -----------------------------------------------------------------------------------------------------------------------------\n0.5.0에는 GRPC 및 REST API를 지원하는 서빙 프레임워크인 Furiosa 서버가 포함되어 있습니다. `pip install furiosa-sdk[server]` 명령어를 실행하여 쉽게 설치할 수 있습니다. 아래 명령어로 실행하면 NPU로 모델을 즉시 서빙할 수 있습니다. 자세한 지침 및 기능은 [모델 서버 (서빙 프레임워크)](..\/software\/serving.html#modelserving)에서 확인할 수 있습니다.\n```bash\nfuriosa server \\\n--model-path MNISTnet_uint8_quant_without_softmax.tflite \\\n--model-name mnist\n```\nFuriosa 모델 패키지 소개 [](#introducing-furiosa-model-package \"이 제목의 고유 링크\") -----------------------------------------------------------------------------------------------------\n0.5.0부터 FuriosaAI NPU에 최적화된 모델을 Python 패키지로 직접 사용할 수 있습니다. `pip install furiosa-sdk[models]` 명령어로 쉽게 설치할 수 있으며, 아래 예시와 같이 세션 API에서 즉시 사용할 수 있습니다.\n```python\nimport asyncio\nfrom furiosa.registry import Model\nfrom furiosa.models.vision import MLCommonsResNet50\nfrom furiosa.runtime import session\n\nresnet50: Model = asyncio.run(MLCommonsResNet50())\nsess = session.create(resnet50.model, device='npu0pe0')\n```\n명령줄 NPU 관리 도구: furiosactl [](#command-line-npu-management-tool-furiosactl \"이 제목의 고유 링크\") --------------------------------------------------------------------------------------------------------------------------\n0.5.0에는 명령줄 NPU 관리 도구인 furiosactl이 포함되어 있습니다. `apt install furiosa-toolkit` 명령어로 설치할 수 있습니다. 이 도구를 사용하여 NPU 장치 상태를 확인하고, 유휴 상태의 NPU를 식별할 수 있습니다. `apt` 서버 구성 지침은 [APT 서버 구성](..\/software\/installation.html#setupaptrepository)에서 확인할 수 있습니다.\n```bash\n$ furiosactl info\n+------+------------------+-------+---------+--------------+---------+\n| NPU  | Name             | Temp. | Power   | PCI-BDF      | PCI-DEV |\n+------+------------------+-------+---------+--------------+---------+\n| npu0 | FuriosaAI Warboy |  34°C | 12.92 W | 0000:01:00.0 | 510:0   |\n+------+------------------+-------+---------+--------------+---------+\n\n$ furiosactl list\n+------+-----------+-----------+--------+\n| NPU  | DEVNAME   | Type      | Status |\n+------+-----------+-----------+--------+\n| npu0 | npu0      | All PE(s) | Ready  |\n|      | npu0pe0   | Single PE | Ready  |\n|      | npu0pe1   | Single PE | Ready  |\n|      | npu0pe0-1 | PE Fusion | Ready  |\n+------+-----------+-----------+--------+\n```\n쿠버네티스 지원 [](#kubernetes-support \"이 제목의 고유 링크\") -----------------------------------------------------------------------\n0.5.0에는 쿠버네티스를 위한 NPU 지원이 포함되어 있습니다. 아래 명령어로 NPU 장치 플러그인과 노드 라벨러를 설치할 수 있으며, 파드를 배포할 때 NPU를 함께 스케줄링할 수 있습니다. 자세한 내용은 [쿠버네티스 지원](..\/software\/kubernetes_support.html#kubernetesintegration)에서 확인할 수 있습니다.\n```bash\nkubectl apply -f https:\/\/raw.githubusercontent.com\/furiosa-ai\/furiosa-sdk\/main\/kubernetes\/deployments\/device-plugin.yaml\nkubectl apply -f https:\/\/raw.githubusercontent.com\/furiosa-ai\/furiosa-sdk\/main\/kubernetes\/deployments\/node-labeller.yaml\n```\n[이전](0.6.0.html \"릴리스 노트 - 0.6.0\") [다음](..\/customer-support\/bugs.html \"버그 보고\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용했습니다.\n```"},{"page_id":"b6c45005-84e1-46e6-a185-42be1be00b6e","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/compiler.html","original_content":"* Compiler * [View page source](..\/_sources\/software\/compiler.rst.txt)\n---\nCompiler [](#compiler \"Permalink to this heading\") ===================================================\nThe FuriosaAI compiler compiles models of formats [TFLite](https:\/\/www.tensorflow.org\/lite) and [Onnx](https:\/\/onnx.ai\/) model (( [OpSet 13](https:\/\/github.com\/onnx\/onnx\/blob\/main\/docs\/Changelog.md#version-13-of-the-default-onnx-operator-set) or lower version), thereby generating programs that execute inference using FuriosaAI NPU and resources (CPU, memory, etc) of the host machine. In this process, the compiler analyses the model at the operator level, optimizes it, and generates a program so as to maximize NPU acceleration and host resources utilization. Even for models that are not well known, so long as supported operators are utilized well, you can design models that are optimized for the NPU .\nYou can find the list of NPU acceleration supported operators at [List of Supported Operators for Warboy Acceleration](..\/npu\/warboy.html#supportedoperators) .\n `furiosa-compiler` [](#furiosa-compiler \"Permalink to this heading\") ---------------------------------------------------------------------\nThe most common ways to use a compiler would be to automatically call it during the process of resetting the inference API or the NPU.\nBut you can directly compile a model and generate a program by using the command line tool `furiosa-compiler` in shell. You can install `furiosa-compiler` command via APT package manager.\n``` $ apt install furiosa-compiler\n```\nThe usage of `furiosa-compiler` is as follows:\n``` $ furiosa-compiler --help\nFuriosa SDK Compiler v0.10.0 (f8f05c8ea 2023-07-31T19:30:30Z)\nUsage: furiosa-compiler [OPTIONS] <SOURCE>\nArguments:   <SOURCE>           Path to source file (tflite, onnx, and other IR formats, such as dfg, cdfg, gir, lir)\nOptions:   -o, --output <OUTPUT>           Writes output to <OUTPUT>\n          [default: output.<TARGET_IR>]\n  -b, --batch-size <BATCH_SIZE>           Specifies the batch size which is effective when SOURCE is TFLite, ONNX, or DFG\n      --target-ir <TARGET_IR>           (experimental) Target IR - possible values: [enf]\n          [default: enf]\n      --target-npu <TARGET_NPU>           Target NPU family - possible values: [warboy, warboy-2pe]\n          [default: warboy-2pe]\n      --dot-graph <DOT_GRAPH>           Filename to write DOT-formatted graph to\n      --analyze-memory <ANALYZE_MEMORY>           Analyzes the memory allocation and save the report to <ANALYZE_MEMORY>\n  -v, --verbose           Shows details about the compilation process\n      --no-cache           Disables the compiler result cache\n  -h, --help           Print help (see a summary with '-h')\n  -V, --version           Print version\n```  `SOURCE` is the file path of [TFLite](https:\/\/www.tensorflow.org\/lite) or [ONNX](https:\/\/onnx.ai\/) .\nYou have to use quantized models through [Model Quantization](quantization.html#modelquantization) for NPU accleration.\nYou can omit the option -o OUTPUT\n, and you can also choose to designate the output file name. When omitted, the default output file name is `output.enf` . Here, enf stands for Executable NPU Format. So if you run as shown below, it will generate a `output.enf` file.\n``` furiosa-compiler foo.onnx\n```\nIf you designate the output file name as below, it will generate a `foo.enf` file.\n``` furiosa-compiler foo.onnx -o foo.enf\n```  `--target-npu` lets the generated binary to designate target NPU는.\nTarget NPUs\n[](#id4 \"Permalink to this table\")\n| NPU Family | Number of PEs | Value | | --- | --- | --- | | Warboy | 1 | warboy | | Warboy | 2 | warboy-2pe |\nIf generated program’s target NPU is Warboy that uses one PE independently, you can run the following command.\n``` furiosa-compiler foo.onnx --target-npu warboy\n```\nWhen 2 PEs are fused, execute as follows.\n``` furiosa-compiler foo.onnx --target-npu warboy-2pe\n```\nThe `--batch-size` option lets you specify batch size\n, the number of samples to be passed as input when executing inference through the inference API. The larger the batch size, the higher the NPU utilization, since more data is given as input and executed at once. This allows the inference process to be shared across the batch, increasing efficiency. However, if the larger batch size results in the necessary memory size exceeding NPU DRAM size, the memory I\/O cost between the host and the NPU may increase and lead to significant performance degradation. The default value of batch size is one. Appropriate value can usually be found through trial and error. For reference, the optimal batch sizes for some models included in the [MLPerf™ Inference Edge v2.0](https:\/\/mlcommons.org\/en\/inference-edge-20\/) benchmark are as follows.\nOptimal Batch Size for Well-known Models\n[](#id5 \"Permalink to this table\")\n| Model | Optimal Batch | | --- | --- | | SSD-MobileNets-v1 | 2 | | Resnet50-v1.5 | 1 | | SSD-ResNet34 | 1 |\nIf your desired batch size is two, you can run the following command.\n``` furiosa-compiler foo.onnx --batch-size 2\n```\nUsing ENF files [](#using-enf-files \"Permalink to this heading\") -----------------------------------------------------------------\nAfter the compilation process, the final output of the FuriosaAI compiler is ENF (Executable NPU Format) type data. In general, the compilation process takes from a few seconds to several minutes depending on the model. Once you have the ENF file, you can reuse it to omit this compilation process.\nThis may be useful if you need to frequently create sessions or serve one model across several machines in an actual operation environment.\nFor example, you can first create an ENF file by referring to [furiosa-compiler](#compilercli) .\nThen, with [PythonSDK](python-sdk.html#pythonsdk) as shown below, you can instantly create a runner without the compilation process by passing the ENF file as an argument to the `create_runner()` function as follows:\n``` from furiosa.runtime import sync\nwith sync.create_runner(\"path\/to\/model.enf\") as runner:   outputs = runner.run(inputs)\n```\nCompiler Cache [](#compiler-cache \"Permalink to this heading\") ---------------------------------------------------------------\nCompiler cache allows to user applications to reuse once-compiled results. It’s very helpful especially when you are developing applications because the compilation usually takes at least a couple of minutes.\nBy default, the compiler cache uses a local file system ( `$HOME\/.cache\/furiosa\/compiler` ) as a cache storage. If you specify a configuration, you can also use Redis as a remote and distributed cache storage.\nThe compiler cache is enabled by default, but you can explicitly enable or disable the cache by setting `FC_CACHE_ENABLED` . This setting is effective in CLI tools, Python SDK, and serving frameworks.\n``` # Enable Compiler Cache export FC_CACHE_ENABLED=1 # Disable Compiler Cache export FC_CACHE_ENABLED=0\n```\nThe default cache location is `$HOME\/.cache\/furiosa\/compiler` , but you can explicitly specify the cache storage by setting the shell environment variable `FC_CACHE_STORE_URL` . If you want to Redis as a cache storage, you can specify some URLs starting with `redis:\/\/` or `rediss:\/\/` (over SSL).\n``` # When you want to specify a cache directory export FC_CACHE_STORE_URL=\/tmp\/cache\n# When you want to specify a Redis cluster as the cache storage export FC_CACHE_STORE_URL=redis:\/\/:<PASSWORD>@127.0.0.1:6379 # When you want to specify a Redis cluster over SSL as the cache storage export FC_CACHE_STORE_URL=rediss:\/\/:<PASSWORD>@127.0.0.1:25945\n```\nThe cache will be valid for 30 days by default, but you can explicitly specify the cache lifetime by setting seconds to the environment variable `FC_CACHE_LIFETIME` .\n``` # 2 hours cache lifetime export FC_CACHE_LIFETIME=7200\n```\nAlso, you can control more the cache behavior according to your purpose as follows:\nCache behaviors according to `FC_CACHE_LIFETIME`  [](#id6 \"Permalink to this table\")\n| Value (secs) | Description | Example | | --- | --- | --- | | *N* > 0 | Cache will be alive for N secs | 7200 (2 hours) | | 0 | All previous cache will be invalidated. (When you want to compile the model without cache) | 0 | | *N* < 0 | Cache will be alive forever without expiration. (it can be useful when you want read-only cache) | -1 |\n[Previous](cli.html \"Command Line Tools\") [Next](quantization.html \"Model Quantization\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 컴파일러 * [페이지 소스 보기](..\/_sources\/software\/compiler.rst.txt)\n---\n컴파일러 [](#compiler \"이 제목의 고유 링크\") ===================================================\nFuriosaAI 컴파일러는 [TFLite](https:\/\/www.tensorflow.org\/lite) 및 [Onnx](https:\/\/onnx.ai\/) 모델 형식을 컴파일하여 FuriosaAI NPU와 호스트 머신의 자원(CPU, 메모리 등)을 사용하여 추론을 실행하는 프로그램을 생성합니다. 이 과정에서 컴파일러는 모델을 연산자 수준에서 분석하고 최적화하여 NPU 가속과 호스트 자원 활용을 극대화하는 프로그램을 생성합니다. 잘 알려지지 않은 모델일지라도 지원되는 연산자를 잘 활용하면 NPU에 최적화된 모델을 설계할 수 있습니다. NPU 가속을 지원하는 연산자의 목록은 [Warboy 가속을 위한 지원 연산자 목록](..\/npu\/warboy.html#supportedoperators)에서 확인할 수 있습니다.\n\n`furiosa-compiler` [](#furiosa-compiler \"이 제목의 고유 링크\") ---------------------------------------------------------------------\n컴파일러를 사용하는 가장 일반적인 방법은 추론 API 또는 NPU를 재설정하는 과정에서 자동으로 호출하는 것입니다. 하지만 셸에서 명령줄 도구 `furiosa-compiler`를 사용하여 모델을 직접 컴파일하고 프로그램을 생성할 수 있습니다. `furiosa-compiler` 명령은 APT 패키지 관리자를 통해 설치할 수 있습니다.\n```bash\n$ apt install furiosa-compiler\n```\n`furiosa-compiler`의 사용법은 다음과 같습니다:\n```bash\n$ furiosa-compiler --help\nFuriosa SDK Compiler v0.10.0 (f8f05c8ea 2023-07-31T19:30:30Z)\nUsage: furiosa-compiler [OPTIONS] <SOURCE>\nArguments:   <SOURCE>           소스 파일 경로 (tflite, onnx 및 dfg, cdfg, gir, lir과 같은 다른 IR 형식)\nOptions:   -o, --output <OUTPUT>           <OUTPUT>에 출력 기록\n          [default: output.<TARGET_IR>]\n  -b, --batch-size <BATCH_SIZE>           SOURCE가 TFLite, ONNX 또는 DFG일 때 효과적인 배치 크기 지정\n      --target-ir <TARGET_IR>           (실험적) 대상 IR - 가능한 값: [enf]\n          [default: enf]\n      --target-npu <TARGET_NPU>           대상 NPU 패밀리 - 가능한 값: [warboy, warboy-2pe]\n          [default: warboy-2pe]\n      --dot-graph <DOT_GRAPH>           DOT 형식의 그래프를 기록할 파일 이름\n      --analyze-memory <ANALYZE_MEMORY>           메모리 할당을 분석하고 보고서를 <ANALYZE_MEMORY>에 저장\n  -v, --verbose           컴파일 과정에 대한 세부 정보 표시\n      --no-cache           컴파일러 결과 캐시 비활성화\n  -h, --help           도움말 출력 (요약은 '-h'로 확인)\n  -V, --version           버전 출력\n```\n`SOURCE`는 [TFLite](https:\/\/www.tensorflow.org\/lite) 또는 [ONNX](https:\/\/onnx.ai\/)의 파일 경로입니다. NPU 가속을 위해서는 [모델 양자화](quantization.html#modelquantization)를 통해 양자화된 모델을 사용해야 합니다. 옵션 -o OUTPUT을 생략할 수 있으며, 출력 파일 이름을 지정할 수도 있습니다. 생략하면 기본 출력 파일 이름은 `output.enf`입니다. 여기서 enf는 실행 가능한 NPU 형식을 의미합니다. 아래와 같이 실행하면 `output.enf` 파일이 생성됩니다.\n```bash\nfuriosa-compiler foo.onnx\n```\n출력 파일 이름을 아래와 같이 지정하면 `foo.enf` 파일이 생성됩니다.\n```bash\nfuriosa-compiler foo.onnx -o foo.enf\n```\n`--target-npu` 옵션은 생성된 바이너리가 대상 NPU를 지정할 수 있게 합니다.\n대상 NPU [](#id4 \"이 표의 고유 링크\")\n| NPU 패밀리 | PE 수 | 값 | | --- | --- | --- | | Warboy | 1 | warboy | | Warboy | 2 | warboy-2pe |\n생성된 프로그램의 대상 NPU가 하나의 PE를 독립적으로 사용하는 Warboy인 경우, 다음 명령을 실행할 수 있습니다.\n```bash\nfuriosa-compiler foo.onnx --target-npu warboy\n```\n2개의 PE가 결합된 경우, 다음과 같이 실행합니다.\n```bash\nfuriosa-compiler foo.onnx --target-npu warboy-2pe\n```\n`--batch-size` 옵션은 추론 API를 통해 추론을 실행할 때 입력으로 전달될 샘플 수를 지정할 수 있습니다. 배치 크기가 클수록 더 많은 데이터가 한 번에 입력되고 실행되므로 NPU 활용도가 높아집니다. 이는 배치 전체에 걸쳐 추론 프로세스를 공유하여 효율성을 높입니다. 그러나 배치 크기가 커져 필요한 메모리 크기가 NPU DRAM 크기를 초과하면 호스트와 NPU 간의 메모리 I\/O 비용이 증가하여 성능 저하가 발생할 수 있습니다. 배치 크기의 기본값은 1입니다. 적절한 값은 보통 시행착오를 통해 찾을 수 있습니다. 참고로, [MLPerf™ Inference Edge v2.0](https:\/\/mlcommons.org\/en\/inference-edge-20\/) 벤치마크에 포함된 일부 모델의 최적 배치 크기는 다음과 같습니다.\n잘 알려진 모델의 최적 배치 크기 [](#id5 \"이 표의 고유 링크\")\n| 모델 | 최적 배치 | | --- | --- | | SSD-MobileNets-v1 | 2 | | Resnet50-v1.5 | 1 | | SSD-ResNet34 | 1 |\n원하는 배치 크기가 2인 경우, 다음 명령을 실행할 수 있습니다.\n```bash\nfuriosa-compiler foo.onnx --batch-size 2\n```\nENF 파일 사용 [](#using-enf-files \"이 제목의 고유 링크\") -----------------------------------------------------------------\n컴파일 과정 후, FuriosaAI 컴파일러의 최종 출력은 ENF(Executable NPU Format) 형식의 데이터입니다. 일반적으로 컴파일 과정은 모델에 따라 몇 초에서 몇 분이 소요됩니다. ENF 파일을 얻으면 이 컴파일 과정을 생략할 수 있습니다. 이는 실제 운영 환경에서 여러 머신에 걸쳐 하나의 모델을 자주 생성하거나 제공해야 할 때 유용할 수 있습니다. 예를 들어, [furiosa-compiler](#compilercli)를 참조하여 먼저 ENF 파일을 생성할 수 있습니다. 그런 다음, 아래와 같이 [PythonSDK](python-sdk.html#pythonsdk)를 사용하여 ENF 파일을 `create_runner()` 함수의 인수로 전달하여 컴파일 과정 없이 즉시 러너를 생성할 수 있습니다:\n```python\nfrom furiosa.runtime import sync\nwith sync.create_runner(\"path\/to\/model.enf\") as runner:\n    outputs = runner.run(inputs)\n```\n컴파일러 캐시 [](#compiler-cache \"이 제목의 고유 링크\") ---------------------------------------------------------------\n컴파일러 캐시는 한 번 컴파일된 결과를 사용자 애플리케이션이 재사용할 수 있게 합니다. 이는 특히 애플리케이션을 개발할 때 매우 유용한데, 컴파일은 보통 몇 분 이상 소요되기 때문입니다. 기본적으로 컴파일러 캐시는 로컬 파일 시스템(`$HOME\/.cache\/furiosa\/compiler`)을 캐시 저장소로 사용합니다. 설정을 지정하면 Redis를 원격 및 분산 캐시 저장소로 사용할 수도 있습니다. 컴파일러 캐시는 기본적으로 활성화되어 있지만, `FC_CACHE_ENABLED`를 설정하여 캐시를 명시적으로 활성화하거나 비활성화할 수 있습니다. 이 설정은 CLI 도구, Python SDK 및 서빙 프레임워크에서 유효합니다.\n```bash\n# 컴파일러 캐시 활성화\nexport FC_CACHE_ENABLED=1\n# 컴파일러 캐시 비활성화\nexport FC_CACHE_ENABLED=0\n```\n기본 캐시 위치는 `$HOME\/.cache\/furiosa\/compiler`이지만, 셸 환경 변수 `FC_CACHE_STORE_URL`을 설정하여 캐시 저장소를 명시적으로 지정할 수 있습니다. Redis를 캐시 저장소로 사용하려면 `redis:\/\/` 또는 `rediss:\/\/`(SSL을 통해)로 시작하는 URL을 지정할 수 있습니다.\n```bash\n# 캐시 디렉토리를 지정하고 싶을 때\nexport FC_CACHE_STORE_URL=\/tmp\/cache\n# Redis 클러스터를 캐시 저장소로 지정하고 싶을 때\nexport FC_CACHE_STORE_URL=redis:\/\/:<PASSWORD>@127.0.0.1:6379\n# SSL을 통해 Redis 클러스터를 캐시 저장소로 지정하고 싶을 때\nexport FC_CACHE_STORE_URL=rediss:\/\/:<PASSWORD>@127.0.0.1:25945\n```\n캐시는 기본적으로 30일 동안 유효하지만, 환경 변수 `FC_CACHE_LIFETIME`에 초 단위로 설정하여 캐시 수명을 명시적으로 지정할 수 있습니다.\n```bash\n# 2시간 캐시 수명\nexport FC_CACHE_LIFETIME=7200\n```\n또한, 목적에 따라 캐시 동작을 더 제어할 수 있습니다:\n`FC_CACHE_LIFETIME`에 따른 캐시 동작 [](#id6 \"이 표의 고유 링크\")\n| 값 (초) | 설명 | 예시 | | --- | --- | --- | | *N* > 0 | 캐시는 N 초 동안 유효 | 7200 (2시간) | | 0 | 이전 모든 캐시가 무효화됩니다. (캐시 없이 모델을 컴파일하고 싶을 때) | 0 | | *N* < 0 | 캐시는 만료 없이 영원히 유효합니다. (읽기 전용 캐시가 필요할 때 유용) | -1 |\n[이전](cli.html \"명령줄 도구\") [다음](quantization.html \"모델 양자화\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용했습니다.\n```"},{"page_id":"a50ce0f1-25f2-4882-a3e5-8da4cb7215b7","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/references.html","original_content":"* References * [View page source](..\/_sources\/software\/references.rst.txt)\n---\nReferences [](#references \"Permalink to this heading\") =======================================================\n* [C Language SDK Reference](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/c\/index.html) * [Python SDK Reference](..\/api\/python\/modules.html) * [Model Zoo Reference](https:\/\/furiosa-ai.github.io\/furiosa-models\/v0.10.0\/)\n[Previous](tutorials.html \"Tutorial and Code Examples\") [Next](..\/api\/python\/modules.html \"Furiosa SDK 0.10.1 API Documentation\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"\n* 참고 문서 * [페이지 소스 보기](..\/_sources\/software\/references.rst.txt)\n---\n참고 문서 [](#references \"이 제목의 고유 링크\") =======================================================\n* [C 언어 SDK 참조](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/c\/index.html) * [Python SDK 참조](..\/api\/python\/modules.html) * [모델 동물원 참조](https:\/\/furiosa-ai.github.io\/furiosa-models\/v0.10.0\/)\n[이전](tutorials.html \"튜토리얼 및 코드 예제\") [다음](..\/api\/python\/modules.html \"Furiosa SDK 0.10.1 API 문서\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용하여 [Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었습니다."},{"page_id":"6c328d98-54c8-4c8e-bea7-5927c3921609","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/serving.html","original_content":"* Model Server (Serving Framework) * [View page source](..\/_sources\/software\/serving.rst.txt)\n---\nModel Server (Serving Framework) [](#model-server-serving-framework \"Permalink to this heading\") =================================================================================================\nTo serve DNN models through GRPC and REST API, you can use [Furiosa Model Server](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/main\/python\/furiosa-server) .\nModel Server provides the endpoints compatible with [KServe Predict Protocol Version 2](https:\/\/github.com\/kserve\/kserve\/blob\/master\/docs\/predict-api\/v2\/required_api.md) .\nIts major features are:\n> * REST\/GRPC endpoints support > * Multiple model serving using multiple NPU devices\nInstallation [](#installation \"Permalink to this heading\") -----------------------------------------------------------\nIts requirements are:\n* Ubuntu 20.04 LTS (Debian bullseye) or higher * [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) * Python 3.8 or higher version\nIf you need Python environment, please refer to [Python execution environment setup](python-sdk.html#setuppython) first.\nInstallation using PIP\nInstallation from source code\nRun the following command\n``` $ pip install 'furiosa-sdk[server]'\n```\nCheck out the source code and run the following command\n``` $ git clone https:\/\/github.com\/furiosa-ai\/furiosa-sdk.git $ cd furiosa-sdk\/python\/furiosa-server $ pip install .\n```\nRunning a Model Server [](#running-a-model-server \"Permalink to this heading\") -------------------------------------------------------------------------------\nYou can run model sever command by running `furiosa\nserver` in your shell.\nTo run simply a model server with `tflite` or `onnx` , you need to specify just the model path and its name as follows:\n``` $ cd furiosa-sdk $ furiosa server \\ --model-path examples\/assets\/quantized_models\/MNISTnet_uint8_quant_without_softmax.tflite \\ --model-name mnist\n```  `--model-path` option allows to specify a path of a model file. If you want to use a specific binding address and port, you can use additionally `--host` , `--host-port` .\nPlease run `furiosa\nserver\n--help` if you want to learn more about the command with various options.\n``` $ furiosa server --help Usage: furiosa server [OPTIONS]\n    Start serving models from FuriosaAI model server\nOptions:     --log-level [ERROR|INFO|WARN|DEBUG|TRACE]                                     [default: LogLevel.INFO]     --model-path TEXT               Path to Model file (tflite, onnx are                                     supported)     --model-name TEXT               Model name used in URL path     --model-version TEXT            Model version used in URL path  [default:                                     default]     --host TEXT                     IP address to bind  [default: 0.0.0.0]     --http-port INTEGER             HTTP port to listen to requests  [default:                                     8080]     --model-config FILENAME         Path to a config file about models with                                     specific configurations     --server-config FILENAME        Path to Model file (tflite, onnx are                                     supported)     --install-completion [bash|zsh|fish|powershell|pwsh]                                     Install completion for the specified shell.     --show-completion [bash|zsh|fish|powershell|pwsh]                                     Show completion for the specified shell, to                                     copy it or customize the installation.     --help                          Show this message and exit.\n```\nRunning a Model Server with a Configuration File [](#running-a-model-server-with-a-configuration-file \"Permalink to this heading\") -----------------------------------------------------------------------------------------------------------------------------------\nIf you need more advanced configurations like compilation options and device options, you can use a configuration file based on Yaml.\n``` model_config_list:   - name: mnist     model: \"samples\/data\/MNISTnet_uint8_quant.tflite\"     version: \"1\"     platform: npu     npu_device: warboy(1)*1     compiler_config:       keep_unsignedness: true       split_unit: 0   - name: ssd     model: \"samples\/data\/SSD512_MOBILENET_V2_BDD_int_without_reshape.tflite\"     version: \"1\"     platform: npu     npu_device: warboy(1)*1\n```\nWhen you run a model sever with a configuration file, you need to specify `--model-config` as follows. You can find the model files described in the above example from [furiosa-models\/samples](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/main\/python\/furiosa-server\/samples) .\n``` $ cd furiosa-sdk\/python\/furiosa-server $ furiosa server --model-config samples\/model_config_example.yaml libfuriosa_hal.so --- v0.11.0, built @ 43c901f 2023-08-02T07:42:42.263133Z  INFO furiosa_rt_core::driver::event_driven::device: DeviceManager has detected 1 NPUs 2023-08-02T07:42:42.267247Z  INFO furiosa_rt_core::driver::event_driven::device: [1] npu:6:1 (warboy-b0, 64dpes) 2023-08-02T07:42:42.267264Z  INFO furiosa_rt_core::driver::event_driven::coord: furiosa-rt (v0.10.0-rc6, rev: d021ff71d, built_at: 2023-07-31T19:05:26Z) is being initialized 2023-08-02T07:42:42.267269Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:1-io-thread-0 thread has started 2023-08-02T07:42:42.267398Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:1-commit-thread thread has started 2023-08-02T07:42:42.267405Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:1-io-thread-1 thread has started 2023-08-02T07:42:42.270837Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libcompiler 0.10.0 (rev: f8f05c built: 2023-07-26T09:49:17Z) 2023-08-02T07:42:42.270851Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libhal-warboy 0.11.0 (rev: 43c901f built: 2023-04-19T14:04:55Z) 2023-08-02T07:42:42.271144Z  INFO furiosa_rt_core::driver::event_driven::coord: [NONAME] Runtime has started 2023-08-02T07:42:42.273772Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0001 is being loaded to npu:6:1 2023-08-02T07:42:42.283260Z  INFO furiosa_rt_core::driver::event_driven::coord: Compiling Model#0001 (target: warboy-b0, 64dpes, file: MNISTnet_uint8_quant.tflite, size: 18.2 kiB) 2023-08-02T07:42:42.299091Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0001 has been compiled successfully (took 0 secs) 2023-08-02T07:42:42.299293Z  INFO furiosa_rt_core::dag: Task Statistics: TaskStats { cpu: 5, npu: 1, alias: 0, coalesce: 0 } 2023-08-02T07:42:42.300701Z  INFO furiosa_rt_core::driver::event_driven::coord: NpuApi (AsyncNpuApiImpl) has started.. 2023-08-02T07:42:42.300721Z  INFO furiosa_rt_core::driver::event_driven::coord: Creating 1 Contexts on npu:6:1 (DRAM usage: 6.0 kiB \/ 16.0 GiB, SRAM usage: 124.0 kiB \/ 64.0 MiB) 2023-08-02T07:42:42.300789Z  INFO furiosa_rt_core::driver::event_driven::coord: npu:6:1 has scheduled to Model#0001 2023-08-02T07:42:42.304216Z  WARN furiosa_rt_core::consts::envs: NPU_DEVNAME will be deprecated. Use FURIOSA_DEVICES instead. 2023-08-02T07:42:42.313084Z  INFO furiosa_rt_core::driver::event_driven::device: DeviceManager has detected 1 NPUs 2023-08-02T07:42:42.315470Z  INFO furiosa_rt_core::driver::event_driven::device: [1] npu:6:0 (warboy-b0, 64dpes) 2023-08-02T07:42:42.315483Z  INFO furiosa_rt_core::driver::event_driven::coord: furiosa-rt (v0.10.0-rc6, rev: d021ff71d, built_at: 2023-07-31T19:05:26Z) is being initialized 2023-08-02T07:42:42.315560Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:0-io-thread-1 thread has started 2023-08-02T07:42:42.315610Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:0-io-thread-0 thread has started 2023-08-02T07:42:42.315657Z  INFO furiosa_rt_core::npu::async_impl::threaded: npu:6:0-commit-thread thread has started 2023-08-02T07:42:42.319127Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libcompiler 0.10.0 (rev: f8f05c built: 2023-07-26T09:49:17Z) 2023-08-02T07:42:42.319141Z  INFO furiosa_rt_core::driver::event_driven::coord: Loaded libhal-warboy 0.11.0 (rev: 43c901f built: 2023-04-19T14:04:55Z) 2023-08-02T07:42:42.319364Z  INFO furiosa_rt_core::driver::event_driven::coord: [NONAME] Runtime has started 2023-08-02T07:42:42.324283Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0002 is being loaded to npu:6:0 2023-08-02T07:42:42.333521Z  INFO furiosa_rt_core::driver::event_driven::coord: Compiling Model#0002 (target: warboy-b0, 64dpes, file: SSD512_MOBILENET_V2_BDD_int_without_reshape.tflite, size: 5.2 MiB) 2023-08-02T07:42:42.814260Z  INFO furiosa_rt_core::driver::event_driven::coord: Model#0002 has been compiled successfully (took 0 secs) 2023-08-02T07:42:42.815406Z  INFO furiosa_rt_core::dag: Task Statistics: TaskStats { cpu: 26, npu: 1, alias: 0, coalesce: 0 } 2023-08-02T07:42:42.893745Z  INFO furiosa_rt_core::driver::event_driven::coord: NpuApi (AsyncNpuApiImpl) has started.. 2023-08-02T07:42:42.893772Z  INFO furiosa_rt_core::driver::event_driven::coord: Creating 1 Contexts on npu:6:0 (DRAM usage: 1.0 MiB \/ 16.0 GiB, SRAM usage: 14.8 MiB \/ 64.0 MiB) 2023-08-02T07:42:42.894265Z  INFO furiosa_rt_core::driver::event_driven::coord: npu:6:0 has scheduled to Model#0002 INFO:     Started server process [2448540] INFO:     Waiting for application startup. INFO:     Application startup complete. INFO:     Uvicorn running on http:\/\/0.0.0.0:8080 (Press CTRL+C to quit)\n```\nOnce a model server starts up, you can call the inference request through HTTP protocol. If the model name is `mnist` and its version `1` , the endpoint of the model will be `http:\/\/<host>:<port>\/v2\/models\/mnist\/version\/1\/infer` , accepting `POST` http request. The following is an example using `curl` to send the inference request and return the response.\nThe following is a Python example, doing same as `curl` does in the above example.\n``` import requests import mnist import numpy as np\nmnist_images = mnist.train_images().reshape((60000, 1, 28, 28)).astype(np.uint8) url = 'http:\/\/localhost:8080\/v2\/models\/mnist\/versions\/1\/infer'\ndata = mnist_images[0:1].flatten().tolist() request = {     \"inputs\": [{         \"name\":         \"mnist\",         \"datatype\": \"UINT8\",         \"shape\": (1, 1, 28, 28),         \"data\": data     }] }\nresponse = requests.post(url, json=request) print(response.json())\n```\nEndpoints [](#endpoints \"Permalink to this heading\") -----------------------------------------------------\nThe following table shows REST API endpoints and its descriptions. The model server is following KServe Predict Protocol Version 2. So, you can find more details from [KServe Predict Protocol Version 2 - HTTP\/REST](https:\/\/github.com\/kserve\/kserve\/blob\/master\/docs\/predict-api\/v2\/required_api.md#httprest) .\nEndpoints of KServe Predict Protocol Version 2\n[](#id1 \"Permalink to this table\")\n| Method and Endpoint | Description | | --- | --- | | GET \/v2\/health\/live | Returns HTTP Ok (200) if the inference server is able to receive and respond to metadata and inference requests. This API can be directly used for the Kubernetes livenessProbe. | | GET \/v2\/health\/ready | Returns HTTP Ok (200) if all the models are ready for inferencing. This API can be directly used for the Kubernetes readinessProbe. | | GET \/v2\/models\/${MODEL\\_NAME}\/versions\/${MODEL\\_VERSION} | Returns a model metadata | | GET \/v2\/models\/${MODEL\\_NAME}\/versions\/${MODEL\\_VERSION}\/ready | Returns HTTP Ok (200) if a specific model is ready for inferencing. | | POST \/v2\/models\/${MODEL\\_NAME}[\/versions\/${MODEL\\_VERSION}]\/infer | Inference request |\n[Previous](profiler.html \"Performance Profiling\") [Next](kubernetes_support.html \"Kubernetes Support\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 모델 서버 (서빙 프레임워크) * [페이지 소스 보기](..\/_sources\/software\/serving.rst.txt)\n---\n모델 서버 (서빙 프레임워크) [](#model-server-serving-framework \"이 제목의 고유 링크\") =================================================================================================\nDNN 모델을 GRPC 및 REST API를 통해 서비스하려면 [Furiosa Model Server](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/main\/python\/furiosa-server)를 사용할 수 있습니다. 모델 서버는 [KServe Predict Protocol Version 2](https:\/\/github.com\/kserve\/kserve\/blob\/master\/docs\/predict-api\/v2\/required_api.md)와 호환되는 엔드포인트를 제공합니다. 주요 기능은 다음과 같습니다:\n> * REST\/GRPC 엔드포인트 지원 > * 여러 NPU 장치를 사용한 다중 모델 서빙\n설치 [](#installation \"이 제목의 고유 링크\") -----------------------------------------------------------\n요구 사항은 다음과 같습니다:\n* Ubuntu 20.04 LTS (Debian bullseye) 이상 * [드라이버, 펌웨어 및 런타임 설치](installation.html#requiredpackages) * Python 3.8 이상 버전\nPython 환경이 필요하다면, 먼저 [Python 실행 환경 설정](python-sdk.html#setuppython)을 참조하세요.\nPIP를 사용한 설치\n소스 코드에서 설치\n다음 명령어를 실행하세요\n```bash\n$ pip install 'furiosa-sdk[server]'\n```\n소스 코드를 확인하고 다음 명령어를 실행하세요\n```bash\n$ git clone https:\/\/github.com\/furiosa-ai\/furiosa-sdk.git\n$ cd furiosa-sdk\/python\/furiosa-server\n$ pip install .\n```\n모델 서버 실행 [](#running-a-model-server \"이 제목의 고유 링크\") -------------------------------------------------------------------------------\n쉘에서 `furiosa server` 명령어를 실행하여 모델 서버를 실행할 수 있습니다. `tflite` 또는 `onnx` 모델 서버를 간단히 실행하려면 모델 경로와 이름만 지정하면 됩니다:\n```bash\n$ cd furiosa-sdk\n$ furiosa server \\\n  --model-path examples\/assets\/quantized_models\/MNISTnet_uint8_quant_without_softmax.tflite \\\n  --model-name mnist\n```\n`--model-path` 옵션은 모델 파일의 경로를 지정할 수 있습니다. 특정 바인딩 주소와 포트를 사용하려면 `--host`, `--host-port`를 추가로 사용할 수 있습니다. 다양한 옵션에 대해 더 알고 싶다면 `furiosa server --help`를 실행하세요.\n```bash\n$ furiosa server --help\nUsage: furiosa server [OPTIONS]\n    Start serving models from FuriosaAI model server\nOptions:\n    --log-level [ERROR|INFO|WARN|DEBUG|TRACE]                                     [default: LogLevel.INFO]\n    --model-path TEXT               Path to Model file (tflite, onnx are                                     supported)\n    --model-name TEXT               Model name used in URL path\n    --model-version TEXT            Model version used in URL path  [default:                                     default]\n    --host TEXT                     IP address to bind  [default: 0.0.0.0]\n    --http-port INTEGER             HTTP port to listen to requests  [default:                                     8080]\n    --model-config FILENAME         Path to a config file about models with                                     specific configurations\n    --server-config FILENAME        Path to Model file (tflite, onnx are                                     supported)\n    --install-completion [bash|zsh|fish|powershell|pwsh]                                     Install completion for the specified shell.\n    --show-completion [bash|zsh|fish|powershell|pwsh]                                     Show completion for the specified shell, to                                     copy it or customize the installation.\n    --help                          Show this message and exit.\n```\n설정 파일을 사용한 모델 서버 실행 [](#running-a-model-server-with-a-configuration-file \"이 제목의 고유 링크\") -----------------------------------------------------------------------------------------------------------------------------------\n컴파일 옵션 및 장치 옵션과 같은 고급 설정이 필요하다면 Yaml 기반의 설정 파일을 사용할 수 있습니다.\n```yaml\nmodel_config_list:\n  - name: mnist\n    model: \"samples\/data\/MNISTnet_uint8_quant.tflite\"\n    version: \"1\"\n    platform: npu\n    npu_device: warboy(1)*1\n    compiler_config:\n      keep_unsignedness: true\n      split_unit: 0\n  - name: ssd\n    model: \"samples\/data\/SSD512_MOBILENET_V2_BDD_int_without_reshape.tflite\"\n    version: \"1\"\n    platform: npu\n    npu_device: warboy(1)*1\n```\n설정 파일을 사용하여 모델 서버를 실행할 때는 `--model-config`를 다음과 같이 지정해야 합니다. 위 예제에 설명된 모델 파일은 [furiosa-models\/samples](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/main\/python\/furiosa-server\/samples)에서 찾을 수 있습니다.\n```bash\n$ cd furiosa-sdk\/python\/furiosa-server\n$ furiosa server --model-config samples\/model_config_example.yaml\n```\n모델 서버가 시작되면 HTTP 프로토콜을 통해 추론 요청을 보낼 수 있습니다. 모델 이름이 `mnist`이고 버전이 `1`이라면, 모델의 엔드포인트는 `http:\/\/<host>:<port>\/v2\/models\/mnist\/version\/1\/infer`가 되며, `POST` HTTP 요청을 받습니다. 다음은 `curl`을 사용하여 추론 요청을 보내고 응답을 반환하는 예제입니다.\n```bash\ncurl -X POST http:\/\/<host>:<port>\/v2\/models\/mnist\/version\/1\/infer -d '{\"inputs\": [{\"name\": \"mnist\", \"datatype\": \"UINT8\", \"shape\": [1, 1, 28, 28], \"data\": [0, 0, 0, ...]}]}'\n```\n다음은 위의 예제에서 `curl`과 동일한 작업을 수행하는 Python 예제입니다.\n```python\nimport requests\nimport mnist\nimport numpy as np\n\nmnist_images = mnist.train_images().reshape((60000, 1, 28, 28)).astype(np.uint8)\nurl = 'http:\/\/localhost:8080\/v2\/models\/mnist\/versions\/1\/infer'\ndata = mnist_images[0:1].flatten().tolist()\nrequest = {\n    \"inputs\": [{\n        \"name\": \"mnist\",\n        \"datatype\": \"UINT8\",\n        \"shape\": (1, 1, 28, 28),\n        \"data\": data\n    }]\n}\nresponse = requests.post(url, json=request)\nprint(response.json())\n```\n엔드포인트 [](#endpoints \"이 제목의 고유 링크\") -----------------------------------------------------\n다음 표는 REST API 엔드포인트와 그 설명을 보여줍니다. 모델 서버는 KServe Predict Protocol Version 2를 따릅니다. 따라서 [KServe Predict Protocol Version 2 - HTTP\/REST](https:\/\/github.com\/kserve\/kserve\/blob\/master\/docs\/predict-api\/v2\/required_api.md#httprest)에서 더 많은 세부 정보를 찾을 수 있습니다.\nKServe Predict Protocol Version 2의 엔드포인트\n[](#id1 \"이 표의 고유 링크\")\n| 메서드 및 엔드포인트 | 설명 |\n| --- | --- |\n| GET \/v2\/health\/live | 추론 서버가 메타데이터 및 추론 요청을 수신하고 응답할 수 있으면 HTTP Ok (200)를 반환합니다. 이 API는 Kubernetes livenessProbe에 직접 사용할 수 있습니다. |\n| GET \/v2\/health\/ready | 모든 모델이 추론 준비가 되면 HTTP Ok (200)를 반환합니다. 이 API는 Kubernetes readinessProbe에 직접 사용할 수 있습니다. |\n| GET \/v2\/models\/${MODEL\\_NAME}\/versions\/${MODEL\\_VERSION} | 모델 메타데이터를 반환합니다 |\n| GET \/v2\/models\/${MODEL\\_NAME}\/versions\/${MODEL\\_VERSION}\/ready | 특정 모델이 추론 준비가 되면 HTTP Ok (200)를 반환합니다. |\n| POST \/v2\/models\/${MODEL\\_NAME}[\/versions\/${MODEL\\_VERSION}]\/infer | 추론 요청 |\n[이전](profiler.html \"성능 프로파일링\") [다음](kubernetes_support.html \"Kubernetes 지원\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용하였습니다.\n```"},{"page_id":"4336ecbc-28cf-41e6-863f-25212b860e38","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/tutorials.html","original_content":"* Tutorial and Code Examples * [View page source](..\/_sources\/software\/tutorials.rst.txt)\n---\nTutorial and Code Examples [](#tutorial-and-code-examples \"Permalink to this heading\") =======================================================================================\nTutorial [](#id1 \"Permalink to this heading\") ----------------------------------------------\n* [How to use Furiosa SDK from Start to Finish](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/HowToUseFuriosaSDKFromStartToFinish.ipynb) * [Basic Inference API](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/GettingStartedWithPythonSDK.ipynb) * [Advanced Inference API](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/AdvancedTopicsInInferenceAPIs.ipynb)\nCode Examples [](#code-examples \"Permalink to this heading\") -------------------------------------------------------------\n* [Comparing Accuracy with CPU-based inference](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/InferenceAccuracyCheck.ipynb) * [Image Classification Model Inference](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/Image_Classification.ipynb) * [SSD Object Detection Model Inference](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/SSD_Object_Detection.ipynb) * [Other Python Code Examples](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/branch-0.10.0\/examples\/inferences)\n[Previous](vm_support.html \"Configuring Warboy Pass-through for Virtual Machine\") [Next](references.html \"References\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 튜토리얼 및 코드 예제 * [페이지 소스 보기](..\/_sources\/software\/tutorials.rst.txt)\n---\n튜토리얼 및 코드 예제 [](#tutorial-and-code-examples \"이 제목의 영구 링크\") =======================================================================================\n튜토리얼 [](#id1 \"이 제목의 영구 링크\") ----------------------------------------------\n* [Furiosa SDK 사용법: 시작부터 끝까지](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/HowToUseFuriosaSDKFromStartToFinish.ipynb) \n* [기본 추론 API](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/GettingStartedWithPythonSDK.ipynb) \n* [고급 추론 API](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/AdvancedTopicsInInferenceAPIs.ipynb)\n\n코드 예제 [](#code-examples \"이 제목의 영구 링크\") -------------------------------------------------------------\n* [CPU 기반 추론과 정확도 비교](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/InferenceAccuracyCheck.ipynb) \n* [이미지 분류 모델 추론](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/Image_Classification.ipynb) \n* [SSD 객체 탐지 모델 추론](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/branch-0.10.0\/examples\/notebooks\/SSD_Object_Detection.ipynb) \n* [기타 Python 코드 예제](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/branch-0.10.0\/examples\/inferences)\n\n[이전](vm_support.html \"가상 머신을 위한 Warboy 패스스루 구성\") [다음](references.html \"참조\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용하였습니다.\n```"},{"page_id":"e527d72f-b0b3-4132-b935-b89558bc7add","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.8.0.html","original_content":"* Release Notes - 0.8.0 * [View page source](..\/_sources\/releases\/0.8.0.rst.txt)\n---\nRelease Notes - 0.8.0 [](#release-notes-0-8-0 \"Permalink to this heading\") ===========================================================================\nFuriosa SDK 0.8.0 is a major release, including many performance enhancements, additional functions, and bug fixes. 0.8.0 also includes the serving framework, a core tool of user application development, as well as major improvements to the Model Zoo.\nComponent Version Information\n[](#id3 \"Permalink to this table\")\n| Package Name | Version | | --- | --- | | NPU Driver | 1.4.0 | | NPU Firmware Tools | 1.2.0 | | NPU Firmware Image | 1.2.0 | | HAL (Hardware Abstraction Layer) | 0.9.0 | | Furiosa Compiler | 0.8.0 | | Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..) | 0.8.0 | | NPU Device Plugin | 0.10.1 | | NPU Feature Discovery | 0.2.0 | | NPU Management CLI (furiosactl) | 0.10.0 |\nInstalling the latest SDK [](#installing-the-latest-sdk \"Permalink to this heading\") -------------------------------------------------------------------------------------\nIf you are using APT repository, the upgrade process is simpler.\n> ``` > apt-get update && apt-get upgrade >  > ```\nIf you wish to designate a specific package for upgrade, execute as below: You can find more details about APT repository setup at [Driver, Firmware, and Runtime Installation](..\/software\/installation.html#requiredpackages) .\n> ``` > apt-get update && \\ > apt-get install -y furiosa-driver-pdma furiosa-libhal-warboy furiosa-libnux furiosactl >  > ```\nYou can upgrade firmware as follows:\n> ``` > apt-get update && \\ > apt-get install -y furiosa-firmware-tools furiosa-firmware-image >  > ```\nYou can upgrade Python package as follows:\n> ``` > pip install --upgrade furiosa-sdk >  > ```\nMajor changes [](#major-changes \"Permalink to this heading\") -------------------------------------------------------------\n### Improvements to serving framework API [](#improvements-to-serving-framework-api \"Permalink to this heading\")\nThe [furiosa-serving](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/branch-0.8.0\/python\/furiosa-serving) is a FastAPI-based serving framework. With the [furiosa-serving](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/branch-0.8.0\/python\/furiosa-serving) ,\nusers can quickly develop Python-based high performance web service applications that utilize NPUs. The 0.8.0 release includes the following major updates.\n**Session pool that enables model serving with multiple NPUs**\nSession pools improve significantly throughput of model APIs by using multiple NPUs. If large inputs can be divided into a number of small inputs, this improvement can be used to reduce the latency of serving applications.\n``` model: NPUServeModel = synchronous(serve.model(\"nux\"))(     \"MNIST\",     location=\".\/assets\/models\/MNISTnet_uint8_quant_without_softmax.tflite\",     # Specify multiple devices     npu_device=\"npu0pe0,npu0pe1,npu1pe0\"     worker_num=4, )\n```\n**Shift from thread-based to asyncio-based NPU query processing**\nSmall and frequent NPU inference queries may now be processed with lower latency. As shown in the example below, applications requiring multiple NPU inferences in a single API query can be processed with better performance.\n``` async def inference(self, tensors: List[np.ndarray]) -> List[np.ndarray]:     # The following code runs multiple inferences at the same time and wait until all requests are completed.     return await asyncio.gather(*(self.model.predict(tensor) for tensor in tensors))\n```\n**Added expanded support for external device & runtime**\nIn complex serving scenarios, additional\/external device and runtime programs may be required, in addition to NPU-based Furiosa Runtime. In this release, the framework has been expanded such that external device and runtime may be used. The first external runtime added is OpenVINO.\n``` imagenet: ServeModel = synchronous(serve.model(\"openvino\"))(     'imagenet',     location='.\/examples\/assets\/models\/image_classification.onnx' )\n```\n**Support for S3 cloud storage repository**\nSet model `location` as S3 URL.\n``` # Load model from S3 (Auth environment variable for aioboto library required) densenet: ServeModel = synchronous(serve.model(\"nux\"))(     'imagenet',  location='s3:\/\/furiosa\/models\/93d63f654f0f192cc4ff5691be60fb9379e9d7fd' )\n```\n**Support for OpenTelemetry compatible tracing**\nWith the [OpenTelemetry Collector](https:\/\/opentelemetry.io\/docs\/collector\/) function, you can now track the execution time of specific code sections of the serving applications.\nTo use this function, you can activate `trace.get_tracer()` , reset the tracer, activate the `tracer.start_as_current_span()` function, and designate the section.\n``` from opentelemetry import trace\ntracer = trace.get_tracer(__name__)\nclass Application:\n        async def process(self, image: Image.Image) -> int:             with tracer.start_as_current_span(\"preprocess\"):                 input_tensors = self.preprocess(image)             with tracer.start_as_current_span(\"inference\"):                 output_tensors = await self.inference(input_tensors)             with tracer.start_as_current_span(\"postprocess\"):                 return self.postprocess(output_tensors)\n```\nThe specification of [OpenTelemetry Collector](https:\/\/opentelemetry.io\/docs\/collector\/) can be done through the configuration of `FURIOSA_SERVING_OTLP_ENDPOINT` , as shown below. The following diagram is an example that visualizes the tracing result with Grafana.\nOther major improvements are as follows:\n* Several inference requests can be executed at once, with serving API now supporting compiler setting   `batch_size` * More threads can share the NPU, with serving API now supporting session option   `worker_num`  ### Profiler [](#profiler \"Permalink to this heading\")\nYou can now analyze the profiler tracing results with [Pandas](https:\/\/pandas.pydata.org\/) ,\na data analysis framework. With this function, you can analyze the tracing result data, allowing you to quickly identify bottlenecks and reasons for model performance changes. More detailed instructions can be found at [Trace analysis using Pandas DataFrame](..\/software\/profiler.html#pandasprofilinganalysis) .\n``` from furiosa.runtime import session, tensor from furiosa.runtime.profiler import RecordFormat, profile\nwith profile(format=RecordFormat.PandasDataFrame) as profiler:     with session.create(\"MNISTnet_uint8_quant_without_softmax.tflite\") as sess:         input_shape = sess.input(0)\n        with profiler.record(\"record\") as record:             for _ in range(0, 2):                 sess.run(tensor.rand(input_shape))\ndf = profiler.get_pandas_dataframe() print(df[df[\"name\"] == \"trace\"][[\"trace_id\", \"name\", \"thread.id\", \"dur\"]])\n```\n### Quantization tool [](#quantization-tool \"Permalink to this heading\")\n[Model Quantization](..\/software\/quantization.html#modelquantization) is a tool that converts pre-trained models to quantized models. This release includes the following major updates.\n* Accuracy improvement when processing SiLU operator * Improved usability of compiler setting   `without_quantize` * Accuracy improvement when processing MatMul\/Gemm operators * Accuracy improvement when processing Add\/Sub\/Mul\/Div operators * NPU acceleration now added for more auto\\_pad properties, when processing Conv\/ConvTranspose\/MaxPool operators * NPU acceleration support for PRelu operator\n### furiosa-toolkit [](#furiosa-toolkit \"Permalink to this heading\")\nThe `furiosactl` command line tool, which has been added to the furiosa-toolkit 0.10.0 release, includes the following improvements.\nThe newly added furiosactl ps\ncommand allows you to print the OS processes which are occupying the NPU device.\n``` # furiosactl ps +-----------+--------+------------------------------------------------------------+ | NPU       | PID    | CMD                                                        | +-----------+--------+------------------------------------------------------------+ | npu0pe0-1 | 132529 | \/usr\/bin\/python3 \/usr\/local\/bin\/uvicorn image_classify:app | +-----------+--------+------------------------------------------------------------+\n```\nThe furiosactl info\ncommand now prints the unique UUID for each device.\n``` $ furiosactl info +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+ | NPU  | Name   | UUID                                 | Firmware        | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+ | npu0 | warboy | 72212674-61BE-4FCA-A2C9-555E4EE67AB5 | v1.1.0, 12180b0 |  49°C | 3.12 W | 0000:24:00.0 | 235:0   | +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+ | npu1 | warboy | DF80FB54-8190-44BC-B9FB-664FA36C754A | v1.1.0, 12180b0 |  54°C | 2.53 W | 0000:6d:00.0 | 511:0   | +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+\n```\nDetailed instructions on installation and usage for furiosactl\ncan be found in [furiosa-toolkit](..\/software\/cli.html#toolkit) .\n### Model Zoo API improvements, added models, and added native post-processing code [](#model-zoo-api-improvements-added-models-and-added-native-post-processing-code \"Permalink to this heading\")\n[furioa-models](https:\/\/furiosa-ai.github.io\/furiosa-models) is a public Model Zoo project, providing FuriosaAI NPU-optimized models. The 0.8.0 release includes the following major updates.\n**YOLOv5 Large\/Medium models added**\nSupport for `YOLOv5l` , `YOLOv5m` , which are SOTA object detection models, have been added. The total list of available models can be found in [Model List](https:\/\/furiosa-ai.github.io\/furiosa-models\/v0.8.0\/#model_list) .\n**Improvements to model class and loading API**\nThe model class has been improved to include pre and post-processing code, while the model loading API has been improved as shown below.\nMore explanation on model class and the API can be found at [Model Object](https:\/\/furiosa-ai.github.io\/furiosa-models\/latest\/model_object\/) .\nBlocking API\nNonblocking API\nBefore update\n``` from furiosa.models.vision import MLCommonsResNet50\nresnet50 = MLCommonsResNet50()\n```\nUpdated code\n``` from furiosa.models.vision import ResNet50\nresnet50 = ResNet50.load()\n```\nBefore update\n``` import asyncio\nfrom furiosa.models.nonblocking.vision import MLCommonsResNet50\nresnet50: Model = asyncio.run(MLCommonsResNet50())\n```\n0.8.0 improvements\n``` import asyncio\nfrom furiosa.models.vision import ResNet50\nresnet50: Model = asyncio.run(ResNet50.load_async())\n```\nThe model post-processing process converts the inference ouput tensor into structural data, which is more accessible for the application. Depending on the model, this may require a longer execution time. The 0.8.0 release includes native post-processing code for ResNet50, SSD-MobileNet, and SSD-ResNet34. Based on internal benchmarks, native post-processing code can reduce latency by up to 70%, depending on the model.\nThe following is a complete example of ResNet50, utilizing native post-processing code. More information can be found at [Pre\/Postprocessing](https:\/\/furiosa-ai.github.io\/furiosa-models\/v0.8.0\/model_object\/#prepostprocessing) .\n> ``` > from furiosa.models.vision import ResNet50 > from furiosa.models.vision.resnet50 import NativePostProcessor, preprocess > from furiosa.runtime import session >  > model = ResNet50.load() >  > postprocessor = NativePostProcessor(model) > with session.create(model) as sess: >     image = preprocess(\"tests\/assets\/cat.jpg\") >     output = sess.run(image).numpy() >     postprocessor.eval(output) >  > ```\nOther changes and updates can be found at [Furiosa Model - 0.8.0 Changelogs](https:\/\/furiosa-ai.github.io\/furiosa-models\/v0.8.0\/changelog\/) .\n[Previous](0.9.0.html \"Release Notes - 0.9.0\") [Next](0.7.0.html \"Release Notes - 0.7.0\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 릴리스 노트 - 0.8.0 * [페이지 소스 보기](..\/_sources\/releases\/0.8.0.rst.txt)\n---\n릴리스 노트 - 0.8.0 [](#release-notes-0-8-0 \"이 제목의 영구 링크\") ===========================================================================\nFuriosa SDK 0.8.0은 주요 릴리스로, 많은 성능 향상, 추가 기능 및 버그 수정을 포함하고 있습니다. 0.8.0에는 사용자 애플리케이션 개발의 핵심 도구인 서빙 프레임워크와 모델 동물원의 주요 개선 사항도 포함되어 있습니다.\n구성 요소 버전 정보\n[](#id3 \"이 표의 영구 링크\")\n| 패키지 이름 | 버전 | | --- | --- | | NPU 드라이버 | 1.4.0 | | NPU 펌웨어 도구 | 1.2.0 | | NPU 펌웨어 이미지 | 1.2.0 | | HAL (하드웨어 추상화 레이어) | 0.9.0 | | Furiosa 컴파일러 | 0.8.0 | | Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..) | 0.8.0 | | NPU 디바이스 플러그인 | 0.10.1 | | NPU 기능 탐지 | 0.2.0 | | NPU 관리 CLI (furiosactl) | 0.10.0 |\n\n최신 SDK 설치 [](#installing-the-latest-sdk \"이 제목의 영구 링크\") -------------------------------------------------------------------------------------\nAPT 저장소를 사용 중이라면 업그레이드 과정이 더 간단합니다.\n> ``` > apt-get update && apt-get upgrade >  > ```\n특정 패키지를 지정하여 업그레이드하려면 아래와 같이 실행하십시오. APT 저장소 설정에 대한 자세한 내용은 [드라이버, 펌웨어 및 런타임 설치](..\/software\/installation.html#requiredpackages)에서 확인할 수 있습니다.\n> ``` > apt-get update && \\ > apt-get install -y furiosa-driver-pdma furiosa-libhal-warboy furiosa-libnux furiosactl >  > ```\n펌웨어를 다음과 같이 업그레이드할 수 있습니다:\n> ``` > apt-get update && \\ > apt-get install -y furiosa-firmware-tools furiosa-firmware-image >  > ```\nPython 패키지를 다음과 같이 업그레이드할 수 있습니다:\n> ``` > pip install --upgrade furiosa-sdk >  > ```\n\n주요 변경 사항 [](#major-changes \"이 제목의 영구 링크\") -------------------------------------------------------------\n### 서빙 프레임워크 API 개선 [](#improvements-to-serving-framework-api \"이 제목의 영구 링크\")\n[furiosa-serving](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/branch-0.8.0\/python\/furiosa-serving)은 FastAPI 기반의 서빙 프레임워크입니다. [furiosa-serving](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/branch-0.8.0\/python\/furiosa-serving)을 사용하면 사용자는 NPU를 활용하는 고성능 웹 서비스 애플리케이션을 빠르게 개발할 수 있습니다. 0.8.0 릴리스에는 다음과 같은 주요 업데이트가 포함되어 있습니다.\n**다중 NPU를 사용한 모델 서빙을 가능하게 하는 세션 풀**\n세션 풀은 다중 NPU를 사용하여 모델 API의 처리량을 크게 향상시킵니다. 큰 입력을 여러 작은 입력으로 나눌 수 있다면, 이 개선 사항을 통해 서빙 애플리케이션의 지연 시간을 줄일 수 있습니다.\n``` model: NPUServeModel = synchronous(serve.model(\"nux\"))(     \"MNIST\",     location=\".\/assets\/models\/MNISTnet_uint8_quant_without_softmax.tflite\",     # 여러 장치를 지정     npu_device=\"npu0pe0,npu0pe1,npu1pe0\"     worker_num=4, )\n```\n**스레드 기반에서 asyncio 기반의 NPU 쿼리 처리로 전환**\n작고 빈번한 NPU 추론 쿼리는 이제 더 낮은 지연 시간으로 처리될 수 있습니다. 아래 예시와 같이, 단일 API 쿼리에서 여러 NPU 추론이 필요한 애플리케이션은 더 나은 성능으로 처리될 수 있습니다.\n``` async def inference(self, tensors: List[np.ndarray]) -> List[np.ndarray]:     # 다음 코드는 여러 추론을 동시에 실행하고 모든 요청이 완료될 때까지 기다립니다.     return await asyncio.gather(*(self.model.predict(tensor) for tensor in tensors))\n```\n**외부 장치 및 런타임에 대한 확장된 지원 추가**\n복잡한 서빙 시나리오에서는 NPU 기반 Furiosa Runtime 외에도 추가\/외부 장치 및 런타임 프로그램이 필요할 수 있습니다. 이번 릴리스에서는 외부 장치 및 런타임을 사용할 수 있도록 프레임워크가 확장되었습니다. 추가된 첫 번째 외부 런타임은 OpenVINO입니다.\n``` imagenet: ServeModel = synchronous(serve.model(\"openvino\"))(     'imagenet',     location='.\/examples\/assets\/models\/image_classification.onnx' )\n```\n**S3 클라우드 스토리지 저장소 지원**\n모델 `location`을 S3 URL로 설정합니다.\n``` # S3에서 모델 로드 (aioboto 라이브러리의 인증 환경 변수 필요) densenet: ServeModel = synchronous(serve.model(\"nux\"))(     'imagenet',  location='s3:\/\/furiosa\/models\/93d63f654f0f192cc4ff5691be60fb9379e9d7fd' )\n```\n**OpenTelemetry 호환 추적 지원**\n[OpenTelemetry Collector](https:\/\/opentelemetry.io\/docs\/collector\/) 기능을 통해 서빙 애플리케이션의 특정 코드 섹션의 실행 시간을 추적할 수 있습니다. 이 기능을 사용하려면 `trace.get_tracer()`를 활성화하고, 트레이서를 재설정하며, `tracer.start_as_current_span()` 기능을 활성화하고 섹션을 지정할 수 있습니다.\n``` from opentelemetry import trace\ntracer = trace.get_tracer(__name__)\nclass Application:\n        async def process(self, image: Image.Image) -> int:             with tracer.start_as_current_span(\"preprocess\"):                 input_tensors = self.preprocess(image)             with tracer.start_as_current_span(\"inference\"):                 output_tensors = await self.inference(input_tensors)             with tracer.start_as_current_span(\"postprocess\"):                 return self.postprocess(output_tensors)\n```\n[OpenTelemetry Collector](https:\/\/opentelemetry.io\/docs\/collector\/)의 사양은 `FURIOSA_SERVING_OTLP_ENDPOINT`의 구성으로 수행할 수 있으며, 아래 다이어그램은 Grafana로 추적 결과를 시각화한 예입니다.\n다른 주요 개선 사항은 다음과 같습니다:\n* 여러 추론 요청을 한 번에 실행할 수 있으며, 서빙 API는 이제 컴파일러 설정 `batch_size`를 지원합니다. * 더 많은 스레드가 NPU를 공유할 수 있으며, 서빙 API는 이제 세션 옵션 `worker_num`을 지원합니다.\n\n### 프로파일러 [](#profiler \"이 제목의 영구 링크\")\n이제 [Pandas](https:\/\/pandas.pydata.org\/)라는 데이터 분석 프레임워크를 사용하여 프로파일러 추적 결과를 분석할 수 있습니다. 이 기능을 통해 추적 결과 데이터를 분석하여 모델 성능 변화의 병목 현상과 원인을 빠르게 식별할 수 있습니다. 자세한 지침은 [Pandas DataFrame을 사용한 추적 분석](..\/software\/profiler.html#pandasprofilinganalysis)에서 확인할 수 있습니다.\n``` from furiosa.runtime import session, tensor from furiosa.runtime.profiler import RecordFormat, profile\nwith profile(format=RecordFormat.PandasDataFrame) as profiler:     with session.create(\"MNISTnet_uint8_quant_without_softmax.tflite\") as sess:         input_shape = sess.input(0)\n        with profiler.record(\"record\") as record:             for _ in range(0, 2):                 sess.run(tensor.rand(input_shape))\ndf = profiler.get_pandas_dataframe() print(df[df[\"name\"] == \"trace\"][[\"trace_id\", \"name\", \"thread.id\", \"dur\"]])\n```\n\n### 양자화 도구 [](#quantization-tool \"이 제목의 영구 링크\")\n[모델 양자화](..\/software\/quantization.html#modelquantization)는 사전 학습된 모델을 양자화된 모델로 변환하는 도구입니다. 이번 릴리스에는 다음과 같은 주요 업데이트가 포함되어 있습니다.\n* SiLU 연산자 처리 시 정확도 향상 * 컴파일러 설정 `without_quantize`의 사용성 개선 * MatMul\/Gemm 연산자 처리 시 정확도 향상 * Add\/Sub\/Mul\/Div 연산자 처리 시 정확도 향상 * Conv\/ConvTranspose\/MaxPool 연산자 처리 시 더 많은 auto_pad 속성에 대한 NPU 가속 추가 * PRelu 연산자에 대한 NPU 가속 지원\n\n### furiosa-toolkit [](#furiosa-toolkit \"이 제목의 영구 링크\")\nfuriosa-toolkit 0.10.0 릴리스에 추가된 `furiosactl` 명령줄 도구에는 다음과 같은 개선 사항이 포함되어 있습니다.\n새로 추가된 furiosactl ps 명령을 통해 NPU 장치를 점유하고 있는 OS 프로세스를 출력할 수 있습니다.\n``` # furiosactl ps +-----------+--------+------------------------------------------------------------+ | NPU       | PID    | CMD                                                        | +-----------+--------+------------------------------------------------------------+ | npu0pe0-1 | 132529 | \/usr\/bin\/python3 \/usr\/local\/bin\/uvicorn image_classify:app | +-----------+--------+------------------------------------------------------------+\n```\nfuriosactl info 명령은 이제 각 장치의 고유 UUID를 출력합니다.\n``` $ furiosactl info +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+ | NPU  | Name   | UUID                                 | Firmware        | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+ | npu0 | warboy | 72212674-61BE-4FCA-A2C9-555E4EE67AB5 | v1.1.0, 12180b0 |  49°C | 3.12 W | 0000:24:00.0 | 235:0   | +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+ | npu1 | warboy | DF80FB54-8190-44BC-B9FB-664FA36C754A | v1.1.0, 12180b0 |  54°C | 2.53 W | 0000:6d:00.0 | 511:0   | +------+--------+--------------------------------------+-----------------+-------+--------+--------------+---------+\n```\nfuriosactl 설치 및 사용에 대한 자세한 지침은 [furiosa-toolkit](..\/software\/cli.html#toolkit)에서 확인할 수 있습니다.\n\n### 모델 동물원 API 개선, 추가 모델 및 네이티브 후처리 코드 추가 [](#model-zoo-api-improvements-added-models-and-added-native-post-processing-code \"이 제목의 영구 링크\")\n[furioa-models](https:\/\/furiosa-ai.github.io\/furiosa-models)는 FuriosaAI NPU에 최적화된 모델을 제공하는 공개 모델 동물원 프로젝트입니다. 0.8.0 릴리스에는 다음과 같은 주요 업데이트가 포함되어 있습니다.\n**YOLOv5 Large\/Medium 모델 추가**\n최신 객체 탐지 모델인 `YOLOv5l`, `YOLOv5m`에 대한 지원이 추가되었습니다. 사용 가능한 모델의 전체 목록은 [모델 목록](https:\/\/furiosa-ai.github.io\/furiosa-models\/v0.8.0\/#model_list)에서 확인할 수 있습니다.\n**모델 클래스 및 로딩 API 개선**\n모델 클래스는 전처리 및 후처리 코드를 포함하도록 개선되었으며, 모델 로딩 API는 아래와 같이 개선되었습니다. 모델 클래스 및 API에 대한 자세한 설명은 [모델 객체](https:\/\/furiosa-ai.github.io\/furiosa-models\/latest\/model_object\/)에서 확인할 수 있습니다.\n차단 API\n비차단 API\n업데이트 전\n``` from furiosa.models.vision import MLCommonsResNet50\nresnet50 = MLCommonsResNet50()\n```\n업데이트된 코드\n``` from furiosa.models.vision import ResNet50\nresnet50 = ResNet50.load()\n```\n업데이트 전\n``` import asyncio\nfrom furiosa.models.nonblocking.vision import MLCommonsResNet50\nresnet50: Model = asyncio.run(MLCommonsResNet50())\n```\n0.8.0 개선 사항\n``` import asyncio\nfrom furiosa.models.vision import ResNet50\nresnet50: Model = asyncio.run(ResNet50.load_async())\n```\n모델 후처리 과정은 추론 출력 텐서를 애플리케이션에서 더 쉽게 접근할 수 있는 구조적 데이터로 변환합니다. 모델에 따라 이 과정은 더 긴 실행 시간이 필요할 수 있습니다. 0.8.0 릴리스에는 ResNet50, SSD-MobileNet, SSD-ResNet34에 대한 네이티브 후처리 코드가 포함되어 있습니다. 내부 벤치마크에 따르면, 네이티브 후처리 코드는 모델에 따라 최대 70%까지 지연 시간을 줄일 수 있습니다.\n다음은 네이티브 후처리 코드를 활용한 ResNet50의 완전한 예입니다. 자세한 정보는 [전\/후처리](https:\/\/furiosa-ai.github.io\/furiosa-models\/v0.8.0\/model_object\/#prepostprocessing)에서 확인할 수 있습니다.\n> ``` > from furiosa.models.vision import ResNet50 > from furiosa.models.vision.resnet50 import NativePostProcessor, preprocess > from furiosa.runtime import session >  > model = ResNet50.load() >  > postprocessor = NativePostProcessor(model) > with session.create(model) as sess: >     image = preprocess(\"tests\/assets\/cat.jpg\") >     output = sess.run(image).numpy() >     postprocessor.eval(output) >  > ```\n다른 변경 사항 및 업데이트는 [Furiosa Model - 0.8.0 변경 로그](https:\/\/furiosa-ai.github.io\/furiosa-models\/v0.8.0\/changelog\/)에서 확인할 수 있습니다.\n[이전](0.9.0.html \"릴리스 노트 - 0.9.0\") [다음](0.7.0.html \"릴리스 노트 - 0.7.0\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며 [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용하였습니다.\n```"},{"page_id":"a643c5bc-d22e-493d-bee8-90e6b58fc9dd","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/kubernetes_support.html","original_content":"* Kubernetes Support * [View page source](..\/_sources\/software\/kubernetes_support.rst.txt)\n---\nKubernetes Support [](#kubernetes-support \"Permalink to this heading\") =======================================================================\n[Kuberentes](https:\/\/kubernetes.io\/) is an open source platform for managing containerized workloads and services. Furiosa SDK provides the following components to support the Kubernetes environment.\n* FuriosaAI NPU Device Plugin (   [Introduction to Kubernetes Device Plugin](https:\/\/kubernetes.io\/docs\/concepts\/extend-kubernetes\/compute-storage-net\/device-plugins\/)   ) * FuriosaAI NPU Feature Discovery (   [Introduction to Node Feature Discovery](https:\/\/kubernetes-sigs.github.io\/node-feature-discovery\/stable\/get-started\/index.html)   )\nThe two components above provide the following functions.\n* Make the Kubernetes cluster aware of the NPUs available to the node. * Through Kubernetes   `spec.containers[].resources.limits`   , schedule the NPU simultaneously when distributing Pod workload. * Identify NPU information of NPU-equipped machine, and register it as node label (you can selectively schedule Pods with this information and   nodeSelector      )      + The node-feature-discovery needs to be installed to the cluster, and the     `nfd-worker`     Pod must be running in the nodes equipped with NPUs.\nThe setup process for Kubernetes support is as follows.\n1. Preparing NPU nodes [](#preparing-npu-nodes \"Permalink to this heading\") ----------------------------------------------------------------------------\nRequirements for Kubernetes nodes are as follows.\n* Ubuntu 20.04 or higher * Intel compatible CPU\nYou also need to install NPU driver and toolkit on each node of NPU-equipped Kubernetes. If the APT server is set up (see [APT server configuration](installation.html#setupaptrepository) ), you can easily install as follows.\n``` apt-get update && apt install -y furiosa-driver-warboy furiosa-toolkit\n```\nOnce the required package is installed as above, you can check for NPU recognition as follows, with the `furiosactl` command included in furiosa-toolkit. If the NPU is not recognized with the command below, try again after rebooting - depending on the environment.\n``` $ furiosactl info +------+------------------+-------+--------+--------------+---------+ | NPU  | Name             | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+------------------+-------+--------+--------------+---------+ | npu0 | FuriosaAI Warboy |  40°C | 1.37 W | 0000:01:00.0 | 509:0   | +------+------------------+-------+--------+--------------+---------+\n```\n2. Installing Node Feature Discovery [](#installing-node-feature-discovery \"Permalink to this heading\") --------------------------------------------------------------------------------------------------------\nIn order to make Kubernetes to recognize NPUs, you need to install Node Feature Discovery. By running the command as shown in the example below, if there is a node label that begins with `feature.node.kubernetes.io\/...` , Node Feature Discovery’s DaemonSet has already been installed\n``` $ kubectl get no -o json | jq '.items[].metadata.labels' {\n  \"beta.kubernetes.io\/arch\": \"amd64\",   \"beta.kubernetes.io\/os\": \"linux\",   \"feature.node.kubernetes.io\/cpu-cpuid.ADX\": \"true\",   \"feature.node.kubernetes.io\/cpu-cpuid.AESNI\": \"true\",   ...\n```\n* If you do not have the Node Feature Discovery in your cluster, refer to the following document.      > + [Quick start \/ Installation](https:\/\/kubernetes-sigs.github.io\/node-feature-discovery\/v0.11\/get-started\/quick-start.html#installation) * The following options must be applied when executing Node Feature Discovery.      + `beta.furiosa.ai`     needs to be included in the     `--extra-label-ns`     option of     `nfd-master`   + In the config file of     `nfd-worker`     ,     \\* Only     `vendor`     in the     `sources.pci.deviceLabelFields`     value     \\*     `\"12\"`     must be included as a value in     `sources.pci.deviceClassWhitelist`  Note\nInstalling Node Feature Discovery is not mandatory, but is recommended. The next step will explain the additional tasks that must be performed if you are not using Node Feature Discovery.\n3. Installing Device Plugin and NPU Feature Discovery [](#installing-device-plugin-and-npu-feature-discovery \"Permalink to this heading\") ------------------------------------------------------------------------------------------------------------------------------------------\nWhen the NPU node is ready, install Device Plugin and NPU Feature Discovery’s DaemonSet as follows.\n``` kubectl apply -f https:\/\/raw.githubusercontent.com\/furiosa-ai\/furiosa-sdk\/v0.7.0\/kubernetes\/deployments\/device-plugin.yaml kubectl apply -f https:\/\/raw.githubusercontent.com\/furiosa-ai\/furiosa-sdk\/v0.7.0\/kubernetes\/deployments\/npu-feature-discovery.yaml\n```\nAfter executing the above command, you can check whether the installed daemonset is functioning normally with the `kubectl\nget\ndaemonset\n-n\nkube-system` command. For reference, the DaemonSet is distributed only to nodes equipped with NPUs, and uses `alpha.furiosa.ai\/npu.family=warboy` information that the Node Feature Discovery ( `feature.node.kubernetes.io\/pci-1ed2.present=true` ) attaches to each node.\n```  $ kubectl get daemonset -n kube-system NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                      AGE  furiosa-device-plugin          3         3         3       3            3           feature.node.kubernetes.io\/pci-1ed2.present=true   128m  furiosa-npu-feature-discovery  3         3         3       3            3           feature.node.kubernetes.io\/pci-1ed2.present=true   162m\n```\nThe metadata attached by the Node Feature Discovery is shown in the following table.\nNPU Node Labels\n[](#id1 \"Permalink to this table\")\n| Label | Value | Description | | --- | --- | --- | | beta.furiosa.ai\/npu.count | 1 | The number of NPUs e x b number of NPUs attached to node | | beta.furiosa.ai\/npu.product | warboy, warboyB0 | NPU Product Name (Code) | | beta.furiosa.ai\/npu.family | warboy, renegade | NPU Architecture (Family) | | beta.furiosa.ai\/machine.vendor | (depends on machine) | Machine Manufacturer | | beta.furiosa.ai\/machine.name | (depends on machine) | The Nmae of Machine (Code) | | beta.furiosa.ai\/driver.version | 1.3.0 | NPU Device Driver Version | | beta.furiosa.ai\/driver.version.major | 1 | Major Version Number of NPU Device Driver Version | | beta.furiosa.ai\/driver.version.minor | 3 | Minor Version Number of NPU Device Driver | | beta.furiosa.ai\/driver.version.patch | 0 | Patch Version Number of NPU Device Driver | | beta.furiosa.ai\/driver.reference | 57ac7b0 | Build Commit Hash of NPU Device Driver |\nIf you want to check node labels, then execute the `kubectl\nget\nnodes\n--show-labels` command. If you see labels which start with `beta.furiosa.ai` Node Feature Discovery is successfully installed.\n``` kubectl get nodes --show-labels\nwarboy-node01     Ready   <none>  65d   v1.20.10   beta.furiosa.ai\/npu.count=1,beta.furiosa.ai\/npu.product=warboy...,kubernetes.io\/os=linux warboy-node02     Ready   <none>  12d   v1.20.10   beta.furiosa.ai\/npu.count=1,beta.furiosa.ai\/npu.product=warboy...,kubernetes.io\/os=linux\n```\n### Device Plugin Configuration [](#device-plugin-configuration \"Permalink to this heading\")\nExecution options for Device Plugin can be set by the argument of command line or configuration file.\n1. Command Line Arguments\nThe option can be set by the `k8s-device-plugin` command as follows.\n``` $ k8s-device-plugin --interval 10\n```\nFor the Pod or DaemonSet specification command line arguments can be set as follows.\n``` apiVersion: v1 kind: Pod metadata:   name: furiosa-device-plugin   namespace: kube-system spec:   containers:     - name: device-plugin       image: ghcr.io\/furiosa-ai\/k8s-device-plugin:latest       command: [\"\/usr\/bin\/k8s-device-plugin\"]       args: [\"--interval\", \"10\"] # (the reset is omitted)\n```\narguments of k8s-device-plugin\n[](#id2 \"Permalink to this table\")\n| Item | Explanation | Default Value | | --- | --- | --- | | default-pe | default core type when pod is allocated (Fusion\/Single) | Fusion | | interval | interval for searching device (seconds) | 10 | | disabled-devices | devices not for allocations (several devices can be designated using comma) |  | | plugin-dir | directory path of kubelet device-plugin | \/var\/lib\/kubelet\/device-plugins | | socket-name | file name of socket created under <plugin-dir> | furiosa-npu | | resource-name | name of NPU resource registered for k8s node | beta.furiosa.ai\/npu |\n2. Setting Configuration File\nYou may set configuration file by executing `k8s-device-plugin` command with argument `config-file` . If `config-file` is set then the other arguments are not permitted.\n``` $ k8s-device-plugin --config-file \/etc\/furiosa\/device-plugin.conf\n```\n\/etc\/furiosa\/device-plugin.conf\n[](#id3 \"Permalink to this code\")\n``` interval: 10 defaultPe: Fusion disabledDevices:             # device npu1 equipped in warboy-node01 will not be used   - devName: npu1     nodeName: warboy-node01 pluginDir: \/var\/lib\/kubelet\/device-plugins socketName: furiosa-npu resourceName: beta.furiosa.ai\/npu\n```\nConfiguration file is a text file with Yaml format. The modification of file contents is applied to Device Plugin immediately. Updated configuration is recorded on log of Device Plugin. (but, modifications on `pluginDir` , `socketName` , or `resourceName` require reboot.)\n[3. Installing Device Plugin and NPU Feature Discovery](#installingdevicepluginandnfd) provides `device-plugin.yaml` which is default configuration file based on ConfigMap. If you want to modify execution options of Device Plugin, modify ConfigMap. Once modified ConfigMap is applied to Pod, Device Plugin reads the ConfigMap and then reflects modification.\n``` $ kubectl edit configmap npu-device-plugin -n kube-system\n```\nconfigmap\/npu-device-plugin\n[](#id4 \"Permalink to this code\")\n``` apiVersion: v1 data:   config.yaml: |     defaultPe: Fusion     interval: 15     disabledDevices:       - devName: npu2         nodeName: npu-001 kind: ConfigMap\n```\n4. Creating a Pod with NPUs [](#creating-a-pod-with-npus \"Permalink to this heading\") --------------------------------------------------------------------------------------\nTo allocate NPU to a Pod, add as shown below to `spec.containers[].resources.limits` .\n``` resources:     limits:         beta.furiosa.ai\/npu: \"1\" # requesting 1 NPU\n```\n[Full example](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/v0.7.0\/kubernetes\/deployments\/pod-example.yaml) for Pod creation is as follows.\n``` $ cat > npu-pod.yaml <<EOL apiVersion: v1 kind: Pod metadata:   name: npu-pod spec:   containers:     - name: npu-pod       image: ubuntu:focal       resources:         limits:           cpu: \"4\"           memory: \"8Gi\"           beta.furiosa.ai\/npu: \"1\"         requests:           cpu: \"4\"           memory: \"8Gi\"           beta.furiosa.ai\/npu: \"1\" EOL\n$ kubectl apply -f npu-pod.yaml\n```\nAfter Pod creation, you can check NPU allocation as follows.\n``` $ kubectl get pods npu-pod -o yaml | grep alpha.furiosa.ai\/npu     beta.furiosa.ai\/npu: \"1\"     beta.furiosa.ai\/npu: \"1\"\n```\nThe SDK application automatically recognizes the allocated NPU device. If there are multiple NPU devices on a node, you can check which device is allocated as follows:\n``` $ kubectl exec npu-pod -it -- \/bin\/bash root@npu-pod:\/# echo $NPU_DEVNAME npu0pe0-1\n```\nIf furiosa-toolkit is installed in the Pod, you can check for more detailed device information using the furiosactl command as shown below.\nSee [APT server configuration](installation.html#setupaptrepository) for installation guide using APT.\n``` root@npu-pod:\/# furiosactl furiosactl controls the FURIOSA NPU.\nFind more information at: https:\/\/furiosa.ai\/\nBasic Commands:     version    Print the furiosactl version information     info       Show information one or many NPU(s)     config     Get\/Set configuration for NPU environment\nUsage:     furiosactl COMMAND\nroot@npu-pod:\/# furiosactl info +------+------------------+-------+--------+--------------+---------+ | NPU  | Name             | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+------------------+-------+--------+--------------+---------+ | npu0 | FuriosaAI Warboy |  40°C | 1.37 W | 0000:01:00.0 | 509:0   | +------+------------------+-------+--------+--------------+---------+\n```\n5. NPU monitoring [](#npu-monitoring \"Permalink to this heading\") ------------------------------------------------------------------\nIf you install `npu-metrics-exporter` , its daemon set and service will be created in your kubernetes cluster. The Pod that is executed through DaemonSet outputs various NPU status information that may be useful for monitoring. The data is expressed in Prometheus format. If Prometheus is installed, and service discovery is active, Prometheus will automatically collect data through the Exporter.\nThe collected data may be reviewed with visualization tools such as Grafana.\nnpu-metrics-exporter collection category list\n[](#id5 \"Permalink to this table\")\n| Name | Details | | --- | --- | | furiosa\\_npu\\_alive | NPU operation status (1:normal) | | furiosa\\_npu\\_uptime | NPU operation time (s) | | furiosa\\_npu\\_error | Number of detected NPU errors | | furiosa\\_npu\\_hw\\_temperature | Temperature of each NPU components (°mC) | | furiosa\\_npu\\_hw\\_power | NPU instantaneous power usage (µW) | | furiosa\\_npu\\_hw\\_voltage | NPU instantaenous voltage (mV) | | furiosa\\_npu\\_hw\\_current | NPU instantaneous current (mA) |\n[Previous](serving.html \"Model Server (Serving Framework)\") [Next](vm_support.html \"Configuring Warboy Pass-through for Virtual Machine\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"\n# 쿠버네티스 지원 [](#kubernetes-support \"이 제목의 영구 링크\")\n\n[Kubernetes](https:\/\/kubernetes.io\/)는 컨테이너화된 워크로드와 서비스를 관리하기 위한 오픈 소스 플랫폼입니다. Furiosa SDK는 쿠버네티스 환경을 지원하기 위해 다음과 같은 구성 요소를 제공합니다.\n\n- FuriosaAI NPU 디바이스 플러그인 ([쿠버네티스 디바이스 플러그인 소개](https:\/\/kubernetes.io\/docs\/concepts\/extend-kubernetes\/compute-storage-net\/device-plugins\/))\n- FuriosaAI NPU 기능 탐지 ([노드 기능 탐지 소개](https:\/\/kubernetes-sigs.github.io\/node-feature-discovery\/stable\/get-started\/index.html))\n\n위 두 구성 요소는 다음과 같은 기능을 제공합니다.\n\n- 쿠버네티스 클러스터가 노드에 사용 가능한 NPU를 인식하도록 합니다.\n- 쿠버네티스 `spec.containers[].resources.limits`를 통해, Pod 워크로드를 분배할 때 NPU를 동시에 스케줄링합니다.\n- NPU가 장착된 머신의 NPU 정보를 식별하고 이를 노드 레이블로 등록합니다 (이 정보를 사용하여 선택적으로 Pod를 스케줄링할 수 있으며, `nodeSelector`를 사용할 수 있습니다).\n  - 노드 기능 탐지를 클러스터에 설치해야 하며, `nfd-worker` Pod가 NPU가 장착된 노드에서 실행되어야 합니다.\n\n쿠버네티스 지원 설정 절차는 다음과 같습니다.\n\n## 1. NPU 노드 준비 [](#preparing-npu-nodes \"이 제목의 영구 링크\")\n\n쿠버네티스 노드에 대한 요구 사항은 다음과 같습니다.\n\n- Ubuntu 20.04 이상\n- Intel 호환 CPU\n\nNPU가 장착된 쿠버네티스의 각 노드에 NPU 드라이버와 툴킷을 설치해야 합니다. APT 서버가 설정되어 있다면 ([APT 서버 설정](installation.html#setupaptrepository) 참조), 다음과 같이 쉽게 설치할 수 있습니다.\n\n```bash\napt-get update && apt install -y furiosa-driver-warboy furiosa-toolkit\n```\n\n위와 같이 필요한 패키지를 설치한 후, furiosa-toolkit에 포함된 `furiosactl` 명령어를 사용하여 NPU 인식을 확인할 수 있습니다. 아래 명령어로 NPU가 인식되지 않는다면, 환경에 따라 재부팅 후 다시 시도해 보세요.\n\n```bash\n$ furiosactl info\n+------+------------------+-------+--------+--------------+---------+\n| NPU  | Name             | Temp. | Power  | PCI-BDF      | PCI-DEV |\n+------+------------------+-------+--------+--------------+---------+\n| npu0 | FuriosaAI Warboy |  40°C | 1.37 W | 0000:01:00.0 | 509:0   |\n+------+------------------+-------+--------+--------------+---------+\n```\n\n## 2. 노드 기능 탐지 설치 [](#installing-node-feature-discovery \"이 제목의 영구 링크\")\n\n쿠버네티스가 NPU를 인식하도록 하려면 노드 기능 탐지를 설치해야 합니다. 아래 예시와 같이 명령어를 실행하여 `feature.node.kubernetes.io\/...`로 시작하는 노드 레이블이 있는 경우, 노드 기능 탐지의 DaemonSet이 이미 설치된 것입니다.\n\n```bash\n$ kubectl get no -o json | jq '.items[].metadata.labels'\n{\n  \"beta.kubernetes.io\/arch\": \"amd64\",\n  \"beta.kubernetes.io\/os\": \"linux\",\n  \"feature.node.kubernetes.io\/cpu-cpuid.ADX\": \"true\",\n  \"feature.node.kubernetes.io\/cpu-cpuid.AESNI\": \"true\",\n  ...\n```\n\n- 클러스터에 노드 기능 탐지가 없는 경우, 다음 문서를 참조하세요.\n  > + [빠른 시작 \/ 설치](https:\/\/kubernetes-sigs.github.io\/node-feature-discovery\/v0.11\/get-started\/quick-start.html#installation)\n- 노드 기능 탐지를 실행할 때 다음 옵션을 적용해야 합니다.\n  + `beta.furiosa.ai`는 `nfd-master`의 `--extra-label-ns` 옵션에 포함되어야 합니다.\n  + `nfd-worker`의 설정 파일에서, `sources.pci.deviceLabelFields` 값에 `vendor`만 포함되어야 하며, `sources.pci.deviceClassWhitelist`에 `\"12\"`가 값으로 포함되어야 합니다.\n\n노드 기능 탐지 설치는 필수는 아니지만 권장됩니다. 노드 기능 탐지를 사용하지 않는 경우 수행해야 할 추가 작업은 다음 단계에서 설명합니다.\n\n## 3. 디바이스 플러그인 및 NPU 기능 탐지 설치 [](#installing-device-plugin-and-npu-feature-discovery \"이 제목의 영구 링크\")\n\nNPU 노드가 준비되면, 다음과 같이 디바이스 플러그인 및 NPU 기능 탐지의 DaemonSet을 설치합니다.\n\n```bash\nkubectl apply -f https:\/\/raw.githubusercontent.com\/furiosa-ai\/furiosa-sdk\/v0.7.0\/kubernetes\/deployments\/device-plugin.yaml\nkubectl apply -f https:\/\/raw.githubusercontent.com\/furiosa-ai\/furiosa-sdk\/v0.7.0\/kubernetes\/deployments\/npu-feature-discovery.yaml\n```\n\n위 명령어를 실행한 후, `kubectl get daemonset -n kube-system` 명령어로 설치된 DaemonSet이 정상적으로 작동하는지 확인할 수 있습니다. 참고로, DaemonSet은 NPU가 장착된 노드에만 분배되며, 노드 기능 탐지(`feature.node.kubernetes.io\/pci-1ed2.present=true`)가 각 노드에 부착하는 `alpha.furiosa.ai\/npu.family=warboy` 정보를 사용합니다.\n\n```bash\n$ kubectl get daemonset -n kube-system\nNAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                      AGE\nfuriosa-device-plugin          3         3         3       3            3           feature.node.kubernetes.io\/pci-1ed2.present=true   128m\nfuriosa-npu-feature-discovery  3         3         3       3            3           feature.node.kubernetes.io\/pci-1ed2.present=true   162m\n```\n\n노드 기능 탐지가 부착한 메타데이터는 다음 표에 나와 있습니다.\n\n### NPU 노드 레이블 [](#id1 \"이 표의 영구 링크\")\n\n| 레이블 | 값 | 설명 |\n| --- | --- | --- |\n| beta.furiosa.ai\/npu.count | 1 | 노드에 연결된 NPU의 수 |\n| beta.furiosa.ai\/npu.product | warboy, warboyB0 | NPU 제품명 (코드) |\n| beta.furiosa.ai\/npu.family | warboy, renegade | NPU 아키텍처 (패밀리) |\n| beta.furiosa.ai\/machine.vendor | (머신에 따라 다름) | 머신 제조사 |\n| beta.furiosa.ai\/machine.name | (머신에 따라 다름) | 머신 이름 (코드) |\n| beta.furiosa.ai\/driver.version | 1.3.0 | NPU 디바이스 드라이버 버전 |\n| beta.furiosa.ai\/driver.version.major | 1 | NPU 디바이스 드라이버의 주요 버전 번호 |\n| beta.furiosa.ai\/driver.version.minor | 3 | NPU 디바이스 드라이버의 부 버전 번호 |\n| beta.furiosa.ai\/driver.version.patch | 0 | NPU 디바이스 드라이버의 패치 버전 번호 |\n| beta.furiosa.ai\/driver.reference | 57ac7b0 | NPU 디바이스 드라이버의 빌드 커밋 해시 |\n\n노드 레이블을 확인하려면 `kubectl get nodes --show-labels` 명령어를 실행하세요. `beta.furiosa.ai`로 시작하는 레이블이 보이면 노드 기능 탐지가 성공적으로 설치된 것입니다.\n\n```bash\nkubectl get nodes --show-labels\nwarboy-node01     Ready   <none>  65d   v1.20.10   beta.furiosa.ai\/npu.count=1,beta.furiosa.ai\/npu.product=warboy...,kubernetes.io\/os=linux\nwarboy-node02     Ready   <none>  12d   v1.20.10   beta.furiosa.ai\/npu.count=1,beta.furiosa.ai\/npu.product=warboy...,kubernetes.io\/os=linux\n```\n\n### 디바이스 플러그인 설정 [](#device-plugin-configuration \"이 제목의 영구 링크\")\n\n디바이스 플러그인의 실행 옵션은 명령줄 인수 또는 설정 파일로 설정할 수 있습니다.\n\n1. 명령줄 인수\n\n옵션은 `k8s-device-plugin` 명령어로 다음과 같이 설정할 수 있습니다.\n\n```bash\n$ k8s-device-plugin --interval 10\n```\n\nPod 또는 DaemonSet 사양의 명령줄 인수는 다음과 같이 설정할 수 있습니다.\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: furiosa-device-plugin\n  namespace: kube-system\nspec:\n  containers:\n    - name: device-plugin\n      image: ghcr.io\/furiosa-ai\/k8s-device-plugin:latest\n      command: [\"\/usr\/bin\/k8s-device-plugin\"]\n      args: [\"--interval\", \"10\"] # (나머지는 생략)\n```\n\n### k8s-device-plugin 인수 [](#id2 \"이 표의 영구 링크\")\n\n| 항목 | 설명 | 기본값 |\n| --- | --- | --- |\n| default-pe | Pod가 할당될 때 기본 코어 유형 (Fusion\/Single) | Fusion |\n| interval | 장치 검색 간격 (초) | 10 |\n| disabled-devices | 할당되지 않을 장치 (여러 장치는 쉼표로 지정 가능) |  |\n| plugin-dir | kubelet 디바이스 플러그인의 디렉토리 경로 | \/var\/lib\/kubelet\/device-plugins |\n| socket-name | <plugin-dir>에 생성된 소켓의 파일 이름 | furiosa-npu |\n| resource-name | k8s 노드에 등록된 NPU 리소스 이름 | beta.furiosa.ai\/npu |\n\n2. 설정 파일 설정\n\n`k8s-device-plugin` 명령어를 `config-file` 인수와 함께 실행하여 설정 파일을 설정할 수 있습니다. `config-file`이 설정되면 다른 인수는 허용되지 않습니다.\n\n```bash\n$ k8s-device-plugin --config-file \/etc\/furiosa\/device-plugin.conf\n```\n\n### \/etc\/furiosa\/device-plugin.conf [](#id3 \"이 코드의 영구 링크\")\n\n```yaml\ninterval: 10\ndefaultPe: Fusion\ndisabledDevices:\n  # warboy-node01에 장착된 npu1 장치는 사용되지 않음\n  - devName: npu1\n    nodeName: warboy-node01\npluginDir: \/var\/lib\/kubelet\/device-plugins\nsocketName: furiosa-npu\nresourceName: beta.furiosa.ai\/npu\n```\n\n설정 파일은 Yaml 형식의 텍스트 파일입니다. 파일 내용의 수정은 디바이스 플러그인에 즉시 적용됩니다. 업데이트된 설정은 디바이스 플러그인의 로그에 기록됩니다. (`pluginDir`, `socketName`, 또는 `resourceName`의 수정은 재부팅이 필요합니다.)\n\n[3. 디바이스 플러그인 및 NPU 기능 탐지 설치](#installingdevicepluginandnfd)에서는 ConfigMap을 기반으로 한 기본 설정 파일인 `device-plugin.yaml`을 제공합니다. 디바이스 플러그인의 실행 옵션을 수정하려면 ConfigMap을 수정하세요. 수정된 ConfigMap이 Pod에 적용되면, 디바이스 플러그인은 ConfigMap을 읽고 수정 내용을 반영합니다.\n\n```bash\n$ kubectl edit configmap npu-device-plugin -n kube-system\n```\n\n### configmap\/npu-device-plugin [](#id4 \"이 코드의 영구 링크\")\n\n```yaml\napiVersion: v1\ndata:\n  config.yaml: |\n    defaultPe: Fusion\n    interval: 15\n    disabledDevices:\n      - devName: npu2\n        nodeName: npu-001\nkind: ConfigMap\n```\n\n## 4. NPU가 포함된 Pod 생성 [](#creating-a-pod-with-npus \"이 제목의 영구 링크\")\n\nPod에 NPU를 할당하려면, `spec.containers[].resources.limits`에 아래와 같이 추가합니다.\n\n```yaml\nresources:\n    limits:\n        beta.furiosa.ai\/npu: \"1\" # 1개의 NPU 요청\n```\n\nPod 생성에 대한 [전체 예제](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/v0.7.0\/kubernetes\/deployments\/pod-example.yaml)는 다음과 같습니다.\n\n```bash\n$ cat > npu-pod.yaml <<EOL\napiVersion: v1\nkind: Pod\nmetadata:\n  name: npu-pod\nspec:\n  containers:\n    - name: npu-pod\n      image: ubuntu:focal\n      resources:\n        limits:\n          cpu: \"4\"\n          memory: \"8Gi\"\n          beta.furiosa.ai\/npu: \"1\"\n        requests:\n          cpu: \"4\"\n          memory: \"8Gi\"\n          beta.furiosa.ai\/npu: \"1\"\nEOL\n\n$ kubectl apply -f npu-pod.yaml\n```\n\nPod 생성 후, NPU 할당을 다음과 같이 확인할 수 있습니다.\n\n```bash\n$ kubectl get pods npu-pod -o yaml | grep alpha.furiosa.ai\/npu\n    beta.furiosa.ai\/npu: \"1\"\n    beta.furiosa.ai\/npu: \"1\"\n```\n\nSDK 애플리케이션은 할당된 NPU 장치를 자동으로 인식합니다. 노드에 여러 NPU 장치가 있는 경우, 다음과 같이 할당된 장치를 확인할 수 있습니다.\n\n```bash\n$ kubectl exec npu-pod -it -- \/bin\/bash\nroot@npu-pod:\/# echo $NPU_DEVNAME\nnpu0pe0-1\n```\n\nPod에 furiosa-toolkit이 설치되어 있다면, 아래와 같이 furiosactl 명령어를 사용하여 더 자세한 장치 정보를 확인할 수 있습니다. APT를 사용한 설치 가이드는 [APT 서버 설정](installation.html#setupaptrepository)을 참조하세요.\n\n```bash\nroot@npu-pod:\/# furiosactl\nfuriosactl은 FURIOSA NPU를 제어합니다.\n자세한 정보는 https:\/\/furiosa.ai\/에서 확인하세요.\n\n기본 명령어:\n    version    furiosactl 버전 정보 출력\n    info       하나 이상의 NPU 정보 표시\n    config     NPU 환경 설정 가져오기\/설정하기\n\n사용법:\n    furiosactl COMMAND\n\nroot@npu-pod:\/# furiosactl info\n+------+------------------+-------+--------+--------------+---------+\n| NPU  | Name             | Temp. | Power  | PCI-BDF      | PCI-DEV |\n+------+------------------+-------+--------+--------------+---------+\n| npu0 | FuriosaAI Warboy |  40°C | 1.37 W | 0000:01:00.0 | 509:0   |\n+------+------------------+-------+--------+--------------+---------+\n```\n\n## 5. NPU 모니터링 [](#npu-monitoring \"이 제목의 영구 링크\")\n\n`npu-metrics-exporter`를 설치하면, 쿠버네티스 클러스터에 DaemonSet과 서비스가 생성됩니다. DaemonSet을 통해 실행되는 Pod는 모니터링에 유용한 다양한 NPU 상태 정보를 출력합니다. 데이터는 Prometheus 형식으로 표현됩니다. Prometheus가 설치되어 있고 서비스 디스커버리가 활성화되어 있으면, Prometheus는 Exporter를 통해 데이터를 자동으로 수집합니다.\n\n수집된 데이터는 Grafana와 같은 시각화 도구를 사용하여 검토할 수 있습니다.\n\n### npu-metrics-exporter 수집 카테고리 목록 [](#id5 \"이 표의 영구 링크\")\n\n| 이름 | 세부사항 |\n| --- | --- |\n| furiosa\\_npu\\_alive | NPU 작동 상태 (1:정상) |\n| furiosa\\_npu\\_uptime | NPU 작동 시간 (초) |\n| furiosa\\_npu\\_error | 감지된 NPU 오류 수 |\n| furiosa\\_npu\\_hw\\_temperature | 각 NPU 구성 요소의 온도 (°mC) |\n| furiosa\\_npu\\_hw\\_power | NPU 순간 전력 사용량 (µW) |\n| furiosa\\_npu\\_hw\\_voltage | NPU 순간 전압 (mV) |\n| furiosa\\_npu\\_hw\\_current | NPU 순간 전류 (mA) |\n\n[이전](serving.html \"모델 서버 (서빙 프레임워크)\") [다음](vm_support.html \"가상 머신을 위한 Warboy 패스스루 구성\")\n\n---\n\n© 저작권 2023 FuriosaAI, Inc.  \n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용했습니다."},{"page_id":"3e1275e1-f809-4d7a-beb7-ec8b8b5de8f5","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/quantization.html","original_content":"* Model Quantization * [View page source](..\/_sources\/software\/quantization.rst.txt)\n---\nModel Quantization [](#model-quantization \"Permalink to this heading\") =======================================================================\nFuriosa SDK and first generation Warboy support INT8 models. To support floating point models, Furiosa SDK provides quantization tools to convert FP16 or FP32 floating point data type models into INT8 data type models. Quantization is a common technique used to increase model processing performance or accelerate hardware. Using the quantization tool provied by Furiosa SDK, a greater variety of models can be accelerated by deploying the NPU.\nQuantization method supported by Furiosa SDK is based on *post-training 8-bit quantization* , and follows [Tensorflow Lite 8-bit quantization specification](https:\/\/www.tensorflow.org\/lite\/performance\/quantization_spec) .\nHow It Works [](#how-it-works \"Permalink to this heading\") -----------------------------------------------------------\nAs shown in the diagram below, the quantization tool receives the ONNX model as input, performs quantization through the following three steps, and outputs the quantized ONNX model.\n1. Graph Optimization 2. Calibration 3. Quantization\nIn the graph optimization process, the topological structure of the graph is changed by adding or replacing operators in the model through analysis of the original model network structure, so that the model can process quantized data with a minimal drop in accuracy.\nIn the calibration process, the data used to train the model is required in order to calibrate the weights of the model.\nAccuracy of Quantized Models [](#accuracy-of-quantized-models \"Permalink to this heading\") -------------------------------------------------------------------------------------------\nThe table below compares the accuracy of the original floating-point models with that of the quantized models obtained using the quantizer and various calibration methods provided by Furiosa SDK:\nQuantization Accuracy\n[](#id1 \"Permalink to this table\")\n| Model | FP Accuracy | INT8 Accuracy (Calibration Method) | INT8 Accuracy ÷ FP Accuracy | | --- | --- | --- | --- | | ConvNext-B | 85.8% | 80.376% (Asymmetric MSE) | 93.678% | | EfficientNet-B0 | 77.698% | 73.556% (Asymmetric 99.99%-Percentile) | 94.669% | | EfficientNetV2-S | 84.228% | 83.566% (Asymmetric 99.99%-Percentile) | 99.214% | | ResNet50 v1.5 | 76.456% | 76.228% (Asymmetric MSE) | 99.702% | | RetinaNet | mAP 0.3757 | mAP 0.37373 (Symmetric Entropy) | 99.476% | | YOLOX-l | mAP 0.497 | mAP 0.48524 (Asymmetric 99.99%-Percentile) | 97.634% | | YOLOv5-l | mAP 0.490 | mAP 0.47443 (Asymmetric MSE) | 96.822% | | YOLOv5-m | mAP 0.454 | mAP 0.43963 (Asymmetric SQNR) | 96.835% |\nModel Quantization APIs [](#model-quantization-apis \"Permalink to this heading\") ---------------------------------------------------------------------------------\nYou can use the APU and command line tool provided in this SDK to convert an ONNX model into an 8bit quantized model.\nRefer to the links below for further instructions.\n* [Python SDK example: How to use Furiosa SDK from start to finish](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/HowToUseFuriosaSDKFromStartToFinish.ipynb) * [Python SDK Quantization example](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/main\/examples\/quantizers) * [Python reference - furiosa.quantizer](https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/api\/python\/furiosa.quantizer.html)\n[Previous](compiler.html \"Compiler\") [Next](performance.html \"Performance Optimization\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 모델 양자화 * [페이지 소스 보기](..\/_sources\/software\/quantization.rst.txt)\n---\n모델 양자화 [](#model-quantization \"이 제목의 영구 링크\") =======================================================================\nFuriosa SDK와 1세대 Warboy는 INT8 모델을 지원합니다. 부동 소수점 모델을 지원하기 위해, Furiosa SDK는 FP16 또는 FP32 부동 소수점 데이터 타입 모델을 INT8 데이터 타입 모델로 변환하는 양자화 도구를 제공합니다. 양자화는 모델 처리 성능을 높이거나 하드웨어를 가속하기 위해 일반적으로 사용되는 기술입니다. Furiosa SDK가 제공하는 양자화 도구를 사용하면, NPU를 배포하여 더 다양한 모델을 가속할 수 있습니다. Furiosa SDK가 지원하는 양자화 방법은 *훈련 후 8비트 양자화*에 기반하며, [Tensorflow Lite 8비트 양자화 사양](https:\/\/www.tensorflow.org\/lite\/performance\/quantization_spec)을 따릅니다.\n\n작동 방식 [](#how-it-works \"이 제목의 영구 링크\") -----------------------------------------------------------\n아래 다이어그램에 표시된 것처럼, 양자화 도구는 ONNX 모델을 입력으로 받아 다음 세 가지 단계를 통해 양자화를 수행하고 양자화된 ONNX 모델을 출력합니다.\n1. 그래프 최적화 2. 보정 3. 양자화\n\n그래프 최적화 과정에서는 원본 모델 네트워크 구조를 분석하여 모델에 연산자를 추가하거나 교체함으로써 그래프의 위상 구조를 변경하여, 모델이 정확도의 최소 손실로 양자화된 데이터를 처리할 수 있도록 합니다.\n\n보정 과정에서는 모델의 가중치를 보정하기 위해 모델을 훈련하는 데 사용된 데이터가 필요합니다.\n\n양자화된 모델의 정확도 [](#accuracy-of-quantized-models \"이 제목의 영구 링크\") -------------------------------------------------------------------------------------------\n아래 표는 Furiosa SDK가 제공하는 양자화 도구와 다양한 보정 방법을 사용하여 얻은 양자화된 모델의 정확도를 원본 부동 소수점 모델의 정확도와 비교한 것입니다:\n\n양자화 정확도 [](#id1 \"이 표의 영구 링크\")\n\n| 모델 | FP 정확도 | INT8 정확도 (보정 방법) | INT8 정확도 ÷ FP 정확도 | | --- | --- | --- | --- | | ConvNext-B | 85.8% | 80.376% (비대칭 MSE) | 93.678% | | EfficientNet-B0 | 77.698% | 73.556% (비대칭 99.99%-백분위수) | 94.669% | | EfficientNetV2-S | 84.228% | 83.566% (비대칭 99.99%-백분위수) | 99.214% | | ResNet50 v1.5 | 76.456% | 76.228% (비대칭 MSE) | 99.702% | | RetinaNet | mAP 0.3757 | mAP 0.37373 (대칭 엔트로피) | 99.476% | | YOLOX-l | mAP 0.497 | mAP 0.48524 (비대칭 99.99%-백분위수) | 97.634% | | YOLOv5-l | mAP 0.490 | mAP 0.47443 (비대칭 MSE) | 96.822% | | YOLOv5-m | mAP 0.454 | mAP 0.43963 (비대칭 SQNR) | 96.835% |\n\n모델 양자화 API [](#model-quantization-apis \"이 제목의 영구 링크\") ---------------------------------------------------------------------------------\n이 SDK에서 제공하는 APU와 커맨드 라인 도구를 사용하여 ONNX 모델을 8비트 양자화 모델로 변환할 수 있습니다. 자세한 지침은 아래 링크를 참조하세요.\n\n* [Python SDK 예제: Furiosa SDK를 처음부터 끝까지 사용하는 방법](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/HowToUseFuriosaSDKFromStartToFinish.ipynb) \n* [Python SDK 양자화 예제](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/main\/examples\/quantizers) \n* [Python 참조 - furiosa.quantizer](https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/api\/python\/furiosa.quantizer.html)\n\n[이전](compiler.html \"컴파일러\") [다음](performance.html \"성능 최적화\")\n---\n© 저작권 2023 FuriosaAI, Inc.\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용합니다.\n```"},{"page_id":"2114c344-3971-4d26-8c09-a001327ce7bb","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/c-sdk.html","original_content":"* C SDK installation and user guide * [View page source](..\/_sources\/software\/c-sdk.rst.txt)\n---\nC SDK installation and user guide [](#c-sdk-installation-and-user-guide \"Permalink to this heading\") =====================================================================================================\nWe explain here how to write FuriosaAI NPU applications using C programming language. The C SDK provides a C ABI-based static library and C header file. Using these, you can write applications in C, C++, or other languages that support C ABI.\nThe provided C SDK is relatively lower-level than [Python SDK](python-sdk.html#pythonsdk) . It can be used when lower latency and higher performance are required, or when Python runtime cannot be used.\nWarning  `furiosa-libnux-dev` and the current C API are being deprecated in the future release.\nAs substitute of the current API, new C API based on the next-generation runtime called FuriosaRT will be introduced with more features in the future release.\nC SDK installation [](#c-sdk-installation \"Permalink to this heading\") -----------------------------------------------------------------------\nThe minimum requirements for C SDK are as follows.\n* Ubuntu 20.04 LTS (Debian bullseye) or higher * System administrator privileges (root) * [FuriosaAI SDK required packages](installation.html#requiredpackages)\nIn order to install and use C SDK, you must install the driver, firmware, and runtime library in accordance with the [Required Package Installation](installation.html#requiredpackages) guide.\nOnce you have installed the required packages, follow the instructions below to install C SDK.\nInstallation using APT server\nTo use FuriosaAI APT, refer to [APT server configuration](installation.html#setupaptrepository) and complete the authentication setting for server connection.\n``` apt-get update && apt-get install -y furiosa-libnux-dev\n```\nCompiling with C SDK [](#compiling-with-c-sdk \"Permalink to this heading\") ---------------------------------------------------------------------------\nOnce you install the package as above, you can compile using the C SDK.\nC header files and static libraries are located in the `\/usr\/include\/furiosa` and `\/usr\/lib\/x86_64-linux-gnu` directories respectively. They are the system paths that gcc looks to find C headers and libraries by default, so you can simply compile C applications with only `-lnux` option as follows:\n``` gcc example.c -lnux\n```\nAlso, you can find C SDK examples and C API reference at [C Language SDK Reference](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/c\/index.html) .\n[Previous](python-sdk.html \"Python SDK installation and user guide\") [Next](cli.html \"Command Line Tools\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"\n* C SDK 설치 및 사용자 가이드 * [페이지 소스 보기](..\/_sources\/software\/c-sdk.rst.txt)\n---\nC SDK 설치 및 사용자 가이드 [](#c-sdk-installation-and-user-guide \"이 제목의 영구 링크\") =====================================================================================================\n여기서는 C 프로그래밍 언어를 사용하여 FuriosaAI NPU 애플리케이션을 작성하는 방법을 설명합니다. C SDK는 C ABI 기반의 정적 라이브러리와 C 헤더 파일을 제공합니다. 이를 사용하여 C, C++ 또는 C ABI를 지원하는 다른 언어로 애플리케이션을 작성할 수 있습니다. 제공된 C SDK는 [Python SDK](python-sdk.html#pythonsdk)보다 상대적으로 낮은 수준입니다. 이는 더 낮은 지연 시간과 높은 성능이 필요하거나 Python 런타임을 사용할 수 없는 경우에 사용할 수 있습니다. 경고 `furiosa-libnux-dev`와 현재 C API는 향후 릴리스에서 사용 중단될 예정입니다. 현재 API의 대체로, 차세대 런타임인 FuriosaRT를 기반으로 한 새로운 C API가 향후 릴리스에서 더 많은 기능과 함께 도입될 예정입니다.\nC SDK 설치 [](#c-sdk-installation \"이 제목의 영구 링크\") -----------------------------------------------------------------------\nC SDK의 최소 요구 사항은 다음과 같습니다.\n* Ubuntu 20.04 LTS (Debian bullseye) 이상 * 시스템 관리자 권한 (root) * [FuriosaAI SDK 필수 패키지](installation.html#requiredpackages)\nC SDK를 설치하고 사용하려면 [필수 패키지 설치](installation.html#requiredpackages) 가이드에 따라 드라이버, 펌웨어 및 런타임 라이브러리를 설치해야 합니다. 필수 패키지를 설치한 후 아래 지침에 따라 C SDK를 설치하십시오.\nAPT 서버를 사용한 설치\nFuriosaAI APT를 사용하려면 [APT 서버 구성](installation.html#setupaptrepository)을 참조하여 서버 연결을 위한 인증 설정을 완료하십시오.\n``` apt-get update && apt-get install -y furiosa-libnux-dev\n```\nC SDK로 컴파일하기 [](#compiling-with-c-sdk \"이 제목의 영구 링크\") ---------------------------------------------------------------------------\n위와 같이 패키지를 설치한 후, C SDK를 사용하여 컴파일할 수 있습니다. C 헤더 파일과 정적 라이브러리는 각각 `\/usr\/include\/furiosa` 및 `\/usr\/lib\/x86_64-linux-gnu` 디렉토리에 위치해 있습니다. 이들은 gcc가 기본적으로 C 헤더와 라이브러리를 찾는 시스템 경로이므로, `-lnux` 옵션만으로 C 애플리케이션을 간단히 컴파일할 수 있습니다:\n``` gcc example.c -lnux\n```\n또한, C SDK 예제 및 C API 참조는 [C 언어 SDK 참조](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/c\/index.html)에서 찾을 수 있습니다.\n[이전](python-sdk.html \"Python SDK 설치 및 사용자 가이드\") [다음](cli.html \"명령줄 도구\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용하여 [Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었습니다."},{"page_id":"2a739e9e-c42e-4c07-82d5-41cf614dd6a9","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/installation.html","original_content":"* Driver, Firmware, and Runtime Installation * [View page source](..\/_sources\/software\/installation.rst.txt)\n---\nDriver, Firmware, and Runtime Installation [](#driver-firmware-and-runtime-installation \"Permalink to this heading\") =====================================================================================================================\nHere, we explain how to install the packages necessary to use the various SW components provided by FuriosaAI. The required packages are composed of kernel drivers, firmware, and runtime library, and they can be easily installed via the APT package manager.\nNote\nYou will be able to login [FuriosaAI IAM](https:\/\/iam.furiosa.ai) and create a new API key upon registration to the FuriosaAI evaluation program. Currently, the request for registration can be done through [contact @\nfuriosa .\nai](mailto:contact%40furiosa.ai) .\nMinimum requirements for SDK installation [](#minimum-requirements-for-sdk-installation \"Permalink to this heading\") ---------------------------------------------------------------------------------------------------------------------\n* Ubuntu 20.04 LTS (Focal Fossa)\/Debian bullseye   or higher * Administrator privileges on system (root) * Internet-accessible network\nAPT server configuration [](#apt-server-configuration \"Permalink to this heading\") -----------------------------------------------------------------------------------\nIn order to use the APT server as provided by FuriosaAI, the APT server must be configured on Ubuntu or Debian Linux as delineated below.\n1. Install the necessary packages to access HTTPS-based APT server.\n``` sudo apt update sudo apt install -y ca-certificates apt-transport-https gnupg wget\n```\n2. Register the FuriosaAI public Signing key.\n``` mkdir -p \/etc\/apt\/keyrings && \\ wget -q -O- https:\/\/archive.furiosa.ai\/furiosa-apt-key.gpg \\ | gpg --dearmor \\ | sudo tee \/etc\/apt\/keyrings\/furiosa-apt-key.gpg > \/dev\/null\n```\n3. Generate a new API key from    [FuriosaAI IAM](https:\/\/iam.furiosa.ai)    , and configure the API key as follows:\n``` sudo tee -a \/etc\/apt\/auth.conf.d\/furiosa.conf > \/dev\/null <<EOT   machine archive.furiosa.ai   login [KEY (ID)]   password [PASSWORD] EOT\nsudo chmod 400 \/etc\/apt\/auth.conf.d\/furiosa.conf\n```\n4. Configure the APT server according to the explanation given in the Linux distribution version tab.\nUbuntu 20.04 (Debian Bullseye)\nUbuntu 22.04 (Debian Bookworm)\nRegister the APT server through the command below.\n``` sudo tee -a \/etc\/apt\/sources.list.d\/furiosa.list <<EOT deb [arch=amd64 signed-by=\/etc\/apt\/keyrings\/furiosa-apt-key.gpg] https:\/\/archive.furiosa.ai\/ubuntu focal restricted EOT\n```\nRegister the APT server through the command below.\n``` sudo tee -a \/etc\/apt\/sources.list.d\/furiosa.list <<EOT deb [arch=amd64 signed-by=\/etc\/apt\/keyrings\/furiosa-apt-key.gpg] https:\/\/archive.furiosa.ai\/ubuntu jammy restricted EOT\n```\nInstalling required packages. [](#installing-required-packages \"Permalink to this heading\") --------------------------------------------------------------------------------------------\nIf you have registered the APT server as above, or registered on the download site, you will be able to install the required packages - NPU kernel driver, firmware, and runtime.\nInstallation using APT server\n``` sudo apt-get update && sudo apt-get install -y furiosa-driver-warboy furiosa-libnux\n```\n### Adding a user to the `furiosa` Group [](#adding-a-user-to-the-furiosa-group \"Permalink to this heading\")\nLinux is a multi-user operating system that enables file and device access for both the owner and users within a specific group. The NPU device driver creates a group called `furiosa` and restricts access to NPU devices exclusively to users who are members of the `furiosa` group. To add a user to a member of `furiosa` group, please run as follows:\n``` sudo usermod -aG furiosa <username>\n```\nReplace <username> with the name of the user you want to add to the `furiosa` group. For example, in order to add the current user (i.e., `$USER` ) to the `furiosa` group, you can run as follows:\n``` sudo usermod -aG furiosa $USER\n```\nUpon logging out and logging back in, the change to the group membership will take effect.\n### Holding\/unholding installed version [](#holding-unholding-installed-version \"Permalink to this heading\")\nFollowing package installation, in order to maintain a stable operating environment, there may be a need to hold the installed packages versions. By using the command below, you will be able to hold the currently installed versions.\n``` sudo apt-mark hold furiosa-driver-warboy furiosa-libhal-warboy furiosa-libnux libonnxruntime\n```\nIn order to unhold and update the current package versions, designate the package that you wish to unhold with the command `apt-mark\nunhold` .\nHere, you can state the name of the package, thereby unholding selectively a specific package. In order to show the properties of an already held package, use the command `apt-mark\nshowhold` .\n``` sudo apt-mark unhold furiosa-driver-warboy furiosa-libhal-warboy furiosa-libnux libonnxruntime\n```\n### Installing a specific version [](#installing-a-specific-version \"Permalink to this heading\")\nIf you need to install a specific version, you may designate the version that you want and install as follows.\n1. Check available versions through    `apt        list`    .\n``` sudo apt list -a furiosa-libnux\n```\n2. State the package name and version as options in the command    `apt-get        install`\n``` sudo apt-get install -y furiosa-libnux=0.9.1-?\n```\nNPU Firmware Update [](#npu-firmware-update \"Permalink to this heading\") -------------------------------------------------------------------------\n[Previous](intro.html \"FuriosaAI SW Stack Introduction\") [Next](python-sdk.html \"Python SDK installation and user guide\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 드라이버, 펌웨어 및 런타임 설치 * [페이지 소스 보기](..\/_sources\/software\/installation.rst.txt)\n---\n드라이버, 펌웨어 및 런타임 설치 [](#driver-firmware-and-runtime-installation \"이 제목의 영구 링크\") =====================================================================================================================\n여기에서는 FuriosaAI에서 제공하는 다양한 SW 구성 요소를 사용하기 위해 필요한 패키지를 설치하는 방법을 설명합니다. 필요한 패키지는 커널 드라이버, 펌웨어 및 런타임 라이브러리로 구성되어 있으며, APT 패키지 관리자를 통해 쉽게 설치할 수 있습니다.\n\n참고\n[FuriosaAI IAM](https:\/\/iam.furiosa.ai)에 로그인하고 FuriosaAI 평가 프로그램에 등록하면 새로운 API 키를 생성할 수 있습니다. 현재 등록 요청은 [contact @ furiosa . ai](mailto:contact%40furiosa.ai)를 통해 가능합니다.\n\nSDK 설치를 위한 최소 요구 사항 [](#minimum-requirements-for-sdk-installation \"이 제목의 영구 링크\") ---------------------------------------------------------------------------------------------------------------------\n* Ubuntu 20.04 LTS (Focal Fossa)\/Debian bullseye 이상 * 시스템 관리자 권한 (root) * 인터넷에 연결된 네트워크\n\nAPT 서버 구성 [](#apt-server-configuration \"이 제목의 영구 링크\") -----------------------------------------------------------------------------------\nFuriosaAI에서 제공하는 APT 서버를 사용하려면, 아래에 설명된 대로 Ubuntu 또는 Debian Linux에서 APT 서버를 구성해야 합니다.\n\n1. HTTPS 기반 APT 서버에 접근하기 위한 필수 패키지를 설치합니다.\n```bash\nsudo apt update\nsudo apt install -y ca-certificates apt-transport-https gnupg wget\n```\n\n2. FuriosaAI 공개 서명 키를 등록합니다.\n```bash\nmkdir -p \/etc\/apt\/keyrings && \\\nwget -q -O- https:\/\/archive.furiosa.ai\/furiosa-apt-key.gpg \\\n| gpg --dearmor \\\n| sudo tee \/etc\/apt\/keyrings\/furiosa-apt-key.gpg > \/dev\/null\n```\n\n3. [FuriosaAI IAM](https:\/\/iam.furiosa.ai)에서 새로운 API 키를 생성하고, 다음과 같이 API 키를 구성합니다:\n```bash\nsudo tee -a \/etc\/apt\/auth.conf.d\/furiosa.conf > \/dev\/null <<EOT\nmachine archive.furiosa.ai\nlogin [KEY (ID)]\npassword [PASSWORD]\nEOT\nsudo chmod 400 \/etc\/apt\/auth.conf.d\/furiosa.conf\n```\n\n4. Linux 배포판 버전 탭에 설명된 대로 APT 서버를 구성합니다.\n\nUbuntu 20.04 (Debian Bullseye)\nUbuntu 22.04 (Debian Bookworm)\n\n아래 명령을 통해 APT 서버를 등록합니다.\n```bash\nsudo tee -a \/etc\/apt\/sources.list.d\/furiosa.list <<EOT\ndeb [arch=amd64 signed-by=\/etc\/apt\/keyrings\/furiosa-apt-key.gpg] https:\/\/archive.furiosa.ai\/ubuntu focal restricted\nEOT\n```\n\n아래 명령을 통해 APT 서버를 등록합니다.\n```bash\nsudo tee -a \/etc\/apt\/sources.list.d\/furiosa.list <<EOT\ndeb [arch=amd64 signed-by=\/etc\/apt\/keyrings\/furiosa-apt-key.gpg] https:\/\/archive.furiosa.ai\/ubuntu jammy restricted\nEOT\n```\n\n필수 패키지 설치 [](#installing-required-packages \"이 제목의 영구 링크\") --------------------------------------------------------------------------------------------\n위와 같이 APT 서버를 등록했거나 다운로드 사이트에 등록한 경우, 필요한 패키지 - NPU 커널 드라이버, 펌웨어 및 런타임을 설치할 수 있습니다.\n\nAPT 서버를 사용한 설치\n```bash\nsudo apt-get update && sudo apt-get install -y furiosa-driver-warboy furiosa-libnux\n```\n\n### `furiosa` 그룹에 사용자 추가 [](#adding-a-user-to-the-furiosa-group \"이 제목의 영구 링크\")\nLinux는 소유자와 특정 그룹 내 사용자가 파일 및 장치에 접근할 수 있는 다중 사용자 운영 체제입니다. NPU 장치 드라이버는 `furiosa`라는 그룹을 생성하고, `furiosa` 그룹의 구성원인 사용자에게만 NPU 장치 접근을 제한합니다. `furiosa` 그룹의 구성원으로 사용자를 추가하려면 다음과 같이 실행하십시오:\n```bash\nsudo usermod -aG furiosa <username>\n```\n<username>을 `furiosa` 그룹에 추가하려는 사용자의 이름으로 교체하십시오. 예를 들어, 현재 사용자(즉, `$USER`)를 `furiosa` 그룹에 추가하려면 다음과 같이 실행할 수 있습니다:\n```bash\nsudo usermod -aG furiosa $USER\n```\n로그아웃 후 다시 로그인하면 그룹 구성원 변경이 적용됩니다.\n\n### 설치된 버전 유지\/해제 [](#holding-unholding-installed-version \"이 제목의 영구 링크\")\n패키지 설치 후 안정적인 운영 환경을 유지하기 위해 설치된 패키지 버전을 유지해야 할 수 있습니다. 아래 명령을 사용하여 현재 설치된 버전을 유지할 수 있습니다.\n```bash\nsudo apt-mark hold furiosa-driver-warboy furiosa-libhal-warboy furiosa-libnux libonnxruntime\n```\n현재 패키지 버전을 해제하고 업데이트하려면, `apt-mark unhold` 명령으로 해제하려는 패키지를 지정하십시오. 이미 유지된 패키지의 속성을 표시하려면 `apt-mark showhold` 명령을 사용하십시오.\n```bash\nsudo apt-mark unhold furiosa-driver-warboy furiosa-libhal-warboy furiosa-libnux libonnxruntime\n```\n\n### 특정 버전 설치 [](#installing-a-specific-version \"이 제목의 영구 링크\")\n특정 버전을 설치해야 하는 경우, 원하는 버전을 지정하고 다음과 같이 설치할 수 있습니다.\n\n1. `apt list`를 통해 사용 가능한 버전을 확인합니다.\n```bash\nsudo apt list -a furiosa-libnux\n```\n\n2. `apt-get install` 명령에서 패키지 이름과 버전을 옵션으로 지정합니다.\n```bash\nsudo apt-get install -y furiosa-libnux=0.9.1-?\n```\n\nNPU 펌웨어 업데이트 [](#npu-firmware-update \"이 제목의 영구 링크\") -------------------------------------------------------------------------\n[이전](intro.html \"FuriosaAI SW 스택 소개\") [다음](python-sdk.html \"Python SDK 설치 및 사용자 가이드\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용했습니다.\n```\n"},{"page_id":"8bcf3c20-b1d0-44c9-bedc-9e24a3ce9586","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/cli.html","original_content":"* Command Line Tools * [View page source](..\/_sources\/software\/cli.rst.txt)\n---\nCommand Line Tools [](#command-line-tools \"Permalink to this heading\") =======================================================================\nThrough the command line tools, Furiosa SDK provides functions such as monitoring NPU device information, compiling models, and checking compatibility between models and SDKs. This section explains how to install and use each command line tool.\nfuriosa-toolkit [](#furiosa-toolkit \"Permalink to this heading\") -----------------------------------------------------------------  `furiosa-toolkit` provides a command line tool that enables users to manage and check the information of NPU devices.\n### furiosa-toolkit installation [](#furiosa-toolkit-installation \"Permalink to this heading\")\nTo use this command line tool, you first need to install the kernel driver as shown in [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) .\nSubsequently, follow the instructions below to install furiosa-toolkit.\nInstallation using APT server\n``` sudo apt-get install -y furiosa-toolkit\n```\n### furiosactl [](#furiosactl \"Permalink to this heading\")\nThe furiosactl command provides a variety of subcommands and has the ability to obtain information or control the device.\n``` furiosactl <sub command> [option] ..\n```\n#### `furiosactl info` [](#furiosactl-info \"Permalink to this heading\")\nAfter installing the kernel driver, you can use the `furiosactl` command to check whether the NPU device is recognized. Currently, this command provides the `furiosactl\ninfo` command to output temperature, power consumption and PCI information of the NPU device. If the device is not visible with this command after mounting it on the machine, [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) to install the driver. If you add the `--full` option to the `info` command, you can see the device’s UUID and serial number information together.\n``` $ furiosactl info +------+--------+----------------+-------+--------+--------------+ | NPU  | Name   | Firmware       | Temp. | Power  | PCI-BDF      | +------+--------+----------------+-------+--------+--------------+ | npu1 | warboy | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 | +------+--------+----------------+-------+--------+--------------+\n$ furiosactl info --full +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+ | NPU  | Name   | UUID                                 | S\/N               | Firmware       | Temp. | Power  | PCI-BDF      | PCI-DEV | +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+ | npu1 | warboy | 00000000-0000-0000-0000-000000000000 | WBYB0000000000000 | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 | 511:0   | +------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\n```\n#### `furiosactl list` [](#furiosactl-list \"Permalink to this heading\")\nThe `list` subcommand provides information about the device files available on the NPU device. You can also check whether each core present in the NPU is in use or idle.\n``` furiosactl list +------+------------------------------+-----------------------------------+ | NPU  | Cores                        | DEVFILES                          | +------+------------------------------+-----------------------------------+ | npu1 | 0 (available), 1 (available) | npu1, npu1pe0, npu1pe1, npu1pe0-1 | +------+------------------------------+-----------------------------------+\n```\n#### `furiosactl ps` [](#furiosactl-ps \"Permalink to this heading\")\nThe `ps` subcommand prints information about the OS process currently occupying the NPU device.\n``` $ furiosactl ps +-----------+--------+------------------------------------------------------------+ | NPU       | PID    | CMD                                                        | +-----------+--------+------------------------------------------------------------+ | npu0pe0-1 | 132529 | \/usr\/bin\/python3 \/usr\/local\/bin\/uvicorn image_classify:app | +-----------+--------+------------------------------------------------------------+\n```\n#### `furiosactl top` (experimental) [](#furiosactl-top-experimental \"Permalink to this heading\")\nThe `top` subcommand is used to view utilization by NPU unit over time. The output has the following meaning By default, utilization is calculated every 1 second, but you can set the calculation interval yourself with the `--interval` option. (unit: ms)\nfuriosa top fields\n[](#id1 \"Permalink to this table\")\n| Item | Description | | --- | --- | | Datetime | Observation time | | PID | Process ID that is using the NPU | | Device | NPU device in use | | NPU(%) | Percentage of time the NPU was used during the observation time. | | Comp(%) | Percentage of time the NPU was used for computation during the observation time | | I\/O (%) | Percentage of time the NPU was used for I\/O out of the time the NPU was used | | Command | Executed command line of the process |\n``` $ furiosactl top --interval 200 NOTE: furiosa top is under development. Usage and output formats may change. Please enter Ctrl+C to stop. Datetime                        PID       Device        NPU(%)   Comp(%)   I\/O(%)   Command 2023-03-21T09:45:56.699483936Z  152616    npu1pe0-1      19.06    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:56.906443888Z  152616    npu1pe0-1      51.09     93.05     6.95   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.110489333Z  152616    npu1pe0-1      46.40     97.98     2.02   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.316060982Z  152616    npu1pe0-1      51.43    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.521140588Z  152616    npu1pe0-1      54.28     94.10     5.90   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.725910558Z  152616    npu1pe0-1      48.93     98.93     1.07   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:57.935041998Z  152616    npu1pe0-1      47.91    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf 2023-03-21T09:45:58.13929122Z   152616    npu1pe0-1      49.06     94.94     5.06   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n```\nfuriosa-bench (Benchmark Tool) [](#furiosa-bench-benchmark-tool \"Permalink to this heading\") ---------------------------------------------------------------------------------------------  `furiosa-bench` command carries out a benchmark with a ONNX or TFLite model and a workload using furiosa-runtime. A benchmark result includes tail latency and QPS.\nThe arguments of the command are as follows:\n``` $ furiosa-bench --help USAGE:   furiosa-bench [OPTIONS] <model-path>\n  OPTIONS:       -b, --batch <number>                       Sets the number of batch size, which should be exponents of two [default: 1]       -o, --output <bench-result-path>           Create json file that has information about the benchmark       -C, --compiler-config <compiler-config>    Sets a file path for compiler configuration (YAML format)       -d, --devices <devices>                    Designates NPU devices to be used (e.g., \"warboy(2)*1\" or \"npu0pe0-1\")       -h, --help                                 Prints help information       -t, --io-threads <number>                  Sets the number of I\/O Threads [default: 1]           --duration <min-duration>              Sets the minimum test time in seconds. Both min_query_count and min_duration should be met to finish the test                                                 [default: 0]       -n, --queries <min-query-count>            Sets the minimum number of test queries. Both min_query_count and min_duration_ms should be met to finish the                                                 test [default: 1]       -T, --trace-output <trace-output>          Sets a file path for profiling result (Chrome Trace JSON format)       -V, --version                              Prints version information       -v, --verbose                              Print verbose log       -w, --workers <number>                     Sets the number of workers [default: 1]           --workload <workload>                  Sets the bench workload which can be either latency-oriented (L) or throughput-oriented (T) [default: L]\n  ARGS:       <model-path>\n```\nMODEL\\_PATH is the file path of ONNX, TFLite or ENF (format produced by [furiosa-compiler](compiler.html#compilercli) ).\nThe following is an example usage of furiosa-bench without an output path option (i.e., `--output` ):\n``` $ furiosa-bench mnist-8.onnx --workload L -n 1000 -w 8 -t 2\n  ======================================================================   This benchmark was executed with latency-workload which prioritizes latency of individual queries over throughput.   1000 queries executed with batch size 1   Latency stats are as follows   QPS(Throughput): 34.40\/s\n  Per-query latency:   Min latency (us)    : 8399   Max latency (us)    : 307568   Mean latency (us)   : 29040   50th percentile (us): 19329   95th percentile (us): 62797   99th percentile (us): 79874   99th percentile (us): 307568\n```\nIf an output path is specified, furiosa-bench will save a JSON document as follows:\n``` $ furiosa-bench mnist-8.onnx --workload L -n 1000 -w 8 -t 2 -o mnist.json $ cat mnist.json\n  {       \"model_data\": {           \"path\": \".\/mnist-8.onnx\",           \"md5\": \"d7cd24a0a76cd492f31065301d468c3d  .\/mnist-8.onnx\"       },       \"compiler_version\": \"0.10.0-dev (rev: 2d862de8a built_at: 2023-07-13T20:05:04Z)\",       \"hal_version\": \"Version: 0.12.0-2+nightly-230716\",       \"git_revision\": \"fe6f77a\",       \"result\": {           \"mode\": \"Latency\",           \"total run time\": \"30025 us\",           \"total num queries\": 1000,           \"batch size\": 1,           \"qps\": \"33.31\/s\",           \"latency stats\": {               \"min\": \"8840 us\",               \"max\": \"113254 us\",               \"mean\": \"29989 us\",               \"50th percentile\": \"18861 us\",               \"95th percentile\": \"64927 us\",               \"99th percentile\": \"87052 us\",               \"99.9th percentile\": \"113254 us\"           }       }   }\n```\nfuriosa [](#furiosa \"Permalink to this heading\") -------------------------------------------------\nThe `furiosa` command is a meta-command line tool that can be used by installing the Python SDK <PythonSDK>\n. Additional subcommands are also added when the extension package is installed.\nIf the Python execution environment is not prepared, refer to [Python execution environment setup](python-sdk.html#setuppython) .\nInstalling command line tool.\n``` $ pip install furiosa-sdk\n```\nVerifying installation.\n``` $ furiosa compile --version libnpu.so --- v2.0, built @ fe1fca3 0.5.0 (rev: 49b97492a built at 2021-12-07 04:07:08) (wrapper: None)\n```\n### furiosa compile [](#furiosa-compile \"Permalink to this heading\")\nThe `compile` command compiles models such as [ONNX](https:\/\/onnx.ai\/) and [TFLite](https:\/\/www.tensorflow.org\/lite) , generating programs that utilize FuriosaAI NPU.\nDetailed explanations and options can be found in the [furiosa-compiler](compiler.html#compilercli) page.\n### furiosa litmus (Model Compatibility Checker) [](#furiosa-litmus-model-compatibility-checker \"Permalink to this heading\")\nThe `litmus` is a tool to check quickly if an [ONNX](https:\/\/onnx.ai\/) model can work normally with Furiosa SDK using NPU. `litmus` goes through all usage steps of Furiosa SDK, including quantization, compilation, and inferences on FuriosaAI NPU. `litmus` is also a useful bug reporting tool. If you specify `--dump` option, `litmus` will collect logs and environment information and dump an archive file. The archive file can be used to report issues.\nThe steps executed by `litmus` command are as follows.\n> * Step1: Load an input model and check it is a valid model. > * Step2: Quantize the model with random calibration. > * Step3: Compile the quantized model. > * Step4: Inference the compiled model using >   `furiosa-bench` >   . This step is skipped if >   `furiosa-bench` >   was not installed.\nUsage:\n``` furiosa-litmus [-h] [--dump OUTPUT_PREFIX] [--skip-quantization] [--target-npu TARGET_NPU] [-v] model_path\n```\nA simple example using `litmus` command is as follows.\n``` $ furiosa litmus model.onnx libfuriosa_hal.so --- v0.11.0, built @ 43c901f INFO:furiosa.common.native:loaded native library libfuriosa_compiler.so.0.10.0 (0.10.0-dev d7548b7f6) furiosa-quantizer 0.10.0 (rev. 9ecebb6) furiosa-litmus 0.10.0 (rev. 9ecebb6) [Step 1] Checking if the model can be loaded and optimized ... [Step 1] Passed [Step 2] Checking if the model can be quantized ... [Step 2] Passed [Step 3] Checking if the model can be compiled for the NPU family [warboy-2pe] ... [1\/6] 🔍   Compiling from onnx to dfg Done in 0.09272794s [2\/6] 🔍   Compiling from dfg to ldfg ▪▪▪▪▪ [1\/3] Splitting graph(LAS)...Done in 9.034934s ▪▪▪▪▪ [2\/3] Lowering graph(LAS)...Done in 20.140083s ▪▪▪▪▪ [3\/3] Optimizing graph...Done in 0.019548794s Done in 29.196825s [3\/6] 🔍   Compiling from ldfg to cdfg Done in 0.001701888s [4\/6] 🔍   Compiling from cdfg to gir Done in 0.015205072s [5\/6] 🔍   Compiling from gir to lir Done in 0.0038304s [6\/6] 🔍   Compiling from lir to enf Done in 0.020943863s ✨  Finished in 29.331545s [Step 3] Passed [Step 4] Perform inference once for data collection... (Optional) ✨  Finished in 0.000001198s ====================================================================== This benchmark was executed with latency-workload which prioritizes latency of individual queries over throughput. 1 queries executed with batch size 1 Latency stats are as follows QPS(Throughput): 125.00\/s\nPer-query latency: Min latency (us)    : 7448 Max latency (us)    : 7448 Mean latency (us)   : 7448 50th percentile (us): 7448 95th percentile (us): 7448 99th percentile (us): 7448 99th percentile (us): 7448 [Step 4] Finished\n```\nIf you have quantized model already, you can skip Step1 and Step2 with `--skip-quantization` option.\n``` $ furiosa litmus --skip-quantization quantized-model.onnx libfuriosa_hal.so --- v0.11.0, built @ 43c901f INFO:furiosa.common.native:loaded native library libfuriosa_compiler.so.0.10.0 (0.10.0-dev d7548b7f6) furiosa-quantizer 0.10.0 (rev. 9ecebb6) furiosa-litmus 0.10.0 (rev. 9ecebb6) [Step 1] Skip model loading and optimization [Step 2] Skip model quantization [Step 1 & Step 2] Load quantized model ... [Step 3] Checking if the model can be compiled for the NPU family [warboy-2pe] ... ...\n```\nYou can use the `--dump\n<path>` option to create a <path>-<unix\\_epoch>.zip\nfile that contains metadata necessary for analysis, such as compilation logs, runtime logs, software versions, and execution environments. If you have any problems, you can get support through [FuriosaAI customer service center](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals) with this zip file.\n``` $ furiosa litmus --dump archive model.onnx libfuriosa_hal.so --- v0.11.0, built @ 43c901f INFO:furiosa.common.native:loaded native library libfuriosa_compiler.so.0.10.0 (0.10.0-dev d7548b7f6) furiosa-quantizer 0.10.0 (rev. 9ecebb6) furiosa-litmus 0.10.0 (rev. 9ecebb6) [Step 1] Checking if the model can be loaded and optimized ... [Step 1] Passed ...\n$ zipinfo -1 archive-1690438803.zip archive-16904388032l4hoi3h\/meta.yaml archive-16904388032l4hoi3h\/compiler\/compiler.log archive-16904388032l4hoi3h\/compiler\/memory-analysis.html archive-16904388032l4hoi3h\/compiler\/model.dot archive-16904388032l4hoi3h\/runtime\/trace.json\n```\n[Previous](c-sdk.html \"C SDK installation and user guide\") [Next](compiler.html \"Compiler\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 명령줄 도구 * [페이지 소스 보기](..\/_sources\/software\/cli.rst.txt)\n---\n명령줄 도구 [](#command-line-tools \"이 제목의 영구 링크\") =======================================================================\n명령줄 도구를 통해 Furiosa SDK는 NPU 장치 정보 모니터링, 모델 컴파일, 모델과 SDK 간의 호환성 확인 등의 기능을 제공합니다. 이 섹션에서는 각 명령줄 도구를 설치하고 사용하는 방법을 설명합니다.\n\nfuriosa-toolkit [](#furiosa-toolkit \"이 제목의 영구 링크\") ----------------------------------------------------------------- `furiosa-toolkit`은 사용자가 NPU 장치의 정보를 관리하고 확인할 수 있는 명령줄 도구를 제공합니다.\n\n### furiosa-toolkit 설치 [](#furiosa-toolkit-installation \"이 제목의 영구 링크\")\n이 명령줄 도구를 사용하려면 먼저 [드라이버, 펌웨어 및 런타임 설치](installation.html#requiredpackages)에서 설명한 대로 커널 드라이버를 설치해야 합니다. 이후 아래의 지침에 따라 furiosa-toolkit을 설치하십시오.\n\nAPT 서버를 사용한 설치\n```bash\nsudo apt-get install -y furiosa-toolkit\n```\n\n### furiosactl [](#furiosactl \"이 제목의 영구 링크\")\nfuriosactl 명령은 다양한 하위 명령을 제공하며 장치의 정보를 얻거나 제어할 수 있는 기능을 가지고 있습니다.\n```bash\nfuriosactl <sub command> [option] ..\n```\n\n#### `furiosactl info` [](#furiosactl-info \"이 제목의 영구 링크\")\n커널 드라이버를 설치한 후, `furiosactl` 명령을 사용하여 NPU 장치가 인식되는지 확인할 수 있습니다. 현재 이 명령은 NPU 장치의 온도, 전력 소모 및 PCI 정보를 출력하는 `furiosactl info` 명령을 제공합니다. 장치를 머신에 장착한 후 이 명령으로 장치가 보이지 않으면, 드라이버를 설치하기 위해 [드라이버, 펌웨어 및 런타임 설치](installation.html#requiredpackages)를 참조하십시오. `info` 명령에 `--full` 옵션을 추가하면 장치의 UUID와 일련 번호 정보를 함께 볼 수 있습니다.\n```bash\n$ furiosactl info\n+------+--------+----------------+-------+--------+--------------+\n| NPU  | Name   | Firmware       | Temp. | Power  | PCI-BDF      |\n+------+--------+----------------+-------+--------+--------------+\n| npu1 | warboy | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 |\n+------+--------+----------------+-------+--------+--------------+\n\n$ furiosactl info --full\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\n| NPU  | Name   | UUID                                 | S\/N               | Firmware       | Temp. | Power  | PCI-BDF      | PCI-DEV |\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\n| npu1 | warboy | 00000000-0000-0000-0000-000000000000 | WBYB0000000000000 | 1.6.0, 3c10fd3 |  54°C | 0.99 W | 0000:44:00.0 | 511:0   |\n+------+--------+--------------------------------------+-------------------+----------------+-------+--------+--------------+---------+\n```\n\n#### `furiosactl list` [](#furiosactl-list \"이 제목의 영구 링크\")\n`list` 하위 명령은 NPU 장치에서 사용 가능한 장치 파일에 대한 정보를 제공합니다. 또한 NPU에 존재하는 각 코어가 사용 중인지 유휴 상태인지 확인할 수 있습니다.\n```bash\nfuriosactl list\n+------+------------------------------+-----------------------------------+\n| NPU  | Cores                        | DEVFILES                          |\n+------+------------------------------+-----------------------------------+\n| npu1 | 0 (available), 1 (available) | npu1, npu1pe0, npu1pe1, npu1pe0-1 |\n+------+------------------------------+-----------------------------------+\n```\n\n#### `furiosactl ps` [](#furiosactl-ps \"이 제목의 영구 링크\")\n`ps` 하위 명령은 현재 NPU 장치를 점유하고 있는 OS 프로세스에 대한 정보를 출력합니다.\n```bash\n$ furiosactl ps\n+-----------+--------+------------------------------------------------------------+\n| NPU       | PID    | CMD                                                        |\n+-----------+--------+------------------------------------------------------------+\n| npu0pe0-1 | 132529 | \/usr\/bin\/python3 \/usr\/local\/bin\/uvicorn image_classify:app |\n+-----------+--------+------------------------------------------------------------+\n```\n\n#### `furiosactl top` (실험적) [](#furiosactl-top-experimental \"이 제목의 영구 링크\")\n`top` 하위 명령은 시간에 따른 NPU 단위의 활용도를 보기 위해 사용됩니다. 기본적으로 활용도는 1초마다 계산되지만, `--interval` 옵션을 사용하여 계산 간격을 직접 설정할 수 있습니다. (단위: ms)\n\nfuriosa top 필드 [](#id1 \"이 표의 영구 링크\")\n| 항목 | 설명 |\n| --- | --- |\n| Datetime | 관찰 시간 |\n| PID | NPU를 사용하는 프로세스 ID |\n| Device | 사용 중인 NPU 장치 |\n| NPU(%) | 관찰 시간 동안 NPU가 사용된 시간의 비율 |\n| Comp(%) | 관찰 시간 동안 NPU가 계산에 사용된 시간의 비율 |\n| I\/O (%) | NPU가 사용된 시간 중 I\/O에 사용된 시간의 비율 |\n| Command | 프로세스의 실행된 명령줄 |\n\n```bash\n$ furiosactl top --interval 200\nNOTE: furiosa top은 개발 중입니다. 사용법 및 출력 형식이 변경될 수 있습니다. 중지하려면 Ctrl+C를 입력하세요.\nDatetime                        PID       Device        NPU(%)   Comp(%)   I\/O(%)   Command\n2023-03-21T09:45:56.699483936Z  152616    npu1pe0-1      19.06    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n2023-03-21T09:45:56.906443888Z  152616    npu1pe0-1      51.09     93.05     6.95   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n2023-03-21T09:45:57.110489333Z  152616    npu1pe0-1      46.40     97.98     2.02   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n2023-03-21T09:45:57.316060982Z  152616    npu1pe0-1      51.43    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n2023-03-21T09:45:57.521140588Z  152616    npu1pe0-1      54.28     94.10     5.90   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n2023-03-21T09:45:57.725910558Z  152616    npu1pe0-1      48.93     98.93     1.07   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n2023-03-21T09:45:57.935041998Z  152616    npu1pe0-1      47.91    100.00     0.00   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n2023-03-21T09:45:58.13929122Z   152616    npu1pe0-1      49.06     94.94     5.06   .\/npu_runtime_test -n 10000 results\/ResNet-CTC_kor1_200_nightly3_128dpes_8batches.enf\n```\n\nfuriosa-bench (벤치마크 도구) [](#furiosa-bench-benchmark-tool \"이 제목의 영구 링크\") --------------------------------------------------------------------------------------------- `furiosa-bench` 명령은 ONNX 또는 TFLite 모델과 furiosa-runtime을 사용한 워크로드로 벤치마크를 수행합니다. 벤치마크 결과에는 꼬리 지연 시간과 QPS가 포함됩니다.\n\n명령의 인수는 다음과 같습니다:\n```bash\n$ furiosa-bench --help\nUSAGE:\n  furiosa-bench [OPTIONS] <model-path>\n\nOPTIONS:\n  -b, --batch <number>                       Sets the number of batch size, which should be exponents of two [default: 1]\n  -o, --output <bench-result-path>           Create json file that has information about the benchmark\n  -C, --compiler-config <compiler-config>    Sets a file path for compiler configuration (YAML format)\n  -d, --devices <devices>                    Designates NPU devices to be used (e.g., \"warboy(2)*1\" or \"npu0pe0-1\")\n  -h, --help                                 Prints help information\n  -t, --io-threads <number>                  Sets the number of I\/O Threads [default: 1]\n      --duration <min-duration>              Sets the minimum test time in seconds. Both min_query_count and min_duration should be met to finish the test\n                                             [default: 0]\n  -n, --queries <min-query-count>            Sets the minimum number of test queries. Both min_query_count and min_duration_ms should be met to finish the\n                                             test [default: 1]\n  -T, --trace-output <trace-output>          Sets a file path for profiling result (Chrome Trace JSON format)\n  -V, --version                              Prints version information\n  -v, --verbose                              Print verbose log\n  -w, --workers <number>                     Sets the number of workers [default: 1]\n      --workload <workload>                  Sets the bench workload which can be either latency-oriented (L) or throughput-oriented (T) [default: L]\n\nARGS:\n  <model-path>\n```\n\nMODEL_PATH는 [furiosa-compiler](compiler.html#compilercli)에서 생성된 ONNX, TFLite 또는 ENF 형식의 파일 경로입니다. 출력 경로 옵션(i.e., `--output`) 없이 furiosa-bench를 사용하는 예는 다음과 같습니다:\n```bash\n$ furiosa-bench mnist-8.onnx --workload L -n 1000 -w 8 -t 2\n  ======================================================================\n  이 벤치마크는 개별 쿼리의 지연 시간을 처리량보다 우선시하는 지연 시간 워크로드로 실행되었습니다.\n  1000개의 쿼리가 배치 크기 1로 실행되었습니다.\n  지연 시간 통계는 다음과 같습니다.\n  QPS(Throughput): 34.40\/s\n\n  쿼리당 지연 시간:\n  Min latency (us)    : 8399\n  Max latency (us)    : 307568\n  Mean latency (us)   : 29040\n  50th percentile (us): 19329\n  95th percentile (us): 62797\n  99th percentile (us): 79874\n  99th percentile (us): 307568\n```\n\n출력 경로가 지정된 경우, furiosa-bench는 다음과 같이 JSON 문서를 저장합니다:\n```bash\n$ furiosa-bench mnist-8.onnx --workload L -n 1000 -w 8 -t 2 -o mnist.json\n$ cat mnist.json\n  {\n      \"model_data\": {\n          \"path\": \".\/mnist-8.onnx\",\n          \"md5\": \"d7cd24a0a76cd492f31065301d468c3d  .\/mnist-8.onnx\"\n      },\n      \"compiler_version\": \"0.10.0-dev (rev: 2d862de8a built_at: 2023-07-13T20:05:04Z)\",\n      \"hal_version\": \"Version: 0.12.0-2+nightly-230716\",\n      \"git_revision\": \"fe6f77a\",\n      \"result\": {\n          \"mode\": \"Latency\",\n          \"total run time\": \"30025 us\",\n          \"total num queries\": 1000,\n          \"batch size\": 1,\n          \"qps\": \"33.31\/s\",\n          \"latency stats\": {\n              \"min\": \"8840 us\",\n              \"max\": \"113254 us\",\n              \"mean\": \"29989 us\",\n              \"50th percentile\": \"18861 us\",\n              \"95th percentile\": \"64927 us\",\n              \"99th percentile\": \"87052 us\",\n              \"99.9th percentile\": \"113254 us\"\n          }\n      }\n  }\n```\n\nfuriosa [](#furiosa \"이 제목의 영구 링크\") -------------------------------------------------\n`furiosa` 명령은 Python SDK <PythonSDK>를 설치하여 사용할 수 있는 메타 명령줄 도구입니다. 확장 패키지를 설치하면 추가 하위 명령도 추가됩니다. Python 실행 환경이 준비되지 않은 경우 [Python 실행 환경 설정](python-sdk.html#setuppython)을 참조하십시오.\n\n명령줄 도구 설치.\n```bash\n$ pip install furiosa-sdk\n```\n\n설치 확인.\n```bash\n$ furiosa compile --version\nlibnpu.so --- v2.0, built @ fe1fca3 0.5.0 (rev: 49b97492a built at 2021-12-07 04:07:08) (wrapper: None)\n```\n\n### furiosa compile [](#furiosa-compile \"이 제목의 영구 링크\")\n`compile` 명령은 [ONNX](https:\/\/onnx.ai\/) 및 [TFLite](https:\/\/www.tensorflow.org\/lite)와 같은 모델을 컴파일하여 FuriosaAI NPU를 활용하는 프로그램을 생성합니다. 자세한 설명과 옵션은 [furiosa-compiler](compiler.html#compilercli) 페이지에서 확인할 수 있습니다.\n\n### furiosa litmus (모델 호환성 검사기) [](#furiosa-litmus-model-compatibility-checker \"이 제목의 영구 링크\")\n`litmus`는 [ONNX](https:\/\/onnx.ai\/) 모델이 NPU를 사용하여 Furiosa SDK와 정상적으로 작동할 수 있는지 빠르게 확인하는 도구입니다. `litmus`는 Furiosa SDK의 모든 사용 단계를 거치며, 양자화, 컴파일 및 FuriosaAI NPU에서의 추론을 포함합니다. `litmus`는 또한 유용한 버그 보고 도구입니다. `--dump` 옵션을 지정하면 `litmus`는 로그와 환경 정보를 수집하여 아카이브 파일을 덤프합니다. 아카이브 파일은 문제를 보고하는 데 사용할 수 있습니다.\n\n`litmus` 명령이 실행하는 단계는 다음과 같습니다.\n> * Step1: 입력 모델을 로드하고 유효한 모델인지 확인합니다.\n> * Step2: 무작위 보정을 통해 모델을 양자화합니다.\n> * Step3: 양자화된 모델을 컴파일합니다.\n> * Step4: 컴파일된 모델을 사용하여 추론을 수행합니다. `furiosa-bench`가 설치되지 않은 경우 이 단계는 건너뜁니다.\n\n사용법:\n```bash\nfuriosa-litmus [-h] [--dump OUTPUT_PREFIX] [--skip-quantization] [--target-npu TARGET_NPU] [-v] model_path\n```\n\n`litmus` 명령을 사용하는 간단한 예는 다음과 같습니다.\n```bash\n$ furiosa litmus model.onnx\nlibfuriosa_hal.so --- v0.11.0, built @ 43c901f\nINFO:furiosa.common.native:loaded native library libfuriosa_compiler.so.0.10.0 (0.10.0-dev d7548b7f6)\nfuriosa-quantizer 0.10.0 (rev. 9ecebb6)\nfuriosa-litmus 0.10.0 (rev. 9ecebb6)\n[Step 1] Checking if the model can be loaded and optimized ...\n[Step 1] Passed\n[Step 2] Checking if the model can be quantized ...\n[Step 2] Passed\n[Step 3] Checking if the model can be compiled for the NPU family [warboy-2pe] ...\n[1\/6] 🔍   Compiling from onnx to dfg Done in 0.09272794s\n[2\/6] 🔍   Compiling from dfg to ldfg ▪▪▪▪▪ [1\/3] Splitting graph(LAS)...Done in 9.034934s ▪▪▪▪▪ [2\/3] Lowering graph(LAS)...Done in 20.140083s ▪▪▪▪▪ [3\/3] Optimizing graph...Done in 0.019548794s Done in 29.196825s\n[3\/6] 🔍   Compiling from ldfg to cdfg Done in 0.001701888s\n[4\/6] 🔍   Compiling from cdfg to gir Done in 0.015205072s\n[5\/6] 🔍   Compiling from gir to lir Done in 0.0038304s\n[6\/6] 🔍   Compiling from lir to enf Done in 0.020943863s\n✨  Finished in 29.331545s\n[Step 3] Passed\n[Step 4] Perform inference once for data collection... (Optional)\n✨  Finished in 0.000001198s\n======================================================================\n이 벤치마크는 개별 쿼리의 지연 시간을 처리량보다 우선시하는 지연 시간 워크로드로 실행되었습니다.\n1개의 쿼리가 배치 크기 1로 실행되었습니다.\n지연 시간 통계는 다음과 같습니다.\nQPS(Throughput): 125.00\/s\n\n쿼리당 지연 시간:\nMin latency (us)    : 7448\nMax latency (us)    : 7448\nMean latency (us)   : 7448\n50th percentile (us): 7448\n95th percentile (us): 7448\n99th percentile (us): 7448\n99th percentile (us): 7448\n[Step 4] Finished\n```\n\n이미 양자화된 모델이 있는 경우 `--skip-quantization` 옵션으로 Step1과 Step2를 건너뛸 수 있습니다.\n```bash\n$ furiosa litmus --skip-quantization quantized-model.onnx\nlibfuriosa_hal.so --- v0.11.0, built @ 43c901f\nINFO:furiosa.common.native:loaded native library libfuriosa_compiler.so.0.10.0 (0.10.0-dev d7548b7f6)\nfuriosa-quantizer 0.10.0 (rev. 9ecebb6)\nfuriosa-litmus 0.10.0 (rev. 9ecebb6)\n[Step 1] Skip model loading and optimization\n[Step 2] Skip model quantization\n[Step 1 & Step 2] Load quantized model ...\n[Step 3] Checking if the model can be compiled for the NPU family [warboy-2pe] ...\n...\n```\n\n`--dump <path>` 옵션을 사용하여 컴파일 로그, 런타임 로그, 소프트웨어 버전 및 실행 환경과 같은 분석에 필요한 메타데이터를 포함하는 <path>-<unix_epoch>.zip 파일을 생성할 수 있습니다. 문제가 있는 경우, 이 zip 파일을 통해 [FuriosaAI 고객 서비스 센터](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals)에서 지원을 받을 수 있습니다.\n```bash\n$ furiosa litmus --dump archive model.onnx\nlibfuriosa_hal.so --- v0.11.0, built @ 43c901f\nINFO:furiosa.common.native:loaded native library libfuriosa_compiler.so.0.10.0 (0.10.0-dev d7548b7f6)\nfuriosa-quantizer 0.10.0 (rev. 9ecebb6)\nfuriosa-litmus 0.10.0 (rev. 9ecebb6)\n[Step 1] Checking if the model can be loaded and optimized ...\n[Step 1] Passed\n...\n$ zipinfo -1 archive-1690438803.zip\narchive-16904388032l4hoi3h\/meta.yaml\narchive-16904388032l4hoi3h\/compiler\/compiler.log\narchive-16904388032l4hoi3h\/compiler\/memory-analysis.html\narchive-16904388032l4hoi3h\/compiler\/model.dot\narchive-16904388032l4hoi3h\/runtime\/trace.json\n```\n\n[이전](c-sdk.html \"C SDK 설치 및 사용자 가이드\") [다음](compiler.html \"컴파일러\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용했습니다.\n```"},{"page_id":"aa27e152-d0c0-4c00-99e9-54d1516e3a5b","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.6.0.html","original_content":"* Release Notes - 0.6.0 * [View page source](..\/_sources\/releases\/0.6.0.rst.txt)\n---\nRelease Notes - 0.6.0 [](#release-notes-0-6-0 \"Permalink to this heading\") ===========================================================================\nFuriosaAI SDK 0.6.0 is a major release. It includes 234 PRs on performance improvements, added functionalities, and bug fixes, as well as approximately 900 commits.\nHow to upgrade [](#how-to-upgrade \"Permalink to this heading\") ---------------------------------------------------------------\nIf you are using the APT repositories, you easily upgrade with the instructions below: More detailed instructions can be found at [Driver, Firmware, and Runtime Installation](..\/software\/installation.html#requiredpackages) .\n> ``` > apt-get update && \\ > apt-get install furiosa-driver-pdma furiosa-libnpu-warboy furiosa-libnux >  > pip uninstall furiosa-sdk-quantizer furiosa-sdk-runtime furiosa-sdk-validator && \\ > pip install --upgrade furiosa-sdk >  > ```\nMajor changes [](#major-changes \"Permalink to this heading\") -------------------------------------------------------------\nThe kernel driver (furiosa-driver-pdma) has been upgraded to 1.2.2, and the user-level driver (furiosa-libnpu-warboy) has been upgraded to 0.5.2, thereby providing more stable and higher NPU performance. Other major changes include the following:\n### Compiler [](#compiler \"Permalink to this heading\")\n* Addition of NPU accelerated operators (see   [List of Supported Operators for Warboy Acceleration](..\/npu\/warboy.html#supportedoperators)   for full list of accelerated operators)      > + Space-to-depth (CRD mode)   > + Transpose   > + Slice (height axis only)   > + Concat (height axis only)   > + Grouped Convolution (if groups <= 128) * Improvements to significantly reduce frequency of CPU tasks in models with   operators that require large memory usage (reduced execution time)\n### Quantizer [](#quantizer \"Permalink to this heading\")\n* Improve model quantization process to ensure idempotency * Remove PyTorch reliance * Improve code quality by removing multiple Pylint warnings * Upgrade multiple library dependencies (e.g. Numpy -> 1.21.5, Pyyaml -> 6.0.0)\n### Python SDK [](#python-sdk \"Permalink to this heading\")\n* Python SDK project structure change      + furiosa-sdk-runtime -> furiosa-sdk   + furiosa-sdk-quantizer -> furiosa-quantizer   + furiosa-sdk-validator -> furiosa-litmus * Validator, a package that checks for model compatibility with Furiosa SDK, is renamed to litmus. Installation instruction has also been updated accordingly.\nSee [furiosa litmus (Model Compatibility Checker)](..\/software\/cli.html#litmus) for more detailed usage instructions.\n> ``` > $ pip install 'furiosa-sdk[litmus]' >  > ```\n#### Furiosa Serving: Addition of FastAPI-based advanced serving library [](#furiosa-serving-addition-of-fastapi-based-advanced-serving-library \"Permalink to this heading\")\nfuriosa-serving is based on FastAPI. It allows you to easily add Python-based business logic or image pre\/postprocessing code, before or after executing model inference API.\nYou can install using the following instructions.\n> ``` > $ pip install 'furiosa-sdk[serving]' >  > ```\nThe usage example is shown below. You can find more detailed instructions at [furiosa-serving Github](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/main\/python\/furiosa-serving) .\n> ``` > from typing import Dict >  > from fastapi import File, UploadFile > from furiosa.server.utils.thread import synchronous > from furiosa.serving import ServeAPI, ServeModel > import numpy as np >  >  > serve = ServeAPI() >  >  > model: ServeModel = synchronous(serve.model)( >     'imagenet', >     location='.\/examples\/assets\/models\/image_classification.onnx' > ) >  > @model.post(\"\/models\/imagenet\/infer\") > async def infer(image: UploadFile = File(...)) -> Dict: >     # Convert image to Numpy array with your preprocess() function >     tensors: List[np.ndarray] = preprocess(image) >  >     # Infer from ServeModel >     result: List[np.ndarray] = await model.predict(tensors) >  >     # Classify model from numpy array with your postprocess() function >     response: Dict = postprocess(result) >  >     return response >  > ```\n[Previous](0.7.0.html \"Release Notes - 0.7.0\") [Next](0.5.0.html \"Release Notes - 0.5.0\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 릴리스 노트 - 0.6.0 * [페이지 소스 보기](..\/_sources\/releases\/0.6.0.rst.txt)\n---\n릴리스 노트 - 0.6.0 [](#release-notes-0-6-0 \"이 제목의 영구 링크\") ===========================================================================\nFuriosaAI SDK 0.6.0은 주요 릴리스입니다. 성능 개선, 기능 추가 및 버그 수정에 대한 234개의 PR과 약 900개의 커밋이 포함되어 있습니다.\n업그레이드 방법 [](#how-to-upgrade \"이 제목의 영구 링크\") ---------------------------------------------------------------\nAPT 저장소를 사용 중인 경우 아래 지침을 통해 쉽게 업그레이드할 수 있습니다. 보다 자세한 지침은 [드라이버, 펌웨어 및 런타임 설치](..\/software\/installation.html#requiredpackages)에서 확인할 수 있습니다.\n> ``` > apt-get update && \\ > apt-get install furiosa-driver-pdma furiosa-libnpu-warboy furiosa-libnux >  > pip uninstall furiosa-sdk-quantizer furiosa-sdk-runtime furiosa-sdk-validator && \\ > pip install --upgrade furiosa-sdk >  > ```\n주요 변경 사항 [](#major-changes \"이 제목의 영구 링크\") -------------------------------------------------------------\n커널 드라이버(furiosa-driver-pdma)가 1.2.2로 업그레이드되었고, 사용자 수준 드라이버(furiosa-libnpu-warboy)가 0.5.2로 업그레이드되어 더 안정적이고 높은 NPU 성능을 제공합니다. 기타 주요 변경 사항은 다음과 같습니다:\n### 컴파일러 [](#compiler \"이 제목의 영구 링크\")\n* NPU 가속 연산자 추가 (전체 가속 연산자 목록은 [Warboy 가속을 위한 지원 연산자 목록](..\/npu\/warboy.html#supportedoperators) 참조)      > + Space-to-depth (CRD 모드)   > + Transpose   > + Slice (높이 축만)   > + Concat (높이 축만)   > + Grouped Convolution (그룹 <= 128인 경우) * 대규모 메모리 사용이 필요한 연산자를 포함한 모델에서 CPU 작업 빈도를 크게 줄여 실행 시간을 단축\n### 양자화기 [](#quantizer \"이 제목의 영구 링크\")\n* 모델 양자화 프로세스를 개선하여 항등성을 보장 * PyTorch 의존성 제거 * 여러 Pylint 경고 제거로 코드 품질 개선 * 여러 라이브러리 종속성 업그레이드 (예: Numpy -> 1.21.5, Pyyaml -> 6.0.0)\n### Python SDK [](#python-sdk \"이 제목의 영구 링크\")\n* Python SDK 프로젝트 구조 변경      + furiosa-sdk-runtime -> furiosa-sdk   + furiosa-sdk-quantizer -> furiosa-quantizer   + furiosa-sdk-validator -> furiosa-litmus * Furiosa SDK와의 모델 호환성을 확인하는 패키지인 Validator가 litmus로 이름이 변경되었습니다. 설치 지침도 이에 맞게 업데이트되었습니다.\n자세한 사용 지침은 [furiosa litmus (모델 호환성 검사기)](..\/software\/cli.html#litmus)를 참조하세요.\n> ``` > $ pip install 'furiosa-sdk[litmus]' >  > ```\n#### Furiosa Serving: FastAPI 기반의 고급 서빙 라이브러리 추가 [](#furiosa-serving-addition-of-fastapi-based-advanced-serving-library \"이 제목의 영구 링크\")\nfuriosa-serving은 FastAPI를 기반으로 합니다. 모델 추론 API를 실행하기 전후에 Python 기반의 비즈니스 로직이나 이미지 전처리\/후처리 코드를 쉽게 추가할 수 있습니다.\n다음 지침을 사용하여 설치할 수 있습니다.\n> ``` > $ pip install 'furiosa-sdk[serving]' >  > ```\n사용 예는 아래에 나와 있습니다. 보다 자세한 지침은 [furiosa-serving Github](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/main\/python\/furiosa-serving)에서 확인할 수 있습니다.\n> ``` > from typing import Dict >  > from fastapi import File, UploadFile > from furiosa.server.utils.thread import synchronous > from furiosa.serving import ServeAPI, ServeModel > import numpy as np >  >  > serve = ServeAPI() >  >  > model: ServeModel = synchronous(serve.model)( >     'imagenet', >     location='.\/examples\/assets\/models\/image_classification.onnx' > ) >  > @model.post(\"\/models\/imagenet\/infer\") > async def infer(image: UploadFile = File(...)) -> Dict: >     # Convert image to Numpy array with your preprocess() function >     tensors: List[np.ndarray] = preprocess(image) >  >     # Infer from ServeModel >     result: List[np.ndarray] = await model.predict(tensors) >  >     # Classify model from numpy array with your postprocess() function >     response: Dict = postprocess(result) >  >     return response >  > ```\n[이전](0.7.0.html \"릴리스 노트 - 0.7.0\") [다음](0.5.0.html \"릴리스 노트 - 0.5.0\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)와 [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용하여 제작되었습니다.\n```"},{"page_id":"ab2af890-296a-47f5-89d9-ffc5cf4a924f","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/python-sdk.html","original_content":"* Python SDK installation and user guide * [View page source](..\/_sources\/software\/python-sdk.rst.txt)\n---\nPython SDK installation and user guide [](#python-sdk-installation-and-user-guide \"Permalink to this heading\") ===============================================================================================================\nFuriosaAI Python SDK is a software development kit for writing Python applications that use the NPU. With the Python SDK, you can utilize various tools, libraries, and frameworks of the Python ecosystem that are most widely used in the AI\/ML field for developing NPU applications. Python SDK consists of various modules and provides an inference API, a quantization API, a command line tool, and a server program for serving.\nRequirements [](#requirements \"Permalink to this heading\") -----------------------------------------------------------\n* Ubuntu 20.04 LTS (Debian bullseye) or higher * [FuriosaAI SDK required packages](installation.html#requiredpackages) * Python 3.8 or higher (See   [Python execution environment setup](#setuppython)   for setup Python environment) * Latest version of pip\nTo install and use Python SDK, follow the [Installing required packages](installation.html#requiredpackages) guide. You need to install the required kernel driver, firmware, and runtime library.\nPython execution environment setup [](#python-execution-environment-setup \"Permalink to this heading\") -------------------------------------------------------------------------------------------------------\nPython SDK requires Python 3.8 or above. Here, we describe Python execution environment configuration.\nNote\nIf you are not using the FuriosaAI Python SDK, or if you are familiar with configuring a Python execution environment, you can skip this section.\nYou can check the Python version currently installed in your system with the command below.\n``` python --version Python 3.8.10\n```\nIf the Python command does not exist, or if your Python version is below 3.8, configure the Python environment by selecting one of the methods below.\n* [Python environment configuration with Conda](#condainstall)   (recommended):   [Conda](https:\/\/docs.conda.io\/projects\/conda\/en\/latest\/index.html)   allows you to   configure a separate, isolated Python environment for specific Python applications.   Conda therefore prevents the package dependency issues or Python version issues that users   often encounter when installing Python applications. * Configure the Python execution environment directly on the   [Configuring Python environment using Linux packages](#setuppythononlinux)   : Linux system.   You can select this option if you are not concerned about conflicts with other Python execution environments.\n### Python environment configuration with Conda [](#python-environment-configuration-with-conda \"Permalink to this heading\")\nConda makes it easy to configure a isolated Python environment for a specific Python application. To find out more about Conda, refer to readings available in [Conda](https:\/\/docs.conda.io\/projects\/conda\/en\/latest\/index.html) .\nYou can get started by downloading the installer as shown below. Select yes\nto all questions when running `sh\n.\/Miniconda3-latest-Linux-x86_64.sh` .\n``` wget https:\/\/repo.anaconda.com\/miniconda\/Miniconda3-latest-Linux-x86_64.sh sh .\/Miniconda3-latest-Linux-x86_64.sh source ~\/.bashrc conda --version\n```\n#### Creating and activating isolated Python execution environment [](#creating-and-activating-isolated-python-execution-environment \"Permalink to this heading\")\nAfter installing Anaconda, you can create an isolated Python execution environment and activate it as needed.\n1. If you want to use Python 3.8, create an execution environment with the name    `furiosa-3.8`    , by using the following command.\n``` conda create -n furiosa-3.8 python=3.8\n```\n2. The created Python 3.8 environment is activated with the    `activate`    command.\n``` conda activate furiosa-3.8 # version check python --version\n```\n3. Once the Python execution environment is activated, install the Python SDK as explained in    [Installing Python SDK package](#installpippackages)    . 4. If you wish to terminate the Python execution environment, use the    `deactivate`    command.\n``` $ conda deactivate\n```\nAn environment created once can be used again at any time with the `activate` command. Packages that have been installed do not need to be reinstalled after activation.\n### Configuring Python environment using Linux packages [](#configuring-python-environment-using-linux-packages \"Permalink to this heading\")\n1. If you can configure the Python environment directly on the system, install the necessary packages as shown below.\n``` sudo apt install -y python3 python3-pip python-is-python3\n```\n2. Check the Python version to ensure proper installation.\n``` python --version Python 3.8.10\n```\nInstalling Python SDK package [](#installing-python-sdk-package \"Permalink to this heading\") ---------------------------------------------------------------------------------------------\nBefore installing the furiosa-sdk, you need to update Python’s package installer to the latest version.\n``` pip install --upgrade pip setuptools wheel\n```\nWarning\nIf you install the furiosa-sdk without updating to the latest version, you may encounter the following error.\n``` ERROR: Could not find a version that satisfies the requirement furiosa-quantizer-impl==0.10.* (from furiosa-quantizer==0.10.*->furiosa-sdk) (from versions: none) ERROR: No matching distribution found for furiosa-quantizer-impl==0.10.* (from furiosa-quantizer==0.10.*->furiosa-sdk)\n```\nInstalling with PIP\nInstalling with the source code\nFuriosaAI Python SDK package is uploaded on the [pypi](https:\/\/pypi.org\/) repository, so you can easily install it as shown by using the `pip` command.\n``` pip install furiosa-sdk\n```\nThe package contains a compiler command line interface and an inference API. Refer to [furiosa-compiler](compiler.html#compilercli) and [Tutorial and Code Examples](tutorials.html#tutorial) for detailed usage guides.\nAdditional functions are provided in the form of Python extra packages, and you can select and install packages as you require from [Extra packages](#pythonextrapackages) .\nFor example, if you need to install server`\nfor model serving and `litmus` to check the compatibility between model and SDK, specify the extension package as follows.\n``` pip install 'furiosa-sdk[server, litmus]'\n```\nDownload the source code from [FuriosaAI Github repository](https:\/\/github.com\/furiosa-ai\/furiosa-sdk) and install the packages in the following order.\n``` git clone https:\/\/github.com\/furiosa-ai\/furiosa-sdk cd furiosa-sdk\/python pip install furiosa-runtime pip install furiosa-tools pip install furiosa-sdk\n```\nIf you wish to install extra packages, install the Python module in the subdirectory of furiosa-sdk\/python. For example, if you want to install a model server, install it according to the order of dependencies as follows.\n``` cd furiosa-sdk\/python pip install furiosa-server\n```\nExtra packages [](#extra-packages \"Permalink to this heading\") ---------------------------------------------------------------\n### Legacy Runtime\/API [](#legacy-runtime-api \"Permalink to this heading\")\nRather than the next-generation runtime and its API newly adoted since 0.10.0, you can install furiosa-sdk with the legacy runtime and API as follows:\n``` pip install 'furiosa-sdk[legacy]'\n```\n### FuriosaAI Models [](#furiosaai-models \"Permalink to this heading\")\nIt can be executed directly on the NPU and provides optimized DNN model architecture, pre-trained model image, among others, in the form of a Python module. You can install them with the following command.\n``` pip install 'furiosa-sdk[models]'\n```\n### Quantizer [](#quantizer \"Permalink to this heading\")\nThe quantizer package provides a set of APIs for converting a model into a quantized model. You can find more information about the quantization function provided by the Furiosa SDK and the NPU at [Model Quantization](quantization.html#modelquantization) .\n``` pip install 'furiosa-sdk[quantizer]'\n```\n### Model Server [](#model-server \"Permalink to this heading\")\nProvides the function of accelerating DNN model with the NPU, and serving it with GRPC or Restful API.\n``` pip install 'furiosa-sdk[server]'\n```\n### Litmus [](#litmus \"Permalink to this heading\")\nA tool to check whether the specified model is compatible with the Furiosa SDK. Here, we simulate execution of processes such as model quantization and compilation.\n``` pip install 'furiosa-sdk[litmus]'\n```\n[Previous](installation.html \"Driver, Firmware, and Runtime Installation\") [Next](c-sdk.html \"C SDK installation and user guide\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"\n* Python SDK 설치 및 사용자 가이드 * [페이지 소스 보기](..\/_sources\/software\/python-sdk.rst.txt)\n---\nPython SDK 설치 및 사용자 가이드 [](#python-sdk-installation-and-user-guide \"이 제목의 영구 링크\") ===============================================================================================================\nFuriosaAI Python SDK는 NPU를 사용하는 Python 애플리케이션을 작성하기 위한 소프트웨어 개발 키트입니다. Python SDK를 사용하면 AI\/ML 분야에서 가장 널리 사용되는 Python 생태계의 다양한 도구, 라이브러리 및 프레임워크를 활용하여 NPU 애플리케이션을 개발할 수 있습니다. Python SDK는 다양한 모듈로 구성되어 있으며, 추론 API, 양자화 API, 명령줄 도구 및 서비스를 위한 서버 프로그램을 제공합니다.\n\n요구 사항 [](#requirements \"이 제목의 영구 링크\") -----------------------------------------------------------\n* Ubuntu 20.04 LTS (Debian bullseye) 이상 * [FuriosaAI SDK 필수 패키지](installation.html#requiredpackages) * Python 3.8 이상 (Python 환경 설정은 [Python 실행 환경 설정](#setuppython)을 참조하세요) * 최신 버전의 pip\n\nPython SDK를 설치하고 사용하려면 [필수 패키지 설치](installation.html#requiredpackages) 가이드를 따르세요. 필수 커널 드라이버, 펌웨어 및 런타임 라이브러리를 설치해야 합니다.\n\nPython 실행 환경 설정 [](#python-execution-environment-setup \"이 제목의 영구 링크\") -------------------------------------------------------------------------------------------------------\nPython SDK는 Python 3.8 이상이 필요합니다. 여기서는 Python 실행 환경 구성을 설명합니다.\n\n참고\nFuriosaAI Python SDK를 사용하지 않거나 Python 실행 환경 구성에 익숙한 경우 이 섹션을 건너뛸 수 있습니다.\n\n현재 시스템에 설치된 Python 버전은 아래 명령어로 확인할 수 있습니다.\n``` \npython --version \nPython 3.8.10\n```\nPython 명령어가 없거나 Python 버전이 3.8 미만인 경우 아래 방법 중 하나를 선택하여 Python 환경을 구성하세요.\n* [Conda를 사용한 Python 환경 구성](#condainstall) (권장): [Conda](https:\/\/docs.conda.io\/projects\/conda\/en\/latest\/index.html)를 사용하면 특정 Python 애플리케이션을 위한 별도의 독립된 Python 환경을 구성할 수 있습니다. Conda는 따라서 Python 애플리케이션 설치 시 자주 발생하는 패키지 종속성 문제나 Python 버전 문제를 방지합니다.\n* [Linux 패키지를 사용하여 Python 환경 구성](#setuppythononlinux): 다른 Python 실행 환경과의 충돌에 대해 걱정하지 않는 경우 이 옵션을 선택할 수 있습니다.\n\n### Conda를 사용한 Python 환경 구성 [](#python-environment-configuration-with-conda \"이 제목의 영구 링크\")\nConda는 특정 Python 애플리케이션을 위한 독립된 Python 환경을 쉽게 구성할 수 있게 해줍니다. Conda에 대한 자세한 내용은 [Conda](https:\/\/docs.conda.io\/projects\/conda\/en\/latest\/index.html)에서 확인할 수 있습니다.\n\n아래와 같이 설치 프로그램을 다운로드하여 시작할 수 있습니다. `sh .\/Miniconda3-latest-Linux-x86_64.sh`를 실행할 때 모든 질문에 대해 '예'를 선택하세요.\n``` \nwget https:\/\/repo.anaconda.com\/miniconda\/Miniconda3-latest-Linux-x86_64.sh \nsh .\/Miniconda3-latest-Linux-x86_64.sh \nsource ~\/.bashrc \nconda --version\n```\n#### 독립된 Python 실행 환경 생성 및 활성화 [](#creating-and-activating-isolated-python-execution-environment \"이 제목의 영구 링크\")\nAnaconda 설치 후, 필요에 따라 독립된 Python 실행 환경을 생성하고 활성화할 수 있습니다.\n1. Python 3.8을 사용하려면, 아래 명령어를 사용하여 `furiosa-3.8`이라는 실행 환경을 생성하세요.\n``` \nconda create -n furiosa-3.8 python=3.8\n```\n2. 생성된 Python 3.8 환경은 `activate` 명령어로 활성화됩니다.\n``` \nconda activate furiosa-3.8 \n# 버전 확인 \npython --version\n```\n3. Python 실행 환경이 활성화되면, [Python SDK 패키지 설치](#installpippackages)에 설명된 대로 Python SDK를 설치하세요.\n4. Python 실행 환경을 종료하려면 `deactivate` 명령어를 사용하세요.\n``` \n$ conda deactivate\n```\n한 번 생성된 환경은 `activate` 명령어로 언제든지 다시 사용할 수 있습니다. 설치된 패키지는 활성화 후 다시 설치할 필요가 없습니다.\n\n### Linux 패키지를 사용하여 Python 환경 구성 [](#configuring-python-environment-using-linux-packages \"이 제목의 영구 링크\")\n1. 시스템에서 직접 Python 환경을 구성할 수 있는 경우, 아래와 같이 필요한 패키지를 설치하세요.\n``` \nsudo apt install -y python3 python3-pip python-is-python3\n```\n2. 올바르게 설치되었는지 확인하기 위해 Python 버전을 확인하세요.\n``` \npython --version \nPython 3.8.10\n```\n\nPython SDK 패키지 설치 [](#installing-python-sdk-package \"이 제목의 영구 링크\") ---------------------------------------------------------------------------------------------\nfuriosa-sdk를 설치하기 전에 Python의 패키지 설치 프로그램을 최신 버전으로 업데이트해야 합니다.\n``` \npip install --upgrade pip setuptools wheel\n```\n경고\n최신 버전으로 업데이트하지 않고 furiosa-sdk를 설치하면 다음과 같은 오류가 발생할 수 있습니다.\n``` \nERROR: Could not find a version that satisfies the requirement furiosa-quantizer-impl==0.10.* (from furiosa-quantizer==0.10.*->furiosa-sdk) (from versions: none) \nERROR: No matching distribution found for furiosa-quantizer-impl==0.10.* (from furiosa-quantizer==0.10.*->furiosa-sdk)\n```\nPIP로 설치\n소스 코드로 설치\nFuriosaAI Python SDK 패키지는 [pypi](https:\/\/pypi.org\/) 저장소에 업로드되어 있으므로 `pip` 명령어를 사용하여 쉽게 설치할 수 있습니다.\n``` \npip install furiosa-sdk\n```\n패키지에는 컴파일러 명령줄 인터페이스와 추론 API가 포함되어 있습니다. 자세한 사용 가이드는 [furiosa-compiler](compiler.html#compilercli) 및 [튜토리얼 및 코드 예제](tutorials.html#tutorial)를 참조하세요.\n\n추가 기능은 Python 추가 패키지 형태로 제공되며, [추가 패키지](#pythonextrapackages)에서 필요한 패키지를 선택하여 설치할 수 있습니다.\n\n예를 들어, 모델 서빙을 위한 서버와 SDK와 모델 간의 호환성을 확인하기 위한 `litmus`를 설치해야 하는 경우, 확장 패키지를 다음과 같이 지정하세요.\n``` \npip install 'furiosa-sdk[server, litmus]'\n```\n[FuriosaAI Github 저장소](https:\/\/github.com\/furiosa-ai\/furiosa-sdk)에서 소스 코드를 다운로드하고 다음 순서로 패키지를 설치하세요.\n``` \ngit clone https:\/\/github.com\/furiosa-ai\/furiosa-sdk \ncd furiosa-sdk\/python \npip install furiosa-runtime \npip install furiosa-tools \npip install furiosa-sdk\n```\n추가 패키지를 설치하려면, furiosa-sdk\/python의 하위 디렉토리에 있는 Python 모듈을 설치하세요. 예를 들어, 모델 서버를 설치하려면 다음과 같이 종속성 순서에 따라 설치하세요.\n``` \ncd furiosa-sdk\/python \npip install furiosa-server\n```\n\n추가 패키지 [](#extra-packages \"이 제목의 영구 링크\") ---------------------------------------------------------------\n### 레거시 런타임\/API [](#legacy-runtime-api \"이 제목의 영구 링크\")\n0.10.0 이후 새로 채택된 차세대 런타임 및 API 대신, 레거시 런타임 및 API로 furiosa-sdk를 설치할 수 있습니다.\n``` \npip install 'furiosa-sdk[legacy]'\n```\n### FuriosaAI 모델 [](#furiosaai-models \"이 제목의 영구 링크\")\nNPU에서 직접 실행할 수 있으며, 최적화된 DNN 모델 아키텍처, 사전 학습된 모델 이미지 등을 Python 모듈 형태로 제공합니다. 다음 명령어로 설치할 수 있습니다.\n``` \npip install 'furiosa-sdk[models]'\n```\n### 양자화기 [](#quantizer \"이 제목의 영구 링크\")\n양자화기 패키지는 모델을 양자화된 모델로 변환하기 위한 API 세트를 제공합니다. Furiosa SDK 및 NPU에서 제공하는 양자화 기능에 대한 자세한 정보는 [모델 양자화](quantization.html#modelquantization)를 참조하세요.\n``` \npip install 'furiosa-sdk[quantizer]'\n```\n### 모델 서버 [](#model-server \"이 제목의 영구 링크\")\nNPU로 DNN 모델을 가속화하고, GRPC 또는 Restful API로 제공하는 기능을 제공합니다.\n``` \npip install 'furiosa-sdk[server]'\n```\n### Litmus [](#litmus \"이 제목의 영구 링크\")\n지정된 모델이 Furiosa SDK와 호환되는지 확인하는 도구입니다. 여기서는 모델 양자화 및 컴파일과 같은 프로세스의 실행을 시뮬레이션합니다.\n``` \npip install 'furiosa-sdk[litmus]'\n```\n[이전](installation.html \"드라이버, 펌웨어 및 런타임 설치\") [다음](c-sdk.html \"C SDK 설치 및 사용자 가이드\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 작성되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공한 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용했습니다."},{"page_id":"bff490c4-d18d-40e2-8cdc-089ac8f529ea","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/software\/intro.html","original_content":"* FuriosaAI SW Stack Introduction * [View page source](..\/_sources\/software\/intro.rst.txt)\n---\nFuriosaAI SW Stack Introduction [](#furiosaai-sw-stack-introduction \"Permalink to this heading\") =================================================================================================\nFuriosaAI provides various SW components to allow the NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials.\nThe above diagram demonstrates the SW stack provided by FuriosaAI, by layers. At the lowest level is the IntroToWarboy\n, FuriosaAI’s first generation NPU.\nThe following outlines the key components.\nKernel Driver and Firmware [](#kernel-driver-and-firmware \"Permalink to this heading\") ---------------------------------------------------------------------------------------\nThe kernel driver allows the Linux operating system to recognize the NPU device and acknowledge it as a Linux device file. If the NPU is not recognized by the operating system, try reinstalling the driver. The firmware provides a low-level API for the NPU device based on the NPU device file recognized by the Linux operating system. The runtime and compiler control the NPU using the low-level API provided by the firmware, thereby executing and scheduling tasks for inference on the NPU using the compiled binary.\nThere is no need for the user to utilize kernel driver and firmware directly, but they must be installed for Furiosa SDK to work. Installation guide can be found in [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) .\nCompiler [](#compiler \"Permalink to this heading\") ---------------------------------------------------\nThe compiler plays a key role in optimizing DNN models and generating executable code in the NPU. Currently, the compiler supports [TFLite](https:\/\/www.tensorflow.org\/lite) and [ONNX](https:\/\/onnx.ai\/) models, and optimizes the models by introducing various latest research work and methods.\nThe compiler provided with Warboy\nsupports NPU acceleration of various operators in the vision area. For operators that are not supported with acceleration on NPU, the compiler compiles them such that the CPU will be utilized.\nAdditionally, the compiler not only accelerates major vision models such as ResNet50, SSD-MobileNet, and EfficientNet, but also models designed by the users so long as supported operators are utilized - to generate code optimized for NPU.\nFor reference, operators supported by NPU acceleration can be found in [List of Supported Operators for Warboy Acceleration](..\/npu\/warboy.html#supportedoperators) .\nThe compiler is embedded within runtime, so users do not need to install it separately. It can be used automatically in the process of creating a session through the Python\/C SDK, or through [furiosa-compiler](compiler.html#compilercli) .\nRuntime [](#runtime \"Permalink to this heading\") -------------------------------------------------\nRuntime analyzes the executable program generated by the compiler, and actually executes the DNN model inference task as described in the program. During compilation, the DNN model inference is optimized, and split into a number of smaller tasks running on NPU and CPU. Runtime is responsible for balancing the available resources, scheduling these tasks in accordance with the workload, and controlling the NPU via firmware for tasks being executed on the NPU.\nRuntime functions are provided as APIs through the Python\/C SDK, which will be described in the section below, and installation instructions can be found in [Driver, Firmware, and Runtime Installation](installation.html#requiredpackages) .\nPython SDK and C SDK [](#python-sdk-and-c-sdk \"Permalink to this heading\") ---------------------------------------------------------------------------\nPython and C SDK are packages that provide runtime functions as Python and C libraries as APIs, respectively. They provide an APIs that create objects called ‘session’, that allows the specified model to infer using the designated device, and enables high-performance inference in a blocking and asynchronous manner. If you need to write an application or service that utilizes the NPU, you can select and install one of the SDKs according to the programming language of the application you are using. Installation and usage of each SDK can be found in [Python SDK installation and user guide](python-sdk.html#pythonsdk) and [C SDK installation and user guide](c-sdk.html#csdk) .\nModel quantizer API [](#model-quantizer-api \"Permalink to this heading\") -------------------------------------------------------------------------\nFuriosaAI SDK and Warboy\nsupport INT8 models, while models with floating point data as weights undergo quantization, and can be used in Warboy\n. To facilitate this quantization process, Furiosa SDK provides a Model quantizer API. More information about the Model quantizer API provided by the Furiosa SDK can be found in [Model Quantization](quantization.html#modelquantization) .\nModel Server [](#model-server \"Permalink to this heading\") -----------------------------------------------------------\nThe model server exposes the DNN model as a GRPC or REST API. Model formats such as [TFLite](https:\/\/www.tensorflow.org\/lite) and [ONNX](https:\/\/onnx.ai\/) contain within them the data type and tensor shape or the input\/output tensors. Using this information, the models are exposed through the commonly used [Predict Protocol - Version 2](https:\/\/github.com\/kserve\/kserve\/blob\/master\/docs\/predict-api\/v2\/required_api.md) .\nWith the model server, users do not need to directly access the NPU through the library and Python\/C SDK, but can access it through a remote API. In addition, horizontal scaling of services can be easily implemented by using multiple model servers serving the same model and using a load balancer.\nThe model server requires low latency and high throughput. Here, the scheduling function of the runtime is utilized. Installation and utilization of the model server can be found in [Model Server (Serving Framework)](serving.html#modelserving) .\nKubernetes Support [](#kubernetes-support \"Permalink to this heading\") -----------------------------------------------------------------------\nKubernetes, a platform for managing containerized workloads and services, is popular with many enterprises. FuriosaAI SW stack also provides native Kubernetes support. Kubernetes Device Plugin enables the Kubernetes cluster to recognize FuriosaAI’s NPUs and schedule them for workloads\/services that require the NPU. This feature helps the allocation of resources when multiple workloads require NPU in a multi-tenant environment such as Kubernetes, and enables efficient utilization of limited NPU resources.\nKubernetes Node Labeller adds the information of the physical NPU mounted on the node, participating in Kubernetes, as metadata to the Kubernetes node object.\nThis function allows the user to identify information of the NPU mounted on the node using Kubernetes API or command line tool, and to distribute workload to nodes that satisfy certain conditions by utilizing the Pod’s `spec.nodeSelector` or `spec.nodeAffinity` .\nInstallation and usage instructions for NPU support in the Kubernetes environment can be found in the [Kubernetes Support](kubernetes_support.html#kubernetesintegration) page.\n[Previous](..\/npu\/warboy.html \"FuriosaAI Warboy\") [Next](installation.html \"Driver, Firmware, and Runtime Installation\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* FuriosaAI SW 스택 소개 * [페이지 소스 보기](..\/_sources\/software\/intro.rst.txt)\n---\n\nFuriosaAI SW 스택 소개 [](#furiosaai-sw-stack-introduction \"이 제목의 영구 링크\") =================================================================================================\n\nFuriosaAI는 NPU를 다양한 애플리케이션과 환경에서 사용할 수 있도록 여러 SW 구성 요소를 제공합니다. 여기에서는 FuriosaAI가 제공하는 SW 스택을 설명하고, 각 구성 요소의 역할과 함께 가이드라인 및 튜토리얼을 제공합니다. 위의 다이어그램은 FuriosaAI가 제공하는 SW 스택을 계층별로 보여줍니다. 가장 낮은 수준에는 FuriosaAI의 1세대 NPU인 IntroToWarboy가 있습니다. 다음은 주요 구성 요소를 설명합니다.\n\n커널 드라이버 및 펌웨어 [](#kernel-driver-and-firmware \"이 제목의 영구 링크\") ---------------------------------------------------------------------------------------\n\n커널 드라이버는 리눅스 운영 체제가 NPU 장치를 인식하고 이를 리눅스 장치 파일로 인식할 수 있도록 합니다. 운영 체제가 NPU를 인식하지 못할 경우 드라이버를 다시 설치해 보십시오. 펌웨어는 리눅스 운영 체제에서 인식한 NPU 장치 파일을 기반으로 NPU 장치에 대한 저수준 API를 제공합니다. 런타임과 컴파일러는 펌웨어가 제공하는 저수준 API를 사용하여 NPU를 제어하고, 컴파일된 바이너리를 사용하여 NPU에서 추론 작업을 실행하고 스케줄링합니다. 사용자가 커널 드라이버와 펌웨어를 직접 사용할 필요는 없지만, Furiosa SDK가 작동하려면 반드시 설치되어야 합니다. 설치 가이드는 [드라이버, 펌웨어 및 런타임 설치](installation.html#requiredpackages)에서 찾을 수 있습니다.\n\n컴파일러 [](#compiler \"이 제목의 영구 링크\") ---------------------------------------------------\n\n컴파일러는 DNN 모델을 최적화하고 NPU에서 실행 가능한 코드를 생성하는 데 중요한 역할을 합니다. 현재 컴파일러는 [TFLite](https:\/\/www.tensorflow.org\/lite) 및 [ONNX](https:\/\/onnx.ai\/) 모델을 지원하며, 최신 연구 작업 및 방법을 도입하여 모델을 최적화합니다. Warboy와 함께 제공되는 컴파일러는 비전 영역의 다양한 연산자에 대한 NPU 가속을 지원합니다. NPU에서 가속이 지원되지 않는 연산자의 경우, 컴파일러는 CPU를 활용하도록 컴파일합니다. 또한, 컴파일러는 ResNet50, SSD-MobileNet, EfficientNet과 같은 주요 비전 모델뿐만 아니라 사용자가 설계한 모델도 지원되는 연산자를 사용할 경우 NPU에 최적화된 코드를 생성합니다. NPU 가속을 지원하는 연산자는 [Warboy 가속을 위한 지원 연산자 목록](..\/npu\/warboy.html#supportedoperators)에서 참조할 수 있습니다. 컴파일러는 런타임에 내장되어 있어 사용자가 별도로 설치할 필요가 없습니다. Python\/C SDK를 통해 세션을 생성하는 과정에서 자동으로 사용할 수 있으며, [furiosa-compiler](compiler.html#compilercli)를 통해서도 사용할 수 있습니다.\n\n런타임 [](#runtime \"이 제목의 영구 링크\") -------------------------------------------------\n\n런타임은 컴파일러가 생성한 실행 가능한 프로그램을 분석하고, 프로그램에 설명된 대로 DNN 모델 추론 작업을 실제로 실행합니다. 컴파일 과정에서 DNN 모델 추론은 최적화되고, NPU와 CPU에서 실행되는 여러 작은 작업으로 분할됩니다. 런타임은 사용 가능한 자원을 균형 있게 조정하고, 작업 부하에 따라 이러한 작업을 스케줄링하며, NPU에서 실행되는 작업을 위해 펌웨어를 통해 NPU를 제어합니다. 런타임 기능은 아래 섹션에서 설명할 Python\/C SDK를 통해 API로 제공되며, 설치 지침은 [드라이버, 펌웨어 및 런타임 설치](installation.html#requiredpackages)에서 찾을 수 있습니다.\n\nPython SDK 및 C SDK [](#python-sdk-and-c-sdk \"이 제목의 영구 링크\") ---------------------------------------------------------------------------\n\nPython 및 C SDK는 각각 Python 및 C 라이브러리로서 API로 제공되는 런타임 기능을 제공합니다. 이들은 지정된 모델이 지정된 장치를 사용하여 추론할 수 있도록 하는 '세션'이라는 객체를 생성하는 API를 제공하며, 차단 및 비동기 방식으로 고성능 추론을 가능하게 합니다. NPU를 활용하는 애플리케이션이나 서비스를 작성해야 하는 경우, 사용 중인 애플리케이션의 프로그래밍 언어에 따라 SDK 중 하나를 선택하여 설치할 수 있습니다. 각 SDK의 설치 및 사용법은 [Python SDK 설치 및 사용자 가이드](python-sdk.html#pythonsdk) 및 [C SDK 설치 및 사용자 가이드](c-sdk.html#csdk)에서 찾을 수 있습니다.\n\n모델 양자화 API [](#model-quantizer-api \"이 제목의 영구 링크\") -------------------------------------------------------------------------\n\nFuriosaAI SDK와 Warboy는 INT8 모델을 지원하며, 가중치로 부동 소수점 데이터를 사용하는 모델은 양자화를 거쳐 Warboy에서 사용할 수 있습니다. 이 양자화 과정을 용이하게 하기 위해 Furiosa SDK는 모델 양자화 API를 제공합니다. Furiosa SDK가 제공하는 모델 양자화 API에 대한 자세한 정보는 [모델 양자화](quantization.html#modelquantization)에서 찾을 수 있습니다.\n\n모델 서버 [](#model-server \"이 제목의 영구 링크\") -----------------------------------------------------------\n\n모델 서버는 DNN 모델을 GRPC 또는 REST API로 노출합니다. [TFLite](https:\/\/www.tensorflow.org\/lite) 및 [ONNX](https:\/\/onnx.ai\/)와 같은 모델 형식은 입력\/출력 텐서의 데이터 유형 및 텐서 형태를 포함하고 있습니다. 이 정보를 사용하여 모델은 일반적으로 사용되는 [Predict Protocol - Version 2](https:\/\/github.com\/kserve\/kserve\/blob\/master\/docs\/predict-api\/v2\/required_api.md)를 통해 노출됩니다. 모델 서버를 사용하면 사용자가 라이브러리 및 Python\/C SDK를 통해 NPU에 직접 접근할 필요 없이 원격 API를 통해 접근할 수 있습니다. 또한, 동일한 모델을 제공하는 여러 모델 서버를 사용하고 로드 밸런서를 사용하여 서비스의 수평 확장을 쉽게 구현할 수 있습니다. 모델 서버는 낮은 지연 시간과 높은 처리량이 필요합니다. 여기서 런타임의 스케줄링 기능이 활용됩니다. 모델 서버의 설치 및 활용은 [모델 서버 (서빙 프레임워크)](serving.html#modelserving)에서 찾을 수 있습니다.\n\n쿠버네티스 지원 [](#kubernetes-support \"이 제목의 영구 링크\") -----------------------------------------------------------------------\n\n컨테이너화된 워크로드 및 서비스를 관리하는 플랫폼인 쿠버네티스는 많은 기업에서 인기가 있습니다. FuriosaAI SW 스택은 네이티브 쿠버네티스 지원도 제공합니다. 쿠버네티스 장치 플러그인은 쿠버네티스 클러스터가 FuriosaAI의 NPU를 인식하고 NPU가 필요한 워크로드\/서비스에 대해 스케줄링할 수 있도록 합니다. 이 기능은 쿠버네티스와 같은 멀티 테넌트 환경에서 여러 워크로드가 NPU를 필요로 할 때 자원 할당을 돕고, 제한된 NPU 자원의 효율적인 활용을 가능하게 합니다. 쿠버네티스 노드 라벨러는 쿠버네티스에 참여하는 노드에 장착된 물리적 NPU의 정보를 쿠버네티스 노드 객체의 메타데이터로 추가합니다. 이 기능을 통해 사용자는 쿠버네티스 API 또는 명령줄 도구를 사용하여 노드에 장착된 NPU의 정보를 식별하고, Pod의 `spec.nodeSelector` 또는 `spec.nodeAffinity`를 활용하여 특정 조건을 만족하는 노드에 워크로드를 분배할 수 있습니다. 쿠버네티스 환경에서 NPU 지원 설치 및 사용 지침은 [쿠버네티스 지원](kubernetes_support.html#kubernetesintegration) 페이지에서 찾을 수 있습니다.\n\n[이전](..\/npu\/warboy.html \"FuriosaAI Warboy\") [다음](installation.html \"드라이버, 펌웨어 및 런타임 설치\")\n\n---\n\n© 저작권 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 빌드되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용했습니다.\n```"},{"page_id":"c40d4e1b-c960-4ffb-883f-e35d53def7a1","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/npu\/warboy.html","original_content":"* FuriosaAI Warboy * [View page source](..\/_sources\/npu\/warboy.rst.txt)\n---\nFuriosaAI Warboy [](#furiosaai-warboy \"Permalink to this heading\") ===================================================================\nFuriosaAI’s first generation NPU Warboy is a chip with an architecture optimized for deep learning inference. It demonstrates high performance for deep learning inference while maintaining cost-efficiency. FuriosaAI Warboy is optimized for inferences with low batch sizes; for inference requests with low batch sizes, all of the chip’s resources are maximally utilized to achieve low latency. The large on-chip memory is also able to retain most major CNN models, thereby eliminating memory bottlenecks, and achieving high energy efficiency.\nWarboy supports key CNN models used in various vision tasks, including Image Classification, Object Detection, OCR, Super Resolution, and Pose Estimation. In particular, the chip demonstrates superior performance in computations such as depthwise\/group convolution, that drive high accuracy and computational efficiency in state-of-the-art CNN models.\nWarboy delivers 64 TOPS performance and includes 32MB of SRAM. Warboy consists of two processing elements (PE), which each delivers 32 TOPS performance and can be deployed independently. With a total performance of 64 TOPS, should there be a need to maximize response speed to models, the two PEs may undergo fusion, to aggregate as a larger, single PE. Depending on the users’ model size or performance requirements the PEs may be 1) fused so as to minimize response time, or 2) utilized independently to maximize throughput.\nFuriosaAI SDK provides the compiler, runtime software, and profiling tools for the FuriosaAI Warboy. It also supports the INT8 quantization scheme, used as a standard in TensorFLow and PyTorch, while providing tools to convert Floating Point models using Post Training Quantization. With the FuriosaAI SDK, users can compile trained or exported models in formats commonly used for inference (TFLite or ONNX), and accelerate them on FuriosaAI Warboy.\nHW Specifications [](#hw-specifications \"Permalink to this heading\") ---------------------------------------------------------------------\nThe chip is built with 5 billion transistors, dimensions of 180mm^2, clock speed of 2GHz, and delivers peak performance of 64 TOPS of INT8. It also supports a maximum of 4266 for LPDDR4x. Warboy has a DRAM bandwidth of 66GB\/s, and supports PCIe Gen4 8x.\nWarboy Hardware Specification\n[](#id1 \"Permalink to this table\")\n| Peak Performance | 64 TOPS | | --- | --- | | On-chip SRAM | 32 MB | | Host Interface | PCIe Gen4 8-lane | | Form Factor | Full-Height Half-Length (FHHL)  Half-Height Half-Length (HHHL) | | Thermal Solution | Passive Fan  Active Fan | | TDP | 40 - 60W (Configurable) | | Operating Temperature | 0 ~ 50℃ | | Clock Speed | 2.0 GHz | | DDR Speed | 4266 Mbps | | Memory Type | LPDDR4X | | Memory Size | 16 GB (max. 32 GB) | | Peak Memory Bandwidth | 66 GB\/s |\nList of Supported Operators for Warboy Acceleration [](#list-of-supported-operators-for-warboy-acceleration \"Permalink to this heading\") -----------------------------------------------------------------------------------------------------------------------------------------\nFuriosaAI Warboy and SDK can accelerate the following operators, as supported by [Tensorflow Lite](https:\/\/www.tensorflow.org\/lite) model and [ONNX](https:\/\/onnx.ai\/) .\nThe names of the operators use [ONNX](https:\/\/onnx.ai\/) as a reference.\nNote\nAny operators cannot be accelerated on Warboy, the operators will run on the CPU. For some operators, even if Warboy acceleration is supported, if certain conditions are not met, they may be split into several operators or may run on the CPU. Some examples of this would be when the weight of the model is larger than Warboy memory, or if the Warboy memory is not sufficient to process a certain computation.\nOperators Accelerated on Warboy\n[](#id2 \"Permalink to this table\")\n| Name of operator | Additional details | | --- | --- | | [Add](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Add) |  | | [AveragePool](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#AveragePool) |  | | [BatchNormalization](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#batchnormalization) | Acceleration supported, only if after Conv | | [Clip](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#clip) |  | | [Concat](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#concat) | Acceleration supported, only for height axis | | [Conv](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#conv) | Acceleration supported, only for group  <= 128 and dilation <= 12 | | [ConvTranspose](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#convtranspose) |  | | [DepthToSpace](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#depthtospace) |  | | [Exp](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#exp) |  | | [Expand](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#expand) |  | | [Flatten](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Flatten) |  | | [Gemm](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#gemm) |  | | [LeakyRelu](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#leakyrelu) |  | | [LpNormalization](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#lpnormalization) | Acceleration supported, only for p = 2 and batch <= 2 | | [MatMul](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#matmul) |  | | [MaxPool](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#maxpool) |  | | [Mean](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#mean) |  | | [Mul](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#mul) |  | | [Pad](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Pad) |  | | [ReduceL2](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#ReduceL2) |  | | [ReduceSum](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#ReduceSum) |  | | [Relu](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Relu) |  | | [Reshape](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#reshape) |  | | [Pow](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Pow) |  | | [SpaceToDepth](https:\/\/github.com\/onnx\/onnx\/blob\/main\/docs\/Operators.md#SpaceToDepth) | Acceleration supported, only for mode=”CRD” and Furiosa SDK version 0.6.0 or higher | | [Sigmoid](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Sigmoid) |  | | [Slice](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#slice) | Acceleration supported, only for height axis | | [Softmax](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Softmax) | Acceleration supported, only for batch <= 2 | | [Softplus](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Softplus) |  | | [Sub](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#sub) |  | | [Split](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Split) |  | | [Sqrt](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Sqrt) |  | | [Transpose](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Transpose) |  | | [Unsqueeze](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#unsqueeze) |  |\nMLPerf [](#mlperf \"Permalink to this heading\") -----------------------------------------------\nResults submitted to MLPerf can be found at [MLPerf™ Inference Edge v2.0 Results](https:\/\/mlcommons.org\/en\/inference-edge-20\/)\n### See Also [](#see-also \"Permalink to this heading\")\n* [MLPerf™ Inference Edge v1.1 Results](https:\/\/mlcommons.org\/en\/inference-edge-11\/) * [MLPerf™ Inference Edge v0.5 Results](https:\/\/mlcommons.org\/en\/inference-edge-05\/)\n[Previous](..\/index.html \"FuriosaAI NPU & SDK 0.10.1 Documents\") [Next](..\/software\/intro.html \"FuriosaAI SW Stack Introduction\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* FuriosaAI Warboy * [페이지 소스 보기](..\/_sources\/npu\/warboy.rst.txt)\n---\n\nFuriosaAI Warboy [](#furiosaai-warboy \"이 제목의 영구 링크\")\n===================================================================\n\nFuriosaAI의 1세대 NPU Warboy는 딥러닝 추론에 최적화된 아키텍처를 가진 칩입니다. 이 칩은 비용 효율성을 유지하면서 딥러닝 추론에 높은 성능을 보여줍니다. FuriosaAI Warboy는 낮은 배치 크기의 추론에 최적화되어 있으며, 낮은 배치 크기의 추론 요청에 대해 칩의 모든 자원을 최대한 활용하여 낮은 지연 시간을 달성합니다. 대용량 온칩 메모리는 대부분의 주요 CNN 모델을 유지할 수 있어 메모리 병목 현상을 제거하고 높은 에너지 효율성을 달성합니다.\n\nWarboy는 이미지 분류, 객체 탐지, OCR, 초해상도, 자세 추정 등 다양한 비전 작업에 사용되는 주요 CNN 모델을 지원합니다. 특히, 이 칩은 최신 CNN 모델에서 높은 정확도와 계산 효율성을 이끄는 depthwise\/group convolution과 같은 계산에서 우수한 성능을 보여줍니다.\n\nWarboy는 64 TOPS의 성능을 제공하며, 32MB의 SRAM을 포함하고 있습니다. Warboy는 각각 32 TOPS의 성능을 제공하고 독립적으로 배치할 수 있는 두 개의 처리 요소(PE)로 구성되어 있습니다. 총 64 TOPS의 성능으로, 모델에 대한 응답 속도를 최대화해야 할 경우 두 개의 PE를 결합하여 더 큰 단일 PE로 집계할 수 있습니다. 사용자의 모델 크기나 성능 요구 사항에 따라 PE는 1) 응답 시간을 최소화하기 위해 결합하거나, 2) 처리량을 최대화하기 위해 독립적으로 사용할 수 있습니다.\n\nFuriosaAI SDK는 FuriosaAI Warboy를 위한 컴파일러, 런타임 소프트웨어 및 프로파일링 도구를 제공합니다. 또한 TensorFlow와 PyTorch에서 표준으로 사용되는 INT8 양자화 방식을 지원하며, Post Training Quantization을 사용하여 부동 소수점 모델을 변환하는 도구를 제공합니다. FuriosaAI SDK를 사용하면 사용자는 일반적으로 추론에 사용되는 형식(TFLite 또는 ONNX)으로 훈련되거나 내보낸 모델을 컴파일하고 FuriosaAI Warboy에서 가속할 수 있습니다.\n\nHW 사양 [](#hw-specifications \"이 제목의 영구 링크\")\n---------------------------------------------------------------------\n\n이 칩은 50억 개의 트랜지스터, 180mm^2의 크기, 2GHz의 클럭 속도로 제작되었으며, INT8의 64 TOPS의 피크 성능을 제공합니다. 또한 LPDDR4x의 최대 4266을 지원합니다. Warboy는 66GB\/s의 DRAM 대역폭을 가지며, PCIe Gen4 8x를 지원합니다.\n\nWarboy 하드웨어 사양\n[](#id1 \"이 표의 영구 링크\")\n\n| 피크 성능 | 64 TOPS |\n| --- | --- |\n| 온칩 SRAM | 32 MB |\n| 호스트 인터페이스 | PCIe Gen4 8-lane |\n| 폼 팩터 | Full-Height Half-Length (FHHL)  Half-Height Half-Length (HHHL) |\n| 열 솔루션 | 수동 팬  능동 팬 |\n| TDP | 40 - 60W (구성 가능) |\n| 작동 온도 | 0 ~ 50℃ |\n| 클럭 속도 | 2.0 GHz |\n| DDR 속도 | 4266 Mbps |\n| 메모리 유형 | LPDDR4X |\n| 메모리 크기 | 16 GB (최대 32 GB) |\n| 피크 메모리 대역폭 | 66 GB\/s |\n\nWarboy 가속을 위한 지원 연산자 목록 [](#list-of-supported-operators-for-warboy-acceleration \"이 제목의 영구 링크\")\n-----------------------------------------------------------------------------------------------------------------------------------------\n\nFuriosaAI Warboy와 SDK는 [Tensorflow Lite](https:\/\/www.tensorflow.org\/lite) 모델과 [ONNX](https:\/\/onnx.ai\/)에서 지원하는 다음 연산자를 가속할 수 있습니다. 연산자의 이름은 [ONNX](https:\/\/onnx.ai\/)를 참조로 사용합니다.\n\n참고\n어떤 연산자는 Warboy에서 가속할 수 없으며, 이러한 연산자는 CPU에서 실행됩니다. 일부 연산자의 경우, Warboy 가속이 지원되더라도 특정 조건이 충족되지 않으면 여러 연산자로 분할되거나 CPU에서 실행될 수 있습니다. 예를 들어, 모델의 가중치가 Warboy 메모리보다 크거나 특정 계산을 처리하기에 Warboy 메모리가 충분하지 않은 경우가 이에 해당합니다.\n\nWarboy에서 가속되는 연산자\n[](#id2 \"이 표의 영구 링크\")\n\n| 연산자 이름 | 추가 세부사항 |\n| --- | --- |\n| [Add](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Add) |  |\n| [AveragePool](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#AveragePool) |  |\n| [BatchNormalization](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#batchnormalization) | 가속 지원, Conv 이후에만 |\n| [Clip](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#clip) |  |\n| [Concat](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#concat) | 가속 지원, 높이 축에만 |\n| [Conv](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#conv) | 가속 지원, 그룹 <= 128 및 dilation <= 12에만 |\n| [ConvTranspose](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#convtranspose) |  |\n| [DepthToSpace](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#depthtospace) |  |\n| [Exp](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#exp) |  |\n| [Expand](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#expand) |  |\n| [Flatten](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Flatten) |  |\n| [Gemm](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#gemm) |  |\n| [LeakyRelu](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#leakyrelu) |  |\n| [LpNormalization](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#lpnormalization) | 가속 지원, p = 2 및 배치 <= 2에만 |\n| [MatMul](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#matmul) |  |\n| [MaxPool](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#maxpool) |  |\n| [Mean](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#mean) |  |\n| [Mul](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#mul) |  |\n| [Pad](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Pad) |  |\n| [ReduceL2](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#ReduceL2) |  |\n| [ReduceSum](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#ReduceSum) |  |\n| [Relu](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Relu) |  |\n| [Reshape](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#reshape) |  |\n| [Pow](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Pow) |  |\n| [SpaceToDepth](https:\/\/github.com\/onnx\/onnx\/blob\/main\/docs\/Operators.md#SpaceToDepth) | 가속 지원, mode=”CRD” 및 Furiosa SDK 버전 0.6.0 이상에만 |\n| [Sigmoid](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Sigmoid) |  |\n| [Slice](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#slice) | 가속 지원, 높이 축에만 |\n| [Softmax](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Softmax) | 가속 지원, 배치 <= 2에만 |\n| [Softplus](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Softplus) |  |\n| [Sub](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#sub) |  |\n| [Split](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Split) |  |\n| [Sqrt](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Sqrt) |  |\n| [Transpose](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#Transpose) |  |\n| [Unsqueeze](https:\/\/github.com\/onnx\/onnx\/blob\/master\/docs\/Operators.md#unsqueeze) |  |\n\nMLPerf [](#mlperf \"이 제목의 영구 링크\")\n-----------------------------------------------\n\nMLPerf에 제출된 결과는 [MLPerf™ Inference Edge v2.0 Results](https:\/\/mlcommons.org\/en\/inference-edge-20\/)에서 확인할 수 있습니다.\n\n### 관련 링크 [](#see-also \"이 제목의 영구 링크\")\n\n* [MLPerf™ Inference Edge v1.1 Results](https:\/\/mlcommons.org\/en\/inference-edge-11\/)\n* [MLPerf™ Inference Edge v0.5 Results](https:\/\/mlcommons.org\/en\/inference-edge-05\/)\n\n[이전](..\/index.html \"FuriosaAI NPU & SDK 0.10.1 문서\") [다음](..\/software\/intro.html \"FuriosaAI SW 스택 소개\")\n---\n\n© 저작권 2023 FuriosaAI, Inc.\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용하였습니다.\n```"},{"page_id":"9197c2b9-4a70-427e-910e-e57d9f467929","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.10.0.html","original_content":"* Release Notes - 0.10.0 * [View page source](..\/_sources\/releases\/0.10.0.rst.txt)\n---\nRelease Notes - 0.10.0 [](#release-notes-0-10-0 \"Permalink to this heading\") =============================================================================\nFuriosa SDK 0.10.0 is a major release which includes the followings:\n* Adds the next generation runtime engine (FuriosaRT) with higher performance and multi-device features * Improves usability of optimization for vision models by removing quantization operators from models * Supports OpenMetrics format in Metrics Exporter and provide more metrics such as NPU utilization * Improves furiosa-litmus to collect and dump from the diagnosis steps for reporting * Removes Python dependencies from   `furiosa-compiler`   command * Adds the new benchmark tool   `furiosa-bench`  This release also includes a number of other feature additions, bug fixes, and performance improvements.\nComponent versions\n[](#id2 \"Permalink to this table\")\n| Package Name | Version | | --- | --- | | NPU Driver | 1.9.2 | | NPU Firmware Tools | 1.5.1 | | NPU Firmware Image | 1.7.3 | | HAL (Hardware Abstraction Layer) | 0.11.0 | | Furiosa Compiler | 0.10.0 | | Furiosa Quantizer | 0.10.0 | | Furiosa Runtime | 0.10.0 | | Python SDK (furiosa-server, furiosa-serving, ..) | 0.10.0 | | NPU Toolkit (furiosactl) | 0.11.0 | | NPU Device Plugin | 0.10.1 | | NPU Feature Discovery | 0.2.0 |\nInstalling the latest SDK or Upgrading [](#installing-the-latest-sdk-or-upgrading \"Permalink to this heading\") ---------------------------------------------------------------------------------------------------------------\nIf you are using APT repository, the upgrade process is simple. Please run as follows. If you are not familiar with how to use FurioaAI’s APT repository, please find more detais from [Driver, Firmware, and Runtime Installation](..\/software\/installation.html#requiredpackages) .\n``` apt-get update && apt-get upgrade\n```\nYou can also upgrade specific packages as follows:\n``` apt-get update && \\ apt-get install -y furiosa-driver-warboy furiosa-libnux\n```\nYou can upgrade firmware as follows:\n``` apt-get update && \\ apt-get install -y furiosa-firmware-tools furiosa-firmware-image\n```\nYou can upgrade Python package as follows:\n``` pip install --upgrade pip setuptools wheel pip install --upgrade furiosa-sdk\n```\nWarning\nWhen installing or upgrading the furiosa-sdk without updating pip to the latest version, you may encounter the following errors.\n``` ERROR: Could not find a version that satisfies the requirement furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk) (from versions: none) ERROR: No matching distribution found for furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk)\n```\nMajor changes [](#major-changes \"Permalink to this heading\") -------------------------------------------------------------\n### Next Generation Runtime Engine, FuriosaRT [](#next-generation-runtime-engine-furiosart \"Permalink to this heading\")\nSDK 0.10.0 includes the next-generation runtime engine called FuriosaRT\n. FuriosaRT is a newly designed runtime library that offers more advanced features and high performance in various workloads. Many components, such as furiosa-litmus, furiosa-bench, and furiosa-seving, are based on FuriosaRT, and the benefits of the new runtime engine are reflected in these components. FuriosaRT provides the backward compatibility with the previous runtime and includes the following new features:\n#### New Runtime API [](#new-runtime-api \"Permalink to this heading\")\nFuriosaRT introduces a native asynchronous API based on Python’s asyncio < <https:\/\/docs.python.org\/3\/library\/asyncio.html> >. The existing APIs were sufficient for batch applications, but it requires extra code to implement high-performance serving applications, handling many concurrent individual requests. The new API natively supports asynchronous execution. With the new API, users can easily write their applications running on existing web frameworks such as [FastAPI](https:\/\/fastapi.tiangolo.com\/)\nThe new API introduced many advanced features, and you can learn more about the details at [Furiosa SDK API Reference - furiosa.runtime](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html)\n#### Multi-device Support and Improvement on Device Configuration [](#multi-device-support-and-improvement-on-device-configuration \"Permalink to this heading\")\nFuriosaRT natively supports multiple devices with a single session. This feature leads to high-performance inference using multiple devices without extra implementations. Furthermore, FuriosaRT adopts more abstracted way to specify NPU devices. Before 0.9.0 release, users used to set device file names (e.g., `npu0pe0-1` ) explicitly in the environment variable `NPU_DEVNAME` or `session.create(..,\ndevice=”..”)` .\nThis way was inconvinient in many cases because users need to find all available device files and specify them manually.\nFuriosaRT allows users to specify NPU arch, count of NPUs in a textutal representation. This representation is allowed in the new environment variable `FURIOSA_DEVICES` as follows:\n``` export FURIOSA_DEVICES=\"warboy(2)*8\"\n```\nThe above example lets FuriosaRT to find 8 Warboys, each of which is configured as two PEs fusion in the system.\n``` export FURIOSA_DEVICES=\"npu:0:0-1,npu:1:0-1\"\n```\nFor backward compatibility, FuriosaRT still supports `NPU_DEVNAME` environment variable. However, `NPU_DEVNAME` will be deprecated in a future release.\nYou can find more details about the device configuration at [Furiosa SDK API Reference - Device Specification](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#device-specification) .\n#### Higher Throughput [](#higher-throughput \"Permalink to this heading\")\nAccording to our benchmark, FuriosaRT shows significantly improved throughput compared to the previous runtime. In particular, [worker\\_num](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#runner-api) configuration became more effective in FuriosaRT. For example, in the previous runtime, higher than 2 `worker_num` did not show significant performance improvement in most cases. However, in FuriosaRT, we observed that performance improvement is still significant even with `worker_num\n>=\n10` .\nWe carried out benchmarking with Resnet50, YOLOv5m, YOLOv5L, SSD ResNet34, and SSD MobileNet models through `furiosa-bench` command introduced in this release. We observed that performance improvement is significantly up to tens of percent depending on the model with `worker_num\n>=\n4` .\n### Model Server and Serving Framework [](#model-server-and-serving-framework \"Permalink to this heading\")  `furiosa-server` and `furioa-serving` are a web server and a web framework respectively for serving models. The improvements of FuriosaRT are also reflected to the model server and serving framework.\n* [Multi-device Support and Improvement on Device Configuration](#release-0-10-0-deviceselector)   can be used to configure multiple NPU devices in   `furiosa-server`   and   `furioa-serving` * New   [asyncio](https:\/\/docs.python.org\/3\/library\/asyncio.html)   -based API that FuriosaRT offers is introduced to handle more concurrent requests with less resources. * The model server and serving framework inherit the performance characteristics of FuriosaRT. Also, more   `worker_num`   can be used to improve the performance of the model server.\nPlease refer to [Model Server (Serving Framework)](..\/software\/serving.html#modelserving) to learn more about the model server and serving framework.\n### Model Quantization Tool [](#model-quantization-tool \"Permalink to this heading\")\nThe furiosa-quantizer is a library that transforms trained models into quantized models through the post-training quantization. In 0.10.0 release, the usability of the quantization tool has been improved, so some parameters of the `furiosa.quantizer.quantize()` API have a few breaking changes.\n#### Motivation for Change [](#motivation-for-change \"Permalink to this heading\")  `furiosa.quantizer.quantize()` function is a core function of the model quantization tool. `furiosa.quantizer.quantize()` transforms an ONNX model into a quantized model and returns it. The function has the parameter `with_quantize` that allows the model to accept directly the `uint8` type instead of `float32` , also enabling skipping the quantization process for inferences when the original data type (e.g., pixel values) is uint8. This option can result in significant performance improvements. For instance, YOLOv5 Large with this option can dramatically reduce the execution time from 60.639 ms to 0.277 ms.\nSimilarly, `normalized_pixel_outputs` option allows to directly use `unt8` type for outputs instead of `float32` . This option can be useful when the model output is an image in RGB format or when it can be directly used as an integer value. This option shows significant performance boosts.\nIn certain applications, two options can reduce execution time by several times to hundreds of times. However, there were the following limitations and feedback:\n* The parameter   `normalized_pixel_outputs`   was ambiguous in expressing the purpose clearly. * `normalized_pixel_outputs`   assumes the output tensor value ranged from 0 to 1 in floating-point, and it had limited in real application. * `with_quantize`   and   `normalized_pixel_outputs`   options only supported   `uint8`   type, and didn’t support   `int8`   type.\n#### What Changed [](#what-changed \"Permalink to this heading\")\n* Removed the parameters   `with_quantize`   and   `normalized_pixel_outputs`   from   `furiosa.quantizer.quantize()` * Instead, added the class   [ModelEditor](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.quantizer.html#furiosa.quantizer.ModelEditor)   , allowing more options for model input\/output types that offers following optimizations:      + `convert_input_type(tensor_name,          TensorType)`     method takes a tensor name, removes the corresponding quantize operator, and changes the input type to a given     `TensorType`     .   + `convert_output_type(tensor_name,          TensorType,          tensor_range)`     method takes a tensor name, removes the corresponding dequantize operator, and changes the output type to     `TensorType`     , then modifies the scale of the model output to a given     `tensor_range`     . * Since the   `convert_{output,input}_type`   methods are based on tensor names, users should be able to find tensor names from an original ONNX model.\nFor that, `furiosa.quantizer` module provides `get_pure_input_names(ModelProto)` and `get_output_names(ModelProto)` functions to retrieve tensor names from the original ONNX model.\nNote\nThe removal of `with_quantize` , `normalized_pixel_outputs` parameters from `furiosa.quantizer.quantize()` is a breaking change that requires modifying existing code.\nPlease refer to [ModelEditor](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.quantizer.html#furiosa.quantizer.ModelEditor) to learn more about the ModelEditor API and find examples from [Tutorial and Code Examples](..\/software\/tutorials.html#tutorial) .\n### Compiler [](#compiler \"Permalink to this heading\")\nSince this release, the compiler supports NPU acceleration for the Dequantize\noperator. So, the latency or throughput of models that include Dequantize\noperators can be enhanced. More details of this performance optimization can be found from [Performance Optimization](..\/software\/performance.html#performanceoptimization) .\nSince 0.10.0, the default lifetime of compiler cache has increased from 2 days to 30 days. Please refer to [Compiler Cache](..\/software\/compiler.html#compilercache) to learn the details of compiler cache feature.  `furiosa-compiler` command in 0.10.0 release also has the following improvements:\n* Add   `furiosa-compiler`   command in addition to   `furiosa-compile`   command * `furiosa-compiler`   and   `furiosa-compile`   commands as a native executable   and do not require any Python runtime environment. * `furiosa-compiler`   is now available as an APT package, you can install via   `apt      install      furiosa-compiler`   . * `furiosa      compile`   is kept for backward compatibility, and it will be removed in a future release.\nPlease visit [furiosa-compiler](..\/software\/compiler.html#compilercli) to learn more about furiosa-compiler\ncommand.\n### Performance Profiler [](#performance-profiler \"Permalink to this heading\")\nThe performance profiler is a tool that helps users to analyze performance by measuring the actual execution time of inferences. Since 0.10.0, [Tracing via Profiler Context](..\/software\/profiler.html#profilerenabledbycontext) API provides the pause\/resume features.\nThis feature allows users to skip unnecessary steps like pre\/post processing or warming up times, leading to the reduction of the profiling overhead and the size of the profile result files. Literally, calling `profile.pause()` method immediately stops the profliling, and `profile.resume()` resumes the profiling again. The profiler will not collect any profiling information between both method calls. Please refer to [Pause\/Resume of Profiler Context](..\/software\/profiler.html#temporarilydisablingprofiler) to learn more about the profiling API.\n### furiosa-litmus [](#furiosa-litmus \"Permalink to this heading\")  `furiosa-litmus` is a command-line tool that checks the compatibility of models with the NPU and Furiosa SDK. Since 0.10.0, `furiosa-litmus` has a new feature to collect logs, profiling information, and an environment information for error reporting. This feature is enabled if `--dump\n<OUTPUT_PREFIX>` option is specified. The collected data is saved into a zip file named `<OUTPUT_PREFIX>-<unix_epoch>.zip` .\n``` $ furiosa-litmus <MODEL_PATH> --dump <OUTPUT_PREFIX>\n```\nThe collected information does not include the model itself but does contain only metadata of the model, memory usage, and environmental information (e.g., Python version, SDK, compiler version, and dependency library versions). You can directly unzip the zip file to check the contents. When reporting bugs, attaching this file will be very helpful for error dianosis and analysis.\n### New Benchmark Tool ‘furiosa-bench’ [](#new-benchmark-tool-furiosa-bench \"Permalink to this heading\")\nThe new benchmark tool, `furiosa-bench` , has been added since 0.10.0. `furiosa-bench` command offers various options to run a diverse workloads with certain runtime settings. Users can choose either latency-oriented or throughput-oriented workload, and can specify the number of devices, how long time to run, and runtime settings. `furiosa-bench` accepts both ONNX and Tflite models as well as an ENF file compiled by the furiosa-compiler. More details about the command can be found at [furiosa-bench (Benchmark Tool)](..\/software\/cli.html#furiosabench) .\nAn example of a throughput benchmark\n``` $ furiosa-bench .\/model.onnx --workload throughput -n 10000 --devices \"warboy(1)*2\" --workers 8 --batch 8\n```\nAn example of a latency benchmark\n``` $ furiosa-bench .\/model.onnx --workload latency -n 10000 --devices \"warboy(2)*1\"\n```  `furiosa-bench` can be installed through apt package manager as follows:\n``` $ apt install furiosa-bench\n```\n### furiosa-toolkit [](#furiosa-toolkit \"Permalink to this heading\")\nfuriosa-toolkit is a collection of command line tools that provide NPU management and NPU device monitoring. Since 0.10.0, `furiosa-toolkit` includes the following improvements:\n**Improvements of furiosactl**\nBefore 0.10.0, the sub-commands like `list` , `info` print out a tabular text. Since 0.10.0, `furiosactl` newly provides `--format` option, allowing to print out the result in a structured format like `json` or `yaml` . It will be useful when a users implements a shell pipeline or a script to process the output of `furiosactl` .\n``` $ furiosactl info --format json [{\"dev_name\":\"npu7\",\"product_name\":\"warboy\",\"device_uuid\":\"<device_uuid>\",\"device_sn\":\"<device_sn>\",\"firmware\":\"1.6.0, 7a3b908\",\"temperature\":\"47°C\",\"power\":\"0.99 W\",\"pci_bdf\":\"0000:d6:00.0\",\"pci_dev\":\"492:0\"}]\n$ furiosactl info --format yaml - dev_name: npu7   product_name: warboy   device_uuid: <device_uuid>   device_sn: <device_sn>   firmware: 1.6.0, 7a3b908   temperature: 47°C   power: 0.98 W   pci_bdf: 0000:d6:00.0   pci_dev: 492:0\n```\nAlso, the subcommand `info` results in two more metrics:\n* NPU Clock Frequency * Entire power consumption of card\n**Improvements of furiosa-npu-metrics-exporter**  `furiosa-npu-metrics-exporter` is a HTTP server to export NPU metrics and status in [OpenMetrics](https:\/\/github.com\/OpenObservability\/OpenMetrics\/blob\/main\/specification\/OpenMetrics.md) format. The metrics that `furiosa-npu-metrics-exporter` exports can be collected by Prometheus and other OpenMetrics compatible collectors.\nSince 0.10.0, `furiosa-npu-metrics-exporter` includes NPU clock frequency and NPU utilization as metrics. NPU utilziation is still an experimental feature, and it is disabled by default. To enable this feature, you need to specify `--enable-npu-utilization` option as follows:\n``` furiosa-npu-metrics-exporter --enable-npu-utilization\n```\nAdditionally, `furiosa-npu-metrics-exporter` is now available as an APT package in addition to the docker image. You can install it as follows:\n``` apt install furiosa-toolkit\n```\n[Previous](..\/api\/python\/furiosa.serving.processors.html \"furiosa.serving.processors package\") [Next](0.9.0.html \"Release Notes - 0.9.0\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 릴리스 노트 - 0.10.0 * [페이지 소스 보기](..\/_sources\/releases\/0.10.0.rst.txt)\n---\n릴리스 노트 - 0.10.0 [](#release-notes-0-10-0 \"이 제목으로의 고유 링크\") =============================================================================\nFuriosa SDK 0.10.0은 다음과 같은 주요 업데이트를 포함하는 대규모 릴리스입니다:\n* 더 높은 성능과 멀티 디바이스 기능을 갖춘 차세대 런타임 엔진(FuriosaRT) 추가 * 모델에서 양자화 연산자를 제거하여 비전 모델 최적화의 사용성 개선 * 메트릭스 익스포터에서 OpenMetrics 형식 지원 및 NPU 활용도와 같은 더 많은 메트릭 제공 * 진단 단계에서 수집 및 덤프를 위한 furiosa-litmus 개선 * `furiosa-compiler` 명령에서 Python 종속성 제거 * 새로운 벤치마크 도구 `furiosa-bench` 추가 이 릴리스에는 또한 여러 기능 추가, 버그 수정 및 성능 개선이 포함되어 있습니다.\n\n구성 요소 버전 [](#id2 \"이 표로의 고유 링크\")\n| 패키지 이름 | 버전 | | --- | --- | | NPU 드라이버 | 1.9.2 | | NPU 펌웨어 도구 | 1.5.1 | | NPU 펌웨어 이미지 | 1.7.3 | | HAL (하드웨어 추상화 계층) | 0.11.0 | | Furiosa 컴파일러 | 0.10.0 | | Furiosa 양자화기 | 0.10.0 | | Furiosa 런타임 | 0.10.0 | | Python SDK (furiosa-server, furiosa-serving, ..) | 0.10.0 | | NPU 툴킷 (furiosactl) | 0.11.0 | | NPU 디바이스 플러그인 | 0.10.1 | | NPU 기능 탐지 | 0.2.0 |\n\n최신 SDK 설치 또는 업그레이드 [](#installing-the-latest-sdk-or-upgrading \"이 제목으로의 고유 링크\") ---------------------------------------------------------------------------------------------------------------\nAPT 저장소를 사용하는 경우, 업그레이드 과정은 간단합니다. 다음과 같이 실행하세요. FuriosaAI의 APT 저장소 사용 방법을 잘 모르는 경우, [드라이버, 펌웨어 및 런타임 설치](..\/software\/installation.html#requiredpackages)에서 자세한 내용을 확인하세요.\n```bash\napt-get update && apt-get upgrade\n```\n특정 패키지를 다음과 같이 업그레이드할 수도 있습니다:\n```bash\napt-get update && \\\napt-get install -y furiosa-driver-warboy furiosa-libnux\n```\n펌웨어를 다음과 같이 업그레이드할 수 있습니다:\n```bash\napt-get update && \\\napt-get install -y furiosa-firmware-tools furiosa-firmware-image\n```\nPython 패키지를 다음과 같이 업그레이드할 수 있습니다:\n```bash\npip install --upgrade pip setuptools wheel\npip install --upgrade furiosa-sdk\n```\n경고\npip을 최신 버전으로 업데이트하지 않고 furiosa-sdk를 설치하거나 업그레이드할 때 다음과 같은 오류가 발생할 수 있습니다.\n```plaintext\nERROR: Could not find a version that satisfies the requirement furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk) (from versions: none)\nERROR: No matching distribution found for furiosa-quantizer-impl==0.9.* (from furiosa-quantizer==0.9.*->furiosa-sdk)\n```\n\n주요 변경 사항 [](#major-changes \"이 제목으로의 고유 링크\") -------------------------------------------------------------\n### 차세대 런타임 엔진, FuriosaRT [](#next-generation-runtime-engine-furiosart \"이 제목으로의 고유 링크\")\nSDK 0.10.0에는 FuriosaRT라는 차세대 런타임 엔진이 포함되어 있습니다. FuriosaRT는 다양한 작업에서 더 고급 기능과 높은 성능을 제공하는 새롭게 설계된 런타임 라이브러리입니다. furiosa-litmus, furiosa-bench, furiosa-serving과 같은 많은 구성 요소가 FuriosaRT를 기반으로 하며, 새로운 런타임 엔진의 이점이 이러한 구성 요소에 반영됩니다. FuriosaRT는 이전 런타임과의 하위 호환성을 제공하며 다음과 같은 새로운 기능을 포함합니다:\n\n#### 새로운 런타임 API [](#new-runtime-api \"이 제목으로의 고유 링크\")\nFuriosaRT는 Python의 asyncio < <https:\/\/docs.python.org\/3\/library\/asyncio.html> >를 기반으로 하는 네이티브 비동기 API를 도입합니다. 기존 API는 배치 애플리케이션에 충분했지만, 많은 동시 개별 요청을 처리하는 고성능 서빙 애플리케이션을 구현하기 위해 추가 코드가 필요했습니다. 새로운 API는 비동기 실행을 네이티브로 지원합니다. 새로운 API를 사용하면 [FastAPI](https:\/\/fastapi.tiangolo.com\/)와 같은 기존 웹 프레임워크에서 실행되는 애플리케이션을 쉽게 작성할 수 있습니다. 새로운 API는 많은 고급 기능을 도입했으며, 자세한 내용은 [Furiosa SDK API 참조 - furiosa.runtime](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html)에서 확인할 수 있습니다.\n\n#### 멀티 디바이스 지원 및 디바이스 구성 개선 [](#multi-device-support-and-improvement-on-device-configuration \"이 제목으로의 고유 링크\")\nFuriosaRT는 단일 세션으로 여러 디바이스를 네이티브로 지원합니다. 이 기능은 추가 구현 없이 여러 디바이스를 사용하여 고성능 추론을 가능하게 합니다. 또한, FuriosaRT는 NPU 디바이스를 지정하는 더 추상화된 방법을 채택합니다. 0.9.0 릴리스 이전에는 사용자가 환경 변수 `NPU_DEVNAME` 또는 `session.create(.., device=”..”)`에 디바이스 파일 이름(예: `npu0pe0-1`)을 명시적으로 설정해야 했습니다. 이 방법은 사용자가 모든 사용 가능한 디바이스 파일을 찾아 수동으로 지정해야 했기 때문에 많은 경우 불편했습니다. FuriosaRT는 사용자가 NPU 아키텍처, NPU 수를 텍스트 표현으로 지정할 수 있도록 합니다. 이 표현은 새로운 환경 변수 `FURIOSA_DEVICES`에서 다음과 같이 허용됩니다:\n```bash\nexport FURIOSA_DEVICES=\"warboy(2)*8\"\n```\n위의 예는 시스템에서 두 개의 PEs 융합으로 구성된 8개의 Warboy를 찾도록 FuriosaRT에 지시합니다.\n```bash\nexport FURIOSA_DEVICES=\"npu:0:0-1,npu:1:0-1\"\n```\n하위 호환성을 위해 FuriosaRT는 여전히 `NPU_DEVNAME` 환경 변수를 지원합니다. 그러나 `NPU_DEVNAME`은 향후 릴리스에서 사용 중단될 예정입니다. 디바이스 구성에 대한 자세한 내용은 [Furiosa SDK API 참조 - 디바이스 사양](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#device-specification)에서 확인할 수 있습니다.\n\n#### 더 높은 처리량 [](#higher-throughput \"이 제목으로의 고유 링크\")\n우리의 벤치마크에 따르면, FuriosaRT는 이전 런타임에 비해 상당히 향상된 처리량을 보여줍니다. 특히, [worker_num](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.runtime.html#runner-api) 구성은 FuriosaRT에서 더 효과적이 되었습니다. 예를 들어, 이전 런타임에서는 대부분의 경우 2 이상의 `worker_num`이 성능 향상을 크게 보여주지 않았습니다. 그러나 FuriosaRT에서는 `worker_num >= 10`에서도 성능 향상이 여전히 상당함을 관찰했습니다. 우리는 이 릴리스에서 도입된 `furiosa-bench` 명령을 통해 Resnet50, YOLOv5m, YOLOv5L, SSD ResNet34, SSD MobileNet 모델로 벤치마킹을 수행했습니다. `worker_num >= 4`에서 모델에 따라 성능 향상이 수십 퍼센트까지 크게 나타났습니다.\n\n### 모델 서버 및 서빙 프레임워크 [](#model-server-and-serving-framework \"이 제목으로의 고유 링크\")\n`furiosa-server`와 `furiosa-serving`은 각각 모델을 서빙하기 위한 웹 서버와 웹 프레임워크입니다. FuriosaRT의 개선 사항은 모델 서버 및 서빙 프레임워크에도 반영됩니다.\n* [멀티 디바이스 지원 및 디바이스 구성 개선](#release-0-10-0-deviceselector)은 `furiosa-server`와 `furiosa-serving`에서 여러 NPU 디바이스를 구성하는 데 사용할 수 있습니다. * FuriosaRT가 제공하는 새로운 [asyncio](https:\/\/docs.python.org\/3\/library\/asyncio.html) 기반 API가 더 적은 리소스로 더 많은 동시 요청을 처리할 수 있도록 도입되었습니다. * 모델 서버 및 서빙 프레임워크는 FuriosaRT의 성능 특성을 상속합니다. 또한, 더 많은 `worker_num`을 사용하여 모델 서버의 성능을 향상시킬 수 있습니다. 모델 서버 및 서빙 프레임워크에 대해 더 알아보려면 [모델 서버 (서빙 프레임워크)](..\/software\/serving.html#modelserving)를 참조하세요.\n\n### 모델 양자화 도구 [](#model-quantization-tool \"이 제목으로의 고유 링크\")\nfuriosa-quantizer는 훈련된 모델을 양자화된 모델로 변환하는 포스트 트레이닝 양자화를 수행하는 라이브러리입니다. 0.10.0 릴리스에서는 양자화 도구의 사용성이 개선되어 `furiosa.quantizer.quantize()` API의 일부 매개변수에 몇 가지 파괴적인 변경이 있습니다.\n\n#### 변경 동기 [](#motivation-for-change \"이 제목으로의 고유 링크\")\n`furiosa.quantizer.quantize()` 함수는 모델 양자화 도구의 핵심 함수입니다. `furiosa.quantizer.quantize()`는 ONNX 모델을 양자화된 모델로 변환하고 반환합니다. 이 함수는 모델이 `float32` 대신 `uint8` 타입을 직접 수용할 수 있도록 하는 `with_quantize` 매개변수를 가지고 있으며, 원래 데이터 타입(예: 픽셀 값)이 `uint8`인 경우 추론을 위한 양자화 과정을 건너뛸 수 있습니다. 이 옵션은 상당한 성능 향상을 가져올 수 있습니다. 예를 들어, 이 옵션을 사용한 YOLOv5 Large는 실행 시간을 60.639 ms에서 0.277 ms로 크게 줄일 수 있습니다. 유사하게, `normalized_pixel_outputs` 옵션은 `float32` 대신 `uint8` 타입을 출력에 직접 사용할 수 있도록 합니다. 이 옵션은 모델 출력이 RGB 형식의 이미지이거나 정수 값으로 직접 사용할 수 있는 경우 유용할 수 있습니다. 이 옵션은 상당한 성능 향상을 보여줍니다. 특정 애플리케이션에서는 두 옵션이 실행 시간을 수십 배에서 수백 배까지 줄일 수 있습니다. 그러나 다음과 같은 제한 사항과 피드백이 있었습니다:\n* `normalized_pixel_outputs` 매개변수는 목적을 명확하게 표현하는 데 모호했습니다. * `normalized_pixel_outputs`는 출력 텐서 값이 부동 소수점에서 0에서 1 사이로 범위가 제한되어 실제 애플리케이션에서 제한적이었습니다. * `with_quantize` 및 `normalized_pixel_outputs` 옵션은 `uint8` 타입만 지원하고 `int8` 타입은 지원하지 않았습니다.\n\n#### 변경된 내용 [](#what-changed \"이 제목으로의 고유 링크\")\n* `furiosa.quantizer.quantize()`에서 `with_quantize` 및 `normalized_pixel_outputs` 매개변수를 제거했습니다. * 대신, 모델 입력\/출력 타입에 대한 더 많은 옵션을 제공하는 [ModelEditor](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.quantizer.html#furiosa.quantizer.ModelEditor) 클래스를 추가하여 다음과 같은 최적화를 제공합니다:   + `convert_input_type(tensor_name, TensorType)` 메서드는 텐서 이름을 받아 해당 양자화 연산자를 제거하고 입력 타입을 주어진 `TensorType`으로 변경합니다.   + `convert_output_type(tensor_name, TensorType, tensor_range)` 메서드는 텐서 이름을 받아 해당 디양자화 연산자를 제거하고 출력 타입을 `TensorType`으로 변경한 후 모델 출력의 스케일을 주어진 `tensor_range`로 수정합니다. * `convert_{output,input}_type` 메서드는 텐서 이름을 기반으로 하므로 사용자는 원래 ONNX 모델에서 텐서 이름을 찾을 수 있어야 합니다. 이를 위해 `furiosa.quantizer` 모듈은 원래 ONNX 모델에서 텐서 이름을 검색하기 위한 `get_pure_input_names(ModelProto)` 및 `get_output_names(ModelProto)` 함수를 제공합니다.\n\n참고\n`furiosa.quantizer.quantize()`에서 `with_quantize`, `normalized_pixel_outputs` 매개변수의 제거는 기존 코드를 수정해야 하는 파괴적인 변경입니다. ModelEditor API에 대해 더 알아보고 [튜토리얼 및 코드 예제](..\/software\/tutorials.html#tutorial)에서 예제를 찾으려면 [ModelEditor](https:\/\/furiosa-ai.github.io\/docs\/v0.10.0\/en\/api\/python\/furiosa.quantizer.html#furiosa.quantizer.ModelEditor)를 참조하세요.\n\n### 컴파일러 [](#compiler \"이 제목으로의 고유 링크\")\n이번 릴리스부터 컴파일러는 디양자화 연산자에 대한 NPU 가속을 지원합니다. 따라서 디양자화 연산자를 포함하는 모델의 지연 시간 또는 처리량이 향상될 수 있습니다. 이 성능 최적화에 대한 자세한 내용은 [성능 최적화](..\/software\/performance.html#performanceoptimization)에서 확인할 수 있습니다. 0.10.0부터 컴파일러 캐시의 기본 수명이 2일에서 30일로 증가했습니다. 컴파일러 캐시 기능에 대한 자세한 내용은 [컴파일러 캐시](..\/software\/compiler.html#compilercache)를 참조하세요. 0.10.0 릴리스의 `furiosa-compiler` 명령에는 다음과 같은 개선 사항이 있습니다:\n* `furiosa-compile` 명령 외에 `furiosa-compiler` 명령 추가 * `furiosa-compiler` 및 `furiosa-compile` 명령은 네이티브 실행 파일로 제공되며 Python 런타임 환경이 필요하지 않습니다. * `furiosa-compiler`는 이제 APT 패키지로 제공되며, `apt install furiosa-compiler`를 통해 설치할 수 있습니다. * `furiosa compile`은 하위 호환성을 위해 유지되며, 향후 릴리스에서 제거될 예정입니다. furiosa-compiler 명령에 대해 더 알아보려면 [furiosa-compiler](..\/software\/compiler.html#compilercli)를 방문하세요.\n\n### 성능 프로파일러 [](#performance-profiler \"이 제목으로의 고유 링크\")\n성능 프로파일러는 추론의 실제 실행 시간을 측정하여 성능을 분석하는 데 도움을 주는 도구입니다. 0.10.0부터 [프로파일러 컨텍스트를 통한 추적](..\/software\/profiler.html#profilerenabledbycontext) API는 일시 중지\/재개 기능을 제공합니다. 이 기능은 사전\/사후 처리 또는 워밍업 시간과 같은 불필요한 단계를 건너뛰어 프로파일링 오버헤드와 프로파일 결과 파일의 크기를 줄이는 데 도움이 됩니다. 문자 그대로, `profile.pause()` 메서드를 호출하면 프로파일링이 즉시 중지되고, `profile.resume()`은 프로파일링을 다시 시작합니다. 프로파일러는 두 메서드 호출 사이에 프로파일링 정보를 수집하지 않습니다. 프로파일링 API에 대해 더 알아보려면 [프로파일러 컨텍스트의 일시 중지\/재개](..\/software\/profiler.html#temporarilydisablingprofiler)를 참조하세요.\n\n### furiosa-litmus [](#furiosa-litmus \"이 제목으로의 고유 링크\")\n`furiosa-litmus`는 NPU 및 Furiosa SDK와 모델의 호환성을 확인하는 명령줄 도구입니다. 0.10.0부터 `furiosa-litmus`는 오류 보고를 위한 로그, 프로파일링 정보 및 환경 정보를 수집하는 새로운 기능을 제공합니다. 이 기능은 `--dump <OUTPUT_PREFIX>` 옵션이 지정된 경우 활성화됩니다. 수집된 데이터는 `<OUTPUT_PREFIX>-<unix_epoch>.zip`이라는 이름의 zip 파일에 저장됩니다.\n```bash\n$ furiosa-litmus <MODEL_PATH> --dump <OUTPUT_PREFIX>\n```\n수집된 정보는 모델 자체를 포함하지 않지만, 모델의 메타데이터, 메모리 사용량 및 환경 정보(예: Python 버전, SDK, 컴파일러 버전 및 종속 라이브러리 버전)만 포함합니다. zip 파일을 직접 압축 해제하여 내용을 확인할 수 있습니다. 버그를 보고할 때 이 파일을 첨부하면 오류 진단 및 분석에 매우 유용합니다.\n\n### 새로운 벤치마크 도구 ‘furiosa-bench’ [](#new-benchmark-tool-furiosa-bench \"이 제목으로의 고유 링크\")\n새로운 벤치마크 도구인 `furiosa-bench`가 0.10.0부터 추가되었습니다. `furiosa-bench` 명령은 특정 런타임 설정으로 다양한 작업을 실행할 수 있는 다양한 옵션을 제공합니다. 사용자는 지연 시간 지향 또는 처리량 지향 작업을 선택할 수 있으며, 디바이스 수, 실행 시간 및 런타임 설정을 지정할 수 있습니다. `furiosa-bench`는 ONNX 및 Tflite 모델뿐만 아니라 furiosa-compiler로 컴파일된 ENF 파일도 수용합니다. 명령에 대한 자세한 내용은 [furiosa-bench (벤치마크 도구)](..\/software\/cli.html#furiosabench)에서 확인할 수 있습니다.\n\n처리량 벤치마크 예제\n```bash\n$ furiosa-bench .\/model.onnx --workload throughput -n 10000 --devices \"warboy(1)*2\" --workers 8 --batch 8\n```\n지연 시간 벤치마크 예제\n```bash\n$ furiosa-bench .\/model.onnx --workload latency -n 10000 --devices \"warboy(2)*1\"\n```\n`furiosa-bench`는 apt 패키지 관리자를 통해 다음과 같이 설치할 수 있습니다:\n```bash\n$ apt install furiosa-bench\n```\n\n### furiosa-toolkit [](#furiosa-toolkit \"이 제목으로의 고유 링크\")\nfuriosa-toolkit은 NPU 관리 및 NPU 디바이스 모니터링을 제공하는 명령줄 도구 모음입니다. 0.10.0부터 `furiosa-toolkit`에는 다음과 같은 개선 사항이 포함됩니다:\n\n**furiosactl의 개선 사항**\n0.10.0 이전에는 `list`, `info`와 같은 하위 명령이 표 형식의 텍스트를 출력했습니다. 0.10.0부터 `furiosactl`은 `--format` 옵션을 새롭게 제공하여 `json` 또는 `yaml`과 같은 구조화된 형식으로 결과를 출력할 수 있습니다. 이는 사용자가 쉘 파이프라인이나 스크립트를 구현하여 `furiosactl`의 출력을 처리할 때 유용할 것입니다.\n```bash\n$ furiosactl info --format json\n[{\"dev_name\":\"npu7\",\"product_name\":\"warboy\",\"device_uuid\":\"<device_uuid>\",\"device_sn\":\"<device_sn>\",\"firmware\":\"1.6.0, 7a3b908\",\"temperature\":\"47°C\",\"power\":\"0.99 W\",\"pci_bdf\":\"0000:d6:00.0\",\"pci_dev\":\"492:0\"}]\n$ furiosactl info --format yaml\n- dev_name: npu7\n  product_name: warboy\n  device_uuid: <device_uuid>\n  device_sn: <device_sn>\n  firmware: 1.6.0, 7a3b908\n  temperature: 47°C\n  power: 0.98 W\n  pci_bdf: 0000:d6:00.0\n  pci_dev: 492:0\n```\n또한, `info` 하위 명령은 두 가지 추가 메트릭을 제공합니다:\n* NPU 클록 주파수 * 카드의 전체 전력 소비\n\n**furiosa-npu-metrics-exporter의 개선 사항**\n`furiosa-npu-metrics-exporter`는 NPU 메트릭 및 상태를 [OpenMetrics](https:\/\/github.com\/OpenObservability\/OpenMetrics\/blob\/main\/specification\/OpenMetrics.md) 형식으로 내보내는 HTTP 서버입니다. `furiosa-npu-metrics-exporter`가 내보내는 메트릭은 Prometheus 및 기타 OpenMetrics 호환 수집기가 수집할 수 있습니다. 0.10.0부터 `furiosa-npu-metrics-exporter`는 NPU 클록 주파수 및 NPU 활용도를 메트릭으로 포함합니다. NPU 활용도는 여전히 실험적 기능이며 기본적으로 비활성화되어 있습니다. 이 기능을 활성화하려면 다음과 같이 `--enable-npu-utilization` 옵션을 지정해야 합니다:\n```bash\nfuriosa-npu-metrics-exporter --enable-npu-utilization\n```\n또한, `furiosa-npu-metrics-exporter`는 도커 이미지 외에도 APT 패키지로 제공됩니다. 다음과 같이 설치할 수 있습니다:\n```bash\napt install furiosa-toolkit\n```\n\n[이전](..\/api\/python\/furiosa.serving.processors.html \"furiosa.serving.processors 패키지\") [다음](0.9.0.html \"릴리스 노트 - 0.9.0\")\n---\n© 2023 FuriosaAI, Inc. 모든 권리 보유.\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용했습니다.\n```"},{"page_id":"180d17f3-caa8-46a6-87f5-74a1c03de3a5","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/","original_content":"* FuriosaAI NPU & SDK 0.10.1 Documents * [View page source](_sources\/index.rst.txt)\n---\nFuriosaAI NPU & SDK 0.10.1 Documents [](#furiosaai-npu-sdk-release-documents \"Permalink to this heading\") ==========================================================================================================\nThis document explains FuriosaAI NPU and its SDKs.\nNote\nFuriosaAI software components include kernel driver, firmware, runtime, C SDK, Python SDK, and command lines tools. Currently, we offer them for only users who register *Early Access Program (EAP)* and agree to *End User Licence Agreement (EULA)* .\nPlease contact [contact @\nfuriosa .\nai](mailto:contact%40furiosa.ai) to learn how to start the EAP.\nFuriosaAI NPU [](#furiosaai-npu \"Permalink to this heading\") -------------------------------------------------------------\n* [Introduction to FuriosaAI Warboy](npu\/warboy.html)   : HW specification, performance, and supported operators\nFuriosaAI Software [](#furiosaai-software \"Permalink to this heading\") -----------------------------------------------------------------------\n* [FuriosaAI SW Stack Introduction](software\/intro.html) * [Driver, Firmware, and Runtime Installation](software\/installation.html) * [Python SDK installation and user guide](software\/python-sdk.html) * [C SDK installation and user guide](software\/c-sdk.html) * [Command Line Tools](software\/cli.html) * [Compiler](software\/compiler.html) * [Model Quantization](software\/quantization.html) * [FuriosaAI Model Zoo](https:\/\/furiosa-ai.github.io\/furiosa-models\/latest\/) * [Kubernetes Support](software\/kubernetes_support.html) * [Configuring Warboy Pass-through for Virtual Machine](software\/vm_support.html)\n### FuriosaAI SDK Tutorial and Examples [](#furiosaai-sdk-tutorial-and-examples \"Permalink to this heading\")\n* [Tutorial: How to use Furiosa SDK from Start to Finish](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/HowToUseFuriosaSDKFromStartToFinish.ipynb) * [Tutorial: Basic Inference API](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/GettingStartedWithPythonSDK.ipynb) * [Tutorial: Advanced Inference API](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/AdvancedTopicsInInferenceAPIs.ipynb) * [Example: Comparing Accuracy with CPU-based Inference](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/InferenceAccuracyCheck.ipynb) * [Example: Image Classification Inference](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/Image_Classification.ipynb) * [Example: SSD Object Detection Inference](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/SSD_Object_Detection.ipynb) * [Other Python SDK Examples](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/main\/examples\/inferences)\n### Serving, Model Deployment, MLOps [](#serving-model-deployment-mlops \"Permalink to this heading\")\n* [Model Server (Serving Framework)](software\/serving.html) * [Kubernetes Support](software\/kubernetes_support.html)\nReferences [](#references \"Permalink to this heading\") -------------------------------------------------------\n* [C Language SDK Reference](https:\/\/furiosa-ai.github.io\/docs\/v0.9.0\/en\/api\/c\/index.html) * [Python SDK Reference](api\/python\/modules.html)\nOther Links [](#other-links \"Permalink to this heading\") ---------------------------------------------------------\n* [FuriosaAI Home](https:\/\/furiosa.ai) * [FuriosaAI Customer Support Center](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals\/) * [Bug Report](customer-support\/bugs.html#bugreport)\n[Next](npu\/warboy.html \"FuriosaAI Warboy\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* FuriosaAI NPU & SDK 0.10.1 문서 * [페이지 소스 보기](_sources\/index.rst.txt)\n---\nFuriosaAI NPU & SDK 0.10.1 문서 [](#furiosaai-npu-sdk-release-documents \"이 제목의 영구 링크\") ==========================================================================================================\n이 문서는 FuriosaAI NPU와 그 SDK에 대해 설명합니다.\n\n참고\nFuriosaAI 소프트웨어 구성 요소에는 커널 드라이버, 펌웨어, 런타임, C SDK, Python SDK, 명령줄 도구가 포함됩니다. 현재 우리는 *Early Access Program (EAP)*에 등록하고 *End User Licence Agreement (EULA)*에 동의한 사용자에게만 제공합니다.\nEAP를 시작하는 방법을 알아보려면 [contact @ furiosa . ai](mailto:contact%40furiosa.ai)로 문의하십시오.\n\nFuriosaAI NPU [](#furiosaai-npu \"이 제목의 영구 링크\") -------------------------------------------------------------\n* [FuriosaAI Warboy 소개](npu\/warboy.html) : 하드웨어 사양, 성능 및 지원되는 연산자\n\nFuriosaAI 소프트웨어 [](#furiosaai-software \"이 제목의 영구 링크\") -----------------------------------------------------------------------\n* [FuriosaAI SW 스택 소개](software\/intro.html) \n* [드라이버, 펌웨어 및 런타임 설치](software\/installation.html) \n* [Python SDK 설치 및 사용자 가이드](software\/python-sdk.html) \n* [C SDK 설치 및 사용자 가이드](software\/c-sdk.html) \n* [명령줄 도구](software\/cli.html) \n* [컴파일러](software\/compiler.html) \n* [모델 양자화](software\/quantization.html) \n* [FuriosaAI 모델 동물원](https:\/\/furiosa-ai.github.io\/furiosa-models\/latest\/) \n* [Kubernetes 지원](software\/kubernetes_support.html) \n* [가상 머신을 위한 Warboy 패스스루 구성](software\/vm_support.html)\n\n### FuriosaAI SDK 튜토리얼 및 예제 [](#furiosaai-sdk-tutorial-and-examples \"이 제목의 영구 링크\")\n* [튜토리얼: Furiosa SDK 사용법 시작부터 끝까지](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/HowToUseFuriosaSDKFromStartToFinish.ipynb) \n* [튜토리얼: 기본 추론 API](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/GettingStartedWithPythonSDK.ipynb) \n* [튜토리얼: 고급 추론 API](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/AdvancedTopicsInInferenceAPIs.ipynb) \n* [예제: CPU 기반 추론과 정확도 비교](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/InferenceAccuracyCheck.ipynb) \n* [예제: 이미지 분류 추론](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/Image_Classification.ipynb) \n* [예제: SSD 객체 탐지 추론](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/blob\/main\/examples\/notebooks\/SSD_Object_Detection.ipynb) \n* [기타 Python SDK 예제](https:\/\/github.com\/furiosa-ai\/furiosa-sdk\/tree\/main\/examples\/inferences)\n\n### 서빙, 모델 배포, MLOps [](#serving-model-deployment-mlops \"이 제목의 영구 링크\")\n* [모델 서버 (서빙 프레임워크)](software\/serving.html) \n* [Kubernetes 지원](software\/kubernetes_support.html)\n\n참고 자료 [](#references \"이 제목의 영구 링크\") -------------------------------------------------------\n* [C 언어 SDK 참조](https:\/\/furiosa-ai.github.io\/docs\/v0.9.0\/en\/api\/c\/index.html) \n* [Python SDK 참조](api\/python\/modules.html)\n\n기타 링크 [](#other-links \"이 제목의 영구 링크\") ---------------------------------------------------------\n* [FuriosaAI 홈](https:\/\/furiosa.ai) \n* [FuriosaAI 고객 지원 센터](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals\/) \n* [버그 보고](customer-support\/bugs.html#bugreport)\n\n[다음](npu\/warboy.html \"FuriosaAI Warboy\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며, [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용했습니다.\n```"},{"page_id":"f6a8c152-6f4c-4ac0-98ae-fbe3093522c6","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/customer-support\/bugs.html","original_content":"* Bug Report * [View page source](..\/_sources\/customer-support\/bugs.rst.txt)\n---\nBug Report [](#bug-report \"Permalink to this heading\") =======================================================\nIf you encounter an unresolvable issue, you can file a bug report at [FuriosaAI customer service center](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals) .\nThe following information should be included in a bug report.\n1. How to reproduce the bug 2. Log or screenshot of the bug 3. SDK version information 4. Compilation log, if model compilation failed\nBy default, when an error happens furiosa-sdk outputs the following message. If you see the following message, file a report to the Bug Report\nsection of [FuriosaAI customer service center](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals) with:\n1. The information given below the    `Information        Dump`    , and 2. The compilation log file (this would be    `\/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log`    in the following example) outputted in the message.\n``` Saving the compilation log into \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log Using furiosa-compiler 0.5.0 (rev: 407c0c51f-modified built at 2021-11-18 22:32:34) 2021-11-22T06:30:28.392114Z  INFO Npu (npu0pe0) is being initialized 2021-11-22T06:30:28.397757Z  INFO NuxInner create with pes: [PeId(0)] [1\/6] 🔍   Compiling from onnx to dfg 2021-11-22T06:30:28.423026Z  INFO [Profiler] Received a termination signal. 2021-11-22T06:30:28.423371Z ERROR fail to compile the model: the static shape of tensor 'input' contains an unsupported dimension value: Some(DimParam(\"batch_size\")) ================================================================================ Information Dump ================================================================================ - Python version: 3.8.10 (default, Sep 28 2021, 16:10:42)  [GCC 9.3.0] - furiosa-libnux path: libnux.so.0.5.0 - furiosa-libnux version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34) - furiosa-compiler version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34) - furiosa-sdk-runtime version: Furiosa SDK Runtime  (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\nPlease check the compiler log at \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log. If you have a problem, please report the log file to https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals with the information dumped above. ================================================================================\n```\nIf you do not see a message as shown above, refer to the instructions below to collect the necessary information yourself to file a bug report at [FuriosaAI customer service center](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals) .\nYou can find the Python runtime version information as shown.\n``` $ python --version Python 3.8.6\n```\nYou can find the SDK version information as shown.\n``` $ python -c \"from furiosa import runtime;print(runtime.__full_version__)\" loaded native library \/usr\/lib64\/libnux.so (0.5.0 407c0c51f) Furiosa SDK Runtime 0.5.0 (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\n```\n[Previous](..\/releases\/0.5.0.html \"Release Notes - 0.5.0\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 버그 리포트 * [페이지 소스 보기](..\/_sources\/customer-support\/bugs.rst.txt)\n---\n버그 리포트 [](#bug-report \"이 제목의 영구 링크\") =======================================================\n해결할 수 없는 문제가 발생하면 [FuriosaAI 고객 서비스 센터](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals)에 버그 리포트를 제출할 수 있습니다.\n버그 리포트에는 다음 정보가 포함되어야 합니다.\n1. 버그를 재현하는 방법 2. 버그의 로그 또는 스크린샷 3. SDK 버전 정보 4. 모델 컴파일 실패 시 컴파일 로그\n기본적으로 오류가 발생하면 furiosa-sdk는 다음 메시지를 출력합니다. 다음 메시지를 보면, [FuriosaAI 고객 서비스 센터](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals)의 버그 리포트 섹션에 다음 정보를 포함하여 보고서를 제출하세요:\n1. `Information Dump` 아래에 제공된 정보, 그리고 2. 메시지에 출력된 컴파일 로그 파일 (예: `\/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log`).\n``` \nSaving the compilation log into \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log \nUsing furiosa-compiler 0.5.0 (rev: 407c0c51f-modified built at 2021-11-18 22:32:34) \n2021-11-22T06:30:28.392114Z  INFO Npu (npu0pe0) is being initialized \n2021-11-22T06:30:28.397757Z  INFO NuxInner create with pes: [PeId(0)] \n[1\/6] 🔍   Compiling from onnx to dfg \n2021-11-22T06:30:28.423026Z  INFO [Profiler] Received a termination signal. \n2021-11-22T06:30:28.423371Z ERROR fail to compile the model: the static shape of tensor 'input' contains an unsupported dimension value: Some(DimParam(\"batch_size\")) \n================================================================================ \nInformation Dump \n================================================================================ \n- Python version: 3.8.10 (default, Sep 28 2021, 16:10:42)  [GCC 9.3.0] \n- furiosa-libnux path: libnux.so.0.5.0 \n- furiosa-libnux version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34) \n- furiosa-compiler version: 0.5.0 (rev: 407c0c51f built at 2021-11-18 22:32:34) \n- furiosa-sdk-runtime version: Furiosa SDK Runtime  (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\n컴파일러 로그를 \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log에서 확인하세요. 문제가 있으면 위에 덤프된 정보와 함께 로그 파일을 https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals에 보고하세요. \n================================================================================\n```\n위와 같은 메시지가 보이지 않으면, [FuriosaAI 고객 서비스 센터](https:\/\/furiosa-ai.atlassian.net\/servicedesk\/customer\/portals)에 버그 리포트를 제출하기 위해 필요한 정보를 수집하는 방법은 아래 지침을 참조하세요.\nPython 런타임 버전 정보는 다음과 같이 확인할 수 있습니다.\n``` \n$ python --version \nPython 3.8.6\n```\nSDK 버전 정보는 다음과 같이 확인할 수 있습니다.\n``` \n$ python -c \"from furiosa import runtime;print(runtime.__full_version__)\" \nloaded native library \/usr\/lib64\/libnux.so (0.5.0 407c0c51f) \nFuriosa SDK Runtime 0.5.0 (libnux 0.5.0 407c0c51f 2021-11-18 22:32:34)\n```\n[이전](..\/releases\/0.5.0.html \"릴리스 노트 - 0.5.0\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 제작되었으며 [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용합니다.\n```"},{"page_id":"ca538c67-ce79-412f-bd1b-c0f73dd818e7","link":"https:\/\/furiosa-ai.github.io\/docs\/latest\/en\/releases\/0.7.0.html","original_content":"* Release Notes - 0.7.0 * [View page source](..\/_sources\/releases\/0.7.0.rst.txt)\n---\nRelease Notes - 0.7.0 [](#release-notes-0-7-0 \"Permalink to this heading\") ===========================================================================\nFuriosa SDK 0.7.0 is a major release, and includes approximately 1,400 commits towards performance enhancement, added functions, and bug fixes.\ncomponent version information\n[](#id1 \"Permalink to this table\")\n| Package name | Version | | --- | --- | | NPU Driver | 1.3.0 | | HAL (Hardware Abstraction Layer) | 0.8.0 | | Furiosa Compiler | 0.7.0 | | Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..) | 0.7.0 | | NPU Device Plugin | 0.10.0 | | NPU Feature Discovery | 0.1.0 | | NPU Management CLI (furiosactl) | 0.9.1 |\nHow to upgrade [](#how-to-upgrade \"Permalink to this heading\") ---------------------------------------------------------------\nThe upgrade is a simple process if you are using an APT repository. Detailed information on APT repository setting and installation can be found in [Driver, Firmware, and Runtime Installation](..\/software\/installation.html#requiredpackages) .\n> ``` > apt-get update && \\ > apt-get install -y furiosa-driver-pdma furiosa-libnux >  > pip install --upgrade furiosa-sdk >  > ```\nKey changes [](#key-changes \"Permalink to this heading\") ---------------------------------------------------------\n### Compiler - More NPU acceleration supports [](#compiler-more-npu-acceleration-supports \"Permalink to this heading\")\nThrough improvements in the compiler, more operators can be accelerated in various use cases. Accelerated operators with its condition adopted by 0.7.0 release are following. You can find the entire list of accelerated operators at [List of Supported Operators for Warboy Acceleration](..\/npu\/warboy.html#supportedoperators) .\n> * Added Linear and Nearest mode support for the Resize operator > * Added DCR mode support for the SpaceToDepth operator > * Added DCR mode support for the DepthToSpace operator > * Added CHW axis support for the Pad operator > * Added C axis support for the Slice operator > * Added acceleration support for operators Tanh, Exp, and Log > * Added C axis support for the Concat operator > * Increased Dilation support to up to x12 > * Added acceleration support for operators Gelu, Erf, and Elu\n### Compiler - Compiler Cache [](#compiler-compiler-cache \"Permalink to this heading\")\nCompiler cache stores the compiled binary into a cache directory, and reuses the cache when the same model is compiled. Also, you can also use Redis as the compiler cache storage. More detailed instructions can be found in [Compiler Cache](..\/software\/compiler.html#compilercache) .\n### Compiler - Compiler Hint [](#compiler-compiler-hint \"Permalink to this heading\")\nWhen running a function that includes compilation, such as `session.create()` , a path that includes the compilation log is printed as follows.\n> ``` > Saving the compilation log into \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log >  > ```\nSince 0.7.0, compilation logs contain compilation hints more helpful to understand the compilation process and give some optimization opportunities.\nThe `cat\n<log\nfile>\n|\ngrep\nHint` command will show you only hint from the log. The hint informs why certain operators are not accelerated as shown in the below example.\n> ``` > cat \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log | grep Hint > 2022-05-24T02:44:11.399402Z  WARN nux::session: Hint [19]: 'LogSoftmax' cannot be accelerated yet > 2022-05-24T02:44:11.399407Z  WARN nux::session: Hint [12]: groups should be bigger than 1 > 2022-05-24T02:44:11.399408Z  WARN nux::session: Hint [17]: Softmax with large batch (36 > 2) cannot be accelerated by Warboy >  > ```\n### Performance Profiling Tools [](#performance-profiling-tools \"Permalink to this heading\")\nThe profiler had been an experimental and closed-beta feature. The release 0.7.0 includes the performance profiler by default. It allow users to view the time taken in each step in the model inference process. You can activate the profiler through a shell environment variable or a profiler context in your Python code.\nMore details can be found in [Performance Profiling](..\/software\/profiler.html#profiling) .\n### Improvements\/Bug fixes of Python SDK [](#improvements-bug-fixes-of-python-sdk \"Permalink to this heading\")\n* Since 0.7.0,   `session.create()`   and   `session.create_async()`   can take the batch size. * Fixed a bug that compiler options passed to   `session.create()`   and   `session.create_async()`   wasn’t effective.\nBelow is an example that uses batch size and compiler option.\n> ``` > config = { >   \"without_quantize\": { >       \"parameters\": [{\"input_min\": 0.0, \"input_max\": 255.0, \"permute\": [0, 2, 3, 1]}] >   } > } >  > with session.create(\"model.onnx\", batch_size=2, compile_config=config) as sess: >   outputs = sess.run(inputs) >  > ```\n### Improvements\/Bug fixes of Quantization tools [](#improvements-bug-fixes-of-quantization-tools \"Permalink to this heading\")\n* You can now infer published tensor shapes even if   axes      property is not designated in ONNX Squeeze operators below version OpSet 12 * Added support not just for Conv receiving tensors with NxCxHxW shapes as input, but also for Conv receiving tensors with NxCxD shapes * Modified “Conv - BatchNormalization” subgraph to be fused to Conv even when Conv does not receive bias as input * Modified to always quantize Sub, Concat, and Pow operators in QDQ format, regardless of whether operands have initial values, so that the model can be processed in a consistent way in the post-quantization process * Modified to prevent ONNX Runtime related warnings in the quantization process and the result model * Reinforced the inspection condition to not miss any cases where tensor shape information cannot be inferred * Modified to allow random calibration not only for models that receive float32 data as inputs, but also for models that receive other decimal or integer types as inputs * Modified to find and terminate in a stable manner when given an already quantized model * Modified to adjust scale of weight appropriately if Conv data input or scale of weight is too small, such that scale of bias becomes 0 * Reinforced conditions for “Gather - MatMul” subgraph to be fused into Gather * Dependent libraries updated to latest version\n### Device Plugin - Configuration file support [](#device-plugin-configuration-file-support \"Permalink to this heading\")\nA function to set the execution option of the NPU Device Plugin used in Kubernetes with a file has been added. As before, option items can be entered as command-line arguments, or options can be specified by selecting a configuration file. Detailed instructions can be found in [Kubernetes Support](..\/software\/kubernetes_support.html#kubernetesintegration) .\n[Previous](0.8.0.html \"Release Notes - 0.8.0\") [Next](0.6.0.html \"Release Notes - 0.6.0\")\n---\n© Copyright 2023 FuriosaAI, Inc..\nBuilt with [Sphinx](https:\/\/www.sphinx-doc.org\/) using a [theme](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme) provided by [Read the Docs](https:\/\/readthedocs.org) .","translated_content":"```markdown\n* 릴리스 노트 - 0.7.0 * [페이지 소스 보기](..\/_sources\/releases\/0.7.0.rst.txt)\n---\n릴리스 노트 - 0.7.0 [](#release-notes-0-7-0 \"이 제목의 고유 링크\") ===========================================================================\nFuriosa SDK 0.7.0은 주요 릴리스로, 성능 향상, 기능 추가 및 버그 수정을 위한 약 1,400개의 커밋이 포함되어 있습니다.\n구성 요소 버전 정보\n[](#id1 \"이 표의 고유 링크\")\n| 패키지 이름 | 버전 | | --- | --- | | NPU 드라이버 | 1.3.0 | | HAL (하드웨어 추상화 레이어) | 0.8.0 | | Furiosa 컴파일러 | 0.7.0 | | Python SDK (furiosa-runtime, furiosa-server, furiosa-serving, furiosa-quantizer, ..) | 0.7.0 | | NPU 디바이스 플러그인 | 0.10.0 | | NPU 기능 탐지 | 0.1.0 | | NPU 관리 CLI (furiosactl) | 0.9.1 |\n\n업그레이드 방법 [](#how-to-upgrade \"이 제목의 고유 링크\") ---------------------------------------------------------------\nAPT 저장소를 사용하는 경우 업그레이드는 간단한 과정입니다. APT 저장소 설정 및 설치에 대한 자세한 정보는 [드라이버, 펌웨어 및 런타임 설치](..\/software\/installation.html#requiredpackages)에서 확인할 수 있습니다.\n> ``` > apt-get update && \\ > apt-get install -y furiosa-driver-pdma furiosa-libnux >  > pip install --upgrade furiosa-sdk >  > ```\n\n주요 변경 사항 [](#key-changes \"이 제목의 고유 링크\") ---------------------------------------------------------\n### 컴파일러 - 더 많은 NPU 가속 지원 [](#compiler-more-npu-acceleration-supports \"이 제목의 고유 링크\")\n컴파일러의 개선을 통해 다양한 사용 사례에서 더 많은 연산자가 가속될 수 있습니다. 0.7.0 릴리스에 채택된 조건과 함께 가속된 연산자는 다음과 같습니다. 가속된 연산자의 전체 목록은 [Warboy 가속을 위한 지원되는 연산자 목록](..\/npu\/warboy.html#supportedoperators)에서 확인할 수 있습니다.\n> * Resize 연산자에 Linear 및 Nearest 모드 지원 추가 > * SpaceToDepth 연산자에 DCR 모드 지원 추가 > * DepthToSpace 연산자에 DCR 모드 지원 추가 > * Pad 연산자에 CHW 축 지원 추가 > * Slice 연산자에 C 축 지원 추가 > * Tanh, Exp, Log 연산자에 대한 가속 지원 추가 > * Concat 연산자에 C 축 지원 추가 > * 최대 x12까지 Dilation 지원 증가 > * Gelu, Erf, Elu 연산자에 대한 가속 지원 추가\n\n### 컴파일러 - 컴파일러 캐시 [](#compiler-compiler-cache \"이 제목의 고유 링크\")\n컴파일러 캐시는 컴파일된 바이너리를 캐시 디렉토리에 저장하고 동일한 모델이 컴파일될 때 캐시를 재사용합니다. 또한, Redis를 컴파일러 캐시 저장소로 사용할 수도 있습니다. 자세한 지침은 [컴파일러 캐시](..\/software\/compiler.html#compilercache)에서 확인할 수 있습니다.\n\n### 컴파일러 - 컴파일러 힌트 [](#compiler-compiler-hint \"이 제목의 고유 링크\")\n`session.create()`와 같은 컴파일을 포함하는 함수를 실행할 때, 컴파일 로그가 포함된 경로가 다음과 같이 출력됩니다.\n> ``` > 컴파일 로그를 \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log에 저장합니다 >  > ```\n0.7.0부터 컴파일 로그에는 컴파일 과정을 이해하고 최적화 기회를 제공하는 더 유용한 컴파일 힌트가 포함됩니다.\n`cat <log file> | grep Hint` 명령어는 로그에서 힌트만 보여줍니다. 힌트는 아래 예시와 같이 특정 연산자가 가속되지 않는 이유를 알려줍니다.\n> ``` > cat \/home\/furiosa\/.local\/state\/furiosa\/logs\/compile-20211121223028-l5w4g6.log | grep Hint > 2022-05-24T02:44:11.399402Z  WARN nux::session: Hint [19]: 'LogSoftmax'는 아직 가속될 수 없습니다 > 2022-05-24T02:44:11.399407Z  WARN nux::session: Hint [12]: 그룹은 1보다 커야 합니다 > 2022-05-24T02:44:11.399408Z  WARN nux::session: Hint [17]: 큰 배치(36 > 2)의 Softmax는 Warboy에 의해 가속될 수 없습니다 >  > ```\n\n### 성능 프로파일링 도구 [](#performance-profiling-tools \"이 제목의 고유 링크\")\n프로파일러는 실험적이고 클로즈드 베타 기능이었습니다. 0.7.0 릴리스에는 성능 프로파일러가 기본적으로 포함되어 있습니다. 사용자는 모델 추론 과정의 각 단계에서 소요된 시간을 확인할 수 있습니다. 프로파일러는 셸 환경 변수 또는 Python 코드의 프로파일러 컨텍스트를 통해 활성화할 수 있습니다.\n자세한 내용은 [성능 프로파일링](..\/software\/profiler.html#profiling)에서 확인할 수 있습니다.\n\n### Python SDK의 개선\/버그 수정 [](#improvements-bug-fixes-of-python-sdk \"이 제목의 고유 링크\")\n* 0.7.0부터 `session.create()` 및 `session.create_async()`는 배치 크기를 받을 수 있습니다. * `session.create()` 및 `session.create_async()`에 전달된 컴파일러 옵션이 효과적이지 않았던 버그를 수정했습니다.\n아래는 배치 크기와 컴파일러 옵션을 사용하는 예시입니다.\n> ``` > config = { >   \"without_quantize\": { >       \"parameters\": [{\"input_min\": 0.0, \"input_max\": 255.0, \"permute\": [0, 2, 3, 1]}] >   } > } >  > with session.create(\"model.onnx\", batch_size=2, compile_config=config) as sess: >   outputs = sess.run(inputs) >  > ```\n\n### 양자화 도구의 개선\/버그 수정 [](#improvements-bug-fixes-of-quantization-tools \"이 제목의 고유 링크\")\n* ONNX Squeeze 연산자에서 axes 속성이 지정되지 않은 경우에도 게시된 텐서 모양을 추론할 수 있습니다. * Conv가 NxCxHxW 형태의 텐서를 입력으로 받는 것뿐만 아니라 NxCxD 형태의 텐서를 받는 경우도 지원합니다. * Conv가 바이어스를 입력으로 받지 않는 경우에도 \"Conv - BatchNormalization\" 서브그래프를 Conv에 융합하도록 수정했습니다. * 피연산자가 초기 값을 가지고 있는지 여부에 관계없이 Sub, Concat, Pow 연산자를 QDQ 형식으로 항상 양자화하도록 수정하여 모델이 후 양자화 과정에서 일관되게 처리될 수 있도록 했습니다. * 양자화 과정과 결과 모델에서 ONNX Runtime 관련 경고를 방지하도록 수정했습니다. * 텐서 모양 정보를 추론할 수 없는 경우를 놓치지 않도록 검사 조건을 강화했습니다. * float32 데이터를 입력으로 받는 모델뿐만 아니라 다른 소수 또는 정수 유형의 데이터를 입력으로 받는 모델에 대해서도 랜덤 보정을 허용하도록 수정했습니다. * 이미 양자화된 모델이 주어졌을 때 안정적으로 찾고 종료하도록 수정했습니다. * Conv 데이터 입력 또는 가중치의 스케일이 너무 작은 경우 바이어스의 스케일이 0이 되지 않도록 가중치의 스케일을 적절히 조정하도록 수정했습니다. * \"Gather - MatMul\" 서브그래프를 Gather에 융합하기 위한 조건을 강화했습니다. * 종속 라이브러리를 최신 버전으로 업데이트했습니다.\n\n### 디바이스 플러그인 - 구성 파일 지원 [](#device-plugin-configuration-file-support \"이 제목의 고유 링크\")\nKubernetes에서 사용되는 NPU 디바이스 플러그인의 실행 옵션을 파일로 설정할 수 있는 기능이 추가되었습니다. 이전과 마찬가지로 옵션 항목은 명령줄 인수로 입력할 수 있으며, 구성 파일을 선택하여 옵션을 지정할 수도 있습니다. 자세한 지침은 [Kubernetes 지원](..\/software\/kubernetes_support.html#kubernetesintegration)에서 확인할 수 있습니다.\n\n[이전](0.8.0.html \"릴리스 노트 - 0.8.0\") [다음](0.6.0.html \"릴리스 노트 - 0.6.0\")\n---\n© 저작권 2023 FuriosaAI, Inc..\n[Sphinx](https:\/\/www.sphinx-doc.org\/)로 [Read the Docs](https:\/\/readthedocs.org)에서 제공하는 [테마](https:\/\/github.com\/readthedocs\/sphinx_rtd_theme)를 사용하여 제작되었습니다.\n```"}]