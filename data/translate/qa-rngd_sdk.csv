page_id,link,original_content,translated_content
3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/intro.html,"* [.rst](../_sources/furiosa_llm/intro.rst ""Download source file"") * .pdf
Furiosa LLM ===========
Furiosa LLM [#](#furiosa-llm ""Link to this heading"") ====================================================
Furiosa LLM provides a high-performance inference engine for LLM models and Multi-Modal LLM models, Furiosa LLM is designed to provide the state-of-the-art serving optimization. The features of Furiosa LLM includes:
* vLLM-compatible API * Efficient KV cache management with PagedAttention * Continuous batching of incoming requests in serving * Quantization: INT4, INT8, FP8, GPTQ, AWQ * Data Parallelism and Pipeline Parallelism across multiple NPUs * Tensor Parallelism (planned in release 2024.2) across multiple NPUs * OpenAI-compatible API server * Various decoding algorithms, greedy search, beam search, top-k/top-p, speculative decoding (planned) * HuggingFace model integration and hub support * HuggingFace PEFT support (planned)
[previous
Running MLPerf™ Inference Benchmark](../getting_started/furiosa_mlperf.html ""previous page"") [next
References](references.html ""next page"")
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../_sources/furiosa_llm/intro.rst ""소스 파일 다운로드"") * .pdf

Furiosa LLM
===========

Furiosa LLM [#](#furiosa-llm ""이 제목으로 링크"") 
====================================================

Furiosa LLM은 LLM 모델과 멀티모달 LLM 모델을 위한 고성능 추론 엔진을 제공합니다. Furiosa LLM은 최첨단 서빙 최적화를 제공하도록 설계되었습니다. Furiosa LLM의 기능은 다음과 같습니다:

* vLLM 호환 API
* PagedAttention을 사용한 효율적인 KV 캐시 관리
* 서빙 시 들어오는 요청의 지속적인 배칭
* 양자화: INT4, INT8, FP8, GPTQ, AWQ
* 여러 NPU에 걸친 데이터 병렬 처리 및 파이프라인 병렬 처리
* 여러 NPU에 걸친 텐서 병렬 처리 (2024.2 릴리스 예정)
* OpenAI 호환 API 서버
* 다양한 디코딩 알고리즘, 탐욕적 탐색, 빔 탐색, top-k/top-p, 추측 디코딩 (예정)
* HuggingFace 모델 통합 및 허브 지원
* HuggingFace PEFT 지원 (예정)

[이전
MLPerf™ 추론 벤치마크 실행](../getting_started/furiosa_mlperf.html ""이전 페이지"") [다음
참조](references.html ""다음 페이지"")

FuriosaAI, Inc.에 의해 제공됨.
© 저작권 2024, FuriosaAI, Inc.
```"
dcd59fbc-fb76-4f34-b6ec-ea88a833b047,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/device_plugin.html,"* [.rst](../../_sources/cloud_native_toolkit/kubernetes/device_plugin.rst ""Download source file"") * .pdf
Installing Furiosa Device Plugin ================================
Contents --------
* [Furiosa Device Plugin](#furiosa-device-plugin)   + [Configuration](#configuration)   + [Deploying Furiosa Device Plugin with Helm](#deploying-furiosa-device-plugin-with-helm)
Installing Furiosa Device Plugin [#](#installing-furiosa-device-plugin ""Link to this heading"") ==============================================================================================
Furiosa Device Plugin [#](#furiosa-device-plugin ""Link to this heading"") ------------------------------------------------------------------------
The Furiosa device plugin implements the [Kubernetes Device Plugin](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) interface for FuriosaAI NPU devices, and its features are as follows:
* discovering the Furiosa NPU devices and register to a Kubernetes cluster. * tracking the health of the devices and report to a Kubernetes cluster. * running AI workload on the top of the Furiosa NPU devices within a Kubernetes cluster.
### Configuration [#](#configuration ""Link to this heading"")
The Furiosa NPU can be integrated into the Kubernetes cluster in various configurations. A single NPU card can either be exposed as a single resource or partitioned into multiple resources. Partitioning into multiple resources allows for more granular control.
The following table shows the available resource strategy:
Resource Strategy
[#](#id1 ""Link to this table"")
| NPU Configuration | Resource Name | Resource Count Per Card | | --- | --- | --- | | legacy | beta.furiosa.ai/npu | 1 | | generic | furiosa.ai/rngd | 1 |
The helm chart of Furiosa device plugin is available at [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) .
Following shows default values of the helm chart.
``` config:   resourceStrategy: generic   debugMode: false   disabledDeviceUUIDListMap:
```
### Deploying Furiosa Device Plugin with Helm [#](#deploying-furiosa-device-plugin-with-helm ""Link to this heading"")
The Furiosa device plugin helm chart is available at [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) . To configure deployment as you need, you can modify `charts/furiosa-device-plugin/values.yaml` .
* If resourceStrategy is not specified, the default value is   `""generic""`   . * If debugMode is not specified, the default value is   `false`   . * If disabledDeviceUUIDListMap is not specified, the default value is empty list   `[]`   .
You can deploy the Furiosa Device Plugin by running the following commands:
``` helm repo add furiosa https://furiosa-ai.github.io/helm-charts helm repo update helm install furiosa-device-plugin furiosa/furiosa-device-plugin -n kube-system
```
[previous
Installing Furiosa Feature Discovery](feature_discovery.html ""previous page"") [next
Installing Furiosa Metrics Exporter](metrics_exporter.html ""next page"")
Contents
* [Furiosa Device Plugin](#furiosa-device-plugin)   + [Configuration](#configuration)   + [Deploying Furiosa Device Plugin with Helm](#deploying-furiosa-device-plugin-with-helm)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","* [.rst](../../_sources/cloud_native_toolkit/kubernetes/device_plugin.rst ""소스 파일 다운로드"") * .pdf

Furiosa 디바이스 플러그인 설치
===============================

목차
--------

* [Furiosa 디바이스 플러그인](#furiosa-device-plugin)  
  + [설정](#configuration)  
  + [Helm을 사용한 Furiosa 디바이스 플러그인 배포](#deploying-furiosa-device-plugin-with-helm)

Furiosa 디바이스 플러그인 설치 [#](#installing-furiosa-device-plugin ""이 제목으로 링크"")
==============================================================================================

Furiosa 디바이스 플러그인 [#](#furiosa-device-plugin ""이 제목으로 링크"")
------------------------------------------------------------------------

Furiosa 디바이스 플러그인은 FuriosaAI NPU 장치를 위한 [Kubernetes 디바이스 플러그인](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) 인터페이스를 구현하며, 그 기능은 다음과 같습니다:

* Furiosa NPU 장치를 발견하고 Kubernetes 클러스터에 등록합니다.
* 장치의 상태를 추적하고 Kubernetes 클러스터에 보고합니다.
* Kubernetes 클러스터 내에서 Furiosa NPU 장치 위에서 AI 작업을 실행합니다.

### 설정 [#](#configuration ""이 제목으로 링크"")

Furiosa NPU는 다양한 설정으로 Kubernetes 클러스터에 통합될 수 있습니다. 단일 NPU 카드는 하나의 리소스로 노출되거나 여러 리소스로 분할될 수 있습니다. 여러 리소스로 분할하면 더 세밀한 제어가 가능합니다.

다음 표는 사용 가능한 리소스 전략을 보여줍니다:

리소스 전략
[#](#id1 ""이 표로 링크"")

| NPU 설정 | 리소스 이름 | 카드당 리소스 수 |
| --- | --- | --- |
| legacy | beta.furiosa.ai/npu | 1 |
| generic | furiosa.ai/rngd | 1 |

Furiosa 디바이스 플러그인의 helm 차트는 [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) 에서 사용할 수 있습니다.

다음은 helm 차트의 기본값을 보여줍니다.

```yaml
config:
  resourceStrategy: generic
  debugMode: false
  disabledDeviceUUIDListMap: []
```

### Helm을 사용한 Furiosa 디바이스 플러그인 배포 [#](#deploying-furiosa-device-plugin-with-helm ""이 제목으로 링크"")

Furiosa 디바이스 플러그인 helm 차트는 [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) 에서 사용할 수 있습니다. 필요한 대로 배포를 구성하려면 `charts/furiosa-device-plugin/values.yaml` 파일을 수정할 수 있습니다.

* resourceStrategy가 지정되지 않으면 기본값은 `""generic""`입니다.
* debugMode가 지정되지 않으면 기본값은 `false`입니다.
* disabledDeviceUUIDListMap이 지정되지 않으면 기본값은 빈 리스트 `[]`입니다.

다음 명령어를 실행하여 Furiosa 디바이스 플러그인을 배포할 수 있습니다:

```bash
helm repo add furiosa https://furiosa-ai.github.io/helm-charts
helm repo update
helm install furiosa-device-plugin furiosa/furiosa-device-plugin -n kube-system
```

[이전
Furiosa 기능 탐지 설치](feature_discovery.html ""이전 페이지"") [다음
Furiosa 메트릭스 익스포터 설치](metrics_exporter.html ""다음 페이지"")

목차
* [Furiosa 디바이스 플러그인](#furiosa-device-plugin)  
  + [설정](#configuration)  
  + [Helm을 사용한 Furiosa 디바이스 플러그인 배포](#deploying-furiosa-device-plugin-with-helm)

By FuriosaAI, Inc.
© 저작권 2024, FuriosaAI, Inc."
3bb3bd0a-7bdc-45a7-8e3e-e556a52d7eda,https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/furiosa_mlperf.html,"* [.rst](../_sources/getting_started/furiosa_mlperf.rst ""Download source file"") * .pdf
Running MLPerf™ Inference Benchmark ===================================
Contents --------
* [Installing   `furiosa-mlperf`   command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1)   + [SYNOPSIS](#synopsis)   + [Examples](#examples) * [Running   `furiosa-mlperf`   in container environment](#running-furiosa-mlperf-in-container-environment)
Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") ===================================================================================================
MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems.
FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack.
Note  `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1.
The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1.
Installing `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command ""Link to this heading"") --------------------------------------------------------------------------------------------------
To install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following:
The minimum requirements for `furiosa-mlperf` are as follows:
* Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers (   [Setting up APT server](prerequisites.html#aptsetup)   ) * About 100GB storage space (only for the Llama 3.1 70B)
Then, please install the `furiosa-mlperf` package as follows:
``` sudo apt update sudo apt install -y furiosa-mlperf
```
This command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` .
Running MLPerf Inference Benchmark [#](#id1 ""Link to this heading"") -------------------------------------------------------------------
### SYNOPSIS [#](#synopsis ""Link to this heading"")
The `furiosa-mlperf` command provides the following subcommands:
``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z)
Usage: furiosa-mlperf <COMMAND>
Commands:   bert-offline       Run BERT benchmark with offline scenario   bert-server        Run BERT benchmark with server scenario   gpt-j-offline      Run GPT-J benchmark with offline scenario   gpt-j-server       Run GPT-J benchmark with server scenario   llama-3.1-offline  Run Llama 3.1 benchmark with offline scenario   llama-3.1-server   Run Llama 3.1 benchmark with server scenario   help               Print this message or the help of the given subcommand(s)
Options:   -h, --help     Print help   -V, --version  Print version
```
### Examples [#](#examples ""Link to this heading"")
* BERT benchmark      The BERT benchmark is based on running with a single RNGD.      + Server Scenario          To run BERT Large serving inference benchmark, you can use the following command:          ```     furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh ""npu:0:*""          ```   + Offline Scenario          To run BERT Large offline inference benchmark, you can use the following command:          ```     furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh ""npu:0:*""          ``` * GPT-J benchmark      The GPT-J benchmark is based on running with a single RNGD.      + Server Scenario          To run GPT-J 6B serving inference benchmark, you can use the following command:          ```     furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result          ```   + Offline Scenario          To run GPT-J 6B offline inference benchmark, you can use the following command:          ```     furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result          ``` * Llama 3.1 benchmark      The Llama 3.1 benchmark is based on running with four RNGDs.      + Server Scenario          To run Llama 3.1 70B serving inference benchmark, you can use the following command:          ```     furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result          ```   + Offline Scenario          To run Llama 3.1 70B offline inference benchmark, you can use the following command:          ```     furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result          ``` * Common      Once the process completes, it writes the results to a file in the specified results directory.   You can open this file to view a summary of the results.      ```   cat gpt-j-offline-result/mlperf_log_summary.txt      ```         ```   ================================================   MLPerf Results Summary   ================================================   SUT name : GPT-J SUT   Scenario : Offline   Mode     : PerformanceOnly   Samples per second: 11.842   Tokens per second (inferred): 817.095   Result is : VALID     Min duration satisfied : Yes     Min queries satisfied : Yes     Early stopping satisfied: Yes      ```
Running `furiosa-mlperf` in container environment [#](#running-furiosa-mlperf-in-container-environment ""Link to this heading"") ------------------------------------------------------------------------------------------------------------------------------
FuriosaAI provides a containerized version of the `furiosa-mlperf` command. If you use the containerized version, you can run the `furiosa-mlperf` command without installing the FuriosaAI Software Stack on your host system or you can run the command on Kubernetes environment.
To run the `furiosa-mlperf` container, you can use the following command:
(Assumes model artifacts exist in `/opt/gpt-j-6b` directory)
``` $ docker run -it --rm --privileged \   -v /opt/gpt-j-6b:gpt-j-6b furiosa-mlperf:2024.1.0 bash
(container) # furiosa-mlperf gpt-j-offline /gpt-j-6b /gpt-j-offline-result
```
To run in a containerized environment, refer to the examples provided in this document.
Warning
The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit)
[previous
Quick Start with Furiosa LLM](furiosa_llm.html ""previous page"") [next
Furiosa LLM](../furiosa_llm/intro.html ""next page"")
Contents
* [Installing   `furiosa-mlperf`   command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1)   + [SYNOPSIS](#synopsis)   + [Examples](#examples) * [Running   `furiosa-mlperf`   in container environment](#running-furiosa-mlperf-in-container-environment)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","
* [.rst](../_sources/getting_started/furiosa_mlperf.rst ""소스 파일 다운로드"") * .pdf

MLPerf™ 추론 벤치마크 실행
==========================

목차
--------

* [`furiosa-mlperf` 명령 설치](#installing-furiosa-mlperf-command)
* [MLPerf 추론 벤치마크 실행](#id1)
  + [개요](#synopsis)
  + [예제](#examples)
* [컨테이너 환경에서 `furiosa-mlperf` 실행](#running-furiosa-mlperf-in-container-environment)

MLPerf™ 추론 벤치마크 실행 [#](#running-mlperf-inference-benchmark ""이 제목으로 링크"")
====================================================================================

MLPerf™는 머신 러닝(ML) 소프트웨어, 하드웨어 및 클라우드 플랫폼의 성능을 평가하는 벤치마크 모음입니다. 일반적으로 다양한 시스템의 성능을 비교하고, 개발자와 최종 사용자가 AI 시스템에 대한 결정을 내리는 데 도움을 줍니다.

FuriosaAI 소프트웨어 스택은 MLPerf 추론 벤치마크를 쉽게 실행할 수 있도록 `furiosa-mlperf` 명령을 제공합니다. 이 섹션에서는 FuriosaAI 소프트웨어 스택을 사용하여 MLPerf™ 추론 벤치마크를 재현하는 방법을 설명합니다.

참고: `furiosa-mlperf`는 MLPerf™ 추론 벤치마크 v4.1을 기반으로 합니다. 유일한 예외는 Llama2 벤치마크를 Llama 3.1을 사용하는 것으로 대체한 것입니다.

`furiosa-mlperf` 명령 설치 [#](#installing-furiosa-mlperf-command ""이 제목으로 링크"")
--------------------------------------------------------------------------------------

`furiosa-mlperf` 명령을 설치하려면 다음과 같이 `furiosa-mlperf`를 설치해야 합니다:

`furiosa-mlperf`의 최소 요구 사항은 다음과 같습니다:

* Ubuntu 20.04 LTS (Debian bullseye) 이상
* 루트 권한 또는 sudo 권한
* APT 서버 구성 및 장치 드라이버 설치 ([APT 서버 설정](prerequisites.html#aptsetup))
* 약 100GB의 저장 공간 (Llama 3.1 70B 전용)

그런 다음, 다음과 같이 `furiosa-mlperf` 패키지를 설치하세요:

```bash
sudo apt update
sudo apt install -y furiosa-mlperf
```

이 명령은 `furiosa-compiler`, `furiosa-mlperf` 및 `furiosa-mlperf-resources` 패키지를 설치합니다.

MLPerf 추론 벤치마크 실행 [#](#id1 ""이 제목으로 링크"")
-------------------------------------------------------

### 개요 [#](#synopsis ""이 제목으로 링크"")

`furiosa-mlperf` 명령은 다음과 같은 하위 명령을 제공합니다:

```plaintext
FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z)
Usage: furiosa-mlperf <COMMAND>
Commands:
  bert-offline       Run BERT benchmark with offline scenario
  bert-server        Run BERT benchmark with server scenario
  gpt-j-offline      Run GPT-J benchmark with offline scenario
  gpt-j-server       Run GPT-J benchmark with server scenario
  llama-3.1-offline  Run Llama 3.1 benchmark with offline scenario
  llama-3.1-server   Run Llama 3.1 benchmark with server scenario
  help               Print this message or the help of the given subcommand(s)

Options:
  -h, --help     Print help
  -V, --version  Print version
```

### 예제 [#](#examples ""이 제목으로 링크"")

* BERT 벤치마크
  BERT 벤치마크는 단일 RNGD로 실행됩니다.
  + 서버 시나리오
    BERT Large 서빙 추론 벤치마크를 실행하려면 다음 명령을 사용할 수 있습니다:
    ```bash
    furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh ""npu:0:*""
    ```
  + 오프라인 시나리오
    BERT Large 오프라인 추론 벤치마크를 실행하려면 다음 명령을 사용할 수 있습니다:
    ```bash
    furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh ""npu:0:*""
    ```

* GPT-J 벤치마크
  GPT-J 벤치마크는 단일 RNGD로 실행됩니다.
  + 서버 시나리오
    GPT-J 6B 서빙 추론 벤치마크를 실행하려면 다음 명령을 사용할 수 있습니다:
    ```bash
    furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result
    ```
  + 오프라인 시나리오
    GPT-J 6B 오프라인 추론 벤치마크를 실행하려면 다음 명령을 사용할 수 있습니다:
    ```bash
    furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result
    ```

* Llama 3.1 벤치마크
  Llama 3.1 벤치마크는 네 개의 RNGD로 실행됩니다.
  + 서버 시나리오
    Llama 3.1 70B 서빙 추론 벤치마크를 실행하려면 다음 명령을 사용할 수 있습니다:
    ```bash
    furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result
    ```
  + 오프라인 시나리오
    Llama 3.1 70B 오프라인 추론 벤치마크를 실행하려면 다음 명령을 사용할 수 있습니다:
    ```bash
    furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result
    ```

* 공통
  프로세스가 완료되면 지정된 결과 디렉토리에 파일로 결과를 기록합니다. 이 파일을 열어 결과 요약을 볼 수 있습니다.
  ```bash
  cat gpt-j-offline-result/mlperf_log_summary.txt
  ```

  ```plaintext
  ================================================
  MLPerf Results Summary
  ================================================
  SUT name : GPT-J SUT
  Scenario : Offline
  Mode     : PerformanceOnly
  Samples per second: 11.842
  Tokens per second (inferred): 817.095
  Result is : VALID
    Min duration satisfied : Yes
    Min queries satisfied : Yes
    Early stopping satisfied: Yes
  ```

컨테이너 환경에서 `furiosa-mlperf` 실행 [#](#running-furiosa-mlperf-in-container-environment ""이 제목으로 링크"")
----------------------------------------------------------------------------------------------------------------

FuriosaAI는 `furiosa-mlperf` 명령의 컨테이너화된 버전을 제공합니다. 컨테이너화된 버전을 사용하면 호스트 시스템에 FuriosaAI 소프트웨어 스택을 설치하지 않고도 `furiosa-mlperf` 명령을 실행할 수 있으며, Kubernetes 환경에서도 명령을 실행할 수 있습니다.

`furiosa-mlperf` 컨테이너를 실행하려면 다음 명령을 사용할 수 있습니다:
(모델 아티팩트가 `/opt/gpt-j-6b` 디렉토리에 존재한다고 가정합니다)

```bash
$ docker run -it --rm --privileged \
  -v /opt/gpt-j-6b:gpt-j-6b furiosa-mlperf:2024.1.0 bash

(container) # furiosa-mlperf gpt-j-offline /gpt-j-6b /gpt-j-offline-result
```

컨테이너화된 환경에서 실행하려면 이 문서에 제공된 예제를 참조하세요.

경고
위의 예제는 간단함을 위해 `--privileged` 옵션을 사용하지만, 보안상의 이유로 권장되지 않습니다. Kubernetes를 사용하는 경우, 권장 방법에 대해서는 다음 페이지를 참조하세요: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit)

[이전
Furiosa LLM 빠른 시작](furiosa_llm.html ""이전 페이지"") [다음
Furiosa LLM](../furiosa_llm/intro.html ""다음 페이지"")

목차
* [`furiosa-mlperf` 명령 설치](#installing-furiosa-mlperf-command)
* [MLPerf 추론 벤치마크 실행](#id1)
  + [개요](#synopsis)
  + [예제](#examples)
* [컨테이너 환경에서 `furiosa-mlperf` 실행](#running-furiosa-mlperf-in-container-environment)

By FuriosaAI, Inc.
© 저작권 2024, FuriosaAI, Inc."
a3d94379-304a-4dbc-8300-39169378bfd5,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/furiosa-llm-serve.html,"* [.rst](../_sources/furiosa_llm/furiosa-llm-serve.rst ""Download source file"") * .pdf
OpenAI Compatible Server ========================
Contents --------
* [Preparing Chat Templates](#preparing-chat-templates) * [Launching the Server](#launching-the-server)   + [Arguments for the serve command](#arguments-for-the-serve-command) * [Examples](#examples)   + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds)   + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api)
OpenAI Compatible Server [#](#openai-compatible-server ""Link to this heading"") ==============================================================================
The `furiosa-llm` package includes an OpenAI-compatible server that can interact with OpenAI clients (and, of course, HTTP clients as well), supporting the `/v1/chat/completions` and `/v1/completions` APIs. In this section, we will explain how to launch the OpenAI-compatible furiosa-llm
server.
Tip
You can learn more about the OpenAI API at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) .
To launch the server, you must prepare: (1) the FuriosaAI LLM Engine artifact and (2) a chat template for the model. To download the FuriosaAI LLM Engine artifact, follow the instructions provided through our distribution channels or contact our sales team. To prepare the chat template, follow the instructions in the following section.
Preparing Chat Templates [#](#preparing-chat-templates ""Link to this heading"") ------------------------------------------------------------------------------
Furiosa SDK 2024.1.0 (alpha) uses Transformers v4.31.0, which does not include a chat template. Therefore, to support `/v1/chat/completions` , you must provide a chat template yourself. This constraint will be removed in future releases.
Warning
This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future.
If you have access to the Llama repositories on Hugging Face, you can obtain the chat template for Llama using the following commands:
``` # Prerequisite: create a separate environment to install the latest Transformers version pip install ""transformers>=4.34.0"" python - <<EOF from transformers import AutoTokenizer tok = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-70B-Instruct') with open('chat_template.tpl', 'w') as f:     f.write(tok.chat_template) EOF
```
Launching the Server [#](#launching-the-server ""Link to this heading"") ----------------------------------------------------------------------
You can launch the server using the furiosa-llm serve
command.
### Arguments for the serve command [#](#arguments-for-the-serve-command ""Link to this heading"")
``` usage: furiosa-llm serve [-h] --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} --artifact ARTIFACT [--host HOST] [--port PORT]                      --chat-template CHAT_TEMPLATE [--response-role RESPONSE_ROLE] [-pp PIPELINE_PARALLEL_SIZE] [-tp TENSOR_PARALLEL_SIZE] [--devices DEVICES]
options: -h, --help            show this help message and exit --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm}                         The model to use. Currently only one model is supported per server. --artifact ARTIFACT   Path to Furiosa LLM Engine artifact --host HOST           Host to bind the server to --port PORT           Port to bind the server to --chat-template CHAT_TEMPLATE                         Path to chat template file (must be a jinja2 template) --response-role RESPONSE_ROLE                         Response role for /v1/chat/completions API (default: 'assistant') -pp PIPELINE_PARALLEL_SIZE, --pipeline-parallel-size PIPELINE_PARALLEL_SIZE                         Number of pipeline stages. -tp TENSOR_PARALLEL_SIZE, --tensor-parallel-size TENSOR_PARALLEL_SIZE                         Number of tensor parallel replicas. --devices DEVICES     Devices to use (e.g. ""npu:0:*,npu:1:*""). If unspecified, all available devices from the host will be used.
```
Examples [#](#examples ""Link to this heading"") ----------------------------------------------
### LLaMA-3.1-70B with 4 RNGDs [#](#llama-3-1-70b-with-4-rngds ""Link to this heading"")
``` furiosa-llm serve \ --model {path to mlperf-llama-3-1-fp8-pp4} \ -tp 4 -pp 4 --devices ""npu:0:*,npu:1:*,npu:2:*,npu:3:*"" \ --chat-template {path to chat template}
```
### LLaMA-3.1-8B with Single RNGD [#](#llama-3-1-8b-with-single-rngd ""Link to this heading"")
``` furiosa-llm serve \ --model {path to mlperf-llama-3-1-8b-fp8} \ -tp 4 -pp 1 --devices ""npu:0:*"" \  # You may choose arbitrary device index, if multiple devices are on host --chat-template {path to chat template}
```
Using OpenAI Client [#](#using-openai-client ""Link to this heading"") --------------------------------------------------------------------
You can use two APIs: `client.chat.completions` and `client.completions` . You can also set `stream=True` to receive a streaming response.
Tip
You can install the OpenAI Python client using the following command:
``` pip install openai
```
``` import openai
HOST = ""localhost:8000"" openai.api_base = f""http://{HOST}/v1"" openai.api_key = ""0000""
stream_chat_completion = openai.ChatCompletion.create(     model="""",     messages=[         {""role"": ""system"", ""content"": ""You are a helpful assistant.""},         {""role"": ""user"", ""content"": ""What is the largest animal in the world?""},     ],     stream=True, )
for completion in stream_chat_completion:     content = completion.choices[0].delta.get(""content"")     if content:         print(content, end="""")
```
The compatibility with OpenAI API [#](#the-compatibility-with-openai-api ""Link to this heading"") ------------------------------------------------------------------------------------------------
Currently, `furiosa
serve` supports the following OpenAI API parameters: You can find more about each parameter at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) .
Warning
Please note that using `use_beam_search` with `stream` is not allowed because the beam search cannot determine the tokens until the end of the sequence.
In 2024.1 release, `n` works only for beam search and it will be fixed in the next release.
* `n` * `temperature` * `top_p` * `top_k` * `early_stopping` * `length_penalty` * `max_tokens` * `min_tokens` * `use_beam_search` * `best_of` 
[previous
Furiosa LLM](intro.html ""previous page"") [next
References](references.html ""next page"")
Contents
* [Preparing Chat Templates](#preparing-chat-templates) * [Launching the Server](#launching-the-server)   + [Arguments for the serve command](#arguments-for-the-serve-command) * [Examples](#examples)   + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds)   + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../_sources/furiosa_llm/furiosa-llm-serve.rst ""소스 파일 다운로드"") * .pdf

OpenAI 호환 서버
========================

목차
--------
* [채팅 템플릿 준비하기](#preparing-chat-templates)
* [서버 실행하기](#launching-the-server)
  + [serve 명령어의 인자](#arguments-for-the-serve-command)
* [예제](#examples)
  + [LLaMA-3.1-70B와 4 RNGDs](#llama-3-1-70b-with-4-rngds)
  + [LLaMA-3.1-8B와 단일 RNGD](#llama-3-1-8b-with-single-rngd)
* [OpenAI 클라이언트 사용하기](#using-openai-client)
* [OpenAI API와의 호환성](#the-compatibility-with-openai-api)

OpenAI 호환 서버 [#](#openai-compatible-server ""이 제목으로 링크"")
==============================================================================
`furiosa-llm` 패키지는 OpenAI 클라이언트(물론 HTTP 클라이언트도 포함)와 상호작용할 수 있는 OpenAI 호환 서버를 포함하고 있으며, `/v1/chat/completions` 및 `/v1/completions` API를 지원합니다. 이 섹션에서는 OpenAI 호환 furiosa-llm 서버를 실행하는 방법을 설명합니다.

팁
OpenAI API에 대한 자세한 내용은 [Completions API](https://platform.openai.com/docs/api-reference/completions) 및 [Chat API](https://platform.openai.com/docs/api-reference/chat)에서 확인할 수 있습니다.

서버를 실행하려면 (1) FuriosaAI LLM 엔진 아티팩트와 (2) 모델을 위한 채팅 템플릿을 준비해야 합니다. FuriosaAI LLM 엔진 아티팩트를 다운로드하려면 배포 채널을 통해 제공된 지침을 따르거나 영업팀에 문의하십시오. 채팅 템플릿을 준비하려면 다음 섹션의 지침을 따르십시오.

채팅 템플릿 준비하기 [#](#preparing-chat-templates ""이 제목으로 링크"")
------------------------------------------------------------------------------
Furiosa SDK 2024.1.0 (알파)는 Transformers v4.31.0을 사용하며, 채팅 템플릿을 포함하지 않습니다. 따라서 `/v1/chat/completions`을 지원하려면 직접 채팅 템플릿을 제공해야 합니다. 이 제약은 향후 릴리스에서 제거될 예정입니다.

경고
이 문서는 Furiosa SDK 2024.1.0 (알파) 버전을 기반으로 하며, 이 문서에 설명된 기능과 API는 향후 변경될 수 있습니다.

Hugging Face의 Llama 저장소에 접근할 수 있는 경우, 다음 명령어를 사용하여 Llama의 채팅 템플릿을 얻을 수 있습니다:

```bash
# 전제 조건: 최신 Transformers 버전을 설치하기 위한 별도의 환경 생성
pip install ""transformers>=4.34.0""
python - <<EOF
from transformers import AutoTokenizer
tok = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-70B-Instruct')
with open('chat_template.tpl', 'w') as f:
    f.write(tok.chat_template)
EOF
```

서버 실행하기 [#](#launching-the-server ""이 제목으로 링크"")
----------------------------------------------------------------------
`furiosa-llm serve` 명령어를 사용하여 서버를 실행할 수 있습니다.

### serve 명령어의 인자 [#](#arguments-for-the-serve-command ""이 제목으로 링크"")

```bash
usage: furiosa-llm serve [-h] --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} --artifact ARTIFACT [--host HOST] [--port PORT]
                     --chat-template CHAT_TEMPLATE [--response-role RESPONSE_ROLE] [-pp PIPELINE_PARALLEL_SIZE] [-tp TENSOR_PARALLEL_SIZE] [--devices DEVICES]

options:
  -h, --help            show this help message and exit
  --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm}
                        사용할 모델. 현재 서버당 하나의 모델만 지원됩니다.
  --artifact ARTIFACT   Furiosa LLM 엔진 아티팩트의 경로
  --host HOST           서버를 바인딩할 호스트
  --port PORT           서버를 바인딩할 포트
  --chat-template CHAT_TEMPLATE
                        채팅 템플릿 파일의 경로 (jinja2 템플릿이어야 함)
  --response-role RESPONSE_ROLE
                        /v1/chat/completions API의 응답 역할 (기본값: 'assistant')
  -pp PIPELINE_PARALLEL_SIZE, --pipeline-parallel-size PIPELINE_PARALLEL_SIZE
                        파이프라인 단계 수.
  -tp TENSOR_PARALLEL_SIZE, --tensor-parallel-size TENSOR_PARALLEL_SIZE
                        텐서 병렬 복제본 수.
  --devices DEVICES     사용할 장치 (예: ""npu:0:*,npu:1:*""). 지정하지 않으면 호스트의 모든 사용 가능한 장치가 사용됩니다.
```

예제 [#](#examples ""이 제목으로 링크"")
----------------------------------------------
### LLaMA-3.1-70B와 4 RNGDs [#](#llama-3-1-70b-with-4-rngds ""이 제목으로 링크"")

```bash
furiosa-llm serve \
  --model {path to mlperf-llama-3-1-fp8-pp4} \
  -tp 4 -pp 4 --devices ""npu:0:*,npu:1:*,npu:2:*,npu:3:*"" \
  --chat-template {path to chat template}
```

### LLaMA-3.1-8B와 단일 RNGD [#](#llama-3-1-8b-with-single-rngd ""이 제목으로 링크"")

```bash
furiosa-llm serve \
  --model {path to mlperf-llama-3-1-8b-fp8} \
  -tp 4 -pp 1 --devices ""npu:0:*"" \  # 호스트에 여러 장치가 있는 경우 임의의 장치 인덱스를 선택할 수 있습니다.
  --chat-template {path to chat template}
```

OpenAI 클라이언트 사용하기 [#](#using-openai-client ""이 제목으로 링크"")
--------------------------------------------------------------------
두 가지 API를 사용할 수 있습니다: `client.chat.completions` 및 `client.completions`. 또한 `stream=True`를 설정하여 스트리밍 응답을 받을 수 있습니다.

팁
다음 명령어를 사용하여 OpenAI Python 클라이언트를 설치할 수 있습니다:

```bash
pip install openai
```

```python
import openai
HOST = ""localhost:8000""
openai.api_base = f""http://{HOST}/v1""
openai.api_key = ""0000""

stream_chat_completion = openai.ChatCompletion.create(
    model="""",
    messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""What is the largest animal in the world?""},
    ],
    stream=True,
)

for completion in stream_chat_completion:
    content = completion.choices[0].delta.get(""content"")
    if content:
        print(content, end="""")
```

OpenAI API와의 호환성 [#](#the-compatibility-with-openai-api ""이 제목으로 링크"")
------------------------------------------------------------------------------------------------
현재 `furiosa serve`는 다음 OpenAI API 매개변수를 지원합니다: 각 매개변수에 대한 자세한 내용은 [Completions API](https://platform.openai.com/docs/api-reference/completions) 및 [Chat API](https://platform.openai.com/docs/api-reference/chat)에서 확인할 수 있습니다.

경고
`stream`과 함께 `use_beam_search`를 사용하는 것은 허용되지 않습니다. 빔 검색은 시퀀스의 끝까지 토큰을 결정할 수 없기 때문입니다. 2024.1 릴리스에서는 `n`이 빔 검색에만 작동하며, 이는 다음 릴리스에서 수정될 예정입니다.

* `n`
* `temperature`
* `top_p`
* `top_k`
* `early_stopping`
* `length_penalty`
* `max_tokens`
* `min_tokens`
* `use_beam_search`
* `best_of`

[이전 Furiosa LLM](intro.html ""이전 페이지"") [다음 참고 문헌](references.html ""다음 페이지"")

목차
* [채팅 템플릿 준비하기](#preparing-chat-templates)
* [서버 실행하기](#launching-the-server)
  + [serve 명령어의 인자](#arguments-for-the-serve-command)
* [예제](#examples)
  + [LLaMA-3.1-70B와 4 RNGDs](#llama-3-1-70b-with-4-rngds)
  + [LLaMA-3.1-8B와 단일 RNGD](#llama-3-1-8b-with-single-rngd)
* [OpenAI 클라이언트 사용하기](#using-openai-client)
* [OpenAI API와의 호환성](#the-compatibility-with-openai-api)

By FuriosaAI, Inc.
© 저작권 2024, FuriosaAI, Inc.
```"
13853744-dc18-4c98-b1c2-4f5806b28514,https://furiosa-ai.github.io/docs-dev/2024.1/en/,"* [.rst](_sources/index.rst ""Download source file"") * .pdf
FuriosaAI Developer Center ==========================
Contents --------
* [Overview](#overview) * [Getting Started](#getting-started) * [Cloud Native Toolkit](#cloud-native-toolkit) * [Device Management](#device-management) * [Customer Support](#customer-support)
FuriosaAI Developer Center [#](#furiosaai-developer-center ""Link to this heading"") ==================================================================================
Welcome to the FuriosaAI Developer Center. FuriosaAI provides an streamlined software stack for deep learning model inference on FuriosaAI NPUs. This document provides a guide to easily perform the entire workflow of writing inference applications, from starting with PyTorch model to model quantization, serving, and production deployment.
Warning
This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future.
Overview [#](#overview ""Link to this heading"") ----------------------------------------------
* [FuriosaAI RNGD](overview/rngd.html#rngd)   : RNGD Hardware Specification, and features * [FuriosaAI’s Software Stack](overview/software_stack.html#softwarestack)   : An overview of the FuriosaAI software stack * [Supported Models](overview/supported_models.html#supportedmodels)   : A list of supported models * [What’s New](whatsnew/index.html#whatsnew)   : New features and changes in the latest release * [Roadmap](overview/roadmap.html#roadmap)   : The future roadmap of FuriosaAI Software Stack
Getting Started [#](#getting-started ""Link to this heading"") ------------------------------------------------------------
* [Installing Prerequisites](getting_started/prerequisites.html#installingprerequisites)   : How to install the prerequisites for FuriosaAI Software Stack * [Quick Start with Furiosa LLM](getting_started/furiosa_llm.html#gettingstartedfuriosallm) * [Running MLPerf™ Inference Benchmark](getting_started/furiosa_mlperf.html#gettingstartedfuriosamlperf)
Cloud Native Toolkit [#](#cloud-native-toolkit ""Link to this heading"") ----------------------------------------------------------------------
* [Cloud Native Toolkit](cloud_native_toolkit/intro.html#cloudnativetoolkit)   : An overview of the Cloud Native Toolkit * [Kubernetes Support](cloud_native_toolkit/kubernetes.html#kubernetes)   : An overview of the Kubernetes Support
Device Management [#](#device-management ""Link to this heading"") ----------------------------------------------------------------
* [furiosa-smi](device_management/furiosa_smi.html#furiosasmi)   : A command line utility for managing FuriosaAI NPUs
Customer Support [#](#customer-support ""Link to this heading"") --------------------------------------------------------------
* [FuriosaAI Homepage](https://furiosa.ai) * [FuriosaAI Forum](https://furiosa-ai.discourse.group/) * [FuriosaAI Customer Portal](https://furiosa-ai.atlassian.net/servicedesk/customer/portals/) * [FuriosaAI Warboy SDK Document](https://furiosa-ai.github.io/docs/latest/en/)
[next
FuriosaAI RNGD](overview/rngd.html ""next page"")
Contents
* [Overview](#overview) * [Getting Started](#getting-started) * [Cloud Native Toolkit](#cloud-native-toolkit) * [Device Management](#device-management) * [Customer Support](#customer-support)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](_sources/index.rst ""소스 파일 다운로드"") * .pdf

FuriosaAI 개발자 센터
==========================

목차
--------

* [개요](#overview) 
* [시작하기](#getting-started) 
* [클라우드 네이티브 툴킷](#cloud-native-toolkit) 
* [디바이스 관리](#device-management) 
* [고객 지원](#customer-support)

FuriosaAI 개발자 센터 [#](#furiosaai-developer-center ""이 제목으로 링크"")
==================================================================================

FuriosaAI 개발자 센터에 오신 것을 환영합니다. FuriosaAI는 FuriosaAI NPU에서 딥러닝 모델 추론을 위한 간소화된 소프트웨어 스택을 제공합니다. 이 문서는 PyTorch 모델에서 시작하여 모델 양자화, 서비스, 그리고 프로덕션 배포까지의 전체 워크플로우를 쉽게 수행할 수 있는 가이드를 제공합니다.

경고
이 문서는 Furiosa SDK 2024.1.0 (알파) 버전을 기반으로 하며, 이 문서에 설명된 기능과 API는 향후 변경될 수 있습니다.

개요 [#](#overview ""이 제목으로 링크"")
----------------------------------------------

* [FuriosaAI RNGD](overview/rngd.html#rngd)   : RNGD 하드웨어 사양 및 기능
* [FuriosaAI의 소프트웨어 스택](overview/software_stack.html#softwarestack)   : FuriosaAI 소프트웨어 스택 개요
* [지원되는 모델](overview/supported_models.html#supportedmodels)   : 지원되는 모델 목록
* [새로운 기능](whatsnew/index.html#whatsnew)   : 최신 릴리스의 새로운 기능 및 변경 사항
* [로드맵](overview/roadmap.html#roadmap)   : FuriosaAI 소프트웨어 스택의 미래 로드맵

시작하기 [#](#getting-started ""이 제목으로 링크"")
------------------------------------------------------------

* [필수 조건 설치](getting_started/prerequisites.html#installingprerequisites)   : FuriosaAI 소프트웨어 스택을 위한 필수 조건 설치 방법
* [Furiosa LLM 빠른 시작](getting_started/furiosa_llm.html#gettingstartedfuriosallm)
* [MLPerf™ 추론 벤치마크 실행](getting_started/furiosa_mlperf.html#gettingstartedfuriosamlperf)

클라우드 네이티브 툴킷 [#](#cloud-native-toolkit ""이 제목으로 링크"")
----------------------------------------------------------------------

* [클라우드 네이티브 툴킷](cloud_native_toolkit/intro.html#cloudnativetoolkit)   : 클라우드 네이티브 툴킷 개요
* [Kubernetes 지원](cloud_native_toolkit/kubernetes.html#kubernetes)   : Kubernetes 지원 개요

디바이스 관리 [#](#device-management ""이 제목으로 링크"")
----------------------------------------------------------------

* [furiosa-smi](device_management/furiosa_smi.html#furiosasmi)   : FuriosaAI NPU 관리를 위한 명령줄 유틸리티

고객 지원 [#](#customer-support ""이 제목으로 링크"")
--------------------------------------------------------------

* [FuriosaAI 홈페이지](https://furiosa.ai)
* [FuriosaAI 포럼](https://furiosa-ai.discourse.group/)
* [FuriosaAI 고객 포털](https://furiosa-ai.atlassian.net/servicedesk/customer/portals/)
* [FuriosaAI Warboy SDK 문서](https://furiosa-ai.github.io/docs/latest/en/)

[next FuriosaAI RNGD](overview/rngd.html ""다음 페이지"")

목차
* [개요](#overview) 
* [시작하기](#getting-started) 
* [클라우드 네이티브 툴킷](#cloud-native-toolkit) 
* [디바이스 관리](#device-management) 
* [고객 지원](#customer-support)

By FuriosaAI, Inc.
© 저작권 2024, FuriosaAI, Inc.
```"
2b294227-f255-492c-b642-46e100e59705,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes.html,"* [.rst](../_sources/cloud_native_toolkit/kubernetes.rst ""Download source file"") * .pdf
Kubernetes Support ==================
Kubernetes Support [#](#kubernetes-support ""Link to this heading"") ==================================================================
We do support the following versions of Kubernetes and CRI runtime:
* Kubernetes: v1.24.0 or later * helm v3.0.0 or later * CRI Runtime:   [containerd](https://github.com/containerd/containerd)   or   [CRI-O](https://github.com/cri-o/cri-o)
Note
Docker is officially deprecated as a container runtime in Kubernetes. It is recommended to use containerd or CRI-O as a container runtime. Otherwise you may face unexpected issues with the device plugin. For more information, see [here](https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/) .
Kubernetes Support
* [Installing Furiosa Feature Discovery](kubernetes/feature_discovery.html) * [Installing Furiosa Device Plugin](kubernetes/device_plugin.html) * [Installing Furiosa Metrics Exporter](kubernetes/metrics_exporter.html) * [Scheduling NPUs](kubernetes/scheduling_npus.html)
[previous
Cloud Native Toolkit](intro.html ""previous page"") [next
Installing Furiosa Feature Discovery](kubernetes/feature_discovery.html ""next page"")
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../_sources/cloud_native_toolkit/kubernetes.rst ""소스 파일 다운로드"") * .pdf

Kubernetes 지원
==================

Kubernetes 지원 [#](#kubernetes-support ""이 제목으로 링크"")
==================================================================

저희는 다음 버전의 Kubernetes와 CRI 런타임을 지원합니다:

* Kubernetes: v1.24.0 이상
* helm v3.0.0 이상
* CRI 런타임: [containerd](https://github.com/containerd/containerd) 또는 [CRI-O](https://github.com/cri-o/cri-o)

참고
Kubernetes에서 Docker는 공식적으로 컨테이너 런타임으로 더 이상 지원되지 않습니다. 컨테이너 런타임으로 containerd 또는 CRI-O를 사용하는 것이 권장됩니다. 그렇지 않으면 디바이스 플러그인과 관련된 예기치 않은 문제가 발생할 수 있습니다. 자세한 내용은 [여기](https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/)를 참조하세요.

Kubernetes 지원

* [Furiosa Feature Discovery 설치](kubernetes/feature_discovery.html)
* [Furiosa Device Plugin 설치](kubernetes/device_plugin.html)
* [Furiosa Metrics Exporter 설치](kubernetes/metrics_exporter.html)
* [NPU 스케줄링](kubernetes/scheduling_npus.html)

[이전
클라우드 네이티브 툴킷](intro.html ""이전 페이지"") [다음
Furiosa Feature Discovery 설치](kubernetes/feature_discovery.html ""다음 페이지"")

FuriosaAI, Inc. 제공
© 저작권 2024, FuriosaAI, Inc.
```"
d4927eaf-a1a0-40bd-9624-79db4213c5fc,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/roadmap.html,"* [.rst](../_sources/overview/roadmap.rst ""Download source file"") * .pdf
Roadmap =======
Contents --------
* [Latest Recent Release](#latest-recent-release) * [Future Releases](#future-releases)   + [2024.2.0 (beta 0) - November, 2024](#beta-0-november-2024)   + [2024.3.0 (beta 1) - December, 2024](#beta-1-december-2024)
Roadmap [#](#roadmap ""Link to this heading"") ============================================
FurisaAI strives to deliver the releases for each month, while offering patch releases. This page shows the forward-looking roadmap of ongoing & upcoming projects and when they are expected to land, broken down by areas on [our software stack](software_stack.html#softwarestack) .
Latest Recent Release [#](#latest-recent-release ""Link to this heading"") ------------------------------------------------------------------------
The latest release is 2024.1.0 (alpha) on October 11, 2024. You can find the release notes [here](../whatsnew/index.html#whatsnew) .
Future Releases [#](#future-releases ""Link to this heading"") ------------------------------------------------------------
Note
The roadmap is subject to change and may not reflect the final product.
### 2024.2.0 (beta 0) - November, 2024 [#](#beta-0-november-2024 ""Link to this heading"")
* Model support:      + Language Models: CodeLLaaMA2, Vicuna, Solar, EXAONE-3.0   + Vision Models: MobileNetV1, MobileNetV2, ResNet152, ResNet50, EfficientNet, YOLOv8m, .. * (Furiosa LLM) Tensor Parallelism support Phase 1: Intra-chip * Torch 2.4.1 support * CPU memory swapping in Furiosa LLM
### 2024.3.0 (beta 1) - December, 2024 [#](#beta-1-december-2024 ""Link to this heading"")
* Model support: TBD * (Furiosa LLM) Tensor Parallelism support Phase 2: Inter-chip * `torch.compile()`   backend * Huggingface Optimum integration
[previous
What’s New](../whatsnew/index.html ""previous page"") [next
Installing Prerequisites](../getting_started/prerequisites.html ""next page"")
Contents
* [Latest Recent Release](#latest-recent-release) * [Future Releases](#future-releases)   + [2024.2.0 (beta 0) - November, 2024](#beta-0-november-2024)   + [2024.3.0 (beta 1) - December, 2024](#beta-1-december-2024)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../_sources/overview/roadmap.rst ""소스 파일 다운로드"") * .pdf

로드맵
======

목차
--------

* [최신 릴리스](#latest-recent-release)
* [향후 릴리스](#future-releases)
  + [2024.2.0 (beta 0) - 2024년 11월](#beta-0-november-2024)
  + [2024.3.0 (beta 1) - 2024년 12월](#beta-1-december-2024)

로드맵 [#](#roadmap ""이 제목으로 링크"")
============================================

FurisaAI는 매달 릴리스를 제공하고 패치 릴리스를 제공합니다. 이 페이지는 진행 중인 프로젝트와 다가오는 프로젝트의 로드맵을 보여주며, [소프트웨어 스택](software_stack.html#softwarestack) 영역별로 언제 출시될지 예상됩니다.

최신 릴리스 [#](#latest-recent-release ""이 제목으로 링크"")
------------------------------------------------------------------------

최신 릴리스는 2024년 10월 11일에 출시된 2024.1.0 (alpha)입니다. 릴리스 노트는 [여기](../whatsnew/index.html#whatsnew)에서 확인할 수 있습니다.

향후 릴리스 [#](#future-releases ""이 제목으로 링크"")
------------------------------------------------------------

참고
로드맵은 변경될 수 있으며 최종 제품을 반영하지 않을 수 있습니다.

### 2024.2.0 (beta 0) - 2024년 11월 [#](#beta-0-november-2024 ""이 제목으로 링크"")

* 모델 지원:
  + 언어 모델: CodeLLaaMA2, Vicuna, Solar, EXAONE-3.0
  + 비전 모델: MobileNetV1, MobileNetV2, ResNet152, ResNet50, EfficientNet, YOLOv8m, ..
* (Furiosa LLM) Tensor 병렬 처리 지원 1단계: 칩 내
* Torch 2.4.1 지원
* Furiosa LLM에서 CPU 메모리 스와핑

### 2024.3.0 (beta 1) - 2024년 12월 [#](#beta-1-december-2024 ""이 제목으로 링크"")

* 모델 지원: TBD
* (Furiosa LLM) Tensor 병렬 처리 지원 2단계: 칩 간
* `torch.compile()` 백엔드
* Huggingface Optimum 통합

[이전
새로운 소식](../whatsnew/index.html ""이전 페이지"") [다음
필수 조건 설치](../getting_started/prerequisites.html ""다음 페이지"")

목차
* [최신 릴리스](#latest-recent-release)
* [향후 릴리스](#future-releases)
  + [2024.2.0 (beta 0) - 2024년 11월](#beta-0-november-2024)
  + [2024.3.0 (beta 1) - 2024년 12월](#beta-1-december-2024)

By FuriosaAI, Inc.
© 저작권 2024, FuriosaAI, Inc.
```"
4f19d9ef-0eb7-40bf-a693-6e2d80ddb54a,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/supported_models.html,"* [.rst](../_sources/overview/supported_models.rst ""Download source file"") * .pdf
Supported Models ================
Contents --------
* [Decoder-only Models](#decoder-only-models) * [Encoder-only Models](#encoder-only-models)
Supported Models [#](#supported-models ""Link to this heading"") ==============================================================
FuriosaAI Software Stack supports a variety of Transformer-based models in HuggingFace Hub. The following is the list of model architectures that are currently supported by Furiosa SDK. If your model is based on the following architectures, you can use Furiosa SDK to compile, quantize, and run the model on FuriosaAI RNGD.
Decoder-only Models [#](#decoder-only-models ""Link to this heading"") --------------------------------------------------------------------
The following models are supported for decoding only:
Decoder-only Models
[#](#id1 ""Link to this table"")
| Architecture | Model Name | Example HuggingFace Models | | --- | --- | --- | | `LlamaForCausalLM` | Llama 2, Llama 3.1 | `meta-llama/Llama-2-70b-hf` , `meta-llama/Meta-Llama-3.1-70B` , `meta-llama/Meta-Llama-3-70B-Instruct` , `meta-llama/Meta-Llama-3.1-8B` , `meta-llama/Llama-3.1-8B-Instruct` , .. | | `GPTJForCausalLM` | GPT-J | `EleutherAI/gpt-j-6b` |
Encoder-only Models [#](#encoder-only-models ""Link to this heading"") --------------------------------------------------------------------
Encoder-only Models
[#](#id2 ""Link to this table"")
| Architecture | Model Name | Example HuggingFace Models | | --- | --- | --- | | `BertForQuestionAnswering` | Bert | `google-bert/bert-large-uncased` , `google-bert/bert-base-uncased` , .. |
[previous
FuriosaAI’s Software Stack](software_stack.html ""previous page"") [next
What’s New](../whatsnew/index.html ""next page"")
Contents
* [Decoder-only Models](#decoder-only-models) * [Encoder-only Models](#encoder-only-models)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../_sources/overview/supported_models.rst ""소스 파일 다운로드"") * .pdf

지원 모델
===============

목차
--------
* [디코더 전용 모델](#decoder-only-models) 
* [인코더 전용 모델](#encoder-only-models)

지원 모델 [#](#supported-models ""이 제목으로 링크"")
==============================================================

FuriosaAI 소프트웨어 스택은 HuggingFace Hub의 다양한 Transformer 기반 모델을 지원합니다. 다음은 Furiosa SDK에서 현재 지원하는 모델 아키텍처 목록입니다. 아래 아키텍처를 기반으로 한 모델이라면 Furiosa SDK를 사용하여 모델을 컴파일, 양자화 및 FuriosaAI RNGD에서 실행할 수 있습니다.

디코더 전용 모델 [#](#decoder-only-models ""이 제목으로 링크"")
--------------------------------------------------------------------

다음 모델은 디코딩 전용으로 지원됩니다:

디코더 전용 모델
[#](#id1 ""이 표로 링크"")

| 아키텍처 | 모델 이름 | 예시 HuggingFace 모델 | 
| --- | --- | --- | 
| `LlamaForCausalLM` | Llama 2, Llama 3.1 | `meta-llama/Llama-2-70b-hf`, `meta-llama/Meta-Llama-3.1-70B`, `meta-llama/Meta-Llama-3-70B-Instruct`, `meta-llama/Meta-Llama-3.1-8B`, `meta-llama/Llama-3.1-8B-Instruct`, .. | 
| `GPTJForCausalLM` | GPT-J | `EleutherAI/gpt-j-6b` |

인코더 전용 모델 [#](#encoder-only-models ""이 제목으로 링크"")
--------------------------------------------------------------------

인코더 전용 모델
[#](#id2 ""이 표로 링크"")

| 아키텍처 | 모델 이름 | 예시 HuggingFace 모델 | 
| --- | --- | --- | 
| `BertForQuestionAnswering` | Bert | `google-bert/bert-large-uncased`, `google-bert/bert-base-uncased`, .. |

[이전
FuriosaAI의 소프트웨어 스택](software_stack.html ""이전 페이지"") [다음
새로운 기능](../whatsnew/index.html ""다음 페이지"")

목차
* [디코더 전용 모델](#decoder-only-models) 
* [인코더 전용 모델](#encoder-only-models)

By FuriosaAI, Inc.
© 저작권 2024, FuriosaAI, Inc.
```"
0acdfa06-7dff-4603-9a5c-dbc4e3310580,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/software_stack.html,"* [.rst](../_sources/overview/software_stack.rst ""Download source file"") * .pdf
FuriosaAI’s Software Stack ==========================
Contents --------
* [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support)
FuriosaAI’s Software Stack [#](#furiosaai-s-software-stack ""Link to this heading"") ==================================================================================
FuriosaAI provides the streamlined software stack to allow FuriosaAI NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials. The above diagram demonstrates the SW stack provided by FuriosaAI, by layers.
The following outlines the key components.
Kernel Driver, Firmware, and PE Runtime [#](#kernel-driver-firmware-and-pe-runtime ""Link to this heading"") ----------------------------------------------------------------------------------------------------------
The kernel device driver enables the Linux operating system to recognize NPU devices and expose them as Linux device files. The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE). PERT is responsible for communicating with the host’s runtime and scheduling, managing the resources of PEs to execute NPU tasks.
Furiosa Compiler [#](#furiosa-compiler ""Link to this heading"") --------------------------------------------------------------
Furiosa compiler analyzes, optimizes a model graph, and generates a NPU executable program. The operation passes involve graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. When `torch.compile()` backend, `FuriosaBackend` is used or `furiosa-llm` is used, the Furiosa Compiler is transparently used to generate NPU executable programs for Runtime.
Furiosa Runtime [#](#furiosa-runtime ""Link to this heading"") ------------------------------------------------------------
Runtime loads multiple executable NPU programs generated by Furiosa compiler, and run them on the NPU. A single model can be compiled into multiple executable programs according to model architectures and applications. Runtime is responsible for scheduling NPU programs and managing computation and memory resource on NPUs and CPUs. Also, Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs.
Furiosa Model Compressor (Quantizer) [#](#furiosa-model-compressor-quantizer ""Link to this heading"") ----------------------------------------------------------------------------------------------------
Furiosa Model Compressor is a library as well as toolkit for model calibration and quantization. Model quantization is a powerful technique to reduce memory footprint, computation cost, inference latency and power consumption. Furiosa Model Compressor provides post-training quantization methods, such as
* BF16 (W16A16) * INT8 Weight-Only (W8A16) * FP8 (W8A8) * INT8 SmoothQuant (W8A8) * INT4 Weight-Only (W4A16 AWQ / GPTQ) (Planned in release 2024.2)
Furiosa LLM [#](#furiosa-llm ""Link to this heading"") ----------------------------------------------------
Furiosa LLM provides a high-performance inference engine for LLM models, such as Llama 3.1 70B, 8B, GPT-J, and Bert. Furiosa LLM is designed to provide the state-of-the-art serving optimization for LLM models. The key features of Furiosa LLM include vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and OpenAI-compatible API server. You can find further information at [Furiosa LLM](../furiosa_llm/intro.html#furiosallm) .
Kubernetes Support [#](#kubernetes-support ""Link to this heading"") ------------------------------------------------------------------
Kubernetes, an open-source platform designed to manage containerized applications and services, is extensively adopted by various companies due to its robust capabilities for deploying, scaling, and automating containerized workloads. FuriosaAI software stack also offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment.
FuriosaAI’s device plugin enables Kubernetes clusters to recognize FuriosaAI’s NPUs and allows NPUs to be scheduled for workloads and services that require them. This feature allows users to easily deploy AI workloads with FuriosaAI NPUs on Kubernetes, enabling efficient resource utilization and scaling.
You can find more information about Kubernetes support in the [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) .
[previous
FuriosaAI RNGD](rngd.html ""previous page"") [next
Supported Models](supported_models.html ""next page"")
Contents
* [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../_sources/overview/software_stack.rst ""소스 파일 다운로드"") * .pdf

FuriosaAI의 소프트웨어 스택
==========================

목차
--------

* [커널 드라이버, 펌웨어 및 PE 런타임](#kernel-driver-firmware-and-pe-runtime)
* [Furiosa 컴파일러](#furiosa-compiler)
* [Furiosa 런타임](#furiosa-runtime)
* [Furiosa 모델 압축기 (양자화기)](#furiosa-model-compressor-quantizer)
* [Furiosa LLM](#furiosa-llm)
* [Kubernetes 지원](#kubernetes-support)

FuriosaAI의 소프트웨어 스택 [#](#furiosaai-s-software-stack ""이 제목으로 링크"")
==================================================================================

FuriosaAI는 다양한 애플리케이션과 환경에서 FuriosaAI NPU를 사용할 수 있도록 간소화된 소프트웨어 스택을 제공합니다. 여기서는 FuriosaAI가 제공하는 SW 스택을 계층별로 설명하며, 각 구성 요소의 역할과 지침 및 튜토리얼을 설명합니다. 위의 다이어그램은 FuriosaAI가 제공하는 SW 스택을 계층별로 보여줍니다.

다음은 주요 구성 요소를 설명합니다.

커널 드라이버, 펌웨어 및 PE 런타임 [#](#kernel-driver-firmware-and-pe-runtime ""이 제목으로 링크"")
----------------------------------------------------------------------------------------------------------

커널 장치 드라이버는 Linux 운영 체제가 NPU 장치를 인식하고 이를 Linux 장치 파일로 노출할 수 있도록 합니다. 펌웨어는 RNGD 카드 내의 SoC에서 실행되며, 처리 요소(PE)에서 실행되는 PE 런타임(PERT)에 저수준 API를 제공합니다. PERT는 호스트의 런타임과 통신하고 NPU 작업을 실행하기 위해 PE의 리소스를 관리 및 스케줄링하는 역할을 합니다.

Furiosa 컴파일러 [#](#furiosa-compiler ""이 제목으로 링크"")
--------------------------------------------------------------

Furiosa 컴파일러는 모델 그래프를 분석하고 최적화하여 NPU 실행 프로그램을 생성합니다. 이 과정은 그래프 수준의 최적화, 연산자 결합, 메모리 할당 최적화, 스케줄링 및 계층 간 데이터 이동 최소화를 포함합니다. `torch.compile()` 백엔드에서 `FuriosaBackend`를 사용하거나 `furiosa-llm`을 사용할 때, Furiosa 컴파일러는 런타임을 위한 NPU 실행 프로그램을 투명하게 생성합니다.

Furiosa 런타임 [#](#furiosa-runtime ""이 제목으로 링크"")
------------------------------------------------------------

런타임은 Furiosa 컴파일러가 생성한 여러 실행 가능한 NPU 프로그램을 로드하고 NPU에서 실행합니다. 단일 모델은 모델 아키텍처와 애플리케이션에 따라 여러 실행 프로그램으로 컴파일될 수 있습니다. 런타임은 NPU 프로그램의 스케줄링과 NPU 및 CPU의 계산 및 메모리 리소스를 관리하는 역할을 합니다. 또한, 런타임은 여러 NPU를 사용할 수 있으며, 여러 NPU에서 모델을 실행하기 위한 단일 진입점을 제공합니다.

Furiosa 모델 압축기 (양자화기) [#](#furiosa-model-compressor-quantizer ""이 제목으로 링크"")
----------------------------------------------------------------------------------------------------

Furiosa 모델 압축기는 모델 보정 및 양자화를 위한 라이브러리 및 도구입니다. 모델 양자화는 메모리 사용량, 계산 비용, 추론 지연 및 전력 소비를 줄이는 강력한 기술입니다. Furiosa 모델 압축기는 다음과 같은 사후 훈련 양자화 방법을 제공합니다:

* BF16 (W16A16)
* INT8 Weight-Only (W8A16)
* FP8 (W8A8)
* INT8 SmoothQuant (W8A8)
* INT4 Weight-Only (W4A16 AWQ / GPTQ) (2024.2 릴리스 예정)

Furiosa LLM [#](#furiosa-llm ""이 제목으로 링크"")
----------------------------------------------------

Furiosa LLM은 Llama 3.1 70B, 8B, GPT-J 및 Bert와 같은 LLM 모델을 위한 고성능 추론 엔진을 제공합니다. Furiosa LLM은 LLM 모델을 위한 최첨단 서빙 최적화를 제공하도록 설계되었습니다. Furiosa LLM의 주요 기능에는 vLLM 호환 API, PagedAttention, 연속 배칭, HuggingFace 허브 지원 및 OpenAI 호환 API 서버가 포함됩니다. 자세한 정보는 [Furiosa LLM](../furiosa_llm/intro.html#furiosallm)에서 확인할 수 있습니다.

Kubernetes 지원 [#](#kubernetes-support ""이 제목으로 링크"")
------------------------------------------------------------------

Kubernetes는 컨테이너화된 애플리케이션 및 서비스를 관리하기 위해 설계된 오픈 소스 플랫폼으로, 컨테이너화된 워크로드의 배포, 확장 및 자동화를 위한 강력한 기능으로 인해 다양한 기업에서 널리 채택되고 있습니다. FuriosaAI 소프트웨어 스택은 Kubernetes와의 네이티브 통합을 제공하여 Kubernetes 환경 내에서 AI 애플리케이션의 원활한 배포 및 관리를 가능하게 합니다.

FuriosaAI의 디바이스 플러그인은 Kubernetes 클러스터가 FuriosaAI의 NPU를 인식하고, NPU가 필요한 워크로드 및 서비스에 스케줄링될 수 있도록 합니다. 이 기능을 통해 사용자는 FuriosaAI NPU를 사용하여 Kubernetes에서 AI 워크로드를 쉽게 배포할 수 있으며, 효율적인 리소스 활용 및 확장이 가능합니다.

Kubernetes 지원에 대한 자세한 정보는 [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit)에서 확인할 수 있습니다.

[이전
FuriosaAI RNGD](rngd.html ""이전 페이지"") [다음
지원 모델](supported_models.html ""다음 페이지"")

목차
* [커널 드라이버, 펌웨어 및 PE 런타임](#kernel-driver-firmware-and-pe-runtime)
* [Furiosa 컴파일러](#furiosa-compiler)
* [Furiosa 런타임](#furiosa-runtime)
* [Furiosa 모델 압축기 (양자화기)](#furiosa-model-compressor-quantizer)
* [Furiosa LLM](#furiosa-llm)
* [Kubernetes 지원](#kubernetes-support)

By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc.
```"
da255daf-c8d3-430e-b976-ad096f3a9ad7,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/references/sampling_params.html,"* [.rst](../../_sources/furiosa_llm/references/sampling_params.rst ""Download source file"") * .pdf
SamplingParams ==============
Contents --------
* [`SamplingParams`](#furiosa_llm.SamplingParams)
SamplingParams [#](#samplingparams ""Link to this heading"") ==========================================================
*class* furiosa\_llm.
SamplingParams
(
*\** ,
*n
:
int
=
1* ,
*best\_of
:
int
|
None
=
None* ,
*temperature
:
float
=
1.0* ,
*top\_p
:
float
=
1.0* ,
*top\_k
:
int
=
-1* ,
*use\_beam\_search
:
bool
=
False* ,
*length\_penalty
:
float
=
1.0* ,
*early\_stopping
:
bool
|
str
=
False* ,
*max\_tokens
:
int
=
16* ,
*min\_tokens
:
int
=
0* )
[[source]](../../_modules/furiosa_llm/sampling_params.html#SamplingParams) [#](#furiosa_llm.SamplingParams ""Link to this definition"")
Bases: `object`  Sampling parameters for text generation.
The default parameters represents greedy search.
Parameters :
* **n**   – Number of output sequences to return for the given prompt. * **best\_of**   – Number of output sequences that are generated from the prompt.   From these   best\_of      sequences, the top   n      sequences are returned.   best\_of      must be greater than or equal to   n      . This is treated as   the beam width when   use\_beam\_search      is True. By default,   best\_of      is set to   n      . * **temperature**   – Float that controls the randomness of the sampling. Lower   values make the model more deterministic, while higher values make   the model more random. Zero means greedy sampling. * **top\_p**   – Float that controls the cumulative probability of the top tokens   to consider. Must be in (0, 1]. Set to 1 to consider all tokens. * **top\_k**   – Integer that controls the number of top tokens to consider. Set   to -1 to consider all tokens. * **use\_beam\_search**   – Whether to use beam search instead of sampling. * **length\_penalty**   – Float that penalizes sequences based on their length.   Used in beam search. * **early\_stopping**   – Controls the stopping condition for beam search. It   accepts the following values:   True      , where the generation stops as   soon as there are   best\_of      complete candidates;   False      , where an   heuristic is applied and the generation stops when is it very   unlikely to find better candidates;   “never”      , where the beam search   procedure only stops when there cannot be better candidates   (canonical beam search algorithm). * **max\_tokens**   – Maximum number of tokens to generate per output sequence. * **min\_tokens**   – Minimum number of tokens to generate per output sequence   before EOS or stop\_token\_ids can be generated
[previous
LLM class](llm.html ""previous page"") [next
Cloud Native Toolkit](../../cloud_native_toolkit/intro.html ""next page"")
Contents
* [`SamplingParams`](#furiosa_llm.SamplingParams)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../../_sources/furiosa_llm/references/sampling_params.rst ""소스 파일 다운로드"") * .pdf

SamplingParams ==============

목차 --------

* [`SamplingParams`](#furiosa_llm.SamplingParams)

SamplingParams [#](#samplingparams ""이 제목으로 링크"") ==========================================================

*class* furiosa\_llm.
SamplingParams
(
*\** ,
*n
:
int
=
1* ,
*best\_of
:
int
|
None
=
None* ,
*temperature
:
float
=
1.0* ,
*top\_p
:
float
=
1.0* ,
*top\_k
:
int
=
-1* ,
*use\_beam\_search
:
bool
=
False* ,
*length\_penalty
:
float
=
1.0* ,
*early\_stopping
:
bool
|
str
=
False* ,
*max\_tokens
:
int
=
16* ,
*min\_tokens
:
int
=
0* )
[[source]](../../_modules/furiosa_llm/sampling_params.html#SamplingParams) [#](#furiosa_llm.SamplingParams ""이 정의로 링크"")

기반: `object`  텍스트 생성에 대한 샘플링 매개변수. 기본 매개변수는 탐욕적 검색을 나타냅니다.

매개변수:

* **n**   – 주어진 프롬프트에 대해 반환할 출력 시퀀스의 수.
* **best\_of**   – 프롬프트에서 생성된 출력 시퀀스의 수. 이 중에서 상위   n      시퀀스가 반환됩니다.   best\_of      는   n      보다 크거나 같아야 합니다.   use\_beam\_search      가 True일 때 빔 너비로 처리됩니다. 기본적으로,   best\_of      는   n      으로 설정됩니다.
* **temperature**   – 샘플링의 무작위성을 제어하는 부동 소수점 값. 값이 낮을수록 모델이 더 결정론적이 되고, 값이 높을수록 모델이 더 무작위적이 됩니다. 0은 탐욕적 샘플링을 의미합니다.
* **top\_p**   – 고려할 상위 토큰의 누적 확률을 제어하는 부동 소수점 값. (0, 1] 범위 내에 있어야 합니다. 모든 토큰을 고려하려면 1로 설정합니다.
* **top\_k**   – 고려할 상위 토큰의 수를 제어하는 정수. 모든 토큰을 고려하려면 -1로 설정합니다.
* **use\_beam\_search**   – 샘플링 대신 빔 검색을 사용할지 여부.
* **length\_penalty**   – 시퀀스의 길이에 따라 페널티를 부과하는 부동 소수점 값. 빔 검색에서 사용됩니다.
* **early\_stopping**   – 빔 검색의 중지 조건을 제어합니다. 다음 값을 허용합니다:   True      , 생성이   best\_of      완전한 후보가 있을 때 즉시 중지됩니다;   False      , 휴리스틱이 적용되어 더 나은 후보를 찾을 가능성이 매우 낮을 때 생성이 중지됩니다;   “never”      , 빔 검색 절차가 더 나은 후보가 있을 수 없을 때만 중지됩니다 (표준 빔 검색 알고리즘).
* **max\_tokens**   – 출력 시퀀스당 생성할 최대 토큰 수.
* **min\_tokens**   – EOS 또는 stop\_token\_ids가 생성되기 전에 출력 시퀀스당 생성할 최소 토큰 수.

[이전
LLM 클래스](llm.html ""이전 페이지"") [다음
Cloud Native Toolkit](../../cloud_native_toolkit/intro.html ""다음 페이지"")

목차

* [`SamplingParams`](#furiosa_llm.SamplingParams)

By FuriosaAI, Inc.  
© 저작권 2024, FuriosaAI, Inc.
```"
edfa6fb4-e88c-4fa8-a6aa-9986213aa6ab,https://furiosa-ai.github.io/docs-dev/2024.1/en/device_management/furiosa_smi.html,"* [.rst](../_sources/device_management/furiosa_smi.rst ""Download source file"") * .pdf
furiosa-smi ===========
Contents --------
* [Installing   `furiosa-smi`   command](#installing-furiosa-smi-command)   + [Synopsis](#synopsis)   + [`furiosa-smi          info`](#furiosa-smi-info)   + [`furiosa-smi          status`](#furiosa-smi-status)   + [`furiosa-smi          ps`](#furiosa-smi-ps)   + [`furiosa-smi          topo`](#furiosa-smi-topo)
furiosa-smi [#](#furiosa-smi ""Link to this heading"") ====================================================
The `furiosa-smi` command provides a variety of subcommands and has the ability to obtain information or control the device.
Installing `furiosa-smi` command [#](#installing-furiosa-smi-command ""Link to this heading"") --------------------------------------------------------------------------------------------
To install the `furiosa-smi` command, you need to install `furiosa-smi` as following:
The minimum requirements for `furiosa-smi` are as follows:
* Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers (   [Setting up APT server](../getting_started/prerequisites.html#aptsetup)   )
Then, please install the `furiosa-smi` package as follows:
``` sudo apt update sudo apt install -y furiosa-smi
```
This command installs packages `furiosa-libsmi` and `furiosa-smi` .
### Synopsis [#](#synopsis ""Link to this heading"")
``` furiosa-smi <sub command> [option] ..
```
### `furiosa-smi info` [#](#furiosa-smi-info ""Link to this heading"")
After installing the kernel driver, you can use the `furiosa-smi` command to check whether the NPU device is recognized. Currently, this command provides the `furiosa-smi
info` command to output temperature, power consumption and PCI information of the NPU device. If the device is not visible with this command after mounting it on the machine, please install the driver. If you add the `--full` option to the `info` command, you can see the device’s UUID and serial number information together.
``` $ furiosa-smi info +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware       | Temp.   | Power   | PCI-BDF      | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0   | 0.0.15+af1daaa | 30.18°C | 53.00 W | 0000:17:00.0 | +------+--------+----------------+---------+---------+--------------+ | rngd | npu1   | 0.0.15+af1daaa | 29.25°C | 53.00 W | 0000:2a:00.0 | +------+--------+----------------+---------+---------+--------------+
$ furiosa-smi info --format full +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ | Arch | Device | UUID                                 | S/N        | Firmware       | Temp.   | Power   | Clock | PCI-BDF      | PCI-DEV | +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ | rngd | npu0   | 3E93AE7C-E8EA-4C62-BED6-AD2EC0461AE8 | RNGDXXXXXX | 0.0.15+af1daaa | 30.18°C | 53.00 W |   N/A | 0000:17:00.0 | 508:0   | +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ | rngd | npu1   | 176DAD0F-1510-475C-91D8-5F79551CF718 | RNGDXXXXXY | 0.0.15+af1daaa | 29.44°C | 53.00 W |   N/A | 0000:2a:00.0 | 506:0   | +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+
```
### `furiosa-smi status` [#](#furiosa-smi-status ""Link to this heading"")
The `status` subcommand provides information about the device files available on the NPU device. You can also check whether each core present in the NPU is in use or idle.
``` $ furiosa-smi status +------+--------+---------------+------------------+ | Arch | Device | Cores         | Core Utilization | +------+--------+---------------+------------------+ |      |        | 0 (occupied), | Core 0: 0.00%,   | |      |        | 1 (occupied), | Core 1: 0.00%,   | |      |        | 2 (occupied), | Core 2: 0.00%,   | | rngd | npu0   | 3 (occupied), | Core 3: 0.00%,   | |      |        | 4 (occupied), | Core 4: 0.00%,   | |      |        | 5 (occupied), | Core 5: 0.00%,   | |      |        | 6 (occupied), | Core 6: 0.00%,   | |      |        | 7 (occupied)  | Core 7: 0.00%    | +------+--------+---------------+------------------+ |      |        | 0 (occupied), | Core 0: 0.00%,   | |      |        | 1 (occupied), | Core 1: 0.00%,   | |      |        | 2 (occupied), | Core 2: 0.00%,   | | rngd | npu1   | 3 (occupied), | Core 3: 0.00%,   | |      |        | 4 (occupied), | Core 4: 0.00%,   | |      |        | 5 (occupied), | Core 5: 0.00%,   | |      |        | 6 (occupied), | Core 6: 0.00%,   | |      |        | 7 (occupied)  | Core 7: 0.00%    | +------+--------+---------------+------------------+
```
### `furiosa-smi ps` [#](#furiosa-smi-ps ""Link to this heading"")
The `ps` subcommand prints information about the OS process currently occupying the NPU device.
``` $ furiosa-smi ps +-----------+--------+------------------------------------------------------------+ | NPU       | PID    | CMD                                                        | +-----------+--------+------------------------------------------------------------+ | npu0pe0-3 | 132529 | /usr/bin/python3 /usr/local/bin/uvicorn gptj:app           | +-----------+--------+------------------------------------------------------------+
```
### `furiosa-smi topo` [#](#furiosa-smi-topo ""Link to this heading"")
The `topo` subcommand shows the topology of the NPU device and its NUMA node.
``` $ furiosa-smi topo +--------+--------------+--------------+-----------+ | Device | npu0         | npu1         | NUMA node | +--------+--------------+--------------+-----------+ | npu0   | Noc          | Interconnect | 0         | +--------+--------------+--------------+-----------+ | npu1   | Interconnect | Noc          | 0         | +--------+--------------+--------------+-----------+
Legend:
  Noc          = Connection within the same npu chip   Bridge       = Devices communicating via one or more PCIe switches   Cpu          = Devices communicating exclusively within a single CPU socket   Interconnect = Devices communicating via inter-socket links (e.g., QPI, GMI)   Unknown      = Connection type is unidentified
```
[previous
Cloud Native Toolkit](../cloud_native_toolkit/intro.html ""previous page"")
Contents
* [Installing   `furiosa-smi`   command](#installing-furiosa-smi-command)   + [Synopsis](#synopsis)   + [`furiosa-smi          info`](#furiosa-smi-info)   + [`furiosa-smi          status`](#furiosa-smi-status)   + [`furiosa-smi          ps`](#furiosa-smi-ps)   + [`furiosa-smi          topo`](#furiosa-smi-topo)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../_sources/device_management/furiosa_smi.rst ""소스 파일 다운로드"") * .pdf

furiosa-smi
===========

목차
--------
* [`furiosa-smi` 명령 설치](#installing-furiosa-smi-command)
  + [개요](#synopsis)
  + [`furiosa-smi info`](#furiosa-smi-info)
  + [`furiosa-smi status`](#furiosa-smi-status)
  + [`furiosa-smi ps`](#furiosa-smi-ps)
  + [`furiosa-smi topo`](#furiosa-smi-topo)

furiosa-smi [#](#furiosa-smi ""이 제목으로 링크"")
====================================================

`furiosa-smi` 명령은 다양한 하위 명령을 제공하며, 장치의 정보를 얻거나 제어할 수 있는 기능을 갖추고 있습니다.

`furiosa-smi` 명령 설치 [#](#installing-furiosa-smi-command ""이 제목으로 링크"")
--------------------------------------------------------------------------------------------

`furiosa-smi` 명령을 설치하려면 다음과 같이 `furiosa-smi`를 설치해야 합니다:

`furiosa-smi`의 최소 요구 사항은 다음과 같습니다:
* Ubuntu 20.04 LTS (Debian bullseye) 이상
* 루트 권한 또는 sudo 권한
* APT 서버 구성 및 장치 드라이버 설치 ([APT 서버 설정](../getting_started/prerequisites.html#aptsetup))

그런 다음, 다음과 같이 `furiosa-smi` 패키지를 설치하세요:

```bash
sudo apt update
sudo apt install -y furiosa-smi
```

이 명령은 `furiosa-libsmi`와 `furiosa-smi` 패키지를 설치합니다.

### 개요 [#](#synopsis ""이 제목으로 링크"")

```bash
furiosa-smi <sub command> [option] ..
```

### `furiosa-smi info` [#](#furiosa-smi-info ""이 제목으로 링크"")

커널 드라이버를 설치한 후, `furiosa-smi` 명령을 사용하여 NPU 장치가 인식되는지 확인할 수 있습니다. 현재 이 명령은 NPU 장치의 온도, 전력 소비 및 PCI 정보를 출력하는 `furiosa-smi info` 명령을 제공합니다. 장치를 머신에 장착한 후 이 명령으로 장치가 보이지 않으면 드라이버를 설치하세요. `info` 명령에 `--full` 옵션을 추가하면 장치의 UUID 및 일련 번호 정보를 함께 볼 수 있습니다.

```bash
$ furiosa-smi info
+------+--------+----------------+---------+---------+--------------+
| Arch | Device | Firmware       | Temp.   | Power   | PCI-BDF      |
+------+--------+----------------+---------+---------+--------------+
| rngd | npu0   | 0.0.15+af1daaa | 30.18°C | 53.00 W | 0000:17:00.0 |
+------+--------+----------------+---------+---------+--------------+
| rngd | npu1   | 0.0.15+af1daaa | 29.25°C | 53.00 W | 0000:2a:00.0 |
+------+--------+----------------+---------+---------+--------------+

$ furiosa-smi info --format full
+------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+
| Arch | Device | UUID                                 | S/N        | Firmware       | Temp.   | Power   | Clock | PCI-BDF      | PCI-DEV |
+------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+
| rngd | npu0   | 3E93AE7C-E8EA-4C62-BED6-AD2EC0461AE8 | RNGDXXXXXX | 0.0.15+af1daaa | 30.18°C | 53.00 W |   N/A | 0000:17:00.0 | 508:0   |
+------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+
| rngd | npu1   | 176DAD0F-1510-475C-91D8-5F79551CF718 | RNGDXXXXXY | 0.0.15+af1daaa | 29.44°C | 53.00 W |   N/A | 0000:2a:00.0 | 506:0   |
+------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+
```

### `furiosa-smi status` [#](#furiosa-smi-status ""이 제목으로 링크"")

`status` 하위 명령은 NPU 장치에서 사용 가능한 장치 파일에 대한 정보를 제공합니다. 또한 NPU에 있는 각 코어가 사용 중인지 유휴 상태인지 확인할 수 있습니다.

```bash
$ furiosa-smi status
+------+--------+---------------+------------------+
| Arch | Device | Cores         | Core Utilization |
+------+--------+---------------+------------------+
|      |        | 0 (occupied), | Core 0: 0.00%,   |
|      |        | 1 (occupied), | Core 1: 0.00%,   |
|      |        | 2 (occupied), | Core 2: 0.00%,   |
| rngd | npu0   | 3 (occupied), | Core 3: 0.00%,   |
|      |        | 4 (occupied), | Core 4: 0.00%,   |
|      |        | 5 (occupied), | Core 5: 0.00%,   |
|      |        | 6 (occupied), | Core 6: 0.00%,   |
|      |        | 7 (occupied)  | Core 7: 0.00%    |
+------+--------+---------------+------------------+
|      |        | 0 (occupied), | Core 0: 0.00%,   |
|      |        | 1 (occupied), | Core 1: 0.00%,   |
|      |        | 2 (occupied), | Core 2: 0.00%,   |
| rngd | npu1   | 3 (occupied), | Core 3: 0.00%,   |
|      |        | 4 (occupied), | Core 4: 0.00%,   |
|      |        | 5 (occupied), | Core 5: 0.00%,   |
|      |        | 6 (occupied), | Core 6: 0.00%,   |
|      |        | 7 (occupied)  | Core 7: 0.00%    |
+------+--------+---------------+------------------+
```

### `furiosa-smi ps` [#](#furiosa-smi-ps ""이 제목으로 링크"")

`ps` 하위 명령은 현재 NPU 장치를 점유하고 있는 OS 프로세스에 대한 정보를 출력합니다.

```bash
$ furiosa-smi ps
+-----------+--------+------------------------------------------------------------+
| NPU       | PID    | CMD                                                        |
+-----------+--------+------------------------------------------------------------+
| npu0pe0-3 | 132529 | /usr/bin/python3 /usr/local/bin/uvicorn gptj:app           |
+-----------+--------+------------------------------------------------------------+
```

### `furiosa-smi topo` [#](#furiosa-smi-topo ""이 제목으로 링크"")

`topo` 하위 명령은 NPU 장치와 그 NUMA 노드의 토폴로지를 보여줍니다.

```bash
$ furiosa-smi topo
+--------+--------------+--------------+-----------+
| Device | npu0         | npu1         | NUMA node |
+--------+--------------+--------------+-----------+
| npu0   | Noc          | Interconnect | 0         |
+--------+--------------+--------------+-----------+
| npu1   | Interconnect | Noc          | 0         |
+--------+--------------+--------------+-----------+

Legend:
  Noc          = 동일한 npu 칩 내의 연결
  Bridge       = 하나 이상의 PCIe 스위치를 통해 통신하는 장치
  Cpu          = 단일 CPU 소켓 내에서만 통신하는 장치
  Interconnect = 소켓 간 링크(e.g., QPI, GMI)를 통해 통신하는 장치
  Unknown      = 연결 유형이 식별되지 않음
```

[이전
클라우드 네이티브 툴킷](../cloud_native_toolkit/intro.html ""이전 페이지"")

목차
* [`furiosa-smi` 명령 설치](#installing-furiosa-smi-command)
  + [개요](#synopsis)
  + [`furiosa-smi info`](#furiosa-smi-info)
  + [`furiosa-smi status`](#furiosa-smi-status)
  + [`furiosa-smi ps`](#furiosa-smi-ps)
  + [`furiosa-smi topo`](#furiosa-smi-topo)

By FuriosaAI, Inc.
© 저작권 2024, FuriosaAI, Inc.
```"
e08e3845-eb5a-4b6f-8530-fb63f81ef7d0,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/references/llm.html,"* [.rst](../../_sources/furiosa_llm/references/llm.rst ""Download source file"") * .pdf
LLM class =========
Contents --------
* [`LLM`](#furiosa_llm.LLM)   + [`LLM.generate()`](#furiosa_llm.LLM.generate)   + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms)
LLM class [#](#llm-class ""Link to this heading"") ================================================
*class* furiosa\_llm.
LLM
(
*pretrained\_id
:
str* ,
*task\_type
:
str
|
None
=
None* ,
*llm\_config
:
LLMConfig
|
None
=
None* ,
*qformat\_path
:
PathLike
|
None
=
None* ,
*qparam\_path
:
PathLike
|
None
=
None* ,
*prefill\_quant\_bin\_path
:
PathLike
|
None
=
None* ,
*decode\_quant\_bin\_path
:
PathLike
|
None
=
None* ,
*config
:
Dict
[
str
,
Any
]
=
{}* ,
*bucket\_config
:
BucketConfig
|
None
=
None* ,
*max\_seq\_len\_to\_capture
:
int
=
2048* ,
*tensor\_parallel\_size
:
int
=
4* ,
*pipeline\_parallel\_size
:
int
=
1* ,
*data\_parallel\_size
:
int
|
None
=
None* ,
*tokenizer
:
PreTrainedTokenizer
|
PreTrainedTokenizerFast
|
None
=
None* ,
*tokenizer\_mode
:
Literal
[
'auto'
,
'slow'
]
=
'auto'* ,
*seed
:
int
|
None
=
None* ,
*devices
:
str
|
Sequence
[
Device
]
|
None
=
None* ,
*param\_file\_path
:
PathLike
|
None
=
None* ,
*param\_saved\_format
:
Literal
[
'safetensors'
,
'pt'
]
=
'safetensors'* ,
*do\_decompositions\_for\_model\_rewrite
:
bool
=
False* ,
*comp\_supertask\_kind
:
Literal
[
'edf'
,
'dfg'
,
'fx'
]
|
None
=
None* ,
*cache\_dir
:
PathLike
|
None
=
PosixPath('/home/hyunsik/.cache/furiosa/llm')* ,
*backend
:
LLMBackend
|
None
=
None* ,
*use\_blockwise\_compile
:
bool
=
True* ,
*num\_blocks\_per\_supertask
:
int
=
1* ,
*embed\_all\_constants\_into\_graph
:
bool
=
False* ,
*paged\_attention\_num\_blocks
:
int
|
None
=
None* ,
*paged\_attention\_block\_size
:
int
=
1* ,
*kv\_cache\_sharing\_across\_beams\_config
:
KvCacheSharingAcrossBeamsConfig
|
None
=
None* ,
*scheduler\_config
:
SchedulerConfig
=
SchedulerConfig(npu\_queue\_limit=2,
max\_processing\_samples=65536,
spare\_blocks\_ratio=0.2,
is\_offline=False)* ,
*packing\_type
:
Literal
[
'IDENTITY'
]
=
'IDENTITY'* ,
*compiler\_config\_overrides
:
Mapping
|
None
=
None* ,
*use\_random\_weight
:
bool
=
False* ,
*num\_pipeline\_builder\_workers
:
int
=
1* ,
*num\_compile\_workers
:
int
=
1* ,
*skip\_engine
:
bool
=
False* ,
*\** ,
*\_cleanup
:
bool
=
True* ,
*\*\*
kwargs* )
[[source]](../../_modules/furiosa_llm/api.html#LLM) [#](#furiosa_llm.LLM ""Link to this definition"")
Bases: `object`  An LLM for generating texts from given prompts and sampling parameters.
Parameters :
* **pretrained\_id**   – The name of the pretrained model. This corresponds to   pretrained\_model\_name\_or\_path in HuggingFace Transformers. * **task\_type**   – The type of the task. This corresponds to task in HuggingFace Transformers.   See   <https://huggingface.co/docs/transformers/main/en/quicktour#pipeline>   for more   details. * **llm\_config**   – The configuration for the LLM. This includes quantization and optimization   configurations. * **qformat\_path**   – The path to the quantization format file. * **qparam\_path**   – The path to the quantization parameter file. * **prefill\_quant\_bin\_path**   – The path to the quantziation prefill bin file. * **decode\_quant\_bin\_path**   – The path to the quantziation decode bin file. * **config**   – The configuration for the HuggingFace Transformers model. This is a dictionary   that includes the configuration for the model. * **bucket\_config**   – Config for bucket generating policy. If not given, the model will use single one batch,   max\_seq\_len\_to\_capture      attention size bucket per   each phase. * **max\_seq\_len\_to\_capture**   – Maximum sequence length covered by LLM engine. Sequence with larger context than this will not be covered.   The default is 2048. * **tensor\_parallel\_size**   – The number of PEs for each tensor parallelism group. The default is 4. * **pipeline\_parallel\_size**   – The number of pipeline stages for pipeline parallelism. The default is 1,   which means no pipeline parallelism. * **data\_parallel\_size**   – The size of the data parallelism group. If not given, it will be inferred from   total avaialble PEs and other parallelism degrees. * **tokenizer**   – The name or path of a HuggingFace Transformers tokenizer. * **tokenizer\_mode**   – The tokenizer mode. “auto” will use the fast tokenizer   if available, and “slow” will always use the slow tokenizer. * **seed**   – The seed to initialize the random number generator for sampling. * **devices**   – The devices to run the model. It can be a single device or a list of devices.   Each device can be either “cpu:X” or “cuda:X” where X is a specific device index.   The default is “cpu:0”. * **param\_file\_path**   – The path to the parameter file to use for pipeline generation.   If not specified, the parameters will be saved in a temporary file which will be   used for pipeline generation. * **param\_saved\_format**   – The format of the parameter file. Only possible value is “safetensors” now.   The default is “safetensors”. * **do\_decompositions\_for\_model\_rewrite**   – Whether to decompose some ops to describe various parallelism strategies   with mppp config. When the value is True, mppp config that matches with the decomposed FX graph should be given. * **comp\_supertask\_kind**   – The format that pipeline’s supertask will be represented as.   Possible values are “fx”,”dfg”, and “edf”, and the default is “fx”. * **cache\_dir**   – The cache directory for all generated files for this LLM instance.   When its value is   `None`   , caching is disabled. The default is “$HOME/.cache/furiosa/llm”. * **backend**   – The backend implementation to run forward() of a model for the LLM.   The default is LLMBackend.TORCH\_PIPELINE\_RUNNER. * **use\_blockwise\_compile**   – If True, each task will be compiled in the unit of transformer block,   and compilation result for transformer block is generated once and reused. * **num\_blocks\_per\_supertask**   – The number of transformer blocks that will be merged into one supertask. This option is valid   only when   use\_blockwise\_compile=True      . The default is 1. * **embed\_all\_constants\_into\_graph**   – Whether to embed constant tensors into graph or make them as input of the graph and save them as separate files. * **paged\_attention\_num\_blocks**   – The maximum number of blocks that each k/v storage per layer can store. This argument must be given   if model uses paged attention. * **paged\_attention\_block\_size**   – The maximum number of tokens that can be stored in a single paged attention block. This argument must be given   if model uses paged attention. * **kv\_cache\_sharing\_across\_beams\_config**   – Configuration for sharing kv cache across beams. This argument must be given if and only if   the model is optimized to share kv cache across beams. If this argument is given, decode phase buckets with batch size of   `batch_size`   \*   `kv_cache_sharing_across_beams_config.beam_width`   will be created. * **scheduler\_config**   – Configuration for the scheduler, allowing to maximum number of tasks which can be queued to HW, maximum number of samples   that can be processed by the scheduler, and ratio of spare blocks that are reserved by scheduler. * **packing\_type**   – Packing algorithm. Possible values are “IDENTITY” only for now * **compiler\_config\_overrides**   – Overrides for the compiler config. This is a dictionary that includes the configuration for the compiler. * **use\_random\_weight**   – If True, the model will be initialized with random weights. * **num\_pipeline\_builder\_workers**   – number of workers used for building pipelines (except for compilation). The default is 1 (no parallelism).   Setting this value larger than 1 reduces pipeline building time, especially for large models, but requires much more memory. * **num\_compile\_workers**   – number of workers used for compilation. The default is 1 (no parallelism). * **skip\_engine**   – If True, the native runtime engine will not be initialized. This is useful when you need   the pipelines for other purposes than running them with the engine.
generate
(
*prompts:
str
|
~typing.List[str],
sampling\_params:
~furiosa\_llm.sampling\_params.SamplingParams
=
SamplingParams(n=1,
best\_of=1,
temperature=1.0,
top\_p=1.0,
top\_k=-1,
use\_beam\_search=False,
length\_penalty=1.0,
early\_stopping=False,
max\_tokens=16min\_tokens=0,
,
prompt\_token\_ids:
~typing.List[int]
|
~typing.List[~typing.List[int]]
|
None
=
None* )
→
RequestOutput
|
List
[
RequestOutput
]
[[source]](../../_modules/furiosa_llm/api.html#LLM.generate) [#](#furiosa_llm.LLM.generate ""Link to this definition"")
Generate texts from given prompts and sampling parameters.
Parameters :
* **prompts**   – The prompts to generate texts. * **sampling\_params**   – The sampling parameters for generating texts. * **prompt\_token\_ids**   – The token ids of the prompts. If not given, the token ids are   generated from the prompts using the tokenizer.
Returns :
A list of RequestOutput
objects containing the generated completions in the same order as the input prompts.
get\_splitted\_gms
(
*get\_input\_constants
:
bool
=
False* )
→
Dict
[
str
,
Tuple
[
GraphModule
,
...
]
|
Tuple
[
Tuple
[
GraphModule
,
Tuple
[
Tensor
|
None
,
...
]
]
,
...
]
]
[[source]](../../_modules/furiosa_llm/api.html#LLM.get_splitted_gms) [#](#furiosa_llm.LLM.get_splitted_gms ""Link to this definition"")
Get sub GraphModules for each pipeline.
Returns :
Dictionary whose key is the pipeline name and value is the tuple containing `GraphModule``s
(computation
supertasks)
and
some
additional
information
if
necessary.
if
``get_input_constants==False` , each value is just a tuple of `GraphModule``s
in
the
pipeline.
Otherwise,
each
value
is
a
tuple
whose
element
is
``GraphModule` in the pipeline and list of input constant tensors, which were originally constant tensors, but converted to input. The list of input constant tensors has same length as corresponding `GraphModule` ’s number of inputs with each element exactly corresponding to the input of the `GraphModule` with same index, but elements with original input tensor indexes are `None` .
Return type :
Dict[str, Union[Tuple[GraphModule, …], Tuple[Tuple[GraphModule, Tuple[Optional[torch.Tensor], …]], …],],]
[previous
References](../references.html ""previous page"") [next
SamplingParams](sampling_params.html ""next page"")
Contents
* [`LLM`](#furiosa_llm.LLM)   + [`LLM.generate()`](#furiosa_llm.LLM.generate)   + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../../_sources/furiosa_llm/references/llm.rst ""소스 파일 다운로드"") * .pdf

LLM 클래스
=========

목차
--------

* [`LLM`](#furiosa_llm.LLM)
  + [`LLM.generate()`](#furiosa_llm.LLM.generate)
  + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms)

LLM 클래스 [#](#llm-class ""이 제목으로 링크"")
===============================================

*class* furiosa\_llm.LLM

(
*pretrained\_id: str*,
*task\_type: str | None = None*,
*llm\_config: LLMConfig | None = None*,
*qformat\_path: PathLike | None = None*,
*qparam\_path: PathLike | None = None*,
*prefill\_quant\_bin\_path: PathLike | None = None*,
*decode\_quant\_bin\_path: PathLike | None = None*,
*config: Dict[str, Any] = {}*,
*bucket\_config: BucketConfig | None = None*,
*max\_seq\_len\_to\_capture: int = 2048*,
*tensor\_parallel\_size: int = 4*,
*pipeline\_parallel\_size: int = 1*,
*data\_parallel\_size: int | None = None*,
*tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast | None = None*,
*tokenizer\_mode: Literal['auto', 'slow'] = 'auto'*,
*seed: int | None = None*,
*devices: str | Sequence[Device] | None = None*,
*param\_file\_path: PathLike | None = None*,
*param\_saved\_format: Literal['safetensors', 'pt'] = 'safetensors'*,
*do\_decompositions\_for\_model\_rewrite: bool = False*,
*comp\_supertask\_kind: Literal['edf', 'dfg', 'fx'] | None = None*,
*cache\_dir: PathLike | None = PosixPath('/home/hyunsik/.cache/furiosa/llm')*,
*backend: LLMBackend | None = None*,
*use\_blockwise\_compile: bool = True*,
*num\_blocks\_per\_supertask: int = 1*,
*embed\_all\_constants\_into\_graph: bool = False*,
*paged\_attention\_num\_blocks: int | None = None*,
*paged\_attention\_block\_size: int = 1*,
*kv\_cache\_sharing\_across\_beams\_config: KvCacheSharingAcrossBeamsConfig | None = None*,
*scheduler\_config: SchedulerConfig = SchedulerConfig(npu\_queue\_limit=2, max\_processing\_samples=65536, spare\_blocks\_ratio=0.2, is\_offline=False)*,
*packing\_type: Literal['IDENTITY'] = 'IDENTITY'*,
*compiler\_config\_overrides: Mapping | None = None*,
*use\_random\_weight: bool = False*,
*num\_pipeline\_builder\_workers: int = 1*,
*num\_compile\_workers: int = 1*,
*skip\_engine: bool = False*,
*\**,
*\_cleanup: bool = True*,
*\*\*kwargs*
)

[[source]](../../_modules/furiosa_llm/api.html#LLM) [#](#furiosa_llm.LLM ""이 정의로 링크"")

기반: `object`  
주어진 프롬프트와 샘플링 매개변수로 텍스트를 생성하는 LLM.

매개변수:

* **pretrained\_id** – 사전 학습된 모델의 이름입니다. 이는 HuggingFace Transformers의 pretrained\_model\_name\_or\_path에 해당합니다.
* **task\_type** – 작업의 유형입니다. 이는 HuggingFace Transformers의 task에 해당합니다. 자세한 내용은 <https://huggingface.co/docs/transformers/main/en/quicktour#pipeline>를 참조하세요.
* **llm\_config** – LLM의 구성입니다. 여기에는 양자화 및 최적화 구성이 포함됩니다.
* **qformat\_path** – 양자화 형식 파일의 경로입니다.
* **qparam\_path** – 양자화 매개변수 파일의 경로입니다.
* **prefill\_quant\_bin\_path** – 양자화 프리필 빈 파일의 경로입니다.
* **decode\_quant\_bin\_path** – 양자화 디코드 빈 파일의 경로입니다.
* **config** – HuggingFace Transformers 모델의 구성입니다. 이는 모델의 구성을 포함하는 사전입니다.
* **bucket\_config** – 버킷 생성 정책의 구성입니다. 주어지지 않으면 모델은 각 단계마다 주어진 최대 시퀀스 길이 버킷을 사용합니다.
* **max\_seq\_len\_to\_capture** – LLM 엔진이 커버하는 최대 시퀀스 길이입니다. 이보다 큰 컨텍스트의 시퀀스는 커버되지 않습니다. 기본값은 2048입니다.
* **tensor\_parallel\_size** – 각 텐서 병렬 그룹의 PE 수입니다. 기본값은 4입니다.
* **pipeline\_parallel\_size** – 파이프라인 병렬 처리를 위한 파이프라인 단계 수입니다. 기본값은 1로, 이는 파이프라인 병렬 처리가 없음을 의미합니다.
* **data\_parallel\_size** – 데이터 병렬 그룹의 크기입니다. 주어지지 않으면, 사용 가능한 총 PE와 다른 병렬 처리 정도에서 추론됩니다.
* **tokenizer** – HuggingFace Transformers 토크나이저의 이름 또는 경로입니다.
* **tokenizer\_mode** – 토크나이저 모드입니다. ""auto""는 가능한 경우 빠른 토크나이저를 사용하고, ""slow""는 항상 느린 토크나이저를 사용합니다.
* **seed** – 샘플링을 위한 난수 생성기를 초기화하는 시드입니다.
* **devices** – 모델을 실행할 장치입니다. 단일 장치 또는 장치 목록일 수 있습니다. 각 장치는 ""cpu:X"" 또는 ""cuda:X"" 형식으로 지정할 수 있습니다. 기본값은 ""cpu:0""입니다.
* **param\_file\_path** – 파이프라인 생성을 위한 매개변수 파일의 경로입니다. 지정되지 않으면, 매개변수는 임시 파일에 저장되어 파이프라인 생성에 사용됩니다.
* **param\_saved\_format** – 매개변수 파일의 형식입니다. 현재 가능한 값은 ""safetensors""뿐입니다. 기본값은 ""safetensors""입니다.
* **do\_decompositions\_for\_model\_rewrite** – mppp 구성으로 다양한 병렬 처리 전략을 설명하기 위해 일부 연산을 분해할지 여부입니다. 값이 True인 경우, 분해된 FX 그래프와 일치하는 mppp 구성이 제공되어야 합니다.
* **comp\_supertask\_kind** – 파이프라인의 슈퍼태스크가 표현될 형식입니다. 가능한 값은 ""fx"", ""dfg"", ""edf""이며, 기본값은 ""fx""입니다.
* **cache\_dir** – 이 LLM 인스턴스를 위한 모든 생성된 파일의 캐시 디렉토리입니다. 값이 `None`이면 캐싱이 비활성화됩니다. 기본값은 ""$HOME/.cache/furiosa/llm""입니다.
* **backend** – LLM 모델의 forward()를 실행할 백엔드 구현입니다. 기본값은 LLMBackend.TORCH\_PIPELINE\_RUNNER입니다.
* **use\_blockwise\_compile** – True인 경우, 각 태스크는 변환기 블록 단위로 컴파일되며, 변환기 블록의 컴파일 결과는 한 번 생성되어 재사용됩니다.
* **num\_blocks\_per\_supertask** – 하나의 슈퍼태스크로 병합될 변환기 블록의 수입니다. 이 옵션은 use\_blockwise\_compile=True일 때만 유효합니다. 기본값은 1입니다.
* **embed\_all\_constants\_into\_graph** – 상수 텐서를 그래프에 포함시킬지 아니면 그래프의 입력으로 만들어 별도의 파일로 저장할지 여부입니다.
* **paged\_attention\_num\_blocks** – 각 레이어의 k/v 저장소가 저장할 수 있는 최대 블록 수입니다. 모델이 페이지드 어텐션을 사용하는 경우 이 인수가 주어져야 합니다.
* **paged\_attention\_block\_size** – 단일 페이지드 어텐션 블록에 저장할 수 있는 최대 토큰 수입니다. 모델이 페이지드 어텐션을 사용하는 경우 이 인수가 주어져야 합니다.
* **kv\_cache\_sharing\_across\_beams\_config** – 빔 간의 kv 캐시 공유를 위한 구성입니다. 모델이 빔 간의 kv 캐시를 공유하도록 최적화된 경우에만 이 인수가 주어져야 합니다. 이 인수가 주어지면, `batch_size` \* `kv_cache_sharing_across_beams_config.beam_width` 크기의 디코드 단계 버킷이 생성됩니다.
* **scheduler\_config** – 스케줄러의 구성으로, HW에 큐잉될 수 있는 최대 태스크 수, 스케줄러가 처리할 수 있는 최대 샘플 수, 스케줄러가 예약하는 여유 블록의 비율을 허용합니다.
* **packing\_type** – 패킹 알고리즘입니다. 현재 가능한 값은 ""IDENTITY""뿐입니다.
* **compiler\_config\_overrides** – 컴파일러 구성의 재정의입니다. 이는 컴파일러의 구성을 포함하는 사전입니다.
* **use\_random\_weight** – True인 경우, 모델은 무작위 가중치로 초기화됩니다.
* **num\_pipeline\_builder\_workers** – 파이프라인을 구축하는 데 사용되는 작업자 수입니다(컴파일 제외). 기본값은 1(병렬 처리 없음)입니다. 이 값을 1보다 크게 설정하면 특히 대형 모델의 경우 파이프라인 구축 시간이 줄어들지만, 훨씬 더 많은 메모리가 필요합니다.
* **num\_compile\_workers** – 컴파일에 사용되는 작업자 수입니다. 기본값은 1(병렬 처리 없음)입니다.
* **skip\_engine** – True인 경우, 네이티브 런타임 엔진이 초기화되지 않습니다. 이는 엔진으로 실행하는 것 외의 다른 목적으로 파이프라인이 필요할 때 유용합니다.

generate
(
*prompts: str | ~typing.List[str],
sampling\_params: ~furiosa\_llm.sampling\_params.SamplingParams = SamplingParams(n=1, best\_of=1, temperature=1.0, top\_p=1.0, top\_k=-1, use\_beam\_search=False, length\_penalty=1.0, early\_stopping=False, max\_tokens=16, min\_tokens=0),
prompt\_token\_ids: ~typing.List[int] | ~typing.List[~typing.List[int]] | None = None*
)
→
RequestOutput | List[RequestOutput]

[[source]](../../_modules/furiosa_llm/api.html#LLM.generate) [#](#furiosa_llm.LLM.generate ""이 정의로 링크"")

주어진 프롬프트와 샘플링 매개변수로 텍스트를 생성합니다.

매개변수:

* **prompts** – 텍스트를 생성할 프롬프트입니다.
* **sampling\_params** – 텍스트 생성을 위한 샘플링 매개변수입니다.
* **prompt\_token\_ids** – 프롬프트의 토큰 ID입니다. 주어지지 않으면, 토크나이저를 사용하여 프롬프트에서 토큰 ID가 생성됩니다.

반환:

입력 프롬프트와 동일한 순서로 생성된 완성을 포함하는 RequestOutput 객체의 목록입니다.

get\_splitted\_gms
(
*get\_input\_constants: bool = False*
)
→
Dict[str, Tuple[GraphModule, ...] | Tuple[Tuple[GraphModule, Tuple[Tensor | None, ...]], ...]]

[[source]](../../_modules/furiosa_llm/api.html#LLM.get_splitted_gms) [#](#furiosa_llm.LLM.get_splitted_gms ""이 정의로 링크"")

각 파이프라인에 대한 하위 GraphModules를 가져옵니다.

반환:

키가 파이프라인 이름이고 값이 `GraphModule`s(계산 슈퍼태스크) 및 필요한 경우 추가 정보를 포함하는 튜플인 사전입니다. `get_input_constants==False`인 경우, 각 값은 파이프라인의 `GraphModule`s의 튜플입니다. 그렇지 않으면, 각 값은 파이프라인의 `GraphModule`과 원래 상수 텐서였지만 입력으로 변환된 입력 상수 텐서 목록을 포함하는 튜플입니다. 입력 상수 텐서 목록은 해당 `GraphModule`의 입력 수와 동일한 길이를 가지며, 각 요소는 동일한 인덱스를 가진 `GraphModule`의 입력에 정확히 해당하지만, 원래 입력 텐서 인덱스를 가진 요소는 `None`입니다.

반환 유형:

Dict[str, Union[Tuple[GraphModule, …], Tuple[Tuple[GraphModule, Tuple[Optional[torch.Tensor], …]], …],]

[이전 참조](../references.html ""이전 페이지"") [다음 SamplingParams](sampling_params.html ""다음 페이지"")

목차

* [`LLM`](#furiosa_llm.LLM)
  + [`LLM.generate()`](#furiosa_llm.LLM.generate)
  + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms)

By FuriosaAI, Inc.

© Copyright 2024, FuriosaAI, Inc.
```"
85efd72a-7fd9-4895-a0c7-090891c1e2cf,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/references.html,"* [.rst](../_sources/furiosa_llm/references.rst ""Download source file"") * .pdf
References ==========
References [#](#references ""Link to this heading"") ==================================================
References
* [LLM class](references/llm.html) * [SamplingParams](references/sampling_params.html)
[previous
OpenAI Compatible Server](furiosa-llm-serve.html ""previous page"") [next
LLM class](references/llm.html ""next page"")
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../_sources/furiosa_llm/references.rst ""소스 파일 다운로드"") * .pdf

참고자료
========

참고자료 [#](#references ""이 제목으로 링크"")
==================================================

참고자료
* [LLM 클래스](references/llm.html) * [SamplingParams](references/sampling_params.html)

[이전
OpenAI 호환 서버](furiosa-llm-serve.html ""이전 페이지"") [다음
LLM 클래스](references/llm.html ""다음 페이지"")

FuriosaAI, Inc. 제공
© 저작권 2024, FuriosaAI, Inc.
```"
fe9686e2-f00f-4e68-87fa-80abbaf03e2b,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/metrics_exporter.html,"* [.rst](../../_sources/cloud_native_toolkit/kubernetes/metrics_exporter.rst ""Download source file"") * .pdf
Installing Furiosa Metrics Exporter ===================================
Contents --------
* [Furiosa Metrics Exporter](#furiosa-metrics-exporter)   + [Metrics](#metrics)   + [Deploying Furiosa Metrics Exporter with Helm](#deploying-furiosa-metrics-exporter-with-helm)
Installing Furiosa Metrics Exporter [#](#installing-furiosa-metrics-exporter ""Link to this heading"") ====================================================================================================
Furiosa Metrics Exporter [#](#furiosa-metrics-exporter ""Link to this heading"") ------------------------------------------------------------------------------
The Furiosa metrics exporter exposes collection of metrics related to FuriosaAI NPU devices in [Prometheus](https://prometheus.io/) format. In a Kubernetes cluster, you can scrape the metrics provided by furiosa-metrics-exporter using Prometheus and visualize them with a Grafana dashboard. This can be easily set up using the [Prometheus Chart](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus) and [Grafana](https://github.com/grafana/helm-charts/tree/main/charts/grafana) Helm charts, along with the furiosa-metrics-exporter Helm chart.
### Metrics [#](#metrics ""Link to this heading"")
The exporter is composed of chain of collectors, each collector is responsible for collecting specific metrics from the Furiosa NPU devices. The following table shows the available collectors and metrics:
NPU Metrics
[#](#id1 ""Link to this table"")
| Collector Name | Metric | Type | Metric Labels | Description | | --- | --- | --- | --- | --- | | Liveness | furiosa\_npu\_alive | guage | arch, core, device, uuid, kubernetes\_node\_name | The liveness of the Furiosa NPU device. | | Error | furiosa\_npu\_error | guage | arch, core, device, uuid, kubernetes\_node\_name, label | The error count of the Furiosa NPU device. | | Temperature | furiosa\_npu\_hw\_temperature | guage | arch, core, device, uuid, kubernetes\_node\_name, label | The temperature of the Furiosa NPU device. | | Power | furiosa\_npu\_hw\_power | guage | arch, core, device, uuid, kubernetes\_node\_name, label | The power consumption of the Furiosa NPU device. | | Core Utilization | furiosa\_npu\_core\_utilization | guage | arch, core, device, uuid, kubernetes\_node\_name | The core utilization of the Furiosa NPU device. |
All metrics share common metric labels such as arch, core, device, kubernetes\_node\_name, and uuid. The following table describes the common metric labels:
Common NPU Metrics Label
[#](#id2 ""Link to this table"")
| Common Metric Label | Description | | --- | --- | | arch | The architecture of the Furiosa NPU device. e.g. warboy, rngd | | core | The core number of the Furiosa NPU device. e.g. 0, 1, 2, 3, 4, 5, 6, 7, 0-1, 2-3, 0-3, 4-5, 6-7, 4-7, 0-7 | | device | The device name of the Furiosa NPU device. e.g. npu0 | | kubernetes\_node\_name | The name of the Kubernetes node where the exporter is running, this attribute can be missing if the exporter is running on the host machine or in a naked container. | | uuid | The UUID of the Furiosa NPU device. |
The metric label “label” is used to describe additional attributes specific to each metric. This approach helps avoid having too many metric definitions and effectively aggregates metrics that share common characteristics.
NPU Metrics Type
[#](#id3 ""Link to this table"")
| Metric Type | Label Attribute | Description | | --- | --- | --- | | Error | axi\_post\_error | Indicates count of axi post error. | | Error | axi\_fetch\_error | Indicates count of axi fetch error. | | Error | axi\_discard\_error | Indicates count of axi discard error. | | Error | axi\_doorbell\_done | Indicates count of axi doorbell done error. | | Error | pcie\_post\_error | Indicates count of PCIe post error. | | Error | pcie\_fetch\_error | Indicates count of PCIe fetch error. | | Error | pcie\_discard\_error | Indicates count of PCIe discard error. | | Error | pcie\_doorbell\_done | Indicates count of PCIe doorbell done error. | | Error | device\_error | Total count of device error. | | Temperature | peak | The highest temperature observed from SoC sensors | | Temperature | ambient | The temperature observed from sensors attached to the board | | Power | rms | Root Mean Square (RMS) value of the power consumed by the device, providing an average power consumption metric over a period of time. |
The following shows real-world example of the metrics:
``` #liveness furiosa_npu_alive{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 1
#error furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_post_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_fetch_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_discard_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_doorbell_done"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_post_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_fetch_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_discard_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_doorbell_done"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""device_error"",uuid=""uuid""} 0
#temperature furiosa_npu_hw_temperature{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""peak"",uuid=""uuid""} 39 furiosa_npu_hw_temperature{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""ambient"",uuid=""uuid""} 35
#power furiosa_npu_hw_power{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""rms"",uuid=""uuid""} 4795000
#core utilization furiosa_npu_core_utilization{arch=""rngd"",core=""0"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""1"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""2"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""3"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""4"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""5"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""6"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""7"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90
```
### Deploying Furiosa Metrics Exporter with Helm [#](#deploying-furiosa-metrics-exporter-with-helm ""Link to this heading"")
The Furiosa metrics exporter helm chart is available at [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) . To configure deployment as you need, you can modify `charts/furiosa-metrics-exporter/values.yaml` . For example, the Furiosa metrics exporter Helm chart automatically creates a Service Object with Prometheus annotations to enable metric scraping automatically. You can modify the values.yaml to change the port or disable the Prometheus annotations if needed. You can deploy the Furiosa Metrics Exporter by running the following commands:
``` helm repo add furiosa https://furiosa-ai.github.io/helm-charts helm repo update helm install furiosa-metrics-exporter furiosa/furiosa-metrics-exporter -n kube-system
```
[previous
Installing Furiosa Device Plugin](device_plugin.html ""previous page"") [next
Scheduling NPUs](scheduling_npus.html ""next page"")
Contents
* [Furiosa Metrics Exporter](#furiosa-metrics-exporter)   + [Metrics](#metrics)   + [Deploying Furiosa Metrics Exporter with Helm](#deploying-furiosa-metrics-exporter-with-helm)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","* [.rst](../../_sources/cloud_native_toolkit/kubernetes/metrics_exporter.rst ""소스 파일 다운로드"") * .pdf

Furiosa Metrics Exporter 설치하기
===================================

목차
--------

* [Furiosa Metrics Exporter](#furiosa-metrics-exporter)  
  + [메트릭](#metrics)  
  + [Helm으로 Furiosa Metrics Exporter 배포하기](#deploying-furiosa-metrics-exporter-with-helm)

Furiosa Metrics Exporter 설치하기 [#](#installing-furiosa-metrics-exporter ""이 제목으로 링크"")
====================================================================================================

Furiosa Metrics Exporter [#](#furiosa-metrics-exporter ""이 제목으로 링크"")
------------------------------------------------------------------------------

Furiosa 메트릭 익스포터는 FuriosaAI NPU 장치와 관련된 메트릭을 [Prometheus](https://prometheus.io/) 형식으로 노출합니다. Kubernetes 클러스터에서, Prometheus를 사용하여 furiosa-metrics-exporter가 제공하는 메트릭을 스크랩하고 Grafana 대시보드를 통해 시각화할 수 있습니다. 이는 [Prometheus Chart](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus) 및 [Grafana](https://github.com/grafana/helm-charts/tree/main/charts/grafana) Helm 차트와 furiosa-metrics-exporter Helm 차트를 사용하여 쉽게 설정할 수 있습니다.

### 메트릭 [#](#metrics ""이 제목으로 링크"")

익스포터는 여러 수집기로 구성되어 있으며, 각 수집기는 Furiosa NPU 장치에서 특정 메트릭을 수집하는 역할을 합니다. 다음 표는 사용 가능한 수집기와 메트릭을 보여줍니다:

NPU 메트릭
[#](#id1 ""이 표로 링크"")

| 수집기 이름 | 메트릭 | 유형 | 메트릭 라벨 | 설명 |
| --- | --- | --- | --- | --- |
| Liveness | furiosa\_npu\_alive | guage | arch, core, device, uuid, kubernetes\_node\_name | Furiosa NPU 장치의 생존 여부. |
| Error | furiosa\_npu\_error | guage | arch, core, device, uuid, kubernetes\_node\_name, label | Furiosa NPU 장치의 오류 수. |
| Temperature | furiosa\_npu\_hw\_temperature | guage | arch, core, device, uuid, kubernetes\_node\_name, label | Furiosa NPU 장치의 온도. |
| Power | furiosa\_npu\_hw\_power | guage | arch, core, device, uuid, kubernetes\_node\_name, label | Furiosa NPU 장치의 전력 소비. |
| Core Utilization | furiosa\_npu\_core\_utilization | guage | arch, core, device, uuid, kubernetes\_node\_name | Furiosa NPU 장치의 코어 사용률. |

모든 메트릭은 arch, core, device, kubernetes\_node\_name, uuid와 같은 공통 메트릭 라벨을 공유합니다. 다음 표는 공통 메트릭 라벨을 설명합니다:

공통 NPU 메트릭 라벨
[#](#id2 ""이 표로 링크"")

| 공통 메트릭 라벨 | 설명 |
| --- | --- |
| arch | Furiosa NPU 장치의 아키텍처. 예: warboy, rngd |
| core | Furiosa NPU 장치의 코어 번호. 예: 0, 1, 2, 3, 4, 5, 6, 7, 0-1, 2-3, 0-3, 4-5, 6-7, 4-7, 0-7 |
| device | Furiosa NPU 장치의 장치 이름. 예: npu0 |
| kubernetes\_node\_name | 익스포터가 실행 중인 Kubernetes 노드의 이름. 익스포터가 호스트 머신이나 독립 컨테이너에서 실행 중인 경우 이 속성이 누락될 수 있습니다. |
| uuid | Furiosa NPU 장치의 UUID. |

메트릭 라벨 ""label""은 각 메트릭에 특정한 추가 속성을 설명하는 데 사용됩니다. 이 접근 방식은 너무 많은 메트릭 정의를 피하고 공통 특성을 공유하는 메트릭을 효과적으로 집계하는 데 도움이 됩니다.

NPU 메트릭 유형
[#](#id3 ""이 표로 링크"")

| 메트릭 유형 | 라벨 속성 | 설명 |
| --- | --- | --- |
| Error | axi\_post\_error | axi post 오류 수를 나타냅니다. |
| Error | axi\_fetch\_error | axi fetch 오류 수를 나타냅니다. |
| Error | axi\_discard\_error | axi discard 오류 수를 나타냅니다. |
| Error | axi\_doorbell\_done | axi doorbell 완료 오류 수를 나타냅니다. |
| Error | pcie\_post\_error | PCIe post 오류 수를 나타냅니다. |
| Error | pcie\_fetch\_error | PCIe fetch 오류 수를 나타냅니다. |
| Error | pcie\_discard\_error | PCIe discard 오류 수를 나타냅니다. |
| Error | pcie\_doorbell\_done | PCIe doorbell 완료 오류 수를 나타냅니다. |
| Error | device\_error | 장치 오류의 총 수. |
| Temperature | peak | SoC 센서에서 관찰된 최고 온도 |
| Temperature | ambient | 보드에 부착된 센서에서 관찰된 온도 |
| Power | rms | 장치가 소비한 전력의 제곱평균근(RMS) 값으로, 일정 기간 동안의 평균 전력 소비 메트릭을 제공합니다. |

다음은 메트릭의 실제 예를 보여줍니다:

``` 
#liveness
furiosa_npu_alive{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 1

#error
furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_post_error"",uuid=""uuid""} 0
furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_fetch_error"",uuid=""uuid""} 0
furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_discard_error"",uuid=""uuid""} 0
furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_doorbell_done"",uuid=""uuid""} 0
furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_post_error"",uuid=""uuid""} 0
furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_fetch_error"",uuid=""uuid""} 0
furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_discard_error"",uuid=""uuid""} 0
furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_doorbell_done"",uuid=""uuid""} 0
furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""device_error"",uuid=""uuid""} 0

#temperature
furiosa_npu_hw_temperature{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""peak"",uuid=""uuid""} 39
furiosa_npu_hw_temperature{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""ambient"",uuid=""uuid""} 35

#power
furiosa_npu_hw_power{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""rms"",uuid=""uuid""} 4795000

#core utilization
furiosa_npu_core_utilization{arch=""rngd"",core=""0"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90
furiosa_npu_core_utilization{arch=""rngd"",core=""1"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90
furiosa_npu_core_utilization{arch=""rngd"",core=""2"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90
furiosa_npu_core_utilization{arch=""rngd"",core=""3"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90
furiosa_npu_core_utilization{arch=""rngd"",core=""4"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90
furiosa_npu_core_utilization{arch=""rngd"",core=""5"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90
furiosa_npu_core_utilization{arch=""rngd"",core=""6"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90
furiosa_npu_core_utilization{arch=""rngd"",core=""7"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90
```

### Helm으로 Furiosa Metrics Exporter 배포하기 [#](#deploying-furiosa-metrics-exporter-with-helm ""이 제목으로 링크"")

Furiosa 메트릭 익스포터 Helm 차트는 [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts)에서 사용할 수 있습니다. 필요한 대로 배포를 구성하려면 `charts/furiosa-metrics-exporter/values.yaml`을 수정할 수 있습니다. 예를 들어, Furiosa 메트릭 익스포터 Helm 차트는 Prometheus 주석이 포함된 서비스 객체를 자동으로 생성하여 메트릭 스크래핑을 자동으로 활성화합니다. 필요에 따라 values.yaml을 수정하여 포트를 변경하거나 Prometheus 주석을 비활성화할 수 있습니다. 다음 명령어를 실행하여 Furiosa Metrics Exporter를 배포할 수 있습니다:

``` 
helm repo add furiosa https://furiosa-ai.github.io/helm-charts
helm repo update
helm install furiosa-metrics-exporter furiosa/furiosa-metrics-exporter -n kube-system
```

[이전
Furiosa Device Plugin 설치](device_plugin.html ""이전 페이지"") [다음
NPU 스케줄링](scheduling_npus.html ""다음 페이지"")

목차
* [Furiosa Metrics Exporter](#furiosa-metrics-exporter)  
  + [메트릭](#metrics)  
  + [Helm으로 Furiosa Metrics Exporter 배포하기](#deploying-furiosa-metrics-exporter-with-helm)

By FuriosaAI, Inc.  
© 저작권 2024, FuriosaAI, Inc."
a1f21dc2-1473-410f-a3ae-e46d451a9c0e,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/feature_discovery.html,"* [.rst](../../_sources/cloud_native_toolkit/kubernetes/feature_discovery.rst ""Download source file"") * .pdf
Installing Furiosa Feature Discovery ====================================
Contents --------
* [Furiosa Feature discovery and NFD](#furiosa-feature-discovery-and-nfd)   + [Labels](#labels)   + [Deploying Furiosa Feature Discovery with Helm](#deploying-furiosa-feature-discovery-with-helm)
Installing Furiosa Feature Discovery [#](#installing-furiosa-feature-discovery ""Link to this heading"") ======================================================================================================
Furiosa Feature discovery and NFD [#](#furiosa-feature-discovery-and-nfd ""Link to this heading"") ------------------------------------------------------------------------------------------------
The Furiosa feature discovery automatically labels Kubernetes nodes with information about FuriosaAI NPU properties, such as the NPU family, count, and driver versions. Using these labels, you can schedule your Kubernetes workloads based on specific NPU requirements.
The Furiosa feature Discovery leverage NFD(Node Feature Discovery) which is a tool that detects hardware features and labels Kubernetes nodes. It is recommended to use NFD and Furiosa Feature Discovery to ensure that the Cloud Native Toolkit is deployed only on nodes equipped with FuriosaAI NPUs.
### Labels [#](#labels ""Link to this heading"")
The followings are the labels that the Furiosa Feature Discovery attaches and what they mean.
Labels
[#](#id1 ""Link to this table"")
| Label | Value | Description | | --- | --- | --- | | furiosa.ai/npu.count | n | # of NPU devices | | furiosa.ai/npu.family | warboy, rngd | Chip family | | furiosa.ai/npu.product | warboy, rngd, rngd-s, rngd-max | Chip product name | | furiosa.ai/npu.driver.version | x.y.z | NPU device driver version | | furiosa.ai/npu.driver.version.major | x | NPU device driver version major part | | furiosa.ai/npu.driver.version.minor | y | NPU device driver version minor part | | furiosa.ai/npu.driver.version.patch | z | NPU device driver version patch part | | furiosa.ai/npu.driver.version.metadata | abcxyz | NPU device driver version metadata |
### Deploying Furiosa Feature Discovery with Helm [#](#deploying-furiosa-feature-discovery-with-helm ""Link to this heading"")
With the helm chart you can easily install Furiosa feature discovery and NFD into your Kubernetes cluster. Following command shows how to install them. The Furiosa device plugin helm chart is available at [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) . To configure deployment as you need, you can modify `charts/furiosa-feature-discovery/values.yaml` .
``` helm repo add furiosa https://furiosa-ai.github.io/helm-charts helm repo update helm install furiosa-feature-discovery furiosa/furiosa-feature-discovery -n kube-system
```
[previous
Kubernetes Support](../kubernetes.html ""previous page"") [next
Installing Furiosa Device Plugin](device_plugin.html ""next page"")
Contents
* [Furiosa Feature discovery and NFD](#furiosa-feature-discovery-and-nfd)   + [Labels](#labels)   + [Deploying Furiosa Feature Discovery with Helm](#deploying-furiosa-feature-discovery-with-helm)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../../_sources/cloud_native_toolkit/kubernetes/feature_discovery.rst ""소스 파일 다운로드"") * .pdf

Furiosa Feature Discovery 설치
====================================

목차
--------
* [Furiosa Feature Discovery와 NFD](#furiosa-feature-discovery-and-nfd)  
  + [레이블](#labels)  
  + [Helm을 사용한 Furiosa Feature Discovery 배포](#deploying-furiosa-feature-discovery-with-helm)

Furiosa Feature Discovery 설치 [#](#installing-furiosa-feature-discovery ""이 제목으로 링크"")
======================================================================================================

Furiosa Feature Discovery와 NFD [#](#furiosa-feature-discovery-and-nfd ""이 제목으로 링크"")
------------------------------------------------------------------------------------------------

Furiosa Feature Discovery는 FuriosaAI NPU의 속성, 예를 들어 NPU 패밀리, 개수, 드라이버 버전 등의 정보를 Kubernetes 노드에 자동으로 레이블링합니다. 이러한 레이블을 사용하여 특정 NPU 요구 사항에 따라 Kubernetes 워크로드를 스케줄링할 수 있습니다.

Furiosa Feature Discovery는 하드웨어 기능을 감지하고 Kubernetes 노드에 레이블을 붙이는 도구인 NFD(Node Feature Discovery)를 활용합니다. FuriosaAI NPU가 장착된 노드에만 Cloud Native Toolkit이 배포되도록 NFD와 Furiosa Feature Discovery를 사용하는 것이 권장됩니다.

### 레이블 [#](#labels ""이 제목으로 링크"")

다음은 Furiosa Feature Discovery가 부착하는 레이블과 그 의미입니다.

| 레이블 | 값 | 설명 |
| --- | --- | --- |
| furiosa.ai/npu.count | n | NPU 장치의 수 |
| furiosa.ai/npu.family | warboy, rngd | 칩 패밀리 |
| furiosa.ai/npu.product | warboy, rngd, rngd-s, rngd-max | 칩 제품명 |
| furiosa.ai/npu.driver.version | x.y.z | NPU 장치 드라이버 버전 |
| furiosa.ai/npu.driver.version.major | x | NPU 장치 드라이버 버전의 주요 부분 |
| furiosa.ai/npu.driver.version.minor | y | NPU 장치 드라이버 버전의 부차적 부분 |
| furiosa.ai/npu.driver.version.patch | z | NPU 장치 드라이버 버전의 패치 부분 |
| furiosa.ai/npu.driver.version.metadata | abcxyz | NPU 장치 드라이버 버전 메타데이터 |

### Helm을 사용한 Furiosa Feature Discovery 배포 [#](#deploying-furiosa-feature-discovery-with-helm ""이 제목으로 링크"")

Helm 차트를 사용하면 Kubernetes 클러스터에 Furiosa Feature Discovery와 NFD를 쉽게 설치할 수 있습니다. 다음 명령어는 설치 방법을 보여줍니다. Furiosa 장치 플러그인 Helm 차트는 [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts)에서 사용할 수 있습니다. 필요한 대로 배포를 구성하려면 `charts/furiosa-feature-discovery/values.yaml`을 수정할 수 있습니다.

```bash
helm repo add furiosa https://furiosa-ai.github.io/helm-charts
helm repo update
helm install furiosa-feature-discovery furiosa/furiosa-feature-discovery -n kube-system
```

[이전
Kubernetes 지원](../kubernetes.html ""이전 페이지"") [다음
Furiosa 장치 플러그인 설치](device_plugin.html ""다음 페이지"")

목차
* [Furiosa Feature Discovery와 NFD](#furiosa-feature-discovery-and-nfd)  
  + [레이블](#labels)  
  + [Helm을 사용한 Furiosa Feature Discovery 배포](#deploying-furiosa-feature-discovery-with-helm)

By FuriosaAI, Inc.
© 저작권 2024, FuriosaAI, Inc.
```"
a53038c2-0668-4963-875e-79abe9c99e2c,https://furiosa-ai.github.io/docs-dev/2024.1/en/whatsnew/index.html,"* [.rst](../_sources/whatsnew/index.rst ""Download source file"") * .pdf
What’s New ==========
Contents --------
* [Furiosa SDK 2024.1.0 (2024-10-11)](#furiosa-sdk-2024-1-0-2024-10-11)   + [Highlights](#highlights)
What’s New [#](#what-s-new ""Link to this heading"") ==================================================
This page describes the changes and functionality available in in the latest releases of Furiosa SDK 2024.1.0.
Furiosa SDK 2024.1.0 (2024-10-11) [#](#furiosa-sdk-2024-1-0-2024-10-11 ""Link to this heading"") ----------------------------------------------------------------------------------------------
2024.1.0 is the first SDK release for RNGD. This release is alpha release, and the features and APIs described in this document may change in the future.
### Highlights [#](#highlights ""Link to this heading"")
* Model Support: LLaMA 3.1 8B/70B, BERT Large, GPT-J 6B * Furiosa Quantizer supports the following quantization methods:      + BF16 (W16A16)   + INT8 Weight-Only (W8A16)   + FP8 (W8A8)   + INT8 SmoothQuant (W8A8) * Furiosa LLM      + Efficient KV cache management with PagedAttention   + Continuous batching support in serving   + OpenAI-compatible API server   + Greedy search and beam search   + Pipeline Parallelism and Data Parallelism across multiple NPUs * `furiosa-mlperf`   command      + Server and Offline scenarios   + BERT, GPT-J, LLaMA 3.1 benchmarks * System Management Interface      + System Management Interface Library and CLI for Furiosa NPU family * Cloud Native Toolkit      + Kubernetes integration for managing and monitoring the Furiosa NPU family
Component version
[#](#id1 ""Link to this table"")
| Package name | Version | | --- | --- | | furiosa-compiler | 2024.1.0 | | furiosa-device-plugin | 2024.1.0 | | furiosa-driver-rngd | 2024.1.0 | | furiosa-feature-discovery | 2024.1.0 | | furiosa-firmware-image-tools | 2024.1.0 | | furiosa-firmware-image-rngd | 0.0.19 | | furiosa-libsmi | 2024.1.0 | | furiosa-llm | 2024.1.0 | | furiosa-llm-models | 2024.1.0 | | furiosa-mlperf | 2024.1.0 | | furiosa-mlperf-resources | 2024.1.0 | | furiosa-model-compressor | 2024.1.0 | | furiosa-model-compressor-impl | 2024.1.0 | | furiosa-native-compiler | 2024.1.0 | | furiosa-native-runtime | 2024.1.0 | | furiosa-smi | 2024.1.0 | | furiosa-torch-ext | 2024.1.0 |
[previous
Supported Models](../overview/supported_models.html ""previous page"") [next
Roadmap](../overview/roadmap.html ""next page"")
Contents
* [Furiosa SDK 2024.1.0 (2024-10-11)](#furiosa-sdk-2024-1-0-2024-10-11)   + [Highlights](#highlights)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../_sources/whatsnew/index.rst ""소스 파일 다운로드"") * .pdf

새로운 기능
==========

목차
--------

* [Furiosa SDK 2024.1.0 (2024-10-11)](#furiosa-sdk-2024-1-0-2024-10-11)  
  + [주요 기능](#highlights)

새로운 기능 [#](#what-s-new ""이 제목으로 링크"")
==================================================

이 페이지는 Furiosa SDK 2024.1.0의 최신 릴리스에서 제공되는 변경 사항과 기능을 설명합니다.

Furiosa SDK 2024.1.0 (2024-10-11) [#](#furiosa-sdk-2024-1-0-2024-10-11 ""이 제목으로 링크"")
----------------------------------------------------------------------------------------------

2024.1.0은 RNGD를 위한 첫 번째 SDK 릴리스입니다. 이 릴리스는 알파 버전이며, 이 문서에 설명된 기능과 API는 향후 변경될 수 있습니다.

### 주요 기능 [#](#highlights ""이 제목으로 링크"")

* 모델 지원: LLaMA 3.1 8B/70B, BERT Large, GPT-J 6B
* Furiosa Quantizer는 다음 양자화 방법을 지원합니다:
  + BF16 (W16A16)
  + INT8 Weight-Only (W8A16)
  + FP8 (W8A8)
  + INT8 SmoothQuant (W8A8)
* Furiosa LLM
  + PagedAttention을 통한 효율적인 KV 캐시 관리
  + 서비스에서의 연속 배칭 지원
  + OpenAI 호환 API 서버
  + Greedy search와 beam search
  + 여러 NPU에 걸친 파이프라인 병렬 처리 및 데이터 병렬 처리
* `furiosa-mlperf` 명령어
  + 서버 및 오프라인 시나리오
  + BERT, GPT-J, LLaMA 3.1 벤치마크
* 시스템 관리 인터페이스
  + Furiosa NPU 패밀리를 위한 시스템 관리 인터페이스 라이브러리 및 CLI
* 클라우드 네이티브 툴킷
  + Furiosa NPU 패밀리의 관리 및 모니터링을 위한 Kubernetes 통합

컴포넌트 버전 [#](#id1 ""이 표로 링크"")

| 패키지 이름 | 버전 |
| --- | --- |
| furiosa-compiler | 2024.1.0 |
| furiosa-device-plugin | 2024.1.0 |
| furiosa-driver-rngd | 2024.1.0 |
| furiosa-feature-discovery | 2024.1.0 |
| furiosa-firmware-image-tools | 2024.1.0 |
| furiosa-firmware-image-rngd | 0.0.19 |
| furiosa-libsmi | 2024.1.0 |
| furiosa-llm | 2024.1.0 |
| furiosa-llm-models | 2024.1.0 |
| furiosa-mlperf | 2024.1.0 |
| furiosa-mlperf-resources | 2024.1.0 |
| furiosa-model-compressor | 2024.1.0 |
| furiosa-model-compressor-impl | 2024.1.0 |
| furiosa-native-compiler | 2024.1.0 |
| furiosa-native-runtime | 2024.1.0 |
| furiosa-smi | 2024.1.0 |
| furiosa-torch-ext | 2024.1.0 |

[이전
지원 모델](../overview/supported_models.html ""이전 페이지"") [다음
로드맵](../overview/roadmap.html ""다음 페이지"")

목차

* [Furiosa SDK 2024.1.0 (2024-10-11)](#furiosa-sdk-2024-1-0-2024-10-11)  
  + [주요 기능](#highlights)

By FuriosaAI, Inc.  
© 저작권 2024, FuriosaAI, Inc.
```"
56ae0c48-fbd4-4d43-b6f7-395542108fa7,https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/furiosa_llm.html,"* [.rst](../_sources/getting_started/furiosa_llm.rst ""Download source file"") * .pdf
Quick Start with Furiosa LLM ============================
Contents --------
* [Installing Furiosa LLM](#installing-furiosa-llm)   + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment)
Quick Start with Furiosa LLM [#](#quick-start-with-furiosa-llm ""Link to this heading"") ======================================================================================
Furiosa LLM is a serving framework for LLM models that utilizes FuriosaAI’s NPU. It provides a Python API compatible with vLLM and a server compatible with the OpenAI API. This document explains how to install and use Furiosa LLM.
Warning
This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future.
Installing Furiosa LLM [#](#installing-furiosa-llm ""Link to this heading"") --------------------------------------------------------------------------
The minimum requirements for Furiosa LLM are as follows:
* Ubuntu 20.04 LTS (Debian bullseye) or later * Administrator privileges on system (root) * [Setting up APT server](prerequisites.html#aptsetup)   and   [Installing Prerequisites](prerequisites.html#installingprerequisites) * Python 3.8, 3.9, or 3.10 * Enough storage space for model weights; e.g., about 100GB for Llama 3.1 70B model
Then, please install the `furiosa-compiler` package as follows:
``` sudo apt install -y furiosa-compiler
```
Also, you need to create a Python virtual environment depending on your environment.
Note
Note that some models, such as meta-llama/Meta-Llama-3.1-8B require you to accept their license, hence, you need to create a HuggingFace account, accept the model’s license, and generate a token. Usually, you can create your token at <https://huggingface.co/settings/tokens> .
Once you get a token, you can authenticate on the HuggingFace Hub as following:
``` huggingface-cli login --token $HF_TOKEN
```
Then, you can install the Furiosa LLM with the following command:
``` pip install furiosa-llm
```
### Offline Batch Inference with Furiosa LLM [#](#offline-batch-inference-with-furiosa-llm ""Link to this heading"")
In this section, we will explain how to perform offline LLM inference using the Python API of Furiosa LLM. First, import the `LLM` class and `SamplingParams` from the furiosa\_llm module. `LLM` class is used to load LLM models and provides the core API for LLM inference. `SamplingParams` allows to specify various parameters for text generation.
``` from furiosa_llm import LLM, SamplingParams
```
Next, we will load an LLM model using the LLM class. The following example loads the meta-llama/Meta-Llama-3.1-8B model from the HuggingFace Hub and performs quantization. The model will be loaded into the memory and ready for inference.
``` llm = LLM(model=""meta-llama/Llama-3.1-8B-Instruct"")
```
After loading the model, you can perform LLM inference by calling the `generate` method.
``` prompts = [   """" ]
sampling_params = SamplingParams(temperature=0.0)
```
Launching the OpenAI-Compatible Server [#](#launching-the-openai-compatible-server ""Link to this heading"") ----------------------------------------------------------------------------------------------------------
You can find more details at [OpenAI Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) .
Running Furiosa LLM in container environment [#](#running-furiosa-llm-in-container-environment ""Link to this heading"") ----------------------------------------------------------------------------------------------------------------------
FuriosaAI provides an image for running `furiosa-llm` in a containerized environment. By using the containerized version, you can run environments that utilize `furiosa-llm` without installing the FuriosaAI Software Stack on your host system, or you can execute it within a Kubernetes environment.
To run the `furiosa-llm` container, you can use the following command:
``` $ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash
(container) # python
```
Warning
The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit)
[previous
Installing Prerequisites](prerequisites.html ""previous page"") [next
Running MLPerf™ Inference Benchmark](furiosa_mlperf.html ""next page"")
Contents
* [Installing Furiosa LLM](#installing-furiosa-llm)   + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../_sources/getting_started/furiosa_llm.rst ""소스 파일 다운로드"") * .pdf

Furiosa LLM 빠른 시작
=====================

목차
----
* [Furiosa LLM 설치](#installing-furiosa-llm)
  + [Furiosa LLM을 사용한 오프라인 배치 추론](#offline-batch-inference-with-furiosa-llm)
* [OpenAI 호환 서버 시작](#launching-the-openai-compatible-server)
* [컨테이너 환경에서 Furiosa LLM 실행](#running-furiosa-llm-in-container-environment)

Furiosa LLM 빠른 시작 [#](#quick-start-with-furiosa-llm ""이 제목으로 링크"")
======================================================================================

Furiosa LLM은 FuriosaAI의 NPU를 활용하는 LLM 모델을 위한 서빙 프레임워크입니다. vLLM과 호환되는 Python API와 OpenAI API와 호환되는 서버를 제공합니다. 이 문서는 Furiosa LLM을 설치하고 사용하는 방법을 설명합니다.

경고
이 문서는 Furiosa SDK 2024.1.0 (알파) 버전을 기반으로 하며, 이 문서에 설명된 기능과 API는 향후 변경될 수 있습니다.

Furiosa LLM 설치 [#](#installing-furiosa-llm ""이 제목으로 링크"")
--------------------------------------------------------------------------

Furiosa LLM의 최소 요구 사항은 다음과 같습니다:
* Ubuntu 20.04 LTS (Debian bullseye) 이상
* 시스템 관리자 권한 (root)
* [APT 서버 설정](prerequisites.html#aptsetup) 및 [필수 구성 요소 설치](prerequisites.html#installingprerequisites)
* Python 3.8, 3.9 또는 3.10
* 모델 가중치를 위한 충분한 저장 공간; 예를 들어, Llama 3.1 70B 모델의 경우 약 100GB

그런 다음, `furiosa-compiler` 패키지를 다음과 같이 설치하십시오:
```bash
sudo apt install -y furiosa-compiler
```

또한, 환경에 따라 Python 가상 환경을 생성해야 합니다.

참고
일부 모델, 예를 들어 meta-llama/Meta-Llama-3.1-8B는 라이선스를 수락해야 하므로 HuggingFace 계정을 생성하고 모델의 라이선스를 수락한 후 토큰을 생성해야 합니다. 일반적으로 <https://huggingface.co/settings/tokens> 에서 토큰을 생성할 수 있습니다.

토큰을 얻으면 다음과 같이 HuggingFace Hub에 인증할 수 있습니다:
```bash
huggingface-cli login --token $HF_TOKEN
```

그런 다음, 다음 명령어로 Furiosa LLM을 설치할 수 있습니다:
```bash
pip install furiosa-llm
```

### Furiosa LLM을 사용한 오프라인 배치 추론 [#](#offline-batch-inference-with-furiosa-llm ""이 제목으로 링크"")

이 섹션에서는 Furiosa LLM의 Python API를 사용하여 오프라인 LLM 추론을 수행하는 방법을 설명합니다. 먼저, `furiosa_llm` 모듈에서 `LLM` 클래스와 `SamplingParams`를 가져옵니다. `LLM` 클래스는 LLM 모델을 로드하고 LLM 추론을 위한 핵심 API를 제공합니다. `SamplingParams`는 텍스트 생성에 대한 다양한 매개변수를 지정할 수 있습니다.

```python
from furiosa_llm import LLM, SamplingParams
```

다음으로, LLM 클래스를 사용하여 LLM 모델을 로드합니다. 다음 예제는 HuggingFace Hub에서 meta-llama/Meta-Llama-3.1-8B 모델을 로드하고 양자화를 수행합니다. 모델은 메모리에 로드되어 추론 준비가 완료됩니다.

```python
llm = LLM(model=""meta-llama/Llama-3.1-8B-Instruct"")
```

모델을 로드한 후, `generate` 메서드를 호출하여 LLM 추론을 수행할 수 있습니다.

```python
prompts = [""""]
sampling_params = SamplingParams(temperature=0.0)
```

OpenAI 호환 서버 시작 [#](#launching-the-openai-compatible-server ""이 제목으로 링크"")
----------------------------------------------------------------------------------------------------------

자세한 내용은 [OpenAI 호환 서버](../furiosa_llm/furiosa-llm-serve.html#openaiserver)에서 확인할 수 있습니다.

컨테이너 환경에서 Furiosa LLM 실행 [#](#running-furiosa-llm-in-container-environment ""이 제목으로 링크"")
----------------------------------------------------------------------------------------------------------------------

FuriosaAI는 컨테이너화된 환경에서 `furiosa-llm`을 실행하기 위한 이미지를 제공합니다. 컨테이너화된 버전을 사용하면 호스트 시스템에 FuriosaAI 소프트웨어 스택을 설치하지 않고도 `furiosa-llm`을 사용하는 환경을 실행하거나 Kubernetes 환경 내에서 실행할 수 있습니다.

`furiosa-llm` 컨테이너를 실행하려면 다음 명령어를 사용할 수 있습니다:

```bash
$ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash
(container) # python
```

경고
위의 예제는 간단함을 위해 `--privileged` 옵션을 사용하지만, 보안상의 이유로 권장되지 않습니다. Kubernetes를 사용하는 경우, 권장 방법에 대해서는 다음 페이지를 참조하십시오: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit)

[이전
필수 구성 요소 설치](prerequisites.html ""이전 페이지"") [다음
MLPerf™ 추론 벤치마크 실행](furiosa_mlperf.html ""다음 페이지"")

목차
* [Furiosa LLM 설치](#installing-furiosa-llm)
  + [Furiosa LLM을 사용한 오프라인 배치 추론](#offline-batch-inference-with-furiosa-llm)
* [OpenAI 호환 서버 시작](#launching-the-openai-compatible-server)
* [컨테이너 환경에서 Furiosa LLM 실행](#running-furiosa-llm-in-container-environment)

By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc.
```"
79971985-74d0-445e-8c78-a80a93ef215b,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/intro.html,"* [.rst](../_sources/cloud_native_toolkit/intro.rst ""Download source file"") * .pdf
Cloud Native Toolkit ====================
Cloud Native Toolkit [#](#cloud-native-toolkit ""Link to this heading"") ======================================================================
FuriosaAI Cloud Native Toolkit is a software stack to enable FuriosaAI’s NPU product in Kubernetes and Container ecosystem.
[previous
SamplingParams](../furiosa_llm/references/sampling_params.html ""previous page"") [next
Kubernetes Support](kubernetes.html ""next page"")
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../_sources/cloud_native_toolkit/intro.rst ""소스 파일 다운로드"") * .pdf

클라우드 네이티브 툴킷
====================

클라우드 네이티브 툴킷 [#](#cloud-native-toolkit ""이 제목으로 링크"")
======================================================================

FuriosaAI 클라우드 네이티브 툴킷은 Kubernetes 및 컨테이너 생태계에서 FuriosaAI의 NPU 제품을 활성화하기 위한 소프트웨어 스택입니다.

[이전
SamplingParams](../furiosa_llm/references/sampling_params.html ""이전 페이지"") [다음
Kubernetes 지원](kubernetes.html ""다음 페이지"")

FuriosaAI, Inc. 제공

© 저작권 2024, FuriosaAI, Inc.
```"
ccf0c765-aa95-4b97-bf6b-fe5650ff8858,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/rngd.html,"* [.rst](../_sources/overview/rngd.rst ""Download source file"") * .pdf
FuriosaAI RNGD ==============
FuriosaAI RNGD [#](#furiosaai-rngd ""Link to this heading"") ==========================================================
FuriosaAI’s second-generation Neural Processing Unit (NPU), RNGD, is a chip designed for deep learning inference, supporting high-performance Large Language Models (LLM), Multi-Modal LLM, Vision models, and other deep learning models.
RNGD is based the Tensor Contraction Processor (TCP) architecture which utilizes TSMC’s 5nm process node, and operates at 1.0 GHz. It offers 512 TOPS and 1024 TOPS of INT8 and INT4 performance respectively. RNGD is configured with two HBM3 modules providing a memory bandwidth of 1.5 TB/s, and supports PCIe Gen5 x16. For multi-tenant environments like Kubernetes and virtual environment, a single RNGD chip can work as 2, 4, 8 individual NPUs, each fully isolated with its own cores and memory bandwidth. RNGD supports Single Root IO Virtualization (SR-IOV) and virtualization for multi-instance NPUs.
Please refer to the followings to learn more about TCP architecture and RNGD:
* [TCP: A Tensor Contraction Processor for AI Workloads, ACM/IEEE ISCA 2024](https://ieeexplore.ieee.org/document/10609575)   (   [PDF](https://furiosa.ai/download/FuriosaAI-tensor-contraction-processor-isca24)   ) * [FuriosaAI RNGD: A Tensor Contraction Processor for Sustainable AI Computing, Hotchips 2024](https://hc2024.hotchips.org/#clip=8jnhm5vdlsow) * [Tensor Contraction Processor: The first future-proof AI chip architecture](https://furiosa.ai/blog/tensor-contraction-processor-ai-chip-architecture)
RNGD Hardware Specification
[#](#id1 ""Link to this table"")
| Architecture | Tensor Contraction Processor | | --- | --- | | Process Node | TSMC 5nm | | Frequency | 1.0GHz | | BF16 | 256TFLOPS | | FP8 | 512TFLOPS | | INT8 | 512TOPS | | INT4 | 1024TOPS | | Memory Bandwidth | HBM3 1.5TB/s | | Memory Capacity | HBM3 48GB | | On-Chip SRAM | 256MB | | Interconnect Interface | PCIe Gen5 x16 | | Thermal Solution | Passive | | Thermal Design Power (TDP) | 150W | | Power Connector | 12VHPWR | | Form Factor | PCIe dual-slot full-height 3/4 Length | | Multi-Instance Support | 8 | | Virtualization Support | Yes | | SR-IOV | 8 Virtual Functions | | ECC Memory Support | Yes | | Secure Boot with Root of Trust | Yes |
[previous
FuriosaAI Developer Center](../index.html ""previous page"") [next
FuriosaAI’s Software Stack](software_stack.html ""next page"")
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../_sources/overview/rngd.rst ""소스 파일 다운로드"") * .pdf

FuriosaAI RNGD
=============

FuriosaAI RNGD [#](#furiosaai-rngd ""이 제목으로 링크"") 
==========================================================

FuriosaAI의 2세대 신경 처리 장치(NPU)인 RNGD는 고성능 대형 언어 모델(LLM), 다중 모달 LLM, 비전 모델 및 기타 딥러닝 모델을 지원하는 딥러닝 추론을 위해 설계된 칩입니다. RNGD는 TSMC의 5nm 공정 노드를 활용하는 Tensor Contraction Processor (TCP) 아키텍처를 기반으로 하며, 1.0 GHz에서 작동합니다. INT8과 INT4 성능에서 각각 512 TOPS와 1024 TOPS를 제공합니다. RNGD는 1.5 TB/s의 메모리 대역폭을 제공하는 두 개의 HBM3 모듈로 구성되어 있으며, PCIe Gen5 x16을 지원합니다. Kubernetes 및 가상 환경과 같은 멀티 테넌트 환경에서는 단일 RNGD 칩이 2, 4, 8개의 개별 NPU로 작동할 수 있으며, 각각 자체 코어와 메모리 대역폭으로 완전히 격리됩니다. RNGD는 단일 루트 IO 가상화(SR-IOV) 및 다중 인스턴스 NPU를 위한 가상화를 지원합니다.

TCP 아키텍처와 RNGD에 대해 더 알아보려면 다음을 참조하십시오:
* [TCP: A Tensor Contraction Processor for AI Workloads, ACM/IEEE ISCA 2024](https://ieeexplore.ieee.org/document/10609575)   (   [PDF](https://furiosa.ai/download/FuriosaAI-tensor-contraction-processor-isca24)   )
* [FuriosaAI RNGD: A Tensor Contraction Processor for Sustainable AI Computing, Hotchips 2024](https://hc2024.hotchips.org/#clip=8jnhm5vdlsow)
* [Tensor Contraction Processor: The first future-proof AI chip architecture](https://furiosa.ai/blog/tensor-contraction-processor-ai-chip-architecture)

RNGD 하드웨어 사양
[#](#id1 ""이 표로 링크"")

| 아키텍처 | Tensor Contraction Processor |
| --- | --- |
| 공정 노드 | TSMC 5nm |
| 주파수 | 1.0GHz |
| BF16 | 256TFLOPS |
| FP8 | 512TFLOPS |
| INT8 | 512TOPS |
| INT4 | 1024TOPS |
| 메모리 대역폭 | HBM3 1.5TB/s |
| 메모리 용량 | HBM3 48GB |
| 온칩 SRAM | 256MB |
| 인터커넥트 인터페이스 | PCIe Gen5 x16 |
| 열 솔루션 | 패시브 |
| 열 설계 전력 (TDP) | 150W |
| 전원 커넥터 | 12VHPWR |
| 폼 팩터 | PCIe 듀얼 슬롯 풀 하이트 3/4 길이 |
| 다중 인스턴스 지원 | 8 |
| 가상화 지원 | 예 |
| SR-IOV | 8 가상 기능 |
| ECC 메모리 지원 | 예 |
| 신뢰의 루트로 안전 부팅 | 예 |

[이전
FuriosaAI 개발자 센터](../index.html ""이전 페이지"") [다음
FuriosaAI의 소프트웨어 스택](software_stack.html ""다음 페이지"")

By FuriosaAI, Inc.
© 저작권 2024, FuriosaAI, Inc.
```"
a214fb49-b797-4d38-b877-597b6bb059eb,https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/prerequisites.html,"* [.rst](../_sources/getting_started/prerequisites.rst ""Download source file"") * .pdf
Installing Prerequisites ========================
Contents --------
* [Requirements](#requirements) * [Verifying if the system has devices](#verifying-if-the-system-has-devices) * [Setting up APT server](#setting-up-apt-server) * [Installing Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware)
Installing Prerequisites [#](#installing-prerequisites ""Link to this heading"") ==============================================================================
We will explain how to install the prerequisite packages necessary for FuriosaAI software stack. The prerequisite packages include device driver, firmware, and PE Runtime. These packages are available in package format for installation on Debian and Ubuntu systems.
Requirements [#](#requirements ""Link to this heading"") ------------------------------------------------------
The minimum requirements are as follows:
* Ubuntu 20.04 LTS (or Debian bullseye) or later * Administrator privileges on system (root)
Verifying if the system has devices [#](#verifying-if-the-system-has-devices ""Link to this heading"") ----------------------------------------------------------------------------------------------------
You can verify the proper installation of FuriosaAI’s devices on your machine by running the following commands:
``` lspci -nn | grep FuriosaAI
```
If the device is properly installed, you should see the PCI information as shown below.
``` 4e:00.0 Processing accelerators [1200]: FuriosaAI, Inc. Device [1ed2:0001] (rev 01)
```
If the `lspci` command is not available, please install the following packages and run the commands to update PCIe ID database:
``` sudo apt update sudo apt install -y pciutils sudo update-pciids
```
Setting up APT server [#](#setting-up-apt-server ""Link to this heading"") ------------------------------------------------------------------------
To use the APT server provided by FuriosaAI, you must configure it on Ubuntu or Debian Linux as outlined below.
1. Install the required packages and register the signing key.
``` sudo apt update sudo apt install -y curl gnupg curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/cloud.google.gpg
```
2. Configure the APT server according to the instructions provided for the Linux distribution versions.
> ``` > echo ""deb [arch=$(dpkg --print-architecture)] http://asia-northeast3-apt.pkg.dev/projects/furiosa-ai $(. /etc/os-release && echo ""$VERSION_CODENAME"") main"" | sudo tee /etc/apt/sources.list.d/furiosa.list >  > ```
Installing Pre-requisite Packages [#](#installing-pre-requisite-packages ""Link to this heading"") ------------------------------------------------------------------------------------------------
If you have registered the APT server as described above, you will be able to install the required packages: the device driver and PE Runtime.
``` sudo apt update sudo apt install furiosa-pert-rngd furiosa-driver-rngd
```
[furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) is an useful CLI tool for listing and managing FuriosaAI NPUs.
``` sudo apt install furiosa-smi
```
Checking NPU devices [#](#checking-npu-devices ""Link to this heading"") ----------------------------------------------------------------------
Once the device driver and [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) are successfully installed, you can check the list of NPU devices as following command:
``` furiosa-smi info
```
Output:
``` +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware       | Temp.   | Power   | PCI-BDF      | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0   | 0.0.16+b4a67ca | 28.88°C | 38.00 W | 0000:4e:00.0 | +------+--------+----------------+---------+---------+--------------+
```
Please refer to [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) to learn more about `furiosa-smi` command.
Upgrading Device Firmware [#](#upgrading-device-firmware ""Link to this heading"") --------------------------------------------------------------------------------
Upgrading firmware versions can improve the performance and stability of the devices. If there is newer firmware in the latest release, you can upgrade them using the following methods:
``` sudo apt install furiosa-firmware-tools-rngd sudo apt install furiosa-firmware-image-rngd
```
Installing the `furiosa-firmware-image-rngd` package will automatically upgrade the firmware. The process takes approximately 3 to 5 minutes per device to complete.
[previous
Roadmap](../overview/roadmap.html ""previous page"") [next
Quick Start with Furiosa LLM](furiosa_llm.html ""next page"")
Contents
* [Requirements](#requirements) * [Verifying if the system has devices](#verifying-if-the-system-has-devices) * [Setting up APT server](#setting-up-apt-server) * [Installing Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../_sources/getting_started/prerequisites.rst ""소스 파일 다운로드"") * .pdf

필수 구성 요소 설치
========================

목차
--------

* [요구 사항](#requirements) 
* [시스템에 장치가 있는지 확인](#verifying-if-the-system-has-devices) 
* [APT 서버 설정](#setting-up-apt-server) 
* [필수 패키지 설치](#installing-pre-requisite-packages) 
* [NPU 장치 확인](#checking-npu-devices) 
* [장치 펌웨어 업그레이드](#upgrading-device-firmware)

필수 구성 요소 설치 [#](#installing-prerequisites ""이 제목으로 링크"")
==============================================================================

FuriosaAI 소프트웨어 스택에 필요한 필수 패키지를 설치하는 방법을 설명합니다. 필수 패키지에는 장치 드라이버, 펌웨어 및 PE 런타임이 포함됩니다. 이러한 패키지는 Debian 및 Ubuntu 시스템에 설치할 수 있는 패키지 형식으로 제공됩니다.

요구 사항 [#](#requirements ""이 제목으로 링크"")
------------------------------------------------------

최소 요구 사항은 다음과 같습니다:

* Ubuntu 20.04 LTS (또는 Debian bullseye) 이상
* 시스템 관리자 권한 (root)

시스템에 장치가 있는지 확인 [#](#verifying-if-the-system-has-devices ""이 제목으로 링크"")
----------------------------------------------------------------------------------------------------

다음 명령을 실행하여 FuriosaAI의 장치가 올바르게 설치되었는지 확인할 수 있습니다:

```bash
lspci -nn | grep FuriosaAI
```

장치가 올바르게 설치된 경우 아래와 같이 PCI 정보를 볼 수 있습니다.

```bash
4e:00.0 Processing accelerators [1200]: FuriosaAI, Inc. Device [1ed2:0001] (rev 01)
```

`lspci` 명령이 사용 가능하지 않은 경우, 다음 패키지를 설치하고 PCIe ID 데이터베이스를 업데이트하기 위해 명령을 실행하십시오:

```bash
sudo apt update
sudo apt install -y pciutils
sudo update-pciids
```

APT 서버 설정 [#](#setting-up-apt-server ""이 제목으로 링크"")
------------------------------------------------------------------------

FuriosaAI에서 제공하는 APT 서버를 사용하려면 아래에 설명된 대로 Ubuntu 또는 Debian Linux에서 설정해야 합니다.

1. 필요한 패키지를 설치하고 서명 키를 등록합니다.

```bash
sudo apt update
sudo apt install -y curl gnupg
curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/cloud.google.gpg
```

2. Linux 배포판 버전에 대한 지침에 따라 APT 서버를 구성합니다.

```bash
echo ""deb [arch=$(dpkg --print-architecture)] http://asia-northeast3-apt.pkg.dev/projects/furiosa-ai $(. /etc/os-release && echo ""$VERSION_CODENAME"") main"" | sudo tee /etc/apt/sources.list.d/furiosa.list
```

필수 패키지 설치 [#](#installing-pre-requisite-packages ""이 제목으로 링크"")
------------------------------------------------------------------------------------------------

위에서 설명한 대로 APT 서버를 등록한 경우, 필요한 패키지인 장치 드라이버 및 PE 런타임을 설치할 수 있습니다.

```bash
sudo apt update
sudo apt install furiosa-pert-rngd furiosa-driver-rngd
```

[furiosa-smi](../device_management/furiosa_smi.html#furiosasmi)는 FuriosaAI NPU를 나열하고 관리하는 데 유용한 CLI 도구입니다.

```bash
sudo apt install furiosa-smi
```

NPU 장치 확인 [#](#checking-npu-devices ""이 제목으로 링크"")
----------------------------------------------------------------------

장치 드라이버와 [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi)가 성공적으로 설치되면, 다음 명령을 사용하여 NPU 장치 목록을 확인할 수 있습니다:

```bash
furiosa-smi info
```

출력:

```plaintext
+------+--------+----------------+---------+---------+--------------+
| Arch | Device | Firmware       | Temp.   | Power   | PCI-BDF      |
+------+--------+----------------+---------+---------+--------------+
| rngd | npu0   | 0.0.16+b4a67ca | 28.88°C | 38.00 W | 0000:4e:00.0 |
+------+--------+----------------+---------+---------+--------------+
```

`furiosa-smi` 명령에 대해 더 알아보려면 [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi)를 참조하십시오.

장치 펌웨어 업그레이드 [#](#upgrading-device-firmware ""이 제목으로 링크"")
--------------------------------------------------------------------------------

펌웨어 버전을 업그레이드하면 장치의 성능과 안정성을 향상시킬 수 있습니다. 최신 릴리스에 새로운 펌웨어가 있는 경우, 다음 방법을 사용하여 업그레이드할 수 있습니다:

```bash
sudo apt install furiosa-firmware-tools-rngd
sudo apt install furiosa-firmware-image-rngd
```

`furiosa-firmware-image-rngd` 패키지를 설치하면 펌웨어가 자동으로 업그레이드됩니다. 이 과정은 장치당 약 3~5분이 소요됩니다.

[이전
로드맵](../overview/roadmap.html ""이전 페이지"") [다음
Furiosa LLM 빠른 시작](furiosa_llm.html ""다음 페이지"")

목차
* [요구 사항](#requirements) 
* [시스템에 장치가 있는지 확인](#verifying-if-the-system-has-devices) 
* [APT 서버 설정](#setting-up-apt-server) 
* [필수 패키지 설치](#installing-pre-requisite-packages) 
* [NPU 장치 확인](#checking-npu-devices) 
* [장치 펌웨어 업그레이드](#upgrading-device-firmware)

By FuriosaAI, Inc.
© 저작권 2024, FuriosaAI, Inc.
```"
1ba93fae-bf2e-42c1-a66d-dabbee880912,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/scheduling_npus.html,"* [.rst](../../_sources/cloud_native_toolkit/kubernetes/scheduling_npus.rst ""Download source file"") * .pdf
Scheduling NPUs ===============
Contents --------
* [Preparing Node](#preparing-node) * [Requesting NPUs](#requesting-npus) * [Scheduling NPUs With Specific Requirements](#scheduling-npus-with-specific-requirements)
Scheduling NPUs [#](#scheduling-npus ""Link to this heading"") ============================================================
This page describes how administrator prepares node and user can consume NPU in Kubernetes.
Preparing Node [#](#preparing-node ""Link to this heading"") ----------------------------------------------------------
As an administrator, you have to install [prerequisites](../../getting_started/prerequisites.html#installingprerequisites) such as driver, firmware on nodes and deploy [Furiosa Device Plugin](device_plugin.html#deviceplugin) .
Once you have installed it, your cluster exposes Furiosa NPUs as schedulable resources, such as `furiosa.ai/rngd` .
To ensure your node is ready, you can examine Capacity and/or Allocatable field of `v1.node` object. Here is an example of node that has 2 RNGD NPUs:
``` ... status:   ...   allocatable:     cpu: ""20""     ephemeral-storage: ""1770585791219""     furiosa.ai/rngd: ""2""     hugepages-1Gi: ""0""     hugepages-2Mi: ""0""     memory: 527727860Ki     pods: ""110""   capacity:     cpu: ""20""     ephemeral-storage: 1921208544Ki     furiosa.ai/rngd: ""2""     hugepages-1Gi: ""0""     hugepages-2Mi: ""0""     memory: 527830260Ki     pods: ""110"" ...
```
The following command should show the `Capacity` field of each node in the Kubernetes cluster.
``` kubectl get nodes -o json | jq -r '.items[] | .metadata.name as $name | .status.capacity | to_entries | map(""    \(.key): \(.value)"") | $name + "":\n  capacity:\n"" + join(""\n"")'
```
Requesting NPUs [#](#requesting-npus ""Link to this heading"") ------------------------------------------------------------
You can consume NPUs from your containers in a Pod by requesting NPU resources, the same way you request CPU or memory.
However, since NPUs are exposed as a custom resource, there are some limitations you should be aware of when requesting NPU resources:
* You can specify NPU   `limits`   without specifying   `requests`   , because kubernetes will use limit as request if request is not specified. * You can specify NPU in both   `limits`   and   `requests`   but these two values must be equal. * You cannot specify NPU   `request`   without specifying   `limits`   .
Here is an example manifest for a Pod that requests 2 RNGD NPUs:
``` apiVersion: v1 kind: Pod metadata:   name: example-npu-request spec:   containers:   - name: furiosa     image: furiosaai/furiosa-smi:latest     imagePullPolicy: IfNotPresent     command: [""sleep""]     args: [""120""]     resources:       limits:         furiosa.ai/rngd: 2
```
Scheduling NPUs With Specific Requirements [#](#scheduling-npus-with-specific-requirements ""Link to this heading"") ------------------------------------------------------------------------------------------------------------------
In certain cases, user may need to schedule NPU workload on node that meet specific hardware or software requirements, such as particular driver versions. If the [Furiosa Feature Discovery](feature_discovery.html#featurediscovery) is deployed in your cluster, nodes are automatically labelled based on their hardware and software configurations including driver version. This allows user to schedule Pod on nodes that meet specific requirements.
Following example shows how to use affinity to schedule a Pod that request 2 RNGD NPUs with specific driver version:
``` apiVersion: v1 kind: Pod metadata:   name: example-npu-scheduling-with-affinity spec:   containers:   - name: furiosa     image: furiosaai/furiosa-smi:latest     imagePullPolicy: IfNotPresent     command: [""sleep""]     args: [""120""]     resources:       limits:         furiosa.ai/rngd: 2   affinity:     nodeAffinity:       requiredDuringSchedulingIgnoredDuringExecution:         nodeSelectorTerms:         - matchExpressions:           - key: furiosa.ai/driver-version             operator: In             values:             - ""1.0.12""
```
[previous
Installing Furiosa Metrics Exporter](metrics_exporter.html ""previous page"") [next
furiosa-smi](../../device_management/furiosa_smi.html ""next page"")
Contents
* [Preparing Node](#preparing-node) * [Requesting NPUs](#requesting-npus) * [Scheduling NPUs With Specific Requirements](#scheduling-npus-with-specific-requirements)
By FuriosaAI, Inc.
© Copyright 2024, FuriosaAI, Inc..","```markdown
* [.rst](../../_sources/cloud_native_toolkit/kubernetes/scheduling_npus.rst ""소스 파일 다운로드"") * .pdf

NPUs 스케줄링
==============

목차
--------
* [노드 준비하기](#preparing-node)
* [NPUs 요청하기](#requesting-npus)
* [특정 요구사항으로 NPUs 스케줄링하기](#scheduling-npus-with-specific-requirements)

NPUs 스케줄링 [#](#scheduling-npus ""이 제목으로 링크"")
===========================================================
이 페이지는 관리자가 노드를 준비하고 사용자가 Kubernetes에서 NPU를 사용할 수 있도록 하는 방법을 설명합니다.

노드 준비하기 [#](#preparing-node ""이 제목으로 링크"")
----------------------------------------------------------
관리자로서, 드라이버, 펌웨어와 같은 [필수 구성 요소](../../getting_started/prerequisites.html#installingprerequisites)를 노드에 설치하고 [Furiosa Device Plugin](device_plugin.html#deviceplugin)을 배포해야 합니다. 설치가 완료되면, 클러스터는 `furiosa.ai/rngd`와 같은 스케줄링 가능한 리소스로 Furiosa NPUs를 노출합니다. 노드가 준비되었는지 확인하려면 `v1.node` 객체의 Capacity 및/또는 Allocatable 필드를 검사할 수 있습니다. 다음은 2개의 RNGD NPU를 가진 노드의 예입니다:

```yaml
... 
status:   
  ...   
  allocatable:     
    cpu: ""20""     
    ephemeral-storage: ""1770585791219""     
    furiosa.ai/rngd: ""2""     
    hugepages-1Gi: ""0""     
    hugepages-2Mi: ""0""     
    memory: 527727860Ki     
    pods: ""110""   
  capacity:     
    cpu: ""20""     
    ephemeral-storage: 1921208544Ki     
    furiosa.ai/rngd: ""2""     
    hugepages-1Gi: ""0""     
    hugepages-2Mi: ""0""     
    memory: 527830260Ki     
    pods: ""110"" 
...
```

다음 명령어는 Kubernetes 클러스터의 각 노드의 `Capacity` 필드를 보여줍니다.

```bash
kubectl get nodes -o json | jq -r '.items[] | .metadata.name as $name | .status.capacity | to_entries | map(""    \(.key): \(.value)"") | $name + "":\n  capacity:\n"" + join(""\n"")'
```

NPUs 요청하기 [#](#requesting-npus ""이 제목으로 링크"")
------------------------------------------------------------
Pod의 컨테이너에서 CPU나 메모리를 요청하는 것과 같은 방식으로 NPU 리소스를 요청하여 NPU를 사용할 수 있습니다. 그러나 NPUs는 커스텀 리소스로 노출되기 때문에 NPU 리소스를 요청할 때 몇 가지 제한 사항이 있습니다:

* `requests`를 지정하지 않고 NPU `limits`를 지정할 수 있습니다. Kubernetes는 요청이 지정되지 않은 경우 제한을 요청으로 사용합니다.
* `limits`와 `requests` 모두에 NPU를 지정할 수 있지만 이 두 값은 동일해야 합니다.
* `limits`를 지정하지 않고 NPU `request`를 지정할 수 없습니다.

다음은 2개의 RNGD NPU를 요청하는 Pod의 예제 매니페스트입니다:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: example-npu-request
spec:
  containers:
  - name: furiosa
    image: furiosaai/furiosa-smi:latest
    imagePullPolicy: IfNotPresent
    command: [""sleep""]
    args: [""120""]
    resources:
      limits:
        furiosa.ai/rngd: 2
```

특정 요구사항으로 NPUs 스케줄링하기 [#](#scheduling-npus-with-specific-requirements ""이 제목으로 링크"")
------------------------------------------------------------------------------------------------------------------
특정 드라이버 버전과 같은 특정 하드웨어 또는 소프트웨어 요구사항을 충족하는 노드에 NPU 작업을 스케줄링해야 하는 경우가 있습니다. 클러스터에 [Furiosa Feature Discovery](feature_discovery.html#featurediscovery)가 배포된 경우, 노드는 드라이버 버전을 포함한 하드웨어 및 소프트웨어 구성에 따라 자동으로 라벨이 지정됩니다. 이를 통해 사용자는 특정 요구사항을 충족하는 노드에 Pod를 스케줄링할 수 있습니다.

다음 예제는 특정 드라이버 버전으로 2개의 RNGD NPU를 요청하는 Pod를 스케줄링하기 위해 affinity를 사용하는 방법을 보여줍니다:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: example-npu-scheduling-with-affinity
spec:
  containers:
  - name: furiosa
    image: furiosaai/furiosa-smi:latest
    imagePullPolicy: IfNotPresent
    command: [""sleep""]
    args: [""120""]
    resources:
      limits:
        furiosa.ai/rngd: 2
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: furiosa.ai/driver-version
            operator: In
            values:
            - ""1.0.12""
```

[이전
Furiosa Metrics Exporter 설치](metrics_exporter.html ""이전 페이지"") [다음
furiosa-smi](../../device_management/furiosa_smi.html ""다음 페이지"")

목차
* [노드 준비하기](#preparing-node)
* [NPUs 요청하기](#requesting-npus)
* [특정 요구사항으로 NPUs 스케줄링하기](#scheduling-npus-with-specific-requirements)

By FuriosaAI, Inc.
© 저작권 2024, FuriosaAI, Inc.
```"
