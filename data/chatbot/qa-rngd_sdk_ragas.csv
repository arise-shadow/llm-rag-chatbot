,user_input,reference_contexts,reference,synthesizer_name
0,"Could you elaborate on the role of MLPerf in evaluating AI systems, particularly in the context of the FuriosaAI Software Stack?","['* [.rst](../_sources/getting_started/furiosa_mlperf.rst ""Download source file"") * .pdf Running MLPerf™ Inference Benchmark =================================== Contents -------- * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1.  Running MLPerf Inference Benchmark](#id1) + [']","MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems and to help developers and end users make decisions about AI systems. The FuriosaAI Software Stack provides the `furiosa-mlperf` command to easily run the MLPerf Inference Benchmark, which is based on MLPerf™ Inference Benchmark v4.1, with the exception of replacing the Llama2 benchmark with one using Llama 3.1.",single_hop_specifc_query_synthesizer
1,What change was made to the MLPerf Inference Benchmark in the FuriosaAI Software Stack?,"['* [.rst](../_sources/getting_started/furiosa_mlperf.rst ""Download source file"") * .pdf Running MLPerf™ Inference Benchmark =================================== Contents -------- * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1.  Running MLPerf Inference Benchmark](#id1) + [']",The FuriosaAI Software Stack replaced the Llama2 benchmark with one using Llama 3.1.,single_hop_specifc_query_synthesizer
2,How can I run a BERT benchmark using the FuriosaAI Software Stack?,"['SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1. Installing `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command ""Link to this heading"") -------------------------------------------------------------------------------------------------- To install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following: The minimum requirements for `furiosa-mlperf` are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers ( [Setting up APT server](prerequisites.html#aptsetup) ) * About 100GB storage space (only for the Llama 3.1 70B) Then, please install the `furiosa-mlperf` package as follows: ``` sudo apt update sudo apt install -y furiosa-mlperf ``` This command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` . Running MLPerf Inference Benchmark [#](#id1 ""Link to this heading"") ------------------------------------------------------------------- ### SYNOPSIS [#](#synopsis ""Link to this heading"") The `furiosa-mlperf` command provides the following subcommands: ``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z) Usage: furiosa-mlperf <COMMAND> Commands: bert-offline Run BERT benchmark with offline scenario bert-server Run BERT benchmark with server scenario gpt-j-offline Run GPT-J benchmark with offline scenario gpt-j-server Run GPT-J benchmark with server scenario llama-3.1-offline Run Llama 3.1 benchmark with offline scenario llama-3.1-server Run Llama 3.1 benchmark with server scenario help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version ``` ### Examples [#](#examples ""Link to this heading"") * BERT benchmark The BERT benchmark is based on running with a single RNGD. + Server Scenario To run BERT Large serving inference benchmark, you can use the following command: ``` furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh ""npu:0:*"" ``` + Offline Scenario To run BERT Large offline inference benchmark, you can use the following command: ``` furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh ""npu:0:*"" ``` * GPT-J benchmark The GPT-J benchmark is based on running with a single RNGD. + Server Scenario To run GPT-J 6B serving inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result ``` + Offline Scenario To run GPT-J 6B offline inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result ``` * Llama 3.1 benchmark The Llama 3.1 benchmark is based on running with four RNGDs. + Server Scenario To run Llama 3.1 70B serving inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result ``` + Offline Scenario To run Llama 3.1 70B offline inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result ``` * Common Once the process completes, it writes the results to a file in the specified results directory. You can open this file to view a summary of the results. ``` cat gpt-j-offline-result/mlperf_log_summary.txt ``` ``` ================================================ MLPerf Results Summary ================================================ SUT name : GPT-J SUT Scenario : Offline Mode : PerformanceOnly Samples per second: 11.842 Tokens per second (inferred): 817.095 Result is : VALID Min duration satisfied : Yes Min queries satisfied : Yes Early stopping satisfied: Yes ```']","To run a BERT Large serving inference benchmark, you can use the following command: `furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh ""npu:0:*""`. For the offline scenario, use: `furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh ""npu:0:*""`.",single_hop_specifc_query_synthesizer
3,Wht is Debian bullseye?,"['SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1. Installing `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command ""Link to this heading"") -------------------------------------------------------------------------------------------------- To install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following: The minimum requirements for `furiosa-mlperf` are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers ( [Setting up APT server](prerequisites.html#aptsetup) ) * About 100GB storage space (only for the Llama 3.1 70B) Then, please install the `furiosa-mlperf` package as follows: ``` sudo apt update sudo apt install -y furiosa-mlperf ``` This command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` . Running MLPerf Inference Benchmark [#](#id1 ""Link to this heading"") ------------------------------------------------------------------- ### SYNOPSIS [#](#synopsis ""Link to this heading"") The `furiosa-mlperf` command provides the following subcommands: ``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z) Usage: furiosa-mlperf <COMMAND> Commands: bert-offline Run BERT benchmark with offline scenario bert-server Run BERT benchmark with server scenario gpt-j-offline Run GPT-J benchmark with offline scenario gpt-j-server Run GPT-J benchmark with server scenario llama-3.1-offline Run Llama 3.1 benchmark with offline scenario llama-3.1-server Run Llama 3.1 benchmark with server scenario help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version ``` ### Examples [#](#examples ""Link to this heading"") * BERT benchmark The BERT benchmark is based on running with a single RNGD. + Server Scenario To run BERT Large serving inference benchmark, you can use the following command: ``` furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh ""npu:0:*"" ``` + Offline Scenario To run BERT Large offline inference benchmark, you can use the following command: ``` furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh ""npu:0:*"" ``` * GPT-J benchmark The GPT-J benchmark is based on running with a single RNGD. + Server Scenario To run GPT-J 6B serving inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result ``` + Offline Scenario To run GPT-J 6B offline inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result ``` * Llama 3.1 benchmark The Llama 3.1 benchmark is based on running with four RNGDs. + Server Scenario To run Llama 3.1 70B serving inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result ``` + Offline Scenario To run Llama 3.1 70B offline inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result ``` * Common Once the process completes, it writes the results to a file in the specified results directory. You can open this file to view a summary of the results. ``` cat gpt-j-offline-result/mlperf_log_summary.txt ``` ``` ================================================ MLPerf Results Summary ================================================ SUT name : GPT-J SUT Scenario : Offline Mode : PerformanceOnly Samples per second: 11.842 Tokens per second (inferred): 817.095 Result is : VALID Min duration satisfied : Yes Min queries satisfied : Yes Early stopping satisfied: Yes ```']","Debian bullseye refers to the version of the Debian operating system that is required as a minimum for installing the `furiosa-mlperf` command, specifically Ubuntu 20.04 LTS or later.",single_hop_specifc_query_synthesizer
4,how do you run furiosaai mlperf in container without installing software stack on host system?,"['Running `furiosa-mlperf` in container environment [#](#running-furiosa-mlperf-in-container-environment ""Link to this heading"") ------------------------------------------------------------------------------------------------------------------------------ FuriosaAI provides a containerized version of the `furiosa-mlperf` command. If you use the containerized version, you can run the `furiosa-mlperf` command without installing the FuriosaAI Software Stack on your host system or you can run the command on Kubernetes environment. To run the `furiosa-mlperf` container, you can use the following command: (Assumes model artifacts exist in `/opt/gpt-j-6b` directory) ``` $ docker run -it --rm --privileged \\ -v /opt/gpt-j-6b:gpt-j-6b furiosa-mlperf:2024.1.0 bash (container) # furiosa-mlperf gpt-j-offline /gpt-j-6b /gpt-j-offline-result ``` To run in a containerized environment, refer to the examples provided in this document. Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Quick Start with Furiosa LLM](furiosa_llm.html ""previous page"") [next Furiosa LLM](../furiosa_llm/intro.html ""next page"") Contents * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","FuriosaAI provides a containerized version of the `furiosa-mlperf` command, allowing you to run it without installing the FuriosaAI Software Stack on your host system. You can use the following command to run the `furiosa-mlperf` container: `$ docker run -it --rm --privileged \ -v /opt/gpt-j-6b:gpt-j-6b furiosa-mlperf:2024.1.0 bash (container) # furiosa-mlperf gpt-j-offline /gpt-j-6b /gpt-j-offline-result`. However, note that using the `--privileged` option is not recommended for security reasons.",single_hop_specifc_query_synthesizer
5,what cloud native toolkit do,"['Running `furiosa-mlperf` in container environment [#](#running-furiosa-mlperf-in-container-environment ""Link to this heading"") ------------------------------------------------------------------------------------------------------------------------------ FuriosaAI provides a containerized version of the `furiosa-mlperf` command. If you use the containerized version, you can run the `furiosa-mlperf` command without installing the FuriosaAI Software Stack on your host system or you can run the command on Kubernetes environment. To run the `furiosa-mlperf` container, you can use the following command: (Assumes model artifacts exist in `/opt/gpt-j-6b` directory) ``` $ docker run -it --rm --privileged \\ -v /opt/gpt-j-6b:gpt-j-6b furiosa-mlperf:2024.1.0 bash (container) # furiosa-mlperf gpt-j-offline /gpt-j-6b /gpt-j-offline-result ``` To run in a containerized environment, refer to the examples provided in this document. Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Quick Start with Furiosa LLM](furiosa_llm.html ""previous page"") [next Furiosa LLM](../furiosa_llm/intro.html ""next page"") Contents * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","If you are using Kubernetes, please refer to the following page for the recommended method: Cloud Native Toolkit.",single_hop_specifc_query_synthesizer
6,What OpenAI server do?,"['* [.rst](../_sources/furiosa_llm/furiosa-llm-serve.rst ""Download source file"") * .pdf OpenAI Compatible Server ======================== Contents -------- * [ Preparing Chat Templates](#preparing-chat-templates) * [ Launching the Server](#launching-the-server) + [ Arguments for the serve command](#arguments-for-the-serve-command) * [ Examples](#examples) + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds) + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [']","The OpenAI Compatible Server is designed to manage AI servers that are compatible with OpenAI APIs, facilitating enhanced client interaction.",single_hop_specifc_query_synthesizer
7,Wht is LLaMA-3.1-70B?,"['* [.rst](../_sources/furiosa_llm/furiosa-llm-serve.rst ""Download source file"") * .pdf OpenAI Compatible Server ======================== Contents -------- * [ Preparing Chat Templates](#preparing-chat-templates) * [ Launching the Server](#launching-the-server) + [ Arguments for the serve command](#arguments-for-the-serve-command) * [ Examples](#examples) + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds) + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [']",LLaMA-3.1-70B is mentioned in the context as part of examples related to launching a server with 4 RNGDs.,single_hop_specifc_query_synthesizer
8,"What FuriosaAI, Inc. do with OpenAI API?","['Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api) OpenAI Compatible Server [#](#openai-compatible-server ""Link to this heading"") ============================================================================== The `furiosa-llm` package includes an OpenAI-compatible server that can interact with OpenAI clients (and, of course, HTTP clients as well), supporting the `/v1/chat/completions` and `/v1/completions` APIs. In this section, we will explain how to launch the OpenAI-compatible furiosa-llm server. Tip You can learn more about the OpenAI API at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) . To launch the server, you must prepare: (1) the FuriosaAI LLM Engine artifact and (2) a chat template for the model. To download the FuriosaAI LLM Engine artifact, follow the instructions provided through our distribution channels or contact our sales team. To prepare the chat template, follow the instructions in the following section. Preparing Chat Templates [#](#preparing-chat-templates ""Link to this heading"") ------------------------------------------------------------------------------ Furiosa SDK 2024.1.0 (alpha) uses Transformers v4.31.0, which does not include a chat template. Therefore, to support `/v1/chat/completions` , you must provide a chat template yourself. This constraint will be removed in future releases. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. If you have access to the Llama repositories on Hugging Face, you can obtain the chat template for Llama using the following commands: ``` # Prerequisite: create a separate environment to install the latest Transformers version pip install ""transformers>=4.34.0"" python - <<EOF from transformers import AutoTokenizer tok = AutoTokenizer.from_pretrained(\'meta-llama/Meta-Llama-3.1-70B-Instruct\') with open(\'chat_template.tpl\', \'w\') as f: f.write(tok.chat_template) EOF ``` Launching the Server [#](#launching-the-server ""Link to this heading"") ---------------------------------------------------------------------- You can launch the server using the furiosa-llm serve command. ### Arguments for the serve command [#](#arguments-for-the-serve-command ""Link to this heading"") ``` usage: furiosa-llm serve [-h] --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} --artifact ARTIFACT [--host HOST] [--port PORT] --chat-template CHAT_TEMPLATE [--response-role RESPONSE_ROLE] [-pp PIPELINE_PARALLEL_SIZE] [-tp TENSOR_PARALLEL_SIZE] [--devices DEVICES] options: -h, --help show this help message and exit --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} The model to use. Currently only one model is supported per server. --artifact ARTIFACT Path to Furiosa LLM Engine artifact --host HOST Host to bind the server to --port PORT Port to bind the server to --chat-template CHAT_TEMPLATE Path to chat template file (must be a jinja2 template) --response-role RESPONSE_ROLE Response role for /v1/chat/completions API (default: \'assistant\') -pp PIPELINE_PARALLEL_SIZE, --pipeline-parallel-size PIPELINE_PARALLEL_SIZE Number of pipeline stages. -tp TENSOR_PARALLEL_SIZE, --tensor-parallel-size TENSOR_PARALLEL_SIZE Number of tensor parallel replicas. --devices DEVICES Devices to use (e.g. ""npu:0:*,npu:1:*""). If unspecified, all available devices from the host will be used. ``` Examples [#](#examples ""Link to this heading"") ---------------------------------------------- ### LLaMA-3.1-70B with 4 RNGDs [#](#llama-3-1-70b-with-4-rngds ""Link to this heading"") ``` furiosa-llm serve \\ --model {path to mlperf-llama-3-1-fp8-pp4} \\ -tp 4 -pp 4 --devices ""npu:0:*,npu:1:*,npu:2:*,npu:3:*"" \\ --chat-template {path to chat template} ``` ### LLaMA-3.1-8B with Single RNGD [#](#llama-3-1-8b-with-single-rngd ""Link to this heading"") ``` furiosa-llm serve \\ --model {path to mlperf-llama-3-1-8b-fp8} \\ -tp 4 -pp 1 --devices ""npu:0:*"" \\ # You may choose arbitrary device index, if multiple devices are on host --chat-template {path to chat template} ``` Using OpenAI Client [#](#using-openai-client ""Link to this heading"") -------------------------------------------------------------------- You can use two APIs: `client.chat.completions` and `client.completions` . You can also set `stream=True` to receive a streaming response. Tip You can install the OpenAI Python client using the following command: ``` pip install openai ``` ``` import openai HOST = ""localhost:8000"" openai.api_base = f""http://{HOST}/v1"" openai.api_key = ""0000"" stream_chat_completion = openai.ChatCompletion.create( model="""", messages=[ {""role"": ""system"", ""content"": ""You are a helpful assistant.""}, {""role"": ""user"", ""content"": ""What is the largest animal in the world?""}, ], stream=True, ) for completion in stream_chat_completion: content = completion.choices[0].delta.get(""content"") if content: print(content, end="""") ``` The compatibility with OpenAI API [#](#the-compatibility-with-openai-api ""Link to this heading"") ------------------------------------------------------------------------------------------------ Currently, `furiosa serve` supports the following OpenAI API parameters: You can find more about each parameter at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) . Warning Please note that using `use_beam_search` with `stream` is not allowed because the beam search cannot determine the tokens until the end of the sequence. In 2024.1 release, `n` works only for beam search and it will be fixed in the next release. * `n` * `temperature` * `top_p` * `top_k` * `early_stopping` * `length_penalty` * `max_tokens` * `min_tokens` * `use_beam_search` * `best_of` [previous Furiosa LLM](intro.html ""previous page"") [next References](references.html ""next page"") Contents * [Preparing Chat Templates](#preparing-chat-templates) * [Launching the Server](#launching-the-server) + [Arguments for the serve command](#arguments-for-the-serve-command) * [Examples](#examples) + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds) + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","FuriosaAI, Inc. provides the `furiosa-llm` package, which includes an OpenAI-compatible server that can interact with OpenAI clients, supporting the `/v1/chat/completions` and `/v1/completions` APIs.",single_hop_specifc_query_synthesizer
9,Wht r the compatibilty issues with Transformers v4.31.0 in Furiosa SDK 2024.1.0?,"['Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api) OpenAI Compatible Server [#](#openai-compatible-server ""Link to this heading"") ============================================================================== The `furiosa-llm` package includes an OpenAI-compatible server that can interact with OpenAI clients (and, of course, HTTP clients as well), supporting the `/v1/chat/completions` and `/v1/completions` APIs. In this section, we will explain how to launch the OpenAI-compatible furiosa-llm server. Tip You can learn more about the OpenAI API at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) . To launch the server, you must prepare: (1) the FuriosaAI LLM Engine artifact and (2) a chat template for the model. To download the FuriosaAI LLM Engine artifact, follow the instructions provided through our distribution channels or contact our sales team. To prepare the chat template, follow the instructions in the following section. Preparing Chat Templates [#](#preparing-chat-templates ""Link to this heading"") ------------------------------------------------------------------------------ Furiosa SDK 2024.1.0 (alpha) uses Transformers v4.31.0, which does not include a chat template. Therefore, to support `/v1/chat/completions` , you must provide a chat template yourself. This constraint will be removed in future releases. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. If you have access to the Llama repositories on Hugging Face, you can obtain the chat template for Llama using the following commands: ``` # Prerequisite: create a separate environment to install the latest Transformers version pip install ""transformers>=4.34.0"" python - <<EOF from transformers import AutoTokenizer tok = AutoTokenizer.from_pretrained(\'meta-llama/Meta-Llama-3.1-70B-Instruct\') with open(\'chat_template.tpl\', \'w\') as f: f.write(tok.chat_template) EOF ``` Launching the Server [#](#launching-the-server ""Link to this heading"") ---------------------------------------------------------------------- You can launch the server using the furiosa-llm serve command. ### Arguments for the serve command [#](#arguments-for-the-serve-command ""Link to this heading"") ``` usage: furiosa-llm serve [-h] --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} --artifact ARTIFACT [--host HOST] [--port PORT] --chat-template CHAT_TEMPLATE [--response-role RESPONSE_ROLE] [-pp PIPELINE_PARALLEL_SIZE] [-tp TENSOR_PARALLEL_SIZE] [--devices DEVICES] options: -h, --help show this help message and exit --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} The model to use. Currently only one model is supported per server. --artifact ARTIFACT Path to Furiosa LLM Engine artifact --host HOST Host to bind the server to --port PORT Port to bind the server to --chat-template CHAT_TEMPLATE Path to chat template file (must be a jinja2 template) --response-role RESPONSE_ROLE Response role for /v1/chat/completions API (default: \'assistant\') -pp PIPELINE_PARALLEL_SIZE, --pipeline-parallel-size PIPELINE_PARALLEL_SIZE Number of pipeline stages. -tp TENSOR_PARALLEL_SIZE, --tensor-parallel-size TENSOR_PARALLEL_SIZE Number of tensor parallel replicas. --devices DEVICES Devices to use (e.g. ""npu:0:*,npu:1:*""). If unspecified, all available devices from the host will be used. ``` Examples [#](#examples ""Link to this heading"") ---------------------------------------------- ### LLaMA-3.1-70B with 4 RNGDs [#](#llama-3-1-70b-with-4-rngds ""Link to this heading"") ``` furiosa-llm serve \\ --model {path to mlperf-llama-3-1-fp8-pp4} \\ -tp 4 -pp 4 --devices ""npu:0:*,npu:1:*,npu:2:*,npu:3:*"" \\ --chat-template {path to chat template} ``` ### LLaMA-3.1-8B with Single RNGD [#](#llama-3-1-8b-with-single-rngd ""Link to this heading"") ``` furiosa-llm serve \\ --model {path to mlperf-llama-3-1-8b-fp8} \\ -tp 4 -pp 1 --devices ""npu:0:*"" \\ # You may choose arbitrary device index, if multiple devices are on host --chat-template {path to chat template} ``` Using OpenAI Client [#](#using-openai-client ""Link to this heading"") -------------------------------------------------------------------- You can use two APIs: `client.chat.completions` and `client.completions` . You can also set `stream=True` to receive a streaming response. Tip You can install the OpenAI Python client using the following command: ``` pip install openai ``` ``` import openai HOST = ""localhost:8000"" openai.api_base = f""http://{HOST}/v1"" openai.api_key = ""0000"" stream_chat_completion = openai.ChatCompletion.create( model="""", messages=[ {""role"": ""system"", ""content"": ""You are a helpful assistant.""}, {""role"": ""user"", ""content"": ""What is the largest animal in the world?""}, ], stream=True, ) for completion in stream_chat_completion: content = completion.choices[0].delta.get(""content"") if content: print(content, end="""") ``` The compatibility with OpenAI API [#](#the-compatibility-with-openai-api ""Link to this heading"") ------------------------------------------------------------------------------------------------ Currently, `furiosa serve` supports the following OpenAI API parameters: You can find more about each parameter at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) . Warning Please note that using `use_beam_search` with `stream` is not allowed because the beam search cannot determine the tokens until the end of the sequence. In 2024.1 release, `n` works only for beam search and it will be fixed in the next release. * `n` * `temperature` * `top_p` * `top_k` * `early_stopping` * `length_penalty` * `max_tokens` * `min_tokens` * `use_beam_search` * `best_of` [previous Furiosa LLM](intro.html ""previous page"") [next References](references.html ""next page"") Contents * [Preparing Chat Templates](#preparing-chat-templates) * [Launching the Server](#launching-the-server) + [Arguments for the serve command](#arguments-for-the-serve-command) * [Examples](#examples) + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds) + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","Transformers v4.31.0 does not include a chat template, which is necessary to support the `/v1/chat/completions` API in Furiosa SDK 2024.1.0. Therefore, users must provide a chat template themselves.",single_hop_specifc_query_synthesizer
10,What is the role of the Furiosa Model Compressor in the software stack?,"['* [.rst](../_sources/overview/software_stack.rst ""Download source file"") * .pdf FuriosaAI’s Software Stack ========================== Contents -------- * [ Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [ Furiosa Compiler](#furiosa-compiler) * [ Furiosa Runtime](#furiosa-runtime) * [ Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [']","The Furiosa Model Compressor, also known as the Quantizer, is a component of FuriosaAI's software stack.",single_hop_specifc_query_synthesizer
11,What is the Furiosa Compiler?,"['* [.rst](../_sources/overview/software_stack.rst ""Download source file"") * .pdf FuriosaAI’s Software Stack ========================== Contents -------- * [ Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [ Furiosa Compiler](#furiosa-compiler) * [ Furiosa Runtime](#furiosa-runtime) * [ Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [']",The Furiosa Compiler is a component of FuriosaAI’s Software Stack.,single_hop_specifc_query_synthesizer
12,What role does the firmware play in the RNGD card within FuriosaAI's software stack?,"['Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) FuriosaAI’s Software Stack [#](#furiosaai-s-software-stack ""Link to this heading"") ================================================================================== FuriosaAI provides the streamlined software stack to allow FuriosaAI NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials. The above diagram demonstrates the SW stack provided by FuriosaAI, by layers. The following outlines the key components. Kernel Driver, Firmware, and PE Runtime [#](#kernel-driver-firmware-and-pe-runtime ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- The kernel device driver enables the Linux operating system to recognize NPU devices and expose them as Linux device files. The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE). PERT is responsible for communicating with the host’s runtime and scheduling, managing the resources of PEs to execute NPU tasks. Furiosa Compiler [#](#furiosa-compiler ""Link to this heading"") -------------------------------------------------------------- Furiosa compiler analyzes, optimizes a model graph, and generates a NPU executable program. The operation passes involve graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. When `torch.compile()` backend, `FuriosaBackend` is used or `furiosa-llm` is used, the Furiosa Compiler is transparently used to generate NPU executable programs for Runtime. Furiosa Runtime [#](#furiosa-runtime ""Link to this heading"") ------------------------------------------------------------ Runtime loads multiple executable NPU programs generated by Furiosa compiler, and run them on the NPU. A single model can be compiled into multiple executable programs according to model architectures and applications. Runtime is responsible for scheduling NPU programs and managing computation and memory resource on NPUs and CPUs. Also, Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs. Furiosa Model Compressor (Quantizer) [#](#furiosa-model-compressor-quantizer ""Link to this heading"") ---------------------------------------------------------------------------------------------------- Furiosa Model Compressor is a library as well as toolkit for model calibration and quantization. Model quantization is a powerful technique to reduce memory footprint, computation cost, inference latency and power consumption. Furiosa Model Compressor provides post-training quantization methods, such as * BF16 (W16A16) * INT8 Weight-Only (W8A16) * FP8 (W8A8) * INT8 SmoothQuant (W8A8) * INT4 Weight-Only (W4A16 AWQ / GPTQ) (Planned in release 2024.2) Furiosa LLM [#](#furiosa-llm ""Link to this heading"") ---------------------------------------------------- Furiosa LLM provides a high-performance inference engine for LLM models, such as Llama 3.1 70B, 8B, GPT-J, and Bert. Furiosa LLM is designed to provide the state-of-the-art serving optimization for LLM models. The key features of Furiosa LLM include vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and OpenAI-compatible API server. You can find further information at [Furiosa LLM](../furiosa_llm/intro.html#furiosallm) . Kubernetes Support [#](#kubernetes-support ""Link to this heading"") ------------------------------------------------------------------ Kubernetes, an open-source platform designed to manage containerized applications and services, is extensively adopted by various companies due to its robust capabilities for deploying, scaling, and automating containerized workloads. FuriosaAI software stack also offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment. FuriosaAI’s device plugin enables Kubernetes clusters to recognize FuriosaAI’s NPUs and allows NPUs to be scheduled for workloads and services that require them. This feature allows users to easily deploy AI workloads with FuriosaAI NPUs on Kubernetes, enabling efficient resource utilization and scaling. You can find more information about Kubernetes support in the [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) . [previous FuriosaAI RNGD](rngd.html ""previous page"") [next Supported Models](supported_models.html ""next page"") Contents * [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']",The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE).,single_hop_specifc_query_synthesizer
13,What is the role of the Furiosa Model Compressor in optimizing models?,"['Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) FuriosaAI’s Software Stack [#](#furiosaai-s-software-stack ""Link to this heading"") ================================================================================== FuriosaAI provides the streamlined software stack to allow FuriosaAI NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials. The above diagram demonstrates the SW stack provided by FuriosaAI, by layers. The following outlines the key components. Kernel Driver, Firmware, and PE Runtime [#](#kernel-driver-firmware-and-pe-runtime ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- The kernel device driver enables the Linux operating system to recognize NPU devices and expose them as Linux device files. The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE). PERT is responsible for communicating with the host’s runtime and scheduling, managing the resources of PEs to execute NPU tasks. Furiosa Compiler [#](#furiosa-compiler ""Link to this heading"") -------------------------------------------------------------- Furiosa compiler analyzes, optimizes a model graph, and generates a NPU executable program. The operation passes involve graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. When `torch.compile()` backend, `FuriosaBackend` is used or `furiosa-llm` is used, the Furiosa Compiler is transparently used to generate NPU executable programs for Runtime. Furiosa Runtime [#](#furiosa-runtime ""Link to this heading"") ------------------------------------------------------------ Runtime loads multiple executable NPU programs generated by Furiosa compiler, and run them on the NPU. A single model can be compiled into multiple executable programs according to model architectures and applications. Runtime is responsible for scheduling NPU programs and managing computation and memory resource on NPUs and CPUs. Also, Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs. Furiosa Model Compressor (Quantizer) [#](#furiosa-model-compressor-quantizer ""Link to this heading"") ---------------------------------------------------------------------------------------------------- Furiosa Model Compressor is a library as well as toolkit for model calibration and quantization. Model quantization is a powerful technique to reduce memory footprint, computation cost, inference latency and power consumption. Furiosa Model Compressor provides post-training quantization methods, such as * BF16 (W16A16) * INT8 Weight-Only (W8A16) * FP8 (W8A8) * INT8 SmoothQuant (W8A8) * INT4 Weight-Only (W4A16 AWQ / GPTQ) (Planned in release 2024.2) Furiosa LLM [#](#furiosa-llm ""Link to this heading"") ---------------------------------------------------- Furiosa LLM provides a high-performance inference engine for LLM models, such as Llama 3.1 70B, 8B, GPT-J, and Bert. Furiosa LLM is designed to provide the state-of-the-art serving optimization for LLM models. The key features of Furiosa LLM include vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and OpenAI-compatible API server. You can find further information at [Furiosa LLM](../furiosa_llm/intro.html#furiosallm) . Kubernetes Support [#](#kubernetes-support ""Link to this heading"") ------------------------------------------------------------------ Kubernetes, an open-source platform designed to manage containerized applications and services, is extensively adopted by various companies due to its robust capabilities for deploying, scaling, and automating containerized workloads. FuriosaAI software stack also offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment. FuriosaAI’s device plugin enables Kubernetes clusters to recognize FuriosaAI’s NPUs and allows NPUs to be scheduled for workloads and services that require them. This feature allows users to easily deploy AI workloads with FuriosaAI NPUs on Kubernetes, enabling efficient resource utilization and scaling. You can find more information about Kubernetes support in the [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) . [previous FuriosaAI RNGD](rngd.html ""previous page"") [next Supported Models](supported_models.html ""next page"") Contents * [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","Furiosa Model Compressor is a library and toolkit for model calibration and quantization, which reduces memory footprint, computation cost, inference latency, and power consumption. It provides post-training quantization methods such as BF16, INT8 Weight-Only, FP8, and INT8 SmoothQuant.",single_hop_specifc_query_synthesizer
14,Wht is the purpse of BucketConfig in the LLM class?,"['Contents -------- * [`LLM`](#furiosa_llm.LLM) + [`LLM.generate()`](#furiosa_llm.LLM.generate) + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms) LLM class [#](#llm-class ""Link to this heading"") ================================================ *class* furiosa\\_llm. LLM ( *pretrained\\_id : str* , *task\\_type : str | None = None* , *llm\\_config : LLMConfig | None = None* , *qformat\\_path : PathLike | None = None* , *qparam\\_path : PathLike | None = None* , *prefill\\_quant\\_bin\\_path : PathLike | None = None* , *decode\\_quant\\_bin\\_path : PathLike | None = None* , *config : Dict [ str , Any ] = {}* , *bucket\\_config : BucketConfig | None = None* , *max\\_seq\\_len\\_to\\_capture : int = 2048* , *tensor\\_parallel\\_size : int = 4* , *pipeline\\_parallel\\_size : int = 1* , *data\\_parallel\\_size : int | None = None* , *tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast | None = None* , *tokenizer\\_mode : Literal [ \'auto\' , \'slow\' ] = \'auto\'* , *seed : int | None = None* , *devices : str | Sequence [ Device ] | None = None* , *param\\_file\\_path : PathLike | None = None* , *param\\_saved\\_format : Literal [ \'safetensors\' , \'pt\' ] = \'safetensors\'* , *do\\_decompositions\\_for\\_model\\_rewrite : bool = False* , *comp\\_supertask\\_kind : Literal [ \'edf\' , \'dfg\' , \'fx\' ] | None = None* , *cache\\_dir : PathLike | None = PosixPath(\'/home/hyunsik/.cache/furiosa/llm\')* , *backend : LLMBackend | None = None* , *use\\_blockwise\\_compile : bool = True* , *num\\_blocks\\_per\\_supertask : int = 1* , *embed\\_all\\_constants\\_into\\_graph : bool = False* , *paged\\_attention\\_num\\_blocks : int | None = None* , *paged\\_attention\\_block\\_size : int = 1* , *kv\\_cache\\_sharing\\_across\\_beams\\_config : KvCacheSharingAcrossBeamsConfig | None = None* , *scheduler\\_config : SchedulerConfig = SchedulerConfig(npu\\_queue\\_limit=2, max\\_processing\\_samples=65536, spare\\_blocks\\_ratio=0.2, is\\_offline=False)* , *packing\\_type : Literal [ \'IDENTITY\' ] = \'IDENTITY\'* , *compiler\\_config\\_overrides : Mapping | None = None* , *use\\_random\\_weight : bool = False* , *num\\_pipeline\\_builder\\_workers : int = 1* , *num\\_compile\\_workers : int = 1* , *skip\\_engine : bool = False* , *\\** , *\\_cleanup : bool = True* , *\\*\\* kwargs* ) [[source]](../../_modules/furiosa_llm/api.html#LLM) [#](#furiosa_llm.LLM ""Link to this definition"") Bases: `object` An LLM for generating texts from given prompts and sampling parameters. Parameters : * **pretrained\\_id** – The name of the pretrained model. This corresponds to pretrained\\_model\\_name\\_or\\_path in HuggingFace Transformers. * **task\\_type** – The type of the task. This corresponds to task in HuggingFace Transformers. See <https://huggingface.co/docs/transformers/main/en/quicktour#pipeline> for more details. * **llm\\_config** – The configuration for the LLM. This includes quantization and optimization configurations. * **qformat\\_path** – The path to the quantization format file. * **qparam\\_path** – The path to the quantization parameter file. * **prefill\\_quant\\_bin\\_path** – The path to the quantziation prefill bin file. * **decode\\_quant\\_bin\\_path** – The path to the quantziation decode bin file. * **config** – The configuration for the HuggingFace Transformers model. This is a dictionary that includes the configuration for the model. * **bucket\\_config** – Config for bucket generating policy. If not given, the model will use single one batch, max\\_seq\\_len\\_to\\_capture attention size bucket per each phase. * **max\\_seq\\_len\\_to\\_capture** – Maximum sequence length covered by LLM engine. Sequence with larger context than this will not be covered. The default is 2048. * **tensor\\_parallel\\_size** – The number of PEs for each tensor parallelism group. The default is 4. * **pipeline\\_parallel\\_size** – The number of pipeline stages for pipeline parallelism. The default is 1, which means no pipeline parallelism. * **data\\_parallel\\_size** – The size of the data parallelism group. If not given, it will be inferred from total avaialble PEs and other parallelism degrees. * **tokenizer** – The name or path of a HuggingFace Transformers tokenizer. * **tokenizer\\_mode** – The tokenizer mode. “auto” will use the fast tokenizer if available, and “slow” will always use the slow tokenizer. * **seed** – The seed to initialize the random number generator for sampling. * **devices** – The devices to run the model. It can be a single device or a list of devices. Each device can be either “cpu:X” or “cuda:X” where X is a specific device index. The default is “cpu:0”. * **param\\_file\\_path** – The path to the parameter file to use for pipeline generation. If not specified, the parameters will be saved in a temporary file which will be used for pipeline generation. * **param\\_saved\\_format** – The format of the parameter file. Only possible value is “safetensors” now. The default is “safetensors”. * **do\\_decompositions\\_for\\_model\\_rewrite** – Whether to decompose some ops to describe various parallelism strategies with mppp config. When the value is True, mppp config that matches with the decomposed FX graph should be given. * **comp\\_supertask\\_kind** – The format that pipeline’s supertask will be represented as. Possible values are “fx”,”dfg”, and “edf”, and the default is “fx”. * **cache\\_dir** – The cache directory for all generated files for this LLM instance. When its value is `None` , caching is disabled. The default is “$HOME/.cache/furiosa/llm”. * **backend** – The backend implementation to run forward() of a model for the LLM. The default is LLMBackend.TORCH\\_PIPELINE\\_RUNNER. * **use\\_blockwise\\_compile** – If True, each task will be compiled in the unit of transformer block, and compilation result for transformer block is generated once and reused. * **num\\_blocks\\_per\\_supertask** – The number of transformer blocks that will be merged into one supertask. This option is valid only when use\\_blockwise\\_compile=True . The default is 1. * **embed\\_all\\_constants\\_into\\_graph** – Whether to embed constant tensors into graph or make them as input of the graph and save them as separate files. * **paged\\_attention\\_num\\_blocks** – The maximum number of blocks that each k/v storage per layer can store. This argument must be given if model uses paged attention. * **paged\\_attention\\_block\\_size** – The maximum number of tokens that can be stored in a single paged attention block. This argument must be given if model uses paged attention. * **kv\\_cache\\_sharing\\_across\\_beams\\_config** – Configuration for sharing kv cache across beams. This argument must be given if and only if the model is optimized to share kv cache across beams. If this argument is given, decode phase buckets with batch size of `batch_size` \\* `kv_cache_sharing_across_beams_config.beam_width` will be created. * **scheduler\\_config** – Configuration for the scheduler, allowing to maximum number of tasks which can be queued to HW, maximum number']","BucketConfig is used for configuring the bucket generating policy. If not provided, the model will use a single batch with a maximum sequence length to capture attention size bucket per each phase.",single_hop_specifc_query_synthesizer
15,what SchedulerConfig do?,"['Contents -------- * [`LLM`](#furiosa_llm.LLM) + [`LLM.generate()`](#furiosa_llm.LLM.generate) + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms) LLM class [#](#llm-class ""Link to this heading"") ================================================ *class* furiosa\\_llm. LLM ( *pretrained\\_id : str* , *task\\_type : str | None = None* , *llm\\_config : LLMConfig | None = None* , *qformat\\_path : PathLike | None = None* , *qparam\\_path : PathLike | None = None* , *prefill\\_quant\\_bin\\_path : PathLike | None = None* , *decode\\_quant\\_bin\\_path : PathLike | None = None* , *config : Dict [ str , Any ] = {}* , *bucket\\_config : BucketConfig | None = None* , *max\\_seq\\_len\\_to\\_capture : int = 2048* , *tensor\\_parallel\\_size : int = 4* , *pipeline\\_parallel\\_size : int = 1* , *data\\_parallel\\_size : int | None = None* , *tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast | None = None* , *tokenizer\\_mode : Literal [ \'auto\' , \'slow\' ] = \'auto\'* , *seed : int | None = None* , *devices : str | Sequence [ Device ] | None = None* , *param\\_file\\_path : PathLike | None = None* , *param\\_saved\\_format : Literal [ \'safetensors\' , \'pt\' ] = \'safetensors\'* , *do\\_decompositions\\_for\\_model\\_rewrite : bool = False* , *comp\\_supertask\\_kind : Literal [ \'edf\' , \'dfg\' , \'fx\' ] | None = None* , *cache\\_dir : PathLike | None = PosixPath(\'/home/hyunsik/.cache/furiosa/llm\')* , *backend : LLMBackend | None = None* , *use\\_blockwise\\_compile : bool = True* , *num\\_blocks\\_per\\_supertask : int = 1* , *embed\\_all\\_constants\\_into\\_graph : bool = False* , *paged\\_attention\\_num\\_blocks : int | None = None* , *paged\\_attention\\_block\\_size : int = 1* , *kv\\_cache\\_sharing\\_across\\_beams\\_config : KvCacheSharingAcrossBeamsConfig | None = None* , *scheduler\\_config : SchedulerConfig = SchedulerConfig(npu\\_queue\\_limit=2, max\\_processing\\_samples=65536, spare\\_blocks\\_ratio=0.2, is\\_offline=False)* , *packing\\_type : Literal [ \'IDENTITY\' ] = \'IDENTITY\'* , *compiler\\_config\\_overrides : Mapping | None = None* , *use\\_random\\_weight : bool = False* , *num\\_pipeline\\_builder\\_workers : int = 1* , *num\\_compile\\_workers : int = 1* , *skip\\_engine : bool = False* , *\\** , *\\_cleanup : bool = True* , *\\*\\* kwargs* ) [[source]](../../_modules/furiosa_llm/api.html#LLM) [#](#furiosa_llm.LLM ""Link to this definition"") Bases: `object` An LLM for generating texts from given prompts and sampling parameters. Parameters : * **pretrained\\_id** – The name of the pretrained model. This corresponds to pretrained\\_model\\_name\\_or\\_path in HuggingFace Transformers. * **task\\_type** – The type of the task. This corresponds to task in HuggingFace Transformers. See <https://huggingface.co/docs/transformers/main/en/quicktour#pipeline> for more details. * **llm\\_config** – The configuration for the LLM. This includes quantization and optimization configurations. * **qformat\\_path** – The path to the quantization format file. * **qparam\\_path** – The path to the quantization parameter file. * **prefill\\_quant\\_bin\\_path** – The path to the quantziation prefill bin file. * **decode\\_quant\\_bin\\_path** – The path to the quantziation decode bin file. * **config** – The configuration for the HuggingFace Transformers model. This is a dictionary that includes the configuration for the model. * **bucket\\_config** – Config for bucket generating policy. If not given, the model will use single one batch, max\\_seq\\_len\\_to\\_capture attention size bucket per each phase. * **max\\_seq\\_len\\_to\\_capture** – Maximum sequence length covered by LLM engine. Sequence with larger context than this will not be covered. The default is 2048. * **tensor\\_parallel\\_size** – The number of PEs for each tensor parallelism group. The default is 4. * **pipeline\\_parallel\\_size** – The number of pipeline stages for pipeline parallelism. The default is 1, which means no pipeline parallelism. * **data\\_parallel\\_size** – The size of the data parallelism group. If not given, it will be inferred from total avaialble PEs and other parallelism degrees. * **tokenizer** – The name or path of a HuggingFace Transformers tokenizer. * **tokenizer\\_mode** – The tokenizer mode. “auto” will use the fast tokenizer if available, and “slow” will always use the slow tokenizer. * **seed** – The seed to initialize the random number generator for sampling. * **devices** – The devices to run the model. It can be a single device or a list of devices. Each device can be either “cpu:X” or “cuda:X” where X is a specific device index. The default is “cpu:0”. * **param\\_file\\_path** – The path to the parameter file to use for pipeline generation. If not specified, the parameters will be saved in a temporary file which will be used for pipeline generation. * **param\\_saved\\_format** – The format of the parameter file. Only possible value is “safetensors” now. The default is “safetensors”. * **do\\_decompositions\\_for\\_model\\_rewrite** – Whether to decompose some ops to describe various parallelism strategies with mppp config. When the value is True, mppp config that matches with the decomposed FX graph should be given. * **comp\\_supertask\\_kind** – The format that pipeline’s supertask will be represented as. Possible values are “fx”,”dfg”, and “edf”, and the default is “fx”. * **cache\\_dir** – The cache directory for all generated files for this LLM instance. When its value is `None` , caching is disabled. The default is “$HOME/.cache/furiosa/llm”. * **backend** – The backend implementation to run forward() of a model for the LLM. The default is LLMBackend.TORCH\\_PIPELINE\\_RUNNER. * **use\\_blockwise\\_compile** – If True, each task will be compiled in the unit of transformer block, and compilation result for transformer block is generated once and reused. * **num\\_blocks\\_per\\_supertask** – The number of transformer blocks that will be merged into one supertask. This option is valid only when use\\_blockwise\\_compile=True . The default is 1. * **embed\\_all\\_constants\\_into\\_graph** – Whether to embed constant tensors into graph or make them as input of the graph and save them as separate files. * **paged\\_attention\\_num\\_blocks** – The maximum number of blocks that each k/v storage per layer can store. This argument must be given if model uses paged attention. * **paged\\_attention\\_block\\_size** – The maximum number of tokens that can be stored in a single paged attention block. This argument must be given if model uses paged attention. * **kv\\_cache\\_sharing\\_across\\_beams\\_config** – Configuration for sharing kv cache across beams. This argument must be given if and only if the model is optimized to share kv cache across beams. If this argument is given, decode phase buckets with batch size of `batch_size` \\* `kv_cache_sharing_across_beams_config.beam_width` will be created. * **scheduler\\_config** – Configuration for the scheduler, allowing to maximum number of tasks which can be queued to HW, maximum number']","SchedulerConfig is a configuration for the scheduler, allowing to set the maximum number of tasks which can be queued to HW, maximum number of processing samples, spare blocks ratio, and whether it is offline.",single_hop_specifc_query_synthesizer
16,"Cud yu pleese explane wut FuriosaAI, Inc. duz in the kontekt of LLM class and its funktionalities?","['* [.rst](../../_sources/furiosa_llm/references/llm.rst ""Download source file"") * .pdf LLM class ========= Contents -------- * [`LLM`](#furiosa_llm.LLM) + [`LLM.generate()`](#furiosa_llm.LLM.generate) + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms) LLM class [#](#llm-class ""Link to this heading"") ================================================ *class* furiosa\\_llm. LLM ( *pretrained\\_id : str* , *task\\_type : str | None = None* , *llm\\_config : LLMConfig | None = None* , *qformat\\_path : PathLike | None = None* , *qparam\\_path : PathLike | None = None* , *prefill\\_quant\\_bin\\_path : PathLike | None = None* , *decode\\_quant\\_bin\\_path : PathLike | None = None* , *config : Dict [ str , Any ] = {}* , *bucket\\_config : BucketConfig | None = None* , *max\\_seq\\_len\\_to\\_capture : int = 2048* , *tensor\\_parallel\\_size : int = 4* , *pipeline\\_parallel\\_size : int = 1* , *data\\_parallel\\_size : int | None = None* , *tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast | None = None* , *tokenizer\\_mode : Literal [ \'auto\' , \'slow\' ] = \'auto\'* , *seed : int | None = None* , *devices : str | Sequence [ Device ] | None = None* , *param\\_file\\_path : PathLike | None = None* , *param\\_saved\\_format : Literal [ \'safetensors\' , \'pt\' ] = \'safetensors\'* , *do\\_decompositions\\_for\\_model\\_rewrite : bool = False* , *comp\\_supertask\\_kind : Literal [ \'edf\' , \'dfg\' , \'fx\' ] | None = None* , *cache\\_dir : PathLike | None = PosixPath(\'/home/hyunsik/.cache/furiosa/llm\')* , *backend : LLMBackend | None = None* , *use\\_blockwise\\_compile : bool = True* , *num\\_blocks\\_per\\_supertask : int = 1* , *embed\\_all\\_constants\\_into\\_graph : bool = False* , *paged\\_attention\\_num\\_blocks : int | None = None* , *paged\\_attention\\_block\\_size : int = 1* , *kv\\_cache\\_sharing\\_across\\_beams\\_config : KvCacheSharingAcrossBeamsConfig | None = None* , *scheduler\\_config : SchedulerConfig = SchedulerConfig(npu\\_queue\\_limit=2, max\\_processing\\_samples=65536, spare\\_blocks\\_ratio=0.2, is\\_offline=False)* , *packing\\_type : Literal [ \'IDENTITY\' ] = \'IDENTITY\'* , *compiler\\_config\\_overrides : Mapping | None = None* , *use\\_random\\_weight : bool = False* , *num\\_pipeline\\_builder\\_workers : int = 1* , *num\\_compile\\_workers : int = 1* , *skip\\_engine : bool = False* , *\\** , *\\_cleanup : bool = True* , *\\*\\* kwargs* ) [[source]](../../_modules/furiosa_llm/api.html#LLM) [#](#furiosa_llm.LLM ""Link to this definition"") Bases: `object` An LLM for generating texts from given prompts and sampling parameters.  generate()`](#furiosa_llm.LLM.generate) + [`LLM.  of samples that can be processed by the scheduler, and ratio of spare blocks that are reserved by scheduler. * **packing\\_type** – Packing algorithm. Possible values are “IDENTITY” only for now * **compiler\\_config\\_overrides** – Overrides for the compiler config. This is a dictionary that includes the configuration for the compiler. * **use\\_random\\_weight** – If True, the model will be initialized with random weights. * **num\\_pipeline\\_builder\\_workers** – number of workers used for building pipelines (except for compilation). The default is 1 (no parallelism). Setting this value larger than 1 reduces pipeline building time, especially for large models, but requires much more memory. * **num\\_compile\\_workers** – number of workers used for compilation. The default is 1 (no parallelism). * **skip\\_engine** – If True, the native runtime engine will not be initialized. This is useful when you need the pipelines for other purposes than running them with the engine. generate ( *prompts: str | ~typing.List[str], sampling\\_params: ~furiosa\\_llm.sampling\\_params.SamplingParams = SamplingParams(n=1, best\\_of=1, temperature=1.0, top\\_p=1.0, top\\_k=-1, use\\_beam\\_search=False, length\\_penalty=1.0, early\\_stopping=False, max\\_tokens=16min\\_tokens=0, , prompt\\_token\\_ids: ~typing.List[int] | ~typing.List[~typing.List[int]] | None = None* ) → RequestOutput | List [ RequestOutput ] [[source]](../../_modules/furiosa_llm/api.html#LLM.generate) [#](#furiosa_llm.LLM.generate ""Link to this definition"") Generate texts from given prompts and sampling parameters. Parameters : * **prompts** – The prompts to generate texts. * **sampling\\_params** – The sampling parameters for generating texts. * **prompt\\_token\\_ids** – The token ids of the prompts. If not given, the token ids are generated from the prompts using the tokenizer. Returns : A list of RequestOutput objects containing the generated completions in the same order as the input prompts. get\\_splitted\\_gms ( *get\\_input\\_constants : bool = False* ) → Dict [ str , Tuple [ GraphModule , ... ] | Tuple [ Tuple [ GraphModule , Tuple [ Tensor | None , ... ] ] , ... ] ] [[source]](../../_modules/furiosa_llm/api.html#LLM.get_splitted_gms) [#](#furiosa_llm.LLM.get_splitted_gms ""Link to this definition"") Get sub GraphModules for each pipeline. Returns : Dictionary whose key is the pipeline name and value is the tuple containing `GraphModule``s (computation supertasks) and some additional information if necessary. if ``get_input_constants==False` , each value is just a tuple of `GraphModule``s in the pipeline. Otherwise, each value is a tuple whose element is ``GraphModule` in the pipeline and list of input constant tensors, which were originally constant tensors, but converted to input. The list of input constant tensors has same length as corresponding `GraphModule` ’s number of inputs with each element exactly corresponding to the input of the `GraphModule` with same index, but elements with original input tensor indexes are `None` . Return type : Dict[str, Union[Tuple[GraphModule, …], Tuple[Tuple[GraphModule, Tuple[Optional[torch.Tensor], …]], …],],] [previous References](../references.html ""previous page"") [next SamplingParams](sampling_params.html ""next page"") Contents * [`LLM`](#furiosa_llm.LLM) + [`LLM.generate()`](#furiosa_llm.LLM.generate) + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","FuriosaAI, Inc. provides an LLM class for generating texts from given prompts and sampling parameters. The LLM class includes functionalities such as generating text using the `generate()` method and obtaining sub GraphModules for each pipeline using the `get_splitted_gms()` method.",single_hop_specifc_query_synthesizer
17,"Could you provide a detailed explanation of the functionalities and configurations available in the LLM class developed by FuriosaAI, Inc.?","['* [.rst](../../_sources/furiosa_llm/references/llm.rst ""Download source file"") * .pdf LLM class ========= Contents -------- * [`LLM`](#furiosa_llm.LLM) + [`LLM.generate()`](#furiosa_llm.LLM.generate) + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms) LLM class [#](#llm-class ""Link to this heading"") ================================================ *class* furiosa\\_llm. LLM ( *pretrained\\_id : str* , *task\\_type : str | None = None* , *llm\\_config : LLMConfig | None = None* , *qformat\\_path : PathLike | None = None* , *qparam\\_path : PathLike | None = None* , *prefill\\_quant\\_bin\\_path : PathLike | None = None* , *decode\\_quant\\_bin\\_path : PathLike | None = None* , *config : Dict [ str , Any ] = {}* , *bucket\\_config : BucketConfig | None = None* , *max\\_seq\\_len\\_to\\_capture : int = 2048* , *tensor\\_parallel\\_size : int = 4* , *pipeline\\_parallel\\_size : int = 1* , *data\\_parallel\\_size : int | None = None* , *tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast | None = None* , *tokenizer\\_mode : Literal [ \'auto\' , \'slow\' ] = \'auto\'* , *seed : int | None = None* , *devices : str | Sequence [ Device ] | None = None* , *param\\_file\\_path : PathLike | None = None* , *param\\_saved\\_format : Literal [ \'safetensors\' , \'pt\' ] = \'safetensors\'* , *do\\_decompositions\\_for\\_model\\_rewrite : bool = False* , *comp\\_supertask\\_kind : Literal [ \'edf\' , \'dfg\' , \'fx\' ] | None = None* , *cache\\_dir : PathLike | None = PosixPath(\'/home/hyunsik/.cache/furiosa/llm\')* , *backend : LLMBackend | None = None* , *use\\_blockwise\\_compile : bool = True* , *num\\_blocks\\_per\\_supertask : int = 1* , *embed\\_all\\_constants\\_into\\_graph : bool = False* , *paged\\_attention\\_num\\_blocks : int | None = None* , *paged\\_attention\\_block\\_size : int = 1* , *kv\\_cache\\_sharing\\_across\\_beams\\_config : KvCacheSharingAcrossBeamsConfig | None = None* , *scheduler\\_config : SchedulerConfig = SchedulerConfig(npu\\_queue\\_limit=2, max\\_processing\\_samples=65536, spare\\_blocks\\_ratio=0.2, is\\_offline=False)* , *packing\\_type : Literal [ \'IDENTITY\' ] = \'IDENTITY\'* , *compiler\\_config\\_overrides : Mapping | None = None* , *use\\_random\\_weight : bool = False* , *num\\_pipeline\\_builder\\_workers : int = 1* , *num\\_compile\\_workers : int = 1* , *skip\\_engine : bool = False* , *\\** , *\\_cleanup : bool = True* , *\\*\\* kwargs* ) [[source]](../../_modules/furiosa_llm/api.html#LLM) [#](#furiosa_llm.LLM ""Link to this definition"") Bases: `object` An LLM for generating texts from given prompts and sampling parameters.  generate()`](#furiosa_llm.LLM.generate) + [`LLM.  of samples that can be processed by the scheduler, and ratio of spare blocks that are reserved by scheduler. * **packing\\_type** – Packing algorithm. Possible values are “IDENTITY” only for now * **compiler\\_config\\_overrides** – Overrides for the compiler config. This is a dictionary that includes the configuration for the compiler. * **use\\_random\\_weight** – If True, the model will be initialized with random weights. * **num\\_pipeline\\_builder\\_workers** – number of workers used for building pipelines (except for compilation). The default is 1 (no parallelism). Setting this value larger than 1 reduces pipeline building time, especially for large models, but requires much more memory. * **num\\_compile\\_workers** – number of workers used for compilation. The default is 1 (no parallelism). * **skip\\_engine** – If True, the native runtime engine will not be initialized. This is useful when you need the pipelines for other purposes than running them with the engine. generate ( *prompts: str | ~typing.List[str], sampling\\_params: ~furiosa\\_llm.sampling\\_params.SamplingParams = SamplingParams(n=1, best\\_of=1, temperature=1.0, top\\_p=1.0, top\\_k=-1, use\\_beam\\_search=False, length\\_penalty=1.0, early\\_stopping=False, max\\_tokens=16min\\_tokens=0, , prompt\\_token\\_ids: ~typing.List[int] | ~typing.List[~typing.List[int]] | None = None* ) → RequestOutput | List [ RequestOutput ] [[source]](../../_modules/furiosa_llm/api.html#LLM.generate) [#](#furiosa_llm.LLM.generate ""Link to this definition"") Generate texts from given prompts and sampling parameters. Parameters : * **prompts** – The prompts to generate texts. * **sampling\\_params** – The sampling parameters for generating texts. * **prompt\\_token\\_ids** – The token ids of the prompts. If not given, the token ids are generated from the prompts using the tokenizer. Returns : A list of RequestOutput objects containing the generated completions in the same order as the input prompts. get\\_splitted\\_gms ( *get\\_input\\_constants : bool = False* ) → Dict [ str , Tuple [ GraphModule , ... ] | Tuple [ Tuple [ GraphModule , Tuple [ Tensor | None , ... ] ] , ... ] ] [[source]](../../_modules/furiosa_llm/api.html#LLM.get_splitted_gms) [#](#furiosa_llm.LLM.get_splitted_gms ""Link to this definition"") Get sub GraphModules for each pipeline. Returns : Dictionary whose key is the pipeline name and value is the tuple containing `GraphModule``s (computation supertasks) and some additional information if necessary. if ``get_input_constants==False` , each value is just a tuple of `GraphModule``s in the pipeline. Otherwise, each value is a tuple whose element is ``GraphModule` in the pipeline and list of input constant tensors, which were originally constant tensors, but converted to input. The list of input constant tensors has same length as corresponding `GraphModule` ’s number of inputs with each element exactly corresponding to the input of the `GraphModule` with same index, but elements with original input tensor indexes are `None` . Return type : Dict[str, Union[Tuple[GraphModule, …], Tuple[Tuple[GraphModule, Tuple[Optional[torch.Tensor], …]], …],],] [previous References](../references.html ""previous page"") [next SamplingParams](sampling_params.html ""next page"") Contents * [`LLM`](#furiosa_llm.LLM) + [`LLM.generate()`](#furiosa_llm.LLM.generate) + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","The LLM class by FuriosaAI, Inc. is designed for generating texts from given prompts and sampling parameters. It includes various configurations such as pretrained_id, task_type, llm_config, and several paths for quantization and configuration files. The class supports a maximum sequence length of 2048, tensor parallel size of 4, and pipeline parallel size of 1. It also allows for data parallel size configuration, tokenizer settings, and device specifications. Additional features include options for random weight initialization, pipeline building workers, and skipping the native runtime engine. The generate method takes prompts and sampling parameters to produce text completions, while the get_splitted_gms method provides sub GraphModules for each pipeline.",single_hop_specifc_query_synthesizer
18,Howw doo yoo installl prerequisits for FuriosaAI softwar stack?,"['<1-hop>\n\n* [.rst](../_sources/getting_started/prerequisites.rst ""Download source file"") * .pdf Installing Prerequisites ======================== Contents -------- * [ Requirements](#requirements) * [ Verifying if the system has devices](#verifying-if-the-system-has-devices) * [ Setting up APT server](#setting-up-apt-server) * [', '<2-hop>\n\nInstalling Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware) Installing Prerequisites [#](#installing-prerequisites ""Link to this heading"") ============================================================================== We will explain how to install the prerequisite packages necessary for FuriosaAI software stack. The prerequisite packages include device driver, firmware, and PE Runtime. These packages are available in package format for installation on Debian and Ubuntu systems. Requirements [#](#requirements ""Link to this heading"") ------------------------------------------------------ The minimum requirements are as follows: * Ubuntu 20.04 LTS (or Debian bullseye) or later * Administrator privileges on system (root) Verifying if the system has devices [#](#verifying-if-the-system-has-devices ""Link to this heading"") ---------------------------------------------------------------------------------------------------- You can verify the proper installation of FuriosaAI’s devices on your machine by running the following commands: ``` lspci -nn | grep FuriosaAI ``` If the device is properly installed, you should see the PCI information as shown below. ``` 4e:00.0 Processing accelerators [1200]: FuriosaAI, Inc. Device [1ed2:0001] (rev 01) ``` If the `lspci` command is not available, please install the following packages and run the commands to update PCIe ID database: ``` sudo apt update sudo apt install -y pciutils sudo update-pciids ``` Setting up APT server [#](#setting-up-apt-server ""Link to this heading"") ------------------------------------------------------------------------ To use the APT server provided by FuriosaAI, you must configure it on Ubuntu or Debian Linux as outlined below. 1. Install the required packages and register the signing key. ``` sudo apt update sudo apt install -y curl gnupg curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/cloud.google.gpg ``` 2. Configure the APT server according to the instructions provided for the Linux distribution versions. > ``` > echo ""deb [arch=$(dpkg --print-architecture)] http://asia-northeast3-apt.pkg.dev/projects/furiosa-ai $(. /etc/os-release && echo ""$VERSION_CODENAME"") main"" | sudo tee /etc/apt/sources.list.d/furiosa.list > > ``` Installing Pre-requisite Packages [#](#installing-pre-requisite-packages ""Link to this heading"") ------------------------------------------------------------------------------------------------ If you have registered the APT server as described above, you will be able to install the required packages: the device driver and PE Runtime. ``` sudo apt update sudo apt install furiosa-pert-rngd furiosa-driver-rngd ``` [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) is an useful CLI tool for listing and managing FuriosaAI NPUs. ``` sudo apt install furiosa-smi ``` Checking NPU devices [#](#checking-npu-devices ""Link to this heading"") ---------------------------------------------------------------------- Once the device driver and [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) are successfully installed, you can check the list of NPU devices as following command: ``` furiosa-smi info ``` Output: ``` +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware | Temp. | Power | PCI-BDF | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0 | 0.0.16+b4a67ca | 28.88°C | 38.00 W | 0000:4e:00.0 | +------+--------+----------------+---------+---------+--------------+ ``` Please refer to [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) to learn more about `furiosa-smi` command. Upgrading Device Firmware [#](#upgrading-device-firmware ""Link to this heading"") -------------------------------------------------------------------------------- Upgrading firmware versions can improve the performance and stability of the devices. If there is newer firmware in the latest release, you can upgrade them using the following methods: ``` sudo apt install furiosa-firmware-tools-rngd sudo apt install furiosa-firmware-image-rngd ``` Installing the `furiosa-firmware-image-rngd` package will automatically upgrade the firmware. The process takes approximately 3 to 5 minutes per device to complete. [previous Roadmap](../overview/roadmap.html ""previous page"") [next Quick Start with Furiosa LLM](furiosa_llm.html ""next page"") Contents * [Requirements](#requirements) * [Verifying if the system has devices](#verifying-if-the-system-has-devices) * [Setting up APT server](#setting-up-apt-server) * [Installing Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","To install prerequisites for the FuriosaAI software stack, you need to set up the APT server on Ubuntu or Debian Linux by installing required packages and registering the signing key. Then, configure the APT server according to the instructions for your Linux distribution. After setting up the APT server, you can install the necessary packages, including the device driver and PE Runtime, using the command `sudo apt install furiosa-pert-rngd furiosa-driver-rngd`.",multi_hop_abstract_query_synthesizer
19,How does Furiosa LLM utilize FuriosaAI's NPU to enhance compatibility with OpenAI APIs?,"['<1-hop>\n\n* [.rst](../_sources/getting_started/furiosa_llm.rst ""Download source file"") * .pdf Quick Start with Furiosa LLM ============================ Contents -------- * [ Installing Furiosa LLM](#installing-furiosa-llm) + [ Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [ Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [', '<2-hop>\n\nRunning Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) Quick Start with Furiosa LLM [#](#quick-start-with-furiosa-llm ""Link to this heading"") ====================================================================================== Furiosa LLM is a serving framework for LLM models that utilizes FuriosaAI’s NPU. It provides a Python API compatible with vLLM and a server compatible with the OpenAI API. This document explains how to install and use Furiosa LLM. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. Installing Furiosa LLM [#](#installing-furiosa-llm ""Link to this heading"") -------------------------------------------------------------------------- The minimum requirements for Furiosa LLM are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Administrator privileges on system (root) * [Setting up APT server](prerequisites.html#aptsetup) and [Installing Prerequisites](prerequisites.html#installingprerequisites) * Python 3.8, 3.9, or 3.10 * Enough storage space for model weights; e.g., about 100GB for Llama 3.1 70B model Then, please install the `furiosa-compiler` package as follows: ``` sudo apt install -y furiosa-compiler ``` Also, you need to create a Python virtual environment depending on your environment. Note Note that some models, such as meta-llama/Meta-Llama-3.1-8B require you to accept their license, hence, you need to create a HuggingFace account, accept the model’s license, and generate a token. Usually, you can create your token at <https://huggingface.co/settings/tokens> . Once you get a token, you can authenticate on the HuggingFace Hub as following: ``` huggingface-cli login --token $HF_TOKEN ``` Then, you can install the Furiosa LLM with the following command: ``` pip install furiosa-llm ``` ### Offline Batch Inference with Furiosa LLM [#](#offline-batch-inference-with-furiosa-llm ""Link to this heading"") In this section, we will explain how to perform offline LLM inference using the Python API of Furiosa LLM. First, import the `LLM` class and `SamplingParams` from the furiosa\\_llm module. `LLM` class is used to load LLM models and provides the core API for LLM inference. `SamplingParams` allows to specify various parameters for text generation. ``` from furiosa_llm import LLM, SamplingParams ``` Next, we will load an LLM model using the LLM class. The following example loads the meta-llama/Meta-Llama-3.1-8B model from the HuggingFace Hub and performs quantization. The model will be loaded into the memory and ready for inference. ``` llm = LLM(model=""meta-llama/Llama-3.1-8B-Instruct"") ``` After loading the model, you can perform LLM inference by calling the `generate` method. ``` prompts = [ """" ] sampling_params = SamplingParams(temperature=0.0) ``` Launching the OpenAI-Compatible Server [#](#launching-the-openai-compatible-server ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- You can find more details at [OpenAI Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) . Running Furiosa LLM in container environment [#](#running-furiosa-llm-in-container-environment ""Link to this heading"") ---------------------------------------------------------------------------------------------------------------------- FuriosaAI provides an image for running `furiosa-llm` in a containerized environment. By using the containerized version, you can run environments that utilize `furiosa-llm` without installing the FuriosaAI Software Stack on your host system, or you can execute it within a Kubernetes environment. To run the `furiosa-llm` container, you can use the following command: ``` $ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash (container) # python ``` Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Installing Prerequisites](prerequisites.html ""previous page"") [next Running MLPerf™ Inference Benchmark](furiosa_mlperf.html ""next page"") Contents * [Installing Furiosa LLM](#installing-furiosa-llm) + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","Furiosa LLM is a serving framework for LLM models that utilizes FuriosaAI’s NPU to provide a Python API compatible with vLLM and a server compatible with the OpenAI API. This integration allows for efficient model serving and inference, leveraging the specialized hardware capabilities of FuriosaAI's NPU to enhance performance and compatibility with OpenAI's ecosystem.",multi_hop_abstract_query_synthesizer
20,"How can Furiosa LLM be integrated into a container environment, and what are the benefits of using this setup?","['<1-hop>\n\n* [.rst](../_sources/getting_started/furiosa_llm.rst ""Download source file"") * .pdf Quick Start with Furiosa LLM ============================ Contents -------- * [ Installing Furiosa LLM](#installing-furiosa-llm) + [ Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [ Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [', '<2-hop>\n\nRunning Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) Quick Start with Furiosa LLM [#](#quick-start-with-furiosa-llm ""Link to this heading"") ====================================================================================== Furiosa LLM is a serving framework for LLM models that utilizes FuriosaAI’s NPU. It provides a Python API compatible with vLLM and a server compatible with the OpenAI API. This document explains how to install and use Furiosa LLM. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. Installing Furiosa LLM [#](#installing-furiosa-llm ""Link to this heading"") -------------------------------------------------------------------------- The minimum requirements for Furiosa LLM are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Administrator privileges on system (root) * [Setting up APT server](prerequisites.html#aptsetup) and [Installing Prerequisites](prerequisites.html#installingprerequisites) * Python 3.8, 3.9, or 3.10 * Enough storage space for model weights; e.g., about 100GB for Llama 3.1 70B model Then, please install the `furiosa-compiler` package as follows: ``` sudo apt install -y furiosa-compiler ``` Also, you need to create a Python virtual environment depending on your environment. Note Note that some models, such as meta-llama/Meta-Llama-3.1-8B require you to accept their license, hence, you need to create a HuggingFace account, accept the model’s license, and generate a token. Usually, you can create your token at <https://huggingface.co/settings/tokens> . Once you get a token, you can authenticate on the HuggingFace Hub as following: ``` huggingface-cli login --token $HF_TOKEN ``` Then, you can install the Furiosa LLM with the following command: ``` pip install furiosa-llm ``` ### Offline Batch Inference with Furiosa LLM [#](#offline-batch-inference-with-furiosa-llm ""Link to this heading"") In this section, we will explain how to perform offline LLM inference using the Python API of Furiosa LLM. First, import the `LLM` class and `SamplingParams` from the furiosa\\_llm module. `LLM` class is used to load LLM models and provides the core API for LLM inference. `SamplingParams` allows to specify various parameters for text generation. ``` from furiosa_llm import LLM, SamplingParams ``` Next, we will load an LLM model using the LLM class. The following example loads the meta-llama/Meta-Llama-3.1-8B model from the HuggingFace Hub and performs quantization. The model will be loaded into the memory and ready for inference. ``` llm = LLM(model=""meta-llama/Llama-3.1-8B-Instruct"") ``` After loading the model, you can perform LLM inference by calling the `generate` method. ``` prompts = [ """" ] sampling_params = SamplingParams(temperature=0.0) ``` Launching the OpenAI-Compatible Server [#](#launching-the-openai-compatible-server ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- You can find more details at [OpenAI Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) . Running Furiosa LLM in container environment [#](#running-furiosa-llm-in-container-environment ""Link to this heading"") ---------------------------------------------------------------------------------------------------------------------- FuriosaAI provides an image for running `furiosa-llm` in a containerized environment. By using the containerized version, you can run environments that utilize `furiosa-llm` without installing the FuriosaAI Software Stack on your host system, or you can execute it within a Kubernetes environment. To run the `furiosa-llm` container, you can use the following command: ``` $ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash (container) # python ``` Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Installing Prerequisites](prerequisites.html ""previous page"") [next Running MLPerf™ Inference Benchmark](furiosa_mlperf.html ""next page"") Contents * [Installing Furiosa LLM](#installing-furiosa-llm) + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","Furiosa LLM can be integrated into a container environment by using the containerized version provided by FuriosaAI. This setup allows you to run environments utilizing Furiosa LLM without installing the FuriosaAI Software Stack on your host system. Additionally, it can be executed within a Kubernetes environment. The benefits of using this setup include ease of deployment and management, as well as the ability to run Furiosa LLM in isolated environments, which enhances compatibility and reduces dependency issues.",multi_hop_abstract_query_synthesizer
21,"How does the FuriosaAI Software Stack facilitate running the MLPerf Inference Benchmark, and what are the installation requirements for the `furiosa-mlperf` command?","['<1-hop>\n\n* [.rst](../_sources/getting_started/furiosa_mlperf.rst ""Download source file"") * .pdf Running MLPerf™ Inference Benchmark =================================== Contents -------- * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1.  Running MLPerf Inference Benchmark](#id1) + [', '<2-hop>\n\nSYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1. Installing `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command ""Link to this heading"") -------------------------------------------------------------------------------------------------- To install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following: The minimum requirements for `furiosa-mlperf` are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers ( [Setting up APT server](prerequisites.html#aptsetup) ) * About 100GB storage space (only for the Llama 3.1 70B) Then, please install the `furiosa-mlperf` package as follows: ``` sudo apt update sudo apt install -y furiosa-mlperf ``` This command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` . Running MLPerf Inference Benchmark [#](#id1 ""Link to this heading"") ------------------------------------------------------------------- ### SYNOPSIS [#](#synopsis ""Link to this heading"") The `furiosa-mlperf` command provides the following subcommands: ``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z) Usage: furiosa-mlperf <COMMAND> Commands: bert-offline Run BERT benchmark with offline scenario bert-server Run BERT benchmark with server scenario gpt-j-offline Run GPT-J benchmark with offline scenario gpt-j-server Run GPT-J benchmark with server scenario llama-3.1-offline Run Llama 3.1 benchmark with offline scenario llama-3.1-server Run Llama 3.1 benchmark with server scenario help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version ``` ### Examples [#](#examples ""Link to this heading"") * BERT benchmark The BERT benchmark is based on running with a single RNGD. + Server Scenario To run BERT Large serving inference benchmark, you can use the following command: ``` furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh ""npu:0:*"" ``` + Offline Scenario To run BERT Large offline inference benchmark, you can use the following command: ``` furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh ""npu:0:*"" ``` * GPT-J benchmark The GPT-J benchmark is based on running with a single RNGD. + Server Scenario To run GPT-J 6B serving inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result ``` + Offline Scenario To run GPT-J 6B offline inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result ``` * Llama 3.1 benchmark The Llama 3.1 benchmark is based on running with four RNGDs. + Server Scenario To run Llama 3.1 70B serving inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result ``` + Offline Scenario To run Llama 3.1 70B offline inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result ``` * Common Once the process completes, it writes the results to a file in the specified results directory. You can open this file to view a summary of the results. ``` cat gpt-j-offline-result/mlperf_log_summary.txt ``` ``` ================================================ MLPerf Results Summary ================================================ SUT name : GPT-J SUT Scenario : Offline Mode : PerformanceOnly Samples per second: 11.842 Tokens per second (inferred): 817.095 Result is : VALID Min duration satisfied : Yes Min queries satisfied : Yes Early stopping satisfied: Yes ```', '<3-hop>\n\nRunning `furiosa-mlperf` in container environment [#](#running-furiosa-mlperf-in-container-environment ""Link to this heading"") ------------------------------------------------------------------------------------------------------------------------------ FuriosaAI provides a containerized version of the `furiosa-mlperf` command. If you use the containerized version, you can run the `furiosa-mlperf` command without installing the FuriosaAI Software Stack on your host system or you can run the command on Kubernetes environment. To run the `furiosa-mlperf` container, you can use the following command: (Assumes model artifacts exist in `/opt/gpt-j-6b` directory) ``` $ docker run -it --rm --privileged \\ -v /opt/gpt-j-6b:gpt-j-6b furiosa-mlperf:2024.1.0 bash (container) # furiosa-mlperf gpt-j-offline /gpt-j-6b /gpt-j-offline-result ``` To run in a containerized environment, refer to the examples provided in this document. Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Quick Start with Furiosa LLM](furiosa_llm.html ""previous page"") [next Furiosa LLM](../furiosa_llm/intro.html ""next page"") Contents * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","The FuriosaAI Software Stack facilitates running the MLPerf Inference Benchmark by providing the `furiosa-mlperf` command, which allows users to easily execute the benchmark tests. This command is based on MLPerf Inference Benchmark v4.1, with a modification replacing the Llama2 benchmark with Llama 3.1. To install the `furiosa-mlperf` command, the minimum requirements include having Ubuntu 20.04 LTS (Debian bullseye) or later, root or sudo permissions, configuring the APT server, installing device drivers, and ensuring about 100GB of storage space for the Llama 3.1 70B model. The installation process involves updating the package list and installing the `furiosa-mlperf` package along with its dependencies, such as `furiosa-compiler` and `furiosa-mlperf-resources`.",multi_hop_abstract_query_synthesizer
22,How to run MLPerf Inference Benchmark using FuriosaAI Software Stack and ensure compatibility with OpenAI Compatible Server?,"['<1-hop>\n\n* [.rst](../_sources/getting_started/furiosa_mlperf.rst ""Download source file"") * .pdf Running MLPerf™ Inference Benchmark =================================== Contents -------- * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1.  Running MLPerf Inference Benchmark](#id1) + [', '<2-hop>\n\nSYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1. Installing `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command ""Link to this heading"") -------------------------------------------------------------------------------------------------- To install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following: The minimum requirements for `furiosa-mlperf` are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers ( [Setting up APT server](prerequisites.html#aptsetup) ) * About 100GB storage space (only for the Llama 3.1 70B) Then, please install the `furiosa-mlperf` package as follows: ``` sudo apt update sudo apt install -y furiosa-mlperf ``` This command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` . Running MLPerf Inference Benchmark [#](#id1 ""Link to this heading"") ------------------------------------------------------------------- ### SYNOPSIS [#](#synopsis ""Link to this heading"") The `furiosa-mlperf` command provides the following subcommands: ``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z) Usage: furiosa-mlperf <COMMAND> Commands: bert-offline Run BERT benchmark with offline scenario bert-server Run BERT benchmark with server scenario gpt-j-offline Run GPT-J benchmark with offline scenario gpt-j-server Run GPT-J benchmark with server scenario llama-3.1-offline Run Llama 3.1 benchmark with offline scenario llama-3.1-server Run Llama 3.1 benchmark with server scenario help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version ``` ### Examples [#](#examples ""Link to this heading"") * BERT benchmark The BERT benchmark is based on running with a single RNGD. + Server Scenario To run BERT Large serving inference benchmark, you can use the following command: ``` furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh ""npu:0:*"" ``` + Offline Scenario To run BERT Large offline inference benchmark, you can use the following command: ``` furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh ""npu:0:*"" ``` * GPT-J benchmark The GPT-J benchmark is based on running with a single RNGD. + Server Scenario To run GPT-J 6B serving inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result ``` + Offline Scenario To run GPT-J 6B offline inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result ``` * Llama 3.1 benchmark The Llama 3.1 benchmark is based on running with four RNGDs. + Server Scenario To run Llama 3.1 70B serving inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result ``` + Offline Scenario To run Llama 3.1 70B offline inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result ``` * Common Once the process completes, it writes the results to a file in the specified results directory. You can open this file to view a summary of the results. ``` cat gpt-j-offline-result/mlperf_log_summary.txt ``` ``` ================================================ MLPerf Results Summary ================================================ SUT name : GPT-J SUT Scenario : Offline Mode : PerformanceOnly Samples per second: 11.842 Tokens per second (inferred): 817.095 Result is : VALID Min duration satisfied : Yes Min queries satisfied : Yes Early stopping satisfied: Yes ```', '<3-hop>\n\nRunning `furiosa-mlperf` in container environment [#](#running-furiosa-mlperf-in-container-environment ""Link to this heading"") ------------------------------------------------------------------------------------------------------------------------------ FuriosaAI provides a containerized version of the `furiosa-mlperf` command. If you use the containerized version, you can run the `furiosa-mlperf` command without installing the FuriosaAI Software Stack on your host system or you can run the command on Kubernetes environment. To run the `furiosa-mlperf` container, you can use the following command: (Assumes model artifacts exist in `/opt/gpt-j-6b` directory) ``` $ docker run -it --rm --privileged \\ -v /opt/gpt-j-6b:gpt-j-6b furiosa-mlperf:2024.1.0 bash (container) # furiosa-mlperf gpt-j-offline /gpt-j-6b /gpt-j-offline-result ``` To run in a containerized environment, refer to the examples provided in this document. Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Quick Start with Furiosa LLM](furiosa_llm.html ""previous page"") [next Furiosa LLM](../furiosa_llm/intro.html ""next page"") Contents * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..', '<4-hop>\n\n* [.rst](../_sources/furiosa_llm/furiosa-llm-serve.rst ""Download source file"") * .pdf OpenAI Compatible Server ======================== Contents -------- * [ Preparing Chat Templates](#preparing-chat-templates) * [ Launching the Server](#launching-the-server) + [ Arguments for the serve command](#arguments-for-the-serve-command) * [ Examples](#examples) + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds) + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [']","To run the MLPerf Inference Benchmark using the FuriosaAI Software Stack, you need to install the `furiosa-mlperf` command, which is based on MLPerf Inference Benchmark v4.1. The installation requires Ubuntu 20.04 LTS or later, root or sudo permissions, and about 100GB of storage space for the Llama 3.1 70B model. Once installed, you can use commands like `furiosa-mlperf llama-3.1-server` to run the Llama 3.1 benchmark in a server scenario. For compatibility with an OpenAI Compatible Server, you can prepare chat templates and launch the server using the appropriate arguments for the serve command, as described in the OpenAI Compatible Server documentation.",multi_hop_abstract_query_synthesizer
23,"What are the steps involved in installing the prerequisites for the FuriosaAI software stack, and how can one verify the proper installation of FuriosaAI devices?","['<1-hop>\n\n* [.rst](../_sources/getting_started/prerequisites.rst ""Download source file"") * .pdf Installing Prerequisites ======================== Contents -------- * [ Requirements](#requirements) * [ Verifying if the system has devices](#verifying-if-the-system-has-devices) * [ Setting up APT server](#setting-up-apt-server) * [', '<2-hop>\n\nInstalling Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware) Installing Prerequisites [#](#installing-prerequisites ""Link to this heading"") ============================================================================== We will explain how to install the prerequisite packages necessary for FuriosaAI software stack. The prerequisite packages include device driver, firmware, and PE Runtime. These packages are available in package format for installation on Debian and Ubuntu systems. Requirements [#](#requirements ""Link to this heading"") ------------------------------------------------------ The minimum requirements are as follows: * Ubuntu 20.04 LTS (or Debian bullseye) or later * Administrator privileges on system (root) Verifying if the system has devices [#](#verifying-if-the-system-has-devices ""Link to this heading"") ---------------------------------------------------------------------------------------------------- You can verify the proper installation of FuriosaAI’s devices on your machine by running the following commands: ``` lspci -nn | grep FuriosaAI ``` If the device is properly installed, you should see the PCI information as shown below. ``` 4e:00.0 Processing accelerators [1200]: FuriosaAI, Inc. Device [1ed2:0001] (rev 01) ``` If the `lspci` command is not available, please install the following packages and run the commands to update PCIe ID database: ``` sudo apt update sudo apt install -y pciutils sudo update-pciids ``` Setting up APT server [#](#setting-up-apt-server ""Link to this heading"") ------------------------------------------------------------------------ To use the APT server provided by FuriosaAI, you must configure it on Ubuntu or Debian Linux as outlined below. 1. Install the required packages and register the signing key. ``` sudo apt update sudo apt install -y curl gnupg curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/cloud.google.gpg ``` 2. Configure the APT server according to the instructions provided for the Linux distribution versions. > ``` > echo ""deb [arch=$(dpkg --print-architecture)] http://asia-northeast3-apt.pkg.dev/projects/furiosa-ai $(. /etc/os-release && echo ""$VERSION_CODENAME"") main"" | sudo tee /etc/apt/sources.list.d/furiosa.list > > ``` Installing Pre-requisite Packages [#](#installing-pre-requisite-packages ""Link to this heading"") ------------------------------------------------------------------------------------------------ If you have registered the APT server as described above, you will be able to install the required packages: the device driver and PE Runtime. ``` sudo apt update sudo apt install furiosa-pert-rngd furiosa-driver-rngd ``` [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) is an useful CLI tool for listing and managing FuriosaAI NPUs. ``` sudo apt install furiosa-smi ``` Checking NPU devices [#](#checking-npu-devices ""Link to this heading"") ---------------------------------------------------------------------- Once the device driver and [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) are successfully installed, you can check the list of NPU devices as following command: ``` furiosa-smi info ``` Output: ``` +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware | Temp. | Power | PCI-BDF | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0 | 0.0.16+b4a67ca | 28.88°C | 38.00 W | 0000:4e:00.0 | +------+--------+----------------+---------+---------+--------------+ ``` Please refer to [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) to learn more about `furiosa-smi` command. Upgrading Device Firmware [#](#upgrading-device-firmware ""Link to this heading"") -------------------------------------------------------------------------------- Upgrading firmware versions can improve the performance and stability of the devices. If there is newer firmware in the latest release, you can upgrade them using the following methods: ``` sudo apt install furiosa-firmware-tools-rngd sudo apt install furiosa-firmware-image-rngd ``` Installing the `furiosa-firmware-image-rngd` package will automatically upgrade the firmware. The process takes approximately 3 to 5 minutes per device to complete. [previous Roadmap](../overview/roadmap.html ""previous page"") [next Quick Start with Furiosa LLM](furiosa_llm.html ""next page"") Contents * [Requirements](#requirements) * [Verifying if the system has devices](#verifying-if-the-system-has-devices) * [Setting up APT server](#setting-up-apt-server) * [Installing Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","To install the prerequisites for the FuriosaAI software stack, you need to follow these steps: First, ensure your system meets the minimum requirements, which include having Ubuntu 20.04 LTS (or Debian bullseye) or later and administrator privileges. Next, set up the APT server by installing required packages and registering the signing key. Then, configure the APT server according to your Linux distribution version. After setting up the APT server, install the necessary packages such as the device driver and PE Runtime using the command `sudo apt install furiosa-pert-rngd furiosa-driver-rngd`. To verify the proper installation of FuriosaAI devices, you can run the command `lspci -nn | grep FuriosaAI` to check for PCI information. If the `lspci` command is unavailable, install `pciutils` and update the PCIe ID database. Additionally, you can use the `furiosa-smi info` command to list and manage FuriosaAI NPUs, ensuring the device driver and `furiosa-smi` are installed.",multi_hop_abstract_query_synthesizer
24,"How does the FuriosaAI Software Stack facilitate the running of the MLPerf Inference Benchmark, and what are the specific requirements for installing the `furiosa-mlperf` command?","['<1-hop>\n\n* [.rst](../_sources/getting_started/furiosa_mlperf.rst ""Download source file"") * .pdf Running MLPerf™ Inference Benchmark =================================== Contents -------- * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1.  Running MLPerf Inference Benchmark](#id1) + [', '<2-hop>\n\nSYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1. Installing `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command ""Link to this heading"") -------------------------------------------------------------------------------------------------- To install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following: The minimum requirements for `furiosa-mlperf` are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers ( [Setting up APT server](prerequisites.html#aptsetup) ) * About 100GB storage space (only for the Llama 3.1 70B) Then, please install the `furiosa-mlperf` package as follows: ``` sudo apt update sudo apt install -y furiosa-mlperf ``` This command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` . Running MLPerf Inference Benchmark [#](#id1 ""Link to this heading"") ------------------------------------------------------------------- ### SYNOPSIS [#](#synopsis ""Link to this heading"") The `furiosa-mlperf` command provides the following subcommands: ``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z) Usage: furiosa-mlperf <COMMAND> Commands: bert-offline Run BERT benchmark with offline scenario bert-server Run BERT benchmark with server scenario gpt-j-offline Run GPT-J benchmark with offline scenario gpt-j-server Run GPT-J benchmark with server scenario llama-3.1-offline Run Llama 3.1 benchmark with offline scenario llama-3.1-server Run Llama 3.1 benchmark with server scenario help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version ``` ### Examples [#](#examples ""Link to this heading"") * BERT benchmark The BERT benchmark is based on running with a single RNGD. + Server Scenario To run BERT Large serving inference benchmark, you can use the following command: ``` furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh ""npu:0:*"" ``` + Offline Scenario To run BERT Large offline inference benchmark, you can use the following command: ``` furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh ""npu:0:*"" ``` * GPT-J benchmark The GPT-J benchmark is based on running with a single RNGD. + Server Scenario To run GPT-J 6B serving inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result ``` + Offline Scenario To run GPT-J 6B offline inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result ``` * Llama 3.1 benchmark The Llama 3.1 benchmark is based on running with four RNGDs. + Server Scenario To run Llama 3.1 70B serving inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result ``` + Offline Scenario To run Llama 3.1 70B offline inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result ``` * Common Once the process completes, it writes the results to a file in the specified results directory. You can open this file to view a summary of the results. ``` cat gpt-j-offline-result/mlperf_log_summary.txt ``` ``` ================================================ MLPerf Results Summary ================================================ SUT name : GPT-J SUT Scenario : Offline Mode : PerformanceOnly Samples per second: 11.842 Tokens per second (inferred): 817.095 Result is : VALID Min duration satisfied : Yes Min queries satisfied : Yes Early stopping satisfied: Yes ```', '<3-hop>\n\nRunning `furiosa-mlperf` in container environment [#](#running-furiosa-mlperf-in-container-environment ""Link to this heading"") ------------------------------------------------------------------------------------------------------------------------------ FuriosaAI provides a containerized version of the `furiosa-mlperf` command. If you use the containerized version, you can run the `furiosa-mlperf` command without installing the FuriosaAI Software Stack on your host system or you can run the command on Kubernetes environment. To run the `furiosa-mlperf` container, you can use the following command: (Assumes model artifacts exist in `/opt/gpt-j-6b` directory) ``` $ docker run -it --rm --privileged \\ -v /opt/gpt-j-6b:gpt-j-6b furiosa-mlperf:2024.1.0 bash (container) # furiosa-mlperf gpt-j-offline /gpt-j-6b /gpt-j-offline-result ``` To run in a containerized environment, refer to the examples provided in this document. Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Quick Start with Furiosa LLM](furiosa_llm.html ""previous page"") [next Furiosa LLM](../furiosa_llm/intro.html ""next page"") Contents * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","The FuriosaAI Software Stack facilitates the running of the MLPerf Inference Benchmark by providing the `furiosa-mlperf` command, which allows users to easily execute the benchmark. This command is based on MLPerf Inference Benchmark v4.1, with a modification replacing the Llama2 benchmark with Llama 3.1. To install the `furiosa-mlperf` command, the minimum requirements include having Ubuntu 20.04 LTS (Debian bullseye) or later, root or sudo permissions, configuring the APT server, installing device drivers, and about 100GB of storage space for the Llama 3.1 70B. The installation process involves updating the APT package list and installing the `furiosa-mlperf` package, which includes `furiosa-compiler`, `furiosa-mlperf`, and `furiosa-mlperf-resources`.",multi_hop_abstract_query_synthesizer
25,"What are the steps involved in the quick start process for setting up Furiosa LLM, and how does it ensure compatibility with OpenAI APIs?","['<1-hop>\n\n* [.rst](../_sources/getting_started/furiosa_llm.rst ""Download source file"") * .pdf Quick Start with Furiosa LLM ============================ Contents -------- * [ Installing Furiosa LLM](#installing-furiosa-llm) + [ Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [ Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [', '<2-hop>\n\nRunning Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) Quick Start with Furiosa LLM [#](#quick-start-with-furiosa-llm ""Link to this heading"") ====================================================================================== Furiosa LLM is a serving framework for LLM models that utilizes FuriosaAI’s NPU. It provides a Python API compatible with vLLM and a server compatible with the OpenAI API. This document explains how to install and use Furiosa LLM. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. Installing Furiosa LLM [#](#installing-furiosa-llm ""Link to this heading"") -------------------------------------------------------------------------- The minimum requirements for Furiosa LLM are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Administrator privileges on system (root) * [Setting up APT server](prerequisites.html#aptsetup) and [Installing Prerequisites](prerequisites.html#installingprerequisites) * Python 3.8, 3.9, or 3.10 * Enough storage space for model weights; e.g., about 100GB for Llama 3.1 70B model Then, please install the `furiosa-compiler` package as follows: ``` sudo apt install -y furiosa-compiler ``` Also, you need to create a Python virtual environment depending on your environment. Note Note that some models, such as meta-llama/Meta-Llama-3.1-8B require you to accept their license, hence, you need to create a HuggingFace account, accept the model’s license, and generate a token. Usually, you can create your token at <https://huggingface.co/settings/tokens> . Once you get a token, you can authenticate on the HuggingFace Hub as following: ``` huggingface-cli login --token $HF_TOKEN ``` Then, you can install the Furiosa LLM with the following command: ``` pip install furiosa-llm ``` ### Offline Batch Inference with Furiosa LLM [#](#offline-batch-inference-with-furiosa-llm ""Link to this heading"") In this section, we will explain how to perform offline LLM inference using the Python API of Furiosa LLM. First, import the `LLM` class and `SamplingParams` from the furiosa\\_llm module. `LLM` class is used to load LLM models and provides the core API for LLM inference. `SamplingParams` allows to specify various parameters for text generation. ``` from furiosa_llm import LLM, SamplingParams ``` Next, we will load an LLM model using the LLM class. The following example loads the meta-llama/Meta-Llama-3.1-8B model from the HuggingFace Hub and performs quantization. The model will be loaded into the memory and ready for inference. ``` llm = LLM(model=""meta-llama/Llama-3.1-8B-Instruct"") ``` After loading the model, you can perform LLM inference by calling the `generate` method. ``` prompts = [ """" ] sampling_params = SamplingParams(temperature=0.0) ``` Launching the OpenAI-Compatible Server [#](#launching-the-openai-compatible-server ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- You can find more details at [OpenAI Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) . Running Furiosa LLM in container environment [#](#running-furiosa-llm-in-container-environment ""Link to this heading"") ---------------------------------------------------------------------------------------------------------------------- FuriosaAI provides an image for running `furiosa-llm` in a containerized environment. By using the containerized version, you can run environments that utilize `furiosa-llm` without installing the FuriosaAI Software Stack on your host system, or you can execute it within a Kubernetes environment. To run the `furiosa-llm` container, you can use the following command: ``` $ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash (container) # python ``` Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Installing Prerequisites](prerequisites.html ""previous page"") [next Running MLPerf™ Inference Benchmark](furiosa_mlperf.html ""next page"") Contents * [Installing Furiosa LLM](#installing-furiosa-llm) + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","The quick start process for setting up Furiosa LLM involves several steps. First, you need to ensure that your system meets the minimum requirements, which include having Ubuntu 20.04 LTS or later, administrator privileges, and Python 3.8, 3.9, or 3.10. You also need to set up an APT server and install prerequisites, including the 'furiosa-compiler' package. After setting up the environment, you create a Python virtual environment and install the Furiosa LLM using the command 'pip install furiosa-llm'. Additionally, for certain models like meta-llama/Meta-Llama-3.1-8B, you must accept the model's license on HuggingFace and authenticate using a token. Furiosa LLM ensures compatibility with OpenAI APIs by providing a server that is compatible with these APIs, allowing for seamless integration and interaction with OpenAI's ecosystem. This compatibility is achieved through the use of FuriosaAI’s NPU and a Python API that aligns with vLLM standards.",multi_hop_abstract_query_synthesizer
26,how to install prerequisites for furiosaai software stack and what steps to verify npu devices?,"['<1-hop>\n\n* [.rst](../_sources/getting_started/prerequisites.rst ""Download source file"") * .pdf Installing Prerequisites ======================== Contents -------- * [ Requirements](#requirements) * [ Verifying if the system has devices](#verifying-if-the-system-has-devices) * [ Setting up APT server](#setting-up-apt-server) * [', '<2-hop>\n\nInstalling Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware) Installing Prerequisites [#](#installing-prerequisites ""Link to this heading"") ============================================================================== We will explain how to install the prerequisite packages necessary for FuriosaAI software stack. The prerequisite packages include device driver, firmware, and PE Runtime. These packages are available in package format for installation on Debian and Ubuntu systems. Requirements [#](#requirements ""Link to this heading"") ------------------------------------------------------ The minimum requirements are as follows: * Ubuntu 20.04 LTS (or Debian bullseye) or later * Administrator privileges on system (root) Verifying if the system has devices [#](#verifying-if-the-system-has-devices ""Link to this heading"") ---------------------------------------------------------------------------------------------------- You can verify the proper installation of FuriosaAI’s devices on your machine by running the following commands: ``` lspci -nn | grep FuriosaAI ``` If the device is properly installed, you should see the PCI information as shown below. ``` 4e:00.0 Processing accelerators [1200]: FuriosaAI, Inc. Device [1ed2:0001] (rev 01) ``` If the `lspci` command is not available, please install the following packages and run the commands to update PCIe ID database: ``` sudo apt update sudo apt install -y pciutils sudo update-pciids ``` Setting up APT server [#](#setting-up-apt-server ""Link to this heading"") ------------------------------------------------------------------------ To use the APT server provided by FuriosaAI, you must configure it on Ubuntu or Debian Linux as outlined below. 1. Install the required packages and register the signing key. ``` sudo apt update sudo apt install -y curl gnupg curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/cloud.google.gpg ``` 2. Configure the APT server according to the instructions provided for the Linux distribution versions. > ``` > echo ""deb [arch=$(dpkg --print-architecture)] http://asia-northeast3-apt.pkg.dev/projects/furiosa-ai $(. /etc/os-release && echo ""$VERSION_CODENAME"") main"" | sudo tee /etc/apt/sources.list.d/furiosa.list > > ``` Installing Pre-requisite Packages [#](#installing-pre-requisite-packages ""Link to this heading"") ------------------------------------------------------------------------------------------------ If you have registered the APT server as described above, you will be able to install the required packages: the device driver and PE Runtime. ``` sudo apt update sudo apt install furiosa-pert-rngd furiosa-driver-rngd ``` [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) is an useful CLI tool for listing and managing FuriosaAI NPUs. ``` sudo apt install furiosa-smi ``` Checking NPU devices [#](#checking-npu-devices ""Link to this heading"") ---------------------------------------------------------------------- Once the device driver and [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) are successfully installed, you can check the list of NPU devices as following command: ``` furiosa-smi info ``` Output: ``` +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware | Temp. | Power | PCI-BDF | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0 | 0.0.16+b4a67ca | 28.88°C | 38.00 W | 0000:4e:00.0 | +------+--------+----------------+---------+---------+--------------+ ``` Please refer to [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) to learn more about `furiosa-smi` command. Upgrading Device Firmware [#](#upgrading-device-firmware ""Link to this heading"") -------------------------------------------------------------------------------- Upgrading firmware versions can improve the performance and stability of the devices. If there is newer firmware in the latest release, you can upgrade them using the following methods: ``` sudo apt install furiosa-firmware-tools-rngd sudo apt install furiosa-firmware-image-rngd ``` Installing the `furiosa-firmware-image-rngd` package will automatically upgrade the firmware. The process takes approximately 3 to 5 minutes per device to complete. [previous Roadmap](../overview/roadmap.html ""previous page"") [next Quick Start with Furiosa LLM](furiosa_llm.html ""next page"") Contents * [Requirements](#requirements) * [Verifying if the system has devices](#verifying-if-the-system-has-devices) * [Setting up APT server](#setting-up-apt-server) * [Installing Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","To install prerequisites for the FuriosaAI software stack, you need to ensure your system meets the minimum requirements, such as running Ubuntu 20.04 LTS or Debian bullseye or later, and having administrator privileges. First, set up the APT server by installing required packages and registering the signing key. Then, configure the APT server for your Linux distribution. After setting up the APT server, install the necessary packages like the device driver and PE Runtime using the command `sudo apt install furiosa-pert-rngd furiosa-driver-rngd`. To verify NPU devices, ensure the device driver and `furiosa-smi` are installed, then use the command `furiosa-smi info` to check the list of NPU devices. This command will display details such as architecture, device, firmware version, temperature, power, and PCI-BDF.",multi_hop_abstract_query_synthesizer
27,"How does the FuriosaAI Software Stack facilitate running the MLPerf Inference Benchmark, and what are the installation requirements for the `furiosa-mlperf` command?","['<1-hop>\n\n* [.rst](../_sources/getting_started/furiosa_mlperf.rst ""Download source file"") * .pdf Running MLPerf™ Inference Benchmark =================================== Contents -------- * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1.  Running MLPerf Inference Benchmark](#id1) + [', '<2-hop>\n\nSYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1. Installing `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command ""Link to this heading"") -------------------------------------------------------------------------------------------------- To install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following: The minimum requirements for `furiosa-mlperf` are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers ( [Setting up APT server](prerequisites.html#aptsetup) ) * About 100GB storage space (only for the Llama 3.1 70B) Then, please install the `furiosa-mlperf` package as follows: ``` sudo apt update sudo apt install -y furiosa-mlperf ``` This command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` . Running MLPerf Inference Benchmark [#](#id1 ""Link to this heading"") ------------------------------------------------------------------- ### SYNOPSIS [#](#synopsis ""Link to this heading"") The `furiosa-mlperf` command provides the following subcommands: ``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z) Usage: furiosa-mlperf <COMMAND> Commands: bert-offline Run BERT benchmark with offline scenario bert-server Run BERT benchmark with server scenario gpt-j-offline Run GPT-J benchmark with offline scenario gpt-j-server Run GPT-J benchmark with server scenario llama-3.1-offline Run Llama 3.1 benchmark with offline scenario llama-3.1-server Run Llama 3.1 benchmark with server scenario help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version ``` ### Examples [#](#examples ""Link to this heading"") * BERT benchmark The BERT benchmark is based on running with a single RNGD. + Server Scenario To run BERT Large serving inference benchmark, you can use the following command: ``` furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh ""npu:0:*"" ``` + Offline Scenario To run BERT Large offline inference benchmark, you can use the following command: ``` furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh ""npu:0:*"" ``` * GPT-J benchmark The GPT-J benchmark is based on running with a single RNGD. + Server Scenario To run GPT-J 6B serving inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result ``` + Offline Scenario To run GPT-J 6B offline inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result ``` * Llama 3.1 benchmark The Llama 3.1 benchmark is based on running with four RNGDs. + Server Scenario To run Llama 3.1 70B serving inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result ``` + Offline Scenario To run Llama 3.1 70B offline inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result ``` * Common Once the process completes, it writes the results to a file in the specified results directory. You can open this file to view a summary of the results. ``` cat gpt-j-offline-result/mlperf_log_summary.txt ``` ``` ================================================ MLPerf Results Summary ================================================ SUT name : GPT-J SUT Scenario : Offline Mode : PerformanceOnly Samples per second: 11.842 Tokens per second (inferred): 817.095 Result is : VALID Min duration satisfied : Yes Min queries satisfied : Yes Early stopping satisfied: Yes ```', '<3-hop>\n\nRunning `furiosa-mlperf` in container environment [#](#running-furiosa-mlperf-in-container-environment ""Link to this heading"") ------------------------------------------------------------------------------------------------------------------------------ FuriosaAI provides a containerized version of the `furiosa-mlperf` command. If you use the containerized version, you can run the `furiosa-mlperf` command without installing the FuriosaAI Software Stack on your host system or you can run the command on Kubernetes environment. To run the `furiosa-mlperf` container, you can use the following command: (Assumes model artifacts exist in `/opt/gpt-j-6b` directory) ``` $ docker run -it --rm --privileged \\ -v /opt/gpt-j-6b:gpt-j-6b furiosa-mlperf:2024.1.0 bash (container) # furiosa-mlperf gpt-j-offline /gpt-j-6b /gpt-j-offline-result ``` To run in a containerized environment, refer to the examples provided in this document. Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Quick Start with Furiosa LLM](furiosa_llm.html ""previous page"") [next Furiosa LLM](../furiosa_llm/intro.html ""next page"") Contents * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","The FuriosaAI Software Stack facilitates running the MLPerf Inference Benchmark by providing the `furiosa-mlperf` command, which allows users to easily execute the benchmark tests. This command is based on MLPerf Inference Benchmark v4.1, with a modification replacing the Llama2 benchmark with Llama 3.1. To install the `furiosa-mlperf` command, the minimum requirements include Ubuntu 20.04 LTS (Debian bullseye) or later, root or sudo permissions, configuring the APT server, installing device drivers, and about 100GB of storage space for the Llama 3.1 70B model. The installation process involves updating the package list and installing the `furiosa-mlperf` package along with `furiosa-compiler` and `furiosa-mlperf-resources`.",multi_hop_abstract_query_synthesizer
28,How does FuriosaAI's LLM class support text generation and what role does the Furiosa Compiler play?,"['<1-hop>\n\nContents -------- * [`LLM`](#furiosa_llm.LLM) + [`LLM.generate()`](#furiosa_llm.LLM.generate) + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms) LLM class [#](#llm-class ""Link to this heading"") ================================================ *class* furiosa\\_llm. LLM ( *pretrained\\_id : str* , *task\\_type : str | None = None* , *llm\\_config : LLMConfig | None = None* , *qformat\\_path : PathLike | None = None* , *qparam\\_path : PathLike | None = None* , *prefill\\_quant\\_bin\\_path : PathLike | None = None* , *decode\\_quant\\_bin\\_path : PathLike | None = None* , *config : Dict [ str , Any ] = {}* , *bucket\\_config : BucketConfig | None = None* , *max\\_seq\\_len\\_to\\_capture : int = 2048* , *tensor\\_parallel\\_size : int = 4* , *pipeline\\_parallel\\_size : int = 1* , *data\\_parallel\\_size : int | None = None* , *tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast | None = None* , *tokenizer\\_mode : Literal [ \'auto\' , \'slow\' ] = \'auto\'* , *seed : int | None = None* , *devices : str | Sequence [ Device ] | None = None* , *param\\_file\\_path : PathLike | None = None* , *param\\_saved\\_format : Literal [ \'safetensors\' , \'pt\' ] = \'safetensors\'* , *do\\_decompositions\\_for\\_model\\_rewrite : bool = False* , *comp\\_supertask\\_kind : Literal [ \'edf\' , \'dfg\' , \'fx\' ] | None = None* , *cache\\_dir : PathLike | None = PosixPath(\'/home/hyunsik/.cache/furiosa/llm\')* , *backend : LLMBackend | None = None* , *use\\_blockwise\\_compile : bool = True* , *num\\_blocks\\_per\\_supertask : int = 1* , *embed\\_all\\_constants\\_into\\_graph : bool = False* , *paged\\_attention\\_num\\_blocks : int | None = None* , *paged\\_attention\\_block\\_size : int = 1* , *kv\\_cache\\_sharing\\_across\\_beams\\_config : KvCacheSharingAcrossBeamsConfig | None = None* , *scheduler\\_config : SchedulerConfig = SchedulerConfig(npu\\_queue\\_limit=2, max\\_processing\\_samples=65536, spare\\_blocks\\_ratio=0.2, is\\_offline=False)* , *packing\\_type : Literal [ \'IDENTITY\' ] = \'IDENTITY\'* , *compiler\\_config\\_overrides : Mapping | None = None* , *use\\_random\\_weight : bool = False* , *num\\_pipeline\\_builder\\_workers : int = 1* , *num\\_compile\\_workers : int = 1* , *skip\\_engine : bool = False* , *\\** , *\\_cleanup : bool = True* , *\\*\\* kwargs* ) [[source]](../../_modules/furiosa_llm/api.html#LLM) [#](#furiosa_llm.LLM ""Link to this definition"") Bases: `object` An LLM for generating texts from given prompts and sampling parameters. Parameters : * **pretrained\\_id** – The name of the pretrained model. This corresponds to pretrained\\_model\\_name\\_or\\_path in HuggingFace Transformers. * **task\\_type** – The type of the task. This corresponds to task in HuggingFace Transformers. See <https://huggingface.co/docs/transformers/main/en/quicktour#pipeline> for more details. * **llm\\_config** – The configuration for the LLM. This includes quantization and optimization configurations. * **qformat\\_path** – The path to the quantization format file. * **qparam\\_path** – The path to the quantization parameter file. * **prefill\\_quant\\_bin\\_path** – The path to the quantziation prefill bin file. * **decode\\_quant\\_bin\\_path** – The path to the quantziation decode bin file. * **config** – The configuration for the HuggingFace Transformers model. This is a dictionary that includes the configuration for the model. * **bucket\\_config** – Config for bucket generating policy. If not given, the model will use single one batch, max\\_seq\\_len\\_to\\_capture attention size bucket per each phase. * **max\\_seq\\_len\\_to\\_capture** – Maximum sequence length covered by LLM engine. Sequence with larger context than this will not be covered. The default is 2048. * **tensor\\_parallel\\_size** – The number of PEs for each tensor parallelism group. The default is 4. * **pipeline\\_parallel\\_size** – The number of pipeline stages for pipeline parallelism. The default is 1, which means no pipeline parallelism. * **data\\_parallel\\_size** – The size of the data parallelism group. If not given, it will be inferred from total avaialble PEs and other parallelism degrees. * **tokenizer** – The name or path of a HuggingFace Transformers tokenizer. * **tokenizer\\_mode** – The tokenizer mode. “auto” will use the fast tokenizer if available, and “slow” will always use the slow tokenizer. * **seed** – The seed to initialize the random number generator for sampling. * **devices** – The devices to run the model. It can be a single device or a list of devices. Each device can be either “cpu:X” or “cuda:X” where X is a specific device index. The default is “cpu:0”. * **param\\_file\\_path** – The path to the parameter file to use for pipeline generation. If not specified, the parameters will be saved in a temporary file which will be used for pipeline generation. * **param\\_saved\\_format** – The format of the parameter file. Only possible value is “safetensors” now. The default is “safetensors”. * **do\\_decompositions\\_for\\_model\\_rewrite** – Whether to decompose some ops to describe various parallelism strategies with mppp config. When the value is True, mppp config that matches with the decomposed FX graph should be given. * **comp\\_supertask\\_kind** – The format that pipeline’s supertask will be represented as. Possible values are “fx”,”dfg”, and “edf”, and the default is “fx”. * **cache\\_dir** – The cache directory for all generated files for this LLM instance. When its value is `None` , caching is disabled. The default is “$HOME/.cache/furiosa/llm”. * **backend** – The backend implementation to run forward() of a model for the LLM. The default is LLMBackend.TORCH\\_PIPELINE\\_RUNNER. * **use\\_blockwise\\_compile** – If True, each task will be compiled in the unit of transformer block, and compilation result for transformer block is generated once and reused. * **num\\_blocks\\_per\\_supertask** – The number of transformer blocks that will be merged into one supertask. This option is valid only when use\\_blockwise\\_compile=True . The default is 1. * **embed\\_all\\_constants\\_into\\_graph** – Whether to embed constant tensors into graph or make them as input of the graph and save them as separate files. * **paged\\_attention\\_num\\_blocks** – The maximum number of blocks that each k/v storage per layer can store. This argument must be given if model uses paged attention. * **paged\\_attention\\_block\\_size** – The maximum number of tokens that can be stored in a single paged attention block. This argument must be given if model uses paged attention. * **kv\\_cache\\_sharing\\_across\\_beams\\_config** – Configuration for sharing kv cache across beams. This argument must be given if and only if the model is optimized to share kv cache across beams. If this argument is given, decode phase buckets with batch size of `batch_size` \\* `kv_cache_sharing_across_beams_config.beam_width` will be created. * **scheduler\\_config** – Configuration for the scheduler, allowing to maximum number of tasks which can be queued to HW, maximum number', '<2-hop>\n\n* [.rst](../../_sources/furiosa_llm/references/llm.rst ""Download source file"") * .pdf LLM class ========= Contents -------- * [`LLM`](#furiosa_llm.LLM) + [`LLM.generate()`](#furiosa_llm.LLM.generate) + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms) LLM class [#](#llm-class ""Link to this heading"") ================================================ *class* furiosa\\_llm. LLM ( *pretrained\\_id : str* , *task\\_type : str | None = None* , *llm\\_config : LLMConfig | None = None* , *qformat\\_path : PathLike | None = None* , *qparam\\_path : PathLike | None = None* , *prefill\\_quant\\_bin\\_path : PathLike | None = None* , *decode\\_quant\\_bin\\_path : PathLike | None = None* , *config : Dict [ str , Any ] = {}* , *bucket\\_config : BucketConfig | None = None* , *max\\_seq\\_len\\_to\\_capture : int = 2048* , *tensor\\_parallel\\_size : int = 4* , *pipeline\\_parallel\\_size : int = 1* , *data\\_parallel\\_size : int | None = None* , *tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast | None = None* , *tokenizer\\_mode : Literal [ \'auto\' , \'slow\' ] = \'auto\'* , *seed : int | None = None* , *devices : str | Sequence [ Device ] | None = None* , *param\\_file\\_path : PathLike | None = None* , *param\\_saved\\_format : Literal [ \'safetensors\' , \'pt\' ] = \'safetensors\'* , *do\\_decompositions\\_for\\_model\\_rewrite : bool = False* , *comp\\_supertask\\_kind : Literal [ \'edf\' , \'dfg\' , \'fx\' ] | None = None* , *cache\\_dir : PathLike | None = PosixPath(\'/home/hyunsik/.cache/furiosa/llm\')* , *backend : LLMBackend | None = None* , *use\\_blockwise\\_compile : bool = True* , *num\\_blocks\\_per\\_supertask : int = 1* , *embed\\_all\\_constants\\_into\\_graph : bool = False* , *paged\\_attention\\_num\\_blocks : int | None = None* , *paged\\_attention\\_block\\_size : int = 1* , *kv\\_cache\\_sharing\\_across\\_beams\\_config : KvCacheSharingAcrossBeamsConfig | None = None* , *scheduler\\_config : SchedulerConfig = SchedulerConfig(npu\\_queue\\_limit=2, max\\_processing\\_samples=65536, spare\\_blocks\\_ratio=0.2, is\\_offline=False)* , *packing\\_type : Literal [ \'IDENTITY\' ] = \'IDENTITY\'* , *compiler\\_config\\_overrides : Mapping | None = None* , *use\\_random\\_weight : bool = False* , *num\\_pipeline\\_builder\\_workers : int = 1* , *num\\_compile\\_workers : int = 1* , *skip\\_engine : bool = False* , *\\** , *\\_cleanup : bool = True* , *\\*\\* kwargs* ) [[source]](../../_modules/furiosa_llm/api.html#LLM) [#](#furiosa_llm.LLM ""Link to this definition"") Bases: `object` An LLM for generating texts from given prompts and sampling parameters.  generate()`](#furiosa_llm.LLM.generate) + [`LLM.  of samples that can be processed by the scheduler, and ratio of spare blocks that are reserved by scheduler. * **packing\\_type** – Packing algorithm. Possible values are “IDENTITY” only for now * **compiler\\_config\\_overrides** – Overrides for the compiler config. This is a dictionary that includes the configuration for the compiler. * **use\\_random\\_weight** – If True, the model will be initialized with random weights. * **num\\_pipeline\\_builder\\_workers** – number of workers used for building pipelines (except for compilation). The default is 1 (no parallelism). Setting this value larger than 1 reduces pipeline building time, especially for large models, but requires much more memory. * **num\\_compile\\_workers** – number of workers used for compilation. The default is 1 (no parallelism). * **skip\\_engine** – If True, the native runtime engine will not be initialized. This is useful when you need the pipelines for other purposes than running them with the engine. generate ( *prompts: str | ~typing.List[str], sampling\\_params: ~furiosa\\_llm.sampling\\_params.SamplingParams = SamplingParams(n=1, best\\_of=1, temperature=1.0, top\\_p=1.0, top\\_k=-1, use\\_beam\\_search=False, length\\_penalty=1.0, early\\_stopping=False, max\\_tokens=16min\\_tokens=0, , prompt\\_token\\_ids: ~typing.List[int] | ~typing.List[~typing.List[int]] | None = None* ) → RequestOutput | List [ RequestOutput ] [[source]](../../_modules/furiosa_llm/api.html#LLM.generate) [#](#furiosa_llm.LLM.generate ""Link to this definition"") Generate texts from given prompts and sampling parameters. Parameters : * **prompts** – The prompts to generate texts. * **sampling\\_params** – The sampling parameters for generating texts. * **prompt\\_token\\_ids** – The token ids of the prompts. If not given, the token ids are generated from the prompts using the tokenizer. Returns : A list of RequestOutput objects containing the generated completions in the same order as the input prompts. get\\_splitted\\_gms ( *get\\_input\\_constants : bool = False* ) → Dict [ str , Tuple [ GraphModule , ... ] | Tuple [ Tuple [ GraphModule , Tuple [ Tensor | None , ... ] ] , ... ] ] [[source]](../../_modules/furiosa_llm/api.html#LLM.get_splitted_gms) [#](#furiosa_llm.LLM.get_splitted_gms ""Link to this definition"") Get sub GraphModules for each pipeline. Returns : Dictionary whose key is the pipeline name and value is the tuple containing `GraphModule``s (computation supertasks) and some additional information if necessary. if ``get_input_constants==False` , each value is just a tuple of `GraphModule``s in the pipeline. Otherwise, each value is a tuple whose element is ``GraphModule` in the pipeline and list of input constant tensors, which were originally constant tensors, but converted to input. The list of input constant tensors has same length as corresponding `GraphModule` ’s number of inputs with each element exactly corresponding to the input of the `GraphModule` with same index, but elements with original input tensor indexes are `None` . Return type : Dict[str, Union[Tuple[GraphModule, …], Tuple[Tuple[GraphModule, Tuple[Optional[torch.Tensor], …]], …],],] [previous References](../references.html ""previous page"") [next SamplingParams](sampling_params.html ""next page"") Contents * [`LLM`](#furiosa_llm.LLM) + [`LLM.generate()`](#furiosa_llm.LLM.generate) + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..', '<3-hop>\n\n* [.rst](../_sources/overview/software_stack.rst ""Download source file"") * .pdf FuriosaAI’s Software Stack ========================== Contents -------- * [ Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [ Furiosa Compiler](#furiosa-compiler) * [ Furiosa Runtime](#furiosa-runtime) * [ Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [']","FuriosaAI's LLM class supports text generation by utilizing the `LLM.generate()` method, which generates texts from given prompts and sampling parameters. The class is designed to handle various configurations, including quantization and optimization, to efficiently generate text. The Furiosa Compiler plays a crucial role in this process by compiling tasks in the unit of transformer blocks, allowing for efficient execution and reuse of compilation results. This integration ensures that the LLM can generate text effectively while leveraging the capabilities of the Furiosa Compiler for optimized performance.",multi_hop_abstract_query_synthesizer
29,"What are the steps involved in installing prerequisites for the FuriosaAI software stack, and how can one verify the installation of FuriosaAI devices?","['<1-hop>\n\n* [.rst](../_sources/getting_started/prerequisites.rst ""Download source file"") * .pdf Installing Prerequisites ======================== Contents -------- * [ Requirements](#requirements) * [ Verifying if the system has devices](#verifying-if-the-system-has-devices) * [ Setting up APT server](#setting-up-apt-server) * [', '<2-hop>\n\nInstalling Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware) Installing Prerequisites [#](#installing-prerequisites ""Link to this heading"") ============================================================================== We will explain how to install the prerequisite packages necessary for FuriosaAI software stack. The prerequisite packages include device driver, firmware, and PE Runtime. These packages are available in package format for installation on Debian and Ubuntu systems. Requirements [#](#requirements ""Link to this heading"") ------------------------------------------------------ The minimum requirements are as follows: * Ubuntu 20.04 LTS (or Debian bullseye) or later * Administrator privileges on system (root) Verifying if the system has devices [#](#verifying-if-the-system-has-devices ""Link to this heading"") ---------------------------------------------------------------------------------------------------- You can verify the proper installation of FuriosaAI’s devices on your machine by running the following commands: ``` lspci -nn | grep FuriosaAI ``` If the device is properly installed, you should see the PCI information as shown below. ``` 4e:00.0 Processing accelerators [1200]: FuriosaAI, Inc. Device [1ed2:0001] (rev 01) ``` If the `lspci` command is not available, please install the following packages and run the commands to update PCIe ID database: ``` sudo apt update sudo apt install -y pciutils sudo update-pciids ``` Setting up APT server [#](#setting-up-apt-server ""Link to this heading"") ------------------------------------------------------------------------ To use the APT server provided by FuriosaAI, you must configure it on Ubuntu or Debian Linux as outlined below. 1. Install the required packages and register the signing key. ``` sudo apt update sudo apt install -y curl gnupg curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/cloud.google.gpg ``` 2. Configure the APT server according to the instructions provided for the Linux distribution versions. > ``` > echo ""deb [arch=$(dpkg --print-architecture)] http://asia-northeast3-apt.pkg.dev/projects/furiosa-ai $(. /etc/os-release && echo ""$VERSION_CODENAME"") main"" | sudo tee /etc/apt/sources.list.d/furiosa.list > > ``` Installing Pre-requisite Packages [#](#installing-pre-requisite-packages ""Link to this heading"") ------------------------------------------------------------------------------------------------ If you have registered the APT server as described above, you will be able to install the required packages: the device driver and PE Runtime. ``` sudo apt update sudo apt install furiosa-pert-rngd furiosa-driver-rngd ``` [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) is an useful CLI tool for listing and managing FuriosaAI NPUs. ``` sudo apt install furiosa-smi ``` Checking NPU devices [#](#checking-npu-devices ""Link to this heading"") ---------------------------------------------------------------------- Once the device driver and [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) are successfully installed, you can check the list of NPU devices as following command: ``` furiosa-smi info ``` Output: ``` +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware | Temp. | Power | PCI-BDF | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0 | 0.0.16+b4a67ca | 28.88°C | 38.00 W | 0000:4e:00.0 | +------+--------+----------------+---------+---------+--------------+ ``` Please refer to [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) to learn more about `furiosa-smi` command. Upgrading Device Firmware [#](#upgrading-device-firmware ""Link to this heading"") -------------------------------------------------------------------------------- Upgrading firmware versions can improve the performance and stability of the devices. If there is newer firmware in the latest release, you can upgrade them using the following methods: ``` sudo apt install furiosa-firmware-tools-rngd sudo apt install furiosa-firmware-image-rngd ``` Installing the `furiosa-firmware-image-rngd` package will automatically upgrade the firmware. The process takes approximately 3 to 5 minutes per device to complete. [previous Roadmap](../overview/roadmap.html ""previous page"") [next Quick Start with Furiosa LLM](furiosa_llm.html ""next page"") Contents * [Requirements](#requirements) * [Verifying if the system has devices](#verifying-if-the-system-has-devices) * [Setting up APT server](#setting-up-apt-server) * [Installing Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","To install the prerequisites for the FuriosaAI software stack, you need to follow these steps: First, ensure your system meets the minimum requirements, such as running Ubuntu 20.04 LTS or Debian bullseye or later, and having administrator privileges. Next, set up the APT server by installing required packages and registering the signing key. Then, configure the APT server according to your Linux distribution version. After setting up the APT server, you can install the necessary packages, including the device driver and PE Runtime, using the command `sudo apt install furiosa-pert-rngd furiosa-driver-rngd`. To verify the installation of FuriosaAI devices, you can use the command `lspci -nn | grep FuriosaAI` to check for PCI information. If the `lspci` command is unavailable, install `pciutils` and update the PCIe ID database. Additionally, after installing the device driver and `furiosa-smi`, you can list NPU devices using the command `furiosa-smi info`.",multi_hop_abstract_query_synthesizer
30,"How does the FuriosaAI Software Stack facilitate the execution of the MLPerf Inference Benchmark, and what are the specific requirements and scenarios for running the Llama 3.1 benchmark?","['<1-hop>\n\n* [.rst](../_sources/getting_started/furiosa_mlperf.rst ""Download source file"") * .pdf Running MLPerf™ Inference Benchmark =================================== Contents -------- * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1.  Running MLPerf Inference Benchmark](#id1) + [', '<2-hop>\n\nSYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1. Installing `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command ""Link to this heading"") -------------------------------------------------------------------------------------------------- To install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following: The minimum requirements for `furiosa-mlperf` are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers ( [Setting up APT server](prerequisites.html#aptsetup) ) * About 100GB storage space (only for the Llama 3.1 70B) Then, please install the `furiosa-mlperf` package as follows: ``` sudo apt update sudo apt install -y furiosa-mlperf ``` This command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` . Running MLPerf Inference Benchmark [#](#id1 ""Link to this heading"") ------------------------------------------------------------------- ### SYNOPSIS [#](#synopsis ""Link to this heading"") The `furiosa-mlperf` command provides the following subcommands: ``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z) Usage: furiosa-mlperf <COMMAND> Commands: bert-offline Run BERT benchmark with offline scenario bert-server Run BERT benchmark with server scenario gpt-j-offline Run GPT-J benchmark with offline scenario gpt-j-server Run GPT-J benchmark with server scenario llama-3.1-offline Run Llama 3.1 benchmark with offline scenario llama-3.1-server Run Llama 3.1 benchmark with server scenario help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version ``` ### Examples [#](#examples ""Link to this heading"") * BERT benchmark The BERT benchmark is based on running with a single RNGD. + Server Scenario To run BERT Large serving inference benchmark, you can use the following command: ``` furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh ""npu:0:*"" ``` + Offline Scenario To run BERT Large offline inference benchmark, you can use the following command: ``` furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh ""npu:0:*"" ``` * GPT-J benchmark The GPT-J benchmark is based on running with a single RNGD. + Server Scenario To run GPT-J 6B serving inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result ``` + Offline Scenario To run GPT-J 6B offline inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result ``` * Llama 3.1 benchmark The Llama 3.1 benchmark is based on running with four RNGDs. + Server Scenario To run Llama 3.1 70B serving inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result ``` + Offline Scenario To run Llama 3.1 70B offline inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result ``` * Common Once the process completes, it writes the results to a file in the specified results directory. You can open this file to view a summary of the results. ``` cat gpt-j-offline-result/mlperf_log_summary.txt ``` ``` ================================================ MLPerf Results Summary ================================================ SUT name : GPT-J SUT Scenario : Offline Mode : PerformanceOnly Samples per second: 11.842 Tokens per second (inferred): 817.095 Result is : VALID Min duration satisfied : Yes Min queries satisfied : Yes Early stopping satisfied: Yes ```', '<3-hop>\n\nRunning `furiosa-mlperf` in container environment [#](#running-furiosa-mlperf-in-container-environment ""Link to this heading"") ------------------------------------------------------------------------------------------------------------------------------ FuriosaAI provides a containerized version of the `furiosa-mlperf` command. If you use the containerized version, you can run the `furiosa-mlperf` command without installing the FuriosaAI Software Stack on your host system or you can run the command on Kubernetes environment. To run the `furiosa-mlperf` container, you can use the following command: (Assumes model artifacts exist in `/opt/gpt-j-6b` directory) ``` $ docker run -it --rm --privileged \\ -v /opt/gpt-j-6b:gpt-j-6b furiosa-mlperf:2024.1.0 bash (container) # furiosa-mlperf gpt-j-offline /gpt-j-6b /gpt-j-offline-result ``` To run in a containerized environment, refer to the examples provided in this document. Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Quick Start with Furiosa LLM](furiosa_llm.html ""previous page"") [next Furiosa LLM](../furiosa_llm/intro.html ""next page"") Contents * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","The FuriosaAI Software Stack facilitates the execution of the MLPerf Inference Benchmark by providing the `furiosa-mlperf` command, which is based on MLPerf™ Inference Benchmark v4.1. This command allows users to easily run the benchmark to evaluate the performance of machine learning software, hardware, and cloud platforms. To install the `furiosa-mlperf` command, users need Ubuntu 20.04 LTS or later, root or sudo permissions, configured APT server, installed device drivers, and about 100GB of storage space for the Llama 3.1 70B model. The Llama 3.1 benchmark can be run in both server and offline scenarios using the `furiosa-mlperf` command. For the server scenario, the command is `furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result`, and for the offline scenario, it is `furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result`. Additionally, FuriosaAI provides a containerized version of the `furiosa-mlperf` command, allowing it to be run without installing the software stack on the host system or in a Kubernetes environment.",multi_hop_abstract_query_synthesizer
31,Wht is the role of sampling parametres in the LLM class for text generation?,"['<1-hop>\n\nContents -------- * [`LLM`](#furiosa_llm.LLM) + [`LLM.generate()`](#furiosa_llm.LLM.generate) + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms) LLM class [#](#llm-class ""Link to this heading"") ================================================ *class* furiosa\\_llm. LLM ( *pretrained\\_id : str* , *task\\_type : str | None = None* , *llm\\_config : LLMConfig | None = None* , *qformat\\_path : PathLike | None = None* , *qparam\\_path : PathLike | None = None* , *prefill\\_quant\\_bin\\_path : PathLike | None = None* , *decode\\_quant\\_bin\\_path : PathLike | None = None* , *config : Dict [ str , Any ] = {}* , *bucket\\_config : BucketConfig | None = None* , *max\\_seq\\_len\\_to\\_capture : int = 2048* , *tensor\\_parallel\\_size : int = 4* , *pipeline\\_parallel\\_size : int = 1* , *data\\_parallel\\_size : int | None = None* , *tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast | None = None* , *tokenizer\\_mode : Literal [ \'auto\' , \'slow\' ] = \'auto\'* , *seed : int | None = None* , *devices : str | Sequence [ Device ] | None = None* , *param\\_file\\_path : PathLike | None = None* , *param\\_saved\\_format : Literal [ \'safetensors\' , \'pt\' ] = \'safetensors\'* , *do\\_decompositions\\_for\\_model\\_rewrite : bool = False* , *comp\\_supertask\\_kind : Literal [ \'edf\' , \'dfg\' , \'fx\' ] | None = None* , *cache\\_dir : PathLike | None = PosixPath(\'/home/hyunsik/.cache/furiosa/llm\')* , *backend : LLMBackend | None = None* , *use\\_blockwise\\_compile : bool = True* , *num\\_blocks\\_per\\_supertask : int = 1* , *embed\\_all\\_constants\\_into\\_graph : bool = False* , *paged\\_attention\\_num\\_blocks : int | None = None* , *paged\\_attention\\_block\\_size : int = 1* , *kv\\_cache\\_sharing\\_across\\_beams\\_config : KvCacheSharingAcrossBeamsConfig | None = None* , *scheduler\\_config : SchedulerConfig = SchedulerConfig(npu\\_queue\\_limit=2, max\\_processing\\_samples=65536, spare\\_blocks\\_ratio=0.2, is\\_offline=False)* , *packing\\_type : Literal [ \'IDENTITY\' ] = \'IDENTITY\'* , *compiler\\_config\\_overrides : Mapping | None = None* , *use\\_random\\_weight : bool = False* , *num\\_pipeline\\_builder\\_workers : int = 1* , *num\\_compile\\_workers : int = 1* , *skip\\_engine : bool = False* , *\\** , *\\_cleanup : bool = True* , *\\*\\* kwargs* ) [[source]](../../_modules/furiosa_llm/api.html#LLM) [#](#furiosa_llm.LLM ""Link to this definition"") Bases: `object` An LLM for generating texts from given prompts and sampling parameters. Parameters : * **pretrained\\_id** – The name of the pretrained model. This corresponds to pretrained\\_model\\_name\\_or\\_path in HuggingFace Transformers. * **task\\_type** – The type of the task. This corresponds to task in HuggingFace Transformers. See <https://huggingface.co/docs/transformers/main/en/quicktour#pipeline> for more details. * **llm\\_config** – The configuration for the LLM. This includes quantization and optimization configurations. * **qformat\\_path** – The path to the quantization format file. * **qparam\\_path** – The path to the quantization parameter file. * **prefill\\_quant\\_bin\\_path** – The path to the quantziation prefill bin file. * **decode\\_quant\\_bin\\_path** – The path to the quantziation decode bin file. * **config** – The configuration for the HuggingFace Transformers model. This is a dictionary that includes the configuration for the model. * **bucket\\_config** – Config for bucket generating policy. If not given, the model will use single one batch, max\\_seq\\_len\\_to\\_capture attention size bucket per each phase. * **max\\_seq\\_len\\_to\\_capture** – Maximum sequence length covered by LLM engine. Sequence with larger context than this will not be covered. The default is 2048. * **tensor\\_parallel\\_size** – The number of PEs for each tensor parallelism group. The default is 4. * **pipeline\\_parallel\\_size** – The number of pipeline stages for pipeline parallelism. The default is 1, which means no pipeline parallelism. * **data\\_parallel\\_size** – The size of the data parallelism group. If not given, it will be inferred from total avaialble PEs and other parallelism degrees. * **tokenizer** – The name or path of a HuggingFace Transformers tokenizer. * **tokenizer\\_mode** – The tokenizer mode. “auto” will use the fast tokenizer if available, and “slow” will always use the slow tokenizer. * **seed** – The seed to initialize the random number generator for sampling. * **devices** – The devices to run the model. It can be a single device or a list of devices. Each device can be either “cpu:X” or “cuda:X” where X is a specific device index. The default is “cpu:0”. * **param\\_file\\_path** – The path to the parameter file to use for pipeline generation. If not specified, the parameters will be saved in a temporary file which will be used for pipeline generation. * **param\\_saved\\_format** – The format of the parameter file. Only possible value is “safetensors” now. The default is “safetensors”. * **do\\_decompositions\\_for\\_model\\_rewrite** – Whether to decompose some ops to describe various parallelism strategies with mppp config. When the value is True, mppp config that matches with the decomposed FX graph should be given. * **comp\\_supertask\\_kind** – The format that pipeline’s supertask will be represented as. Possible values are “fx”,”dfg”, and “edf”, and the default is “fx”. * **cache\\_dir** – The cache directory for all generated files for this LLM instance. When its value is `None` , caching is disabled. The default is “$HOME/.cache/furiosa/llm”. * **backend** – The backend implementation to run forward() of a model for the LLM. The default is LLMBackend.TORCH\\_PIPELINE\\_RUNNER. * **use\\_blockwise\\_compile** – If True, each task will be compiled in the unit of transformer block, and compilation result for transformer block is generated once and reused. * **num\\_blocks\\_per\\_supertask** – The number of transformer blocks that will be merged into one supertask. This option is valid only when use\\_blockwise\\_compile=True . The default is 1. * **embed\\_all\\_constants\\_into\\_graph** – Whether to embed constant tensors into graph or make them as input of the graph and save them as separate files. * **paged\\_attention\\_num\\_blocks** – The maximum number of blocks that each k/v storage per layer can store. This argument must be given if model uses paged attention. * **paged\\_attention\\_block\\_size** – The maximum number of tokens that can be stored in a single paged attention block. This argument must be given if model uses paged attention. * **kv\\_cache\\_sharing\\_across\\_beams\\_config** – Configuration for sharing kv cache across beams. This argument must be given if and only if the model is optimized to share kv cache across beams. If this argument is given, decode phase buckets with batch size of `batch_size` \\* `kv_cache_sharing_across_beams_config.beam_width` will be created. * **scheduler\\_config** – Configuration for the scheduler, allowing to maximum number of tasks which can be queued to HW, maximum number', '<2-hop>\n\n* [.rst](../../_sources/furiosa_llm/references/llm.rst ""Download source file"") * .pdf LLM class ========= Contents -------- * [`LLM`](#furiosa_llm.LLM) + [`LLM.generate()`](#furiosa_llm.LLM.generate) + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms) LLM class [#](#llm-class ""Link to this heading"") ================================================ *class* furiosa\\_llm. LLM ( *pretrained\\_id : str* , *task\\_type : str | None = None* , *llm\\_config : LLMConfig | None = None* , *qformat\\_path : PathLike | None = None* , *qparam\\_path : PathLike | None = None* , *prefill\\_quant\\_bin\\_path : PathLike | None = None* , *decode\\_quant\\_bin\\_path : PathLike | None = None* , *config : Dict [ str , Any ] = {}* , *bucket\\_config : BucketConfig | None = None* , *max\\_seq\\_len\\_to\\_capture : int = 2048* , *tensor\\_parallel\\_size : int = 4* , *pipeline\\_parallel\\_size : int = 1* , *data\\_parallel\\_size : int | None = None* , *tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast | None = None* , *tokenizer\\_mode : Literal [ \'auto\' , \'slow\' ] = \'auto\'* , *seed : int | None = None* , *devices : str | Sequence [ Device ] | None = None* , *param\\_file\\_path : PathLike | None = None* , *param\\_saved\\_format : Literal [ \'safetensors\' , \'pt\' ] = \'safetensors\'* , *do\\_decompositions\\_for\\_model\\_rewrite : bool = False* , *comp\\_supertask\\_kind : Literal [ \'edf\' , \'dfg\' , \'fx\' ] | None = None* , *cache\\_dir : PathLike | None = PosixPath(\'/home/hyunsik/.cache/furiosa/llm\')* , *backend : LLMBackend | None = None* , *use\\_blockwise\\_compile : bool = True* , *num\\_blocks\\_per\\_supertask : int = 1* , *embed\\_all\\_constants\\_into\\_graph : bool = False* , *paged\\_attention\\_num\\_blocks : int | None = None* , *paged\\_attention\\_block\\_size : int = 1* , *kv\\_cache\\_sharing\\_across\\_beams\\_config : KvCacheSharingAcrossBeamsConfig | None = None* , *scheduler\\_config : SchedulerConfig = SchedulerConfig(npu\\_queue\\_limit=2, max\\_processing\\_samples=65536, spare\\_blocks\\_ratio=0.2, is\\_offline=False)* , *packing\\_type : Literal [ \'IDENTITY\' ] = \'IDENTITY\'* , *compiler\\_config\\_overrides : Mapping | None = None* , *use\\_random\\_weight : bool = False* , *num\\_pipeline\\_builder\\_workers : int = 1* , *num\\_compile\\_workers : int = 1* , *skip\\_engine : bool = False* , *\\** , *\\_cleanup : bool = True* , *\\*\\* kwargs* ) [[source]](../../_modules/furiosa_llm/api.html#LLM) [#](#furiosa_llm.LLM ""Link to this definition"") Bases: `object` An LLM for generating texts from given prompts and sampling parameters.  generate()`](#furiosa_llm.LLM.generate) + [`LLM.  of samples that can be processed by the scheduler, and ratio of spare blocks that are reserved by scheduler. * **packing\\_type** – Packing algorithm. Possible values are “IDENTITY” only for now * **compiler\\_config\\_overrides** – Overrides for the compiler config. This is a dictionary that includes the configuration for the compiler. * **use\\_random\\_weight** – If True, the model will be initialized with random weights. * **num\\_pipeline\\_builder\\_workers** – number of workers used for building pipelines (except for compilation). The default is 1 (no parallelism). Setting this value larger than 1 reduces pipeline building time, especially for large models, but requires much more memory. * **num\\_compile\\_workers** – number of workers used for compilation. The default is 1 (no parallelism). * **skip\\_engine** – If True, the native runtime engine will not be initialized. This is useful when you need the pipelines for other purposes than running them with the engine. generate ( *prompts: str | ~typing.List[str], sampling\\_params: ~furiosa\\_llm.sampling\\_params.SamplingParams = SamplingParams(n=1, best\\_of=1, temperature=1.0, top\\_p=1.0, top\\_k=-1, use\\_beam\\_search=False, length\\_penalty=1.0, early\\_stopping=False, max\\_tokens=16min\\_tokens=0, , prompt\\_token\\_ids: ~typing.List[int] | ~typing.List[~typing.List[int]] | None = None* ) → RequestOutput | List [ RequestOutput ] [[source]](../../_modules/furiosa_llm/api.html#LLM.generate) [#](#furiosa_llm.LLM.generate ""Link to this definition"") Generate texts from given prompts and sampling parameters. Parameters : * **prompts** – The prompts to generate texts. * **sampling\\_params** – The sampling parameters for generating texts. * **prompt\\_token\\_ids** – The token ids of the prompts. If not given, the token ids are generated from the prompts using the tokenizer. Returns : A list of RequestOutput objects containing the generated completions in the same order as the input prompts. get\\_splitted\\_gms ( *get\\_input\\_constants : bool = False* ) → Dict [ str , Tuple [ GraphModule , ... ] | Tuple [ Tuple [ GraphModule , Tuple [ Tensor | None , ... ] ] , ... ] ] [[source]](../../_modules/furiosa_llm/api.html#LLM.get_splitted_gms) [#](#furiosa_llm.LLM.get_splitted_gms ""Link to this definition"") Get sub GraphModules for each pipeline. Returns : Dictionary whose key is the pipeline name and value is the tuple containing `GraphModule``s (computation supertasks) and some additional information if necessary. if ``get_input_constants==False` , each value is just a tuple of `GraphModule``s in the pipeline. Otherwise, each value is a tuple whose element is ``GraphModule` in the pipeline and list of input constant tensors, which were originally constant tensors, but converted to input. The list of input constant tensors has same length as corresponding `GraphModule` ’s number of inputs with each element exactly corresponding to the input of the `GraphModule` with same index, but elements with original input tensor indexes are `None` . Return type : Dict[str, Union[Tuple[GraphModule, …], Tuple[Tuple[GraphModule, Tuple[Optional[torch.Tensor], …]], …],],] [previous References](../references.html ""previous page"") [next SamplingParams](sampling_params.html ""next page"") Contents * [`LLM`](#furiosa_llm.LLM) + [`LLM.generate()`](#furiosa_llm.LLM.generate) + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","The LLM class is designed for generating texts from given prompts using sampling parameters. These sampling parameters, such as temperature, top_p, and top_k, influence the text generation process by controlling aspects like randomness and diversity in the output. The parameters are part of the LLM.generate() function, which takes prompts and sampling parameters to produce generated text.",multi_hop_abstract_query_synthesizer
32,"What are the steps involved in installing the prerequisites for the FuriosaAI software stack, and how does setting up the APT server facilitate this process?","['<1-hop>\n\n* [.rst](../_sources/getting_started/prerequisites.rst ""Download source file"") * .pdf Installing Prerequisites ======================== Contents -------- * [ Requirements](#requirements) * [ Verifying if the system has devices](#verifying-if-the-system-has-devices) * [ Setting up APT server](#setting-up-apt-server) * [', '<2-hop>\n\nInstalling Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware) Installing Prerequisites [#](#installing-prerequisites ""Link to this heading"") ============================================================================== We will explain how to install the prerequisite packages necessary for FuriosaAI software stack. The prerequisite packages include device driver, firmware, and PE Runtime. These packages are available in package format for installation on Debian and Ubuntu systems. Requirements [#](#requirements ""Link to this heading"") ------------------------------------------------------ The minimum requirements are as follows: * Ubuntu 20.04 LTS (or Debian bullseye) or later * Administrator privileges on system (root) Verifying if the system has devices [#](#verifying-if-the-system-has-devices ""Link to this heading"") ---------------------------------------------------------------------------------------------------- You can verify the proper installation of FuriosaAI’s devices on your machine by running the following commands: ``` lspci -nn | grep FuriosaAI ``` If the device is properly installed, you should see the PCI information as shown below. ``` 4e:00.0 Processing accelerators [1200]: FuriosaAI, Inc. Device [1ed2:0001] (rev 01) ``` If the `lspci` command is not available, please install the following packages and run the commands to update PCIe ID database: ``` sudo apt update sudo apt install -y pciutils sudo update-pciids ``` Setting up APT server [#](#setting-up-apt-server ""Link to this heading"") ------------------------------------------------------------------------ To use the APT server provided by FuriosaAI, you must configure it on Ubuntu or Debian Linux as outlined below. 1. Install the required packages and register the signing key. ``` sudo apt update sudo apt install -y curl gnupg curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/cloud.google.gpg ``` 2. Configure the APT server according to the instructions provided for the Linux distribution versions. > ``` > echo ""deb [arch=$(dpkg --print-architecture)] http://asia-northeast3-apt.pkg.dev/projects/furiosa-ai $(. /etc/os-release && echo ""$VERSION_CODENAME"") main"" | sudo tee /etc/apt/sources.list.d/furiosa.list > > ``` Installing Pre-requisite Packages [#](#installing-pre-requisite-packages ""Link to this heading"") ------------------------------------------------------------------------------------------------ If you have registered the APT server as described above, you will be able to install the required packages: the device driver and PE Runtime. ``` sudo apt update sudo apt install furiosa-pert-rngd furiosa-driver-rngd ``` [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) is an useful CLI tool for listing and managing FuriosaAI NPUs. ``` sudo apt install furiosa-smi ``` Checking NPU devices [#](#checking-npu-devices ""Link to this heading"") ---------------------------------------------------------------------- Once the device driver and [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) are successfully installed, you can check the list of NPU devices as following command: ``` furiosa-smi info ``` Output: ``` +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware | Temp. | Power | PCI-BDF | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0 | 0.0.16+b4a67ca | 28.88°C | 38.00 W | 0000:4e:00.0 | +------+--------+----------------+---------+---------+--------------+ ``` Please refer to [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) to learn more about `furiosa-smi` command. Upgrading Device Firmware [#](#upgrading-device-firmware ""Link to this heading"") -------------------------------------------------------------------------------- Upgrading firmware versions can improve the performance and stability of the devices. If there is newer firmware in the latest release, you can upgrade them using the following methods: ``` sudo apt install furiosa-firmware-tools-rngd sudo apt install furiosa-firmware-image-rngd ``` Installing the `furiosa-firmware-image-rngd` package will automatically upgrade the firmware. The process takes approximately 3 to 5 minutes per device to complete. [previous Roadmap](../overview/roadmap.html ""previous page"") [next Quick Start with Furiosa LLM](furiosa_llm.html ""next page"") Contents * [Requirements](#requirements) * [Verifying if the system has devices](#verifying-if-the-system-has-devices) * [Setting up APT server](#setting-up-apt-server) * [Installing Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","To install the prerequisites for the FuriosaAI software stack, you need to follow several steps. First, ensure that your system meets the minimum requirements, which include having Ubuntu 20.04 LTS or Debian bullseye or later, and having administrator privileges. Next, verify if the system has FuriosaAI devices installed by using the command `lspci -nn | grep FuriosaAI`. If the command is unavailable, install `pciutils` and update the PCIe ID database. Then, set up the APT server by installing required packages and registering the signing key using `curl` and `gnupg`. Configure the APT server by adding the FuriosaAI package source to your system's sources list. Once the APT server is configured, you can install the necessary packages, such as the device driver and PE Runtime, using the command `sudo apt install furiosa-pert-rngd furiosa-driver-rngd`. Setting up the APT server is crucial as it allows you to access and install these packages efficiently, ensuring that your system is ready to run the FuriosaAI software stack.",multi_hop_abstract_query_synthesizer
33,Howw doo yoo installl the prerequisitess for the FuriosaAI software stackk andd whatt are the stepz to set upp the APT serverr on Ubuntu or Debian systems?,"['<1-hop>\n\n* [.rst](../_sources/getting_started/prerequisites.rst ""Download source file"") * .pdf Installing Prerequisites ======================== Contents -------- * [ Requirements](#requirements) * [ Verifying if the system has devices](#verifying-if-the-system-has-devices) * [ Setting up APT server](#setting-up-apt-server) * [', '<2-hop>\n\nInstalling Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware) Installing Prerequisites [#](#installing-prerequisites ""Link to this heading"") ============================================================================== We will explain how to install the prerequisite packages necessary for FuriosaAI software stack. The prerequisite packages include device driver, firmware, and PE Runtime. These packages are available in package format for installation on Debian and Ubuntu systems. Requirements [#](#requirements ""Link to this heading"") ------------------------------------------------------ The minimum requirements are as follows: * Ubuntu 20.04 LTS (or Debian bullseye) or later * Administrator privileges on system (root) Verifying if the system has devices [#](#verifying-if-the-system-has-devices ""Link to this heading"") ---------------------------------------------------------------------------------------------------- You can verify the proper installation of FuriosaAI’s devices on your machine by running the following commands: ``` lspci -nn | grep FuriosaAI ``` If the device is properly installed, you should see the PCI information as shown below. ``` 4e:00.0 Processing accelerators [1200]: FuriosaAI, Inc. Device [1ed2:0001] (rev 01) ``` If the `lspci` command is not available, please install the following packages and run the commands to update PCIe ID database: ``` sudo apt update sudo apt install -y pciutils sudo update-pciids ``` Setting up APT server [#](#setting-up-apt-server ""Link to this heading"") ------------------------------------------------------------------------ To use the APT server provided by FuriosaAI, you must configure it on Ubuntu or Debian Linux as outlined below. 1. Install the required packages and register the signing key. ``` sudo apt update sudo apt install -y curl gnupg curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/cloud.google.gpg ``` 2. Configure the APT server according to the instructions provided for the Linux distribution versions. > ``` > echo ""deb [arch=$(dpkg --print-architecture)] http://asia-northeast3-apt.pkg.dev/projects/furiosa-ai $(. /etc/os-release && echo ""$VERSION_CODENAME"") main"" | sudo tee /etc/apt/sources.list.d/furiosa.list > > ``` Installing Pre-requisite Packages [#](#installing-pre-requisite-packages ""Link to this heading"") ------------------------------------------------------------------------------------------------ If you have registered the APT server as described above, you will be able to install the required packages: the device driver and PE Runtime. ``` sudo apt update sudo apt install furiosa-pert-rngd furiosa-driver-rngd ``` [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) is an useful CLI tool for listing and managing FuriosaAI NPUs. ``` sudo apt install furiosa-smi ``` Checking NPU devices [#](#checking-npu-devices ""Link to this heading"") ---------------------------------------------------------------------- Once the device driver and [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) are successfully installed, you can check the list of NPU devices as following command: ``` furiosa-smi info ``` Output: ``` +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware | Temp. | Power | PCI-BDF | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0 | 0.0.16+b4a67ca | 28.88°C | 38.00 W | 0000:4e:00.0 | +------+--------+----------------+---------+---------+--------------+ ``` Please refer to [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) to learn more about `furiosa-smi` command. Upgrading Device Firmware [#](#upgrading-device-firmware ""Link to this heading"") -------------------------------------------------------------------------------- Upgrading firmware versions can improve the performance and stability of the devices. If there is newer firmware in the latest release, you can upgrade them using the following methods: ``` sudo apt install furiosa-firmware-tools-rngd sudo apt install furiosa-firmware-image-rngd ``` Installing the `furiosa-firmware-image-rngd` package will automatically upgrade the firmware. The process takes approximately 3 to 5 minutes per device to complete. [previous Roadmap](../overview/roadmap.html ""previous page"") [next Quick Start with Furiosa LLM](furiosa_llm.html ""next page"") Contents * [Requirements](#requirements) * [Verifying if the system has devices](#verifying-if-the-system-has-devices) * [Setting up APT server](#setting-up-apt-server) * [Installing Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","To install the prerequisites for the FuriosaAI software stack, you need to ensure that your system meets the minimum requirements, which include having Ubuntu 20.04 LTS or Debian bullseye or later, and administrator privileges. The prerequisite packages include the device driver, firmware, and PE Runtime, which are available in package format for installation on Debian and Ubuntu systems. You can verify the installation of FuriosaAI’s devices by using the command `lspci -nn | grep FuriosaAI`. If the `lspci` command is not available, you should install `pciutils` and update the PCIe ID database. To set up the APT server, you must install the required packages and register the signing key using the command `sudo apt update` followed by `sudo apt install -y curl gnupg`. Then, configure the APT server by adding the FuriosaAI repository to your sources list using the command: `echo ""deb [arch=$(dpkg --print-architecture)] http://asia-northeast3-apt.pkg.dev/projects/furiosa-ai $(. /etc/os-release && echo ""$VERSION_CODENAME"") main"" | sudo tee /etc/apt/sources.list.d/furiosa.list`.",multi_hop_abstract_query_synthesizer
34,How does the FuriosaAI Software Stack facilitate machine learning performance evaluation and text generation using the MLPerf Inference Benchmark and LLM class?,"['<1-hop>\n\n* [.rst](../_sources/getting_started/furiosa_mlperf.rst ""Download source file"") * .pdf Running MLPerf™ Inference Benchmark =================================== Contents -------- * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1.  Running MLPerf Inference Benchmark](#id1) + [', '<2-hop>\n\nSYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1. Installing `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command ""Link to this heading"") -------------------------------------------------------------------------------------------------- To install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following: The minimum requirements for `furiosa-mlperf` are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers ( [Setting up APT server](prerequisites.html#aptsetup) ) * About 100GB storage space (only for the Llama 3.1 70B) Then, please install the `furiosa-mlperf` package as follows: ``` sudo apt update sudo apt install -y furiosa-mlperf ``` This command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` . Running MLPerf Inference Benchmark [#](#id1 ""Link to this heading"") ------------------------------------------------------------------- ### SYNOPSIS [#](#synopsis ""Link to this heading"") The `furiosa-mlperf` command provides the following subcommands: ``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z) Usage: furiosa-mlperf <COMMAND> Commands: bert-offline Run BERT benchmark with offline scenario bert-server Run BERT benchmark with server scenario gpt-j-offline Run GPT-J benchmark with offline scenario gpt-j-server Run GPT-J benchmark with server scenario llama-3.1-offline Run Llama 3.1 benchmark with offline scenario llama-3.1-server Run Llama 3.1 benchmark with server scenario help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version ``` ### Examples [#](#examples ""Link to this heading"") * BERT benchmark The BERT benchmark is based on running with a single RNGD. + Server Scenario To run BERT Large serving inference benchmark, you can use the following command: ``` furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh ""npu:0:*"" ``` + Offline Scenario To run BERT Large offline inference benchmark, you can use the following command: ``` furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh ""npu:0:*"" ``` * GPT-J benchmark The GPT-J benchmark is based on running with a single RNGD. + Server Scenario To run GPT-J 6B serving inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result ``` + Offline Scenario To run GPT-J 6B offline inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result ``` * Llama 3.1 benchmark The Llama 3.1 benchmark is based on running with four RNGDs. + Server Scenario To run Llama 3.1 70B serving inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result ``` + Offline Scenario To run Llama 3.1 70B offline inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result ``` * Common Once the process completes, it writes the results to a file in the specified results directory. You can open this file to view a summary of the results. ``` cat gpt-j-offline-result/mlperf_log_summary.txt ``` ``` ================================================ MLPerf Results Summary ================================================ SUT name : GPT-J SUT Scenario : Offline Mode : PerformanceOnly Samples per second: 11.842 Tokens per second (inferred): 817.095 Result is : VALID Min duration satisfied : Yes Min queries satisfied : Yes Early stopping satisfied: Yes ```', '<3-hop>\n\nContents -------- * [`LLM`](#furiosa_llm.LLM) + [`LLM.generate()`](#furiosa_llm.LLM.generate) + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms) LLM class [#](#llm-class ""Link to this heading"") ================================================ *class* furiosa\\_llm. LLM ( *pretrained\\_id : str* , *task\\_type : str | None = None* , *llm\\_config : LLMConfig | None = None* , *qformat\\_path : PathLike | None = None* , *qparam\\_path : PathLike | None = None* , *prefill\\_quant\\_bin\\_path : PathLike | None = None* , *decode\\_quant\\_bin\\_path : PathLike | None = None* , *config : Dict [ str , Any ] = {}* , *bucket\\_config : BucketConfig | None = None* , *max\\_seq\\_len\\_to\\_capture : int = 2048* , *tensor\\_parallel\\_size : int = 4* , *pipeline\\_parallel\\_size : int = 1* , *data\\_parallel\\_size : int | None = None* , *tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast | None = None* , *tokenizer\\_mode : Literal [ \'auto\' , \'slow\' ] = \'auto\'* , *seed : int | None = None* , *devices : str | Sequence [ Device ] | None = None* , *param\\_file\\_path : PathLike | None = None* , *param\\_saved\\_format : Literal [ \'safetensors\' , \'pt\' ] = \'safetensors\'* , *do\\_decompositions\\_for\\_model\\_rewrite : bool = False* , *comp\\_supertask\\_kind : Literal [ \'edf\' , \'dfg\' , \'fx\' ] | None = None* , *cache\\_dir : PathLike | None = PosixPath(\'/home/hyunsik/.cache/furiosa/llm\')* , *backend : LLMBackend | None = None* , *use\\_blockwise\\_compile : bool = True* , *num\\_blocks\\_per\\_supertask : int = 1* , *embed\\_all\\_constants\\_into\\_graph : bool = False* , *paged\\_attention\\_num\\_blocks : int | None = None* , *paged\\_attention\\_block\\_size : int = 1* , *kv\\_cache\\_sharing\\_across\\_beams\\_config : KvCacheSharingAcrossBeamsConfig | None = None* , *scheduler\\_config : SchedulerConfig = SchedulerConfig(npu\\_queue\\_limit=2, max\\_processing\\_samples=65536, spare\\_blocks\\_ratio=0.2, is\\_offline=False)* , *packing\\_type : Literal [ \'IDENTITY\' ] = \'IDENTITY\'* , *compiler\\_config\\_overrides : Mapping | None = None* , *use\\_random\\_weight : bool = False* , *num\\_pipeline\\_builder\\_workers : int = 1* , *num\\_compile\\_workers : int = 1* , *skip\\_engine : bool = False* , *\\** , *\\_cleanup : bool = True* , *\\*\\* kwargs* ) [[source]](../../_modules/furiosa_llm/api.html#LLM) [#](#furiosa_llm.LLM ""Link to this definition"") Bases: `object` An LLM for generating texts from given prompts and sampling parameters. Parameters : * **pretrained\\_id** – The name of the pretrained model. This corresponds to pretrained\\_model\\_name\\_or\\_path in HuggingFace Transformers. * **task\\_type** – The type of the task. This corresponds to task in HuggingFace Transformers. See <https://huggingface.co/docs/transformers/main/en/quicktour#pipeline> for more details. * **llm\\_config** – The configuration for the LLM. This includes quantization and optimization configurations. * **qformat\\_path** – The path to the quantization format file. * **qparam\\_path** – The path to the quantization parameter file. * **prefill\\_quant\\_bin\\_path** – The path to the quantziation prefill bin file. * **decode\\_quant\\_bin\\_path** – The path to the quantziation decode bin file. * **config** – The configuration for the HuggingFace Transformers model. This is a dictionary that includes the configuration for the model. * **bucket\\_config** – Config for bucket generating policy. If not given, the model will use single one batch, max\\_seq\\_len\\_to\\_capture attention size bucket per each phase. * **max\\_seq\\_len\\_to\\_capture** – Maximum sequence length covered by LLM engine. Sequence with larger context than this will not be covered. The default is 2048. * **tensor\\_parallel\\_size** – The number of PEs for each tensor parallelism group. The default is 4. * **pipeline\\_parallel\\_size** – The number of pipeline stages for pipeline parallelism. The default is 1, which means no pipeline parallelism. * **data\\_parallel\\_size** – The size of the data parallelism group. If not given, it will be inferred from total avaialble PEs and other parallelism degrees. * **tokenizer** – The name or path of a HuggingFace Transformers tokenizer. * **tokenizer\\_mode** – The tokenizer mode. “auto” will use the fast tokenizer if available, and “slow” will always use the slow tokenizer. * **seed** – The seed to initialize the random number generator for sampling. * **devices** – The devices to run the model. It can be a single device or a list of devices. Each device can be either “cpu:X” or “cuda:X” where X is a specific device index. The default is “cpu:0”. * **param\\_file\\_path** – The path to the parameter file to use for pipeline generation. If not specified, the parameters will be saved in a temporary file which will be used for pipeline generation. * **param\\_saved\\_format** – The format of the parameter file. Only possible value is “safetensors” now. The default is “safetensors”. * **do\\_decompositions\\_for\\_model\\_rewrite** – Whether to decompose some ops to describe various parallelism strategies with mppp config. When the value is True, mppp config that matches with the decomposed FX graph should be given. * **comp\\_supertask\\_kind** – The format that pipeline’s supertask will be represented as. Possible values are “fx”,”dfg”, and “edf”, and the default is “fx”. * **cache\\_dir** – The cache directory for all generated files for this LLM instance. When its value is `None` , caching is disabled. The default is “$HOME/.cache/furiosa/llm”. * **backend** – The backend implementation to run forward() of a model for the LLM. The default is LLMBackend.TORCH\\_PIPELINE\\_RUNNER. * **use\\_blockwise\\_compile** – If True, each task will be compiled in the unit of transformer block, and compilation result for transformer block is generated once and reused. * **num\\_blocks\\_per\\_supertask** – The number of transformer blocks that will be merged into one supertask. This option is valid only when use\\_blockwise\\_compile=True . The default is 1. * **embed\\_all\\_constants\\_into\\_graph** – Whether to embed constant tensors into graph or make them as input of the graph and save them as separate files. * **paged\\_attention\\_num\\_blocks** – The maximum number of blocks that each k/v storage per layer can store. This argument must be given if model uses paged attention. * **paged\\_attention\\_block\\_size** – The maximum number of tokens that can be stored in a single paged attention block. This argument must be given if model uses paged attention. * **kv\\_cache\\_sharing\\_across\\_beams\\_config** – Configuration for sharing kv cache across beams. This argument must be given if and only if the model is optimized to share kv cache across beams. If this argument is given, decode phase buckets with batch size of `batch_size` \\* `kv_cache_sharing_across_beams_config.beam_width` will be created. * **scheduler\\_config** – Configuration for the scheduler, allowing to maximum number of tasks which can be queued to HW, maximum number', '<4-hop>\n\n* [.rst](../../_sources/furiosa_llm/references/llm.rst ""Download source file"") * .pdf LLM class ========= Contents -------- * [`LLM`](#furiosa_llm.LLM) + [`LLM.generate()`](#furiosa_llm.LLM.generate) + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms) LLM class [#](#llm-class ""Link to this heading"") ================================================ *class* furiosa\\_llm. LLM ( *pretrained\\_id : str* , *task\\_type : str | None = None* , *llm\\_config : LLMConfig | None = None* , *qformat\\_path : PathLike | None = None* , *qparam\\_path : PathLike | None = None* , *prefill\\_quant\\_bin\\_path : PathLike | None = None* , *decode\\_quant\\_bin\\_path : PathLike | None = None* , *config : Dict [ str , Any ] = {}* , *bucket\\_config : BucketConfig | None = None* , *max\\_seq\\_len\\_to\\_capture : int = 2048* , *tensor\\_parallel\\_size : int = 4* , *pipeline\\_parallel\\_size : int = 1* , *data\\_parallel\\_size : int | None = None* , *tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast | None = None* , *tokenizer\\_mode : Literal [ \'auto\' , \'slow\' ] = \'auto\'* , *seed : int | None = None* , *devices : str | Sequence [ Device ] | None = None* , *param\\_file\\_path : PathLike | None = None* , *param\\_saved\\_format : Literal [ \'safetensors\' , \'pt\' ] = \'safetensors\'* , *do\\_decompositions\\_for\\_model\\_rewrite : bool = False* , *comp\\_supertask\\_kind : Literal [ \'edf\' , \'dfg\' , \'fx\' ] | None = None* , *cache\\_dir : PathLike | None = PosixPath(\'/home/hyunsik/.cache/furiosa/llm\')* , *backend : LLMBackend | None = None* , *use\\_blockwise\\_compile : bool = True* , *num\\_blocks\\_per\\_supertask : int = 1* , *embed\\_all\\_constants\\_into\\_graph : bool = False* , *paged\\_attention\\_num\\_blocks : int | None = None* , *paged\\_attention\\_block\\_size : int = 1* , *kv\\_cache\\_sharing\\_across\\_beams\\_config : KvCacheSharingAcrossBeamsConfig | None = None* , *scheduler\\_config : SchedulerConfig = SchedulerConfig(npu\\_queue\\_limit=2, max\\_processing\\_samples=65536, spare\\_blocks\\_ratio=0.2, is\\_offline=False)* , *packing\\_type : Literal [ \'IDENTITY\' ] = \'IDENTITY\'* , *compiler\\_config\\_overrides : Mapping | None = None* , *use\\_random\\_weight : bool = False* , *num\\_pipeline\\_builder\\_workers : int = 1* , *num\\_compile\\_workers : int = 1* , *skip\\_engine : bool = False* , *\\** , *\\_cleanup : bool = True* , *\\*\\* kwargs* ) [[source]](../../_modules/furiosa_llm/api.html#LLM) [#](#furiosa_llm.LLM ""Link to this definition"") Bases: `object` An LLM for generating texts from given prompts and sampling parameters.  generate()`](#furiosa_llm.LLM.generate) + [`LLM.  of samples that can be processed by the scheduler, and ratio of spare blocks that are reserved by scheduler. * **packing\\_type** – Packing algorithm. Possible values are “IDENTITY” only for now * **compiler\\_config\\_overrides** – Overrides for the compiler config. This is a dictionary that includes the configuration for the compiler. * **use\\_random\\_weight** – If True, the model will be initialized with random weights. * **num\\_pipeline\\_builder\\_workers** – number of workers used for building pipelines (except for compilation). The default is 1 (no parallelism). Setting this value larger than 1 reduces pipeline building time, especially for large models, but requires much more memory. * **num\\_compile\\_workers** – number of workers used for compilation. The default is 1 (no parallelism). * **skip\\_engine** – If True, the native runtime engine will not be initialized. This is useful when you need the pipelines for other purposes than running them with the engine. generate ( *prompts: str | ~typing.List[str], sampling\\_params: ~furiosa\\_llm.sampling\\_params.SamplingParams = SamplingParams(n=1, best\\_of=1, temperature=1.0, top\\_p=1.0, top\\_k=-1, use\\_beam\\_search=False, length\\_penalty=1.0, early\\_stopping=False, max\\_tokens=16min\\_tokens=0, , prompt\\_token\\_ids: ~typing.List[int] | ~typing.List[~typing.List[int]] | None = None* ) → RequestOutput | List [ RequestOutput ] [[source]](../../_modules/furiosa_llm/api.html#LLM.generate) [#](#furiosa_llm.LLM.generate ""Link to this definition"") Generate texts from given prompts and sampling parameters. Parameters : * **prompts** – The prompts to generate texts. * **sampling\\_params** – The sampling parameters for generating texts. * **prompt\\_token\\_ids** – The token ids of the prompts. If not given, the token ids are generated from the prompts using the tokenizer. Returns : A list of RequestOutput objects containing the generated completions in the same order as the input prompts. get\\_splitted\\_gms ( *get\\_input\\_constants : bool = False* ) → Dict [ str , Tuple [ GraphModule , ... ] | Tuple [ Tuple [ GraphModule , Tuple [ Tensor | None , ... ] ] , ... ] ] [[source]](../../_modules/furiosa_llm/api.html#LLM.get_splitted_gms) [#](#furiosa_llm.LLM.get_splitted_gms ""Link to this definition"") Get sub GraphModules for each pipeline. Returns : Dictionary whose key is the pipeline name and value is the tuple containing `GraphModule``s (computation supertasks) and some additional information if necessary. if ``get_input_constants==False` , each value is just a tuple of `GraphModule``s in the pipeline. Otherwise, each value is a tuple whose element is ``GraphModule` in the pipeline and list of input constant tensors, which were originally constant tensors, but converted to input. The list of input constant tensors has same length as corresponding `GraphModule` ’s number of inputs with each element exactly corresponding to the input of the `GraphModule` with same index, but elements with original input tensor indexes are `None` . Return type : Dict[str, Union[Tuple[GraphModule, …], Tuple[Tuple[GraphModule, Tuple[Optional[torch.Tensor], …]], …],],] [previous References](../references.html ""previous page"") [next SamplingParams](sampling_params.html ""next page"") Contents * [`LLM`](#furiosa_llm.LLM) + [`LLM.generate()`](#furiosa_llm.LLM.generate) + [`LLM.get_splitted_gms()`](#furiosa_llm.LLM.get_splitted_gms) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","The FuriosaAI Software Stack facilitates machine learning performance evaluation through the MLPerf Inference Benchmark, which is a suite used to evaluate the performance of machine learning software, hardware, and cloud platforms. The stack provides the `furiosa-mlperf` command to easily run these benchmarks, including scenarios for models like BERT, GPT-J, and Llama 3.1, in both server and offline modes. For text generation, the FuriosaAI Software Stack includes the LLM class, which is designed to generate texts from given prompts using various sampling parameters. The LLM class supports configurations for quantization, optimization, and parallelism, allowing for efficient text generation tasks.",multi_hop_abstract_query_synthesizer
35,"How does the Furiosa LLM Engine ensure compatibility with OpenAI's API, and what are the necessary steps to launch an OpenAI-compatible server using the furiosa-llm package?","['<1-hop>\n\nUsing OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api) OpenAI Compatible Server [#](#openai-compatible-server ""Link to this heading"") ============================================================================== The `furiosa-llm` package includes an OpenAI-compatible server that can interact with OpenAI clients (and, of course, HTTP clients as well), supporting the `/v1/chat/completions` and `/v1/completions` APIs. In this section, we will explain how to launch the OpenAI-compatible furiosa-llm server. Tip You can learn more about the OpenAI API at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) . To launch the server, you must prepare: (1) the FuriosaAI LLM Engine artifact and (2) a chat template for the model. To download the FuriosaAI LLM Engine artifact, follow the instructions provided through our distribution channels or contact our sales team. To prepare the chat template, follow the instructions in the following section. Preparing Chat Templates [#](#preparing-chat-templates ""Link to this heading"") ------------------------------------------------------------------------------ Furiosa SDK 2024.1.0 (alpha) uses Transformers v4.31.0, which does not include a chat template. Therefore, to support `/v1/chat/completions` , you must provide a chat template yourself. This constraint will be removed in future releases. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. If you have access to the Llama repositories on Hugging Face, you can obtain the chat template for Llama using the following commands: ``` # Prerequisite: create a separate environment to install the latest Transformers version pip install ""transformers>=4.34.0"" python - <<EOF from transformers import AutoTokenizer tok = AutoTokenizer.from_pretrained(\'meta-llama/Meta-Llama-3.1-70B-Instruct\') with open(\'chat_template.tpl\', \'w\') as f: f.write(tok.chat_template) EOF ``` Launching the Server [#](#launching-the-server ""Link to this heading"") ---------------------------------------------------------------------- You can launch the server using the furiosa-llm serve command. ### Arguments for the serve command [#](#arguments-for-the-serve-command ""Link to this heading"") ``` usage: furiosa-llm serve [-h] --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} --artifact ARTIFACT [--host HOST] [--port PORT] --chat-template CHAT_TEMPLATE [--response-role RESPONSE_ROLE] [-pp PIPELINE_PARALLEL_SIZE] [-tp TENSOR_PARALLEL_SIZE] [--devices DEVICES] options: -h, --help show this help message and exit --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} The model to use. Currently only one model is supported per server. --artifact ARTIFACT Path to Furiosa LLM Engine artifact --host HOST Host to bind the server to --port PORT Port to bind the server to --chat-template CHAT_TEMPLATE Path to chat template file (must be a jinja2 template) --response-role RESPONSE_ROLE Response role for /v1/chat/completions API (default: \'assistant\') -pp PIPELINE_PARALLEL_SIZE, --pipeline-parallel-size PIPELINE_PARALLEL_SIZE Number of pipeline stages. -tp TENSOR_PARALLEL_SIZE, --tensor-parallel-size TENSOR_PARALLEL_SIZE Number of tensor parallel replicas. --devices DEVICES Devices to use (e.g. ""npu:0:*,npu:1:*""). If unspecified, all available devices from the host will be used. ``` Examples [#](#examples ""Link to this heading"") ---------------------------------------------- ### LLaMA-3.1-70B with 4 RNGDs [#](#llama-3-1-70b-with-4-rngds ""Link to this heading"") ``` furiosa-llm serve \\ --model {path to mlperf-llama-3-1-fp8-pp4} \\ -tp 4 -pp 4 --devices ""npu:0:*,npu:1:*,npu:2:*,npu:3:*"" \\ --chat-template {path to chat template} ``` ### LLaMA-3.1-8B with Single RNGD [#](#llama-3-1-8b-with-single-rngd ""Link to this heading"") ``` furiosa-llm serve \\ --model {path to mlperf-llama-3-1-8b-fp8} \\ -tp 4 -pp 1 --devices ""npu:0:*"" \\ # You may choose arbitrary device index, if multiple devices are on host --chat-template {path to chat template} ``` Using OpenAI Client [#](#using-openai-client ""Link to this heading"") -------------------------------------------------------------------- You can use two APIs: `client.chat.completions` and `client.completions` . You can also set `stream=True` to receive a streaming response. Tip You can install the OpenAI Python client using the following command: ``` pip install openai ``` ``` import openai HOST = ""localhost:8000"" openai.api_base = f""http://{HOST}/v1"" openai.api_key = ""0000"" stream_chat_completion = openai.ChatCompletion.create( model="""", messages=[ {""role"": ""system"", ""content"": ""You are a helpful assistant.""}, {""role"": ""user"", ""content"": ""What is the largest animal in the world?""}, ], stream=True, ) for completion in stream_chat_completion: content = completion.choices[0].delta.get(""content"") if content: print(content, end="""") ``` The compatibility with OpenAI API [#](#the-compatibility-with-openai-api ""Link to this heading"") ------------------------------------------------------------------------------------------------ Currently, `furiosa serve` supports the following OpenAI API parameters: You can find more about each parameter at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) . Warning Please note that using `use_beam_search` with `stream` is not allowed because the beam search cannot determine the tokens until the end of the sequence. In 2024.1 release, `n` works only for beam search and it will be fixed in the next release. * `n` * `temperature` * `top_p` * `top_k` * `early_stopping` * `length_penalty` * `max_tokens` * `min_tokens` * `use_beam_search` * `best_of` [previous Furiosa LLM](intro.html ""previous page"") [next References](references.html ""next page"") Contents * [Preparing Chat Templates](#preparing-chat-templates) * [Launching the Server](#launching-the-server) + [Arguments for the serve command](#arguments-for-the-serve-command) * [Examples](#examples) + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds) + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..', '<2-hop>\n\n* [.rst](../_sources/furiosa_llm/furiosa-llm-serve.rst ""Download source file"") * .pdf OpenAI Compatible Server ======================== Contents -------- * [ Preparing Chat Templates](#preparing-chat-templates) * [ Launching the Server](#launching-the-server) + [ Arguments for the serve command](#arguments-for-the-serve-command) * [ Examples](#examples) + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds) + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [']","The Furiosa LLM Engine ensures compatibility with OpenAI's API by supporting the `/v1/chat/completions` and `/v1/completions` APIs, allowing interaction with OpenAI clients. To launch an OpenAI-compatible server using the furiosa-llm package, you need to prepare the FuriosaAI LLM Engine artifact and a chat template for the model. The server can be launched using the `furiosa-llm serve` command, specifying the model, artifact path, and chat template file. The server supports various OpenAI API parameters such as `n`, `temperature`, `top_p`, `top_k`, and others, although some constraints like the use of `use_beam_search` with `stream` are noted. Additionally, the Furiosa SDK 2024.1.0 (alpha) version requires users to provide their own chat template, as it does not include one by default.",multi_hop_specific_query_synthesizer
36,How does the Furiosa LLM enhance AI server capabilities with its OpenAI-compatible API and Kubernetes support?,"['<1-hop>\n\n* [.rst](../_sources/getting_started/furiosa_llm.rst ""Download source file"") * .pdf Quick Start with Furiosa LLM ============================ Contents -------- * [ Installing Furiosa LLM](#installing-furiosa-llm) + [ Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [ Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [', '<2-hop>\n\nFuriosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) FuriosaAI’s Software Stack [#](#furiosaai-s-software-stack ""Link to this heading"") ================================================================================== FuriosaAI provides the streamlined software stack to allow FuriosaAI NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials. The above diagram demonstrates the SW stack provided by FuriosaAI, by layers. The following outlines the key components. Kernel Driver, Firmware, and PE Runtime [#](#kernel-driver-firmware-and-pe-runtime ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- The kernel device driver enables the Linux operating system to recognize NPU devices and expose them as Linux device files. The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE). PERT is responsible for communicating with the host’s runtime and scheduling, managing the resources of PEs to execute NPU tasks. Furiosa Compiler [#](#furiosa-compiler ""Link to this heading"") -------------------------------------------------------------- Furiosa compiler analyzes, optimizes a model graph, and generates a NPU executable program. The operation passes involve graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. When `torch.compile()` backend, `FuriosaBackend` is used or `furiosa-llm` is used, the Furiosa Compiler is transparently used to generate NPU executable programs for Runtime. Furiosa Runtime [#](#furiosa-runtime ""Link to this heading"") ------------------------------------------------------------ Runtime loads multiple executable NPU programs generated by Furiosa compiler, and run them on the NPU. A single model can be compiled into multiple executable programs according to model architectures and applications. Runtime is responsible for scheduling NPU programs and managing computation and memory resource on NPUs and CPUs. Also, Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs. Furiosa Model Compressor (Quantizer) [#](#furiosa-model-compressor-quantizer ""Link to this heading"") ---------------------------------------------------------------------------------------------------- Furiosa Model Compressor is a library as well as toolkit for model calibration and quantization. Model quantization is a powerful technique to reduce memory footprint, computation cost, inference latency and power consumption. Furiosa Model Compressor provides post-training quantization methods, such as * BF16 (W16A16) * INT8 Weight-Only (W8A16) * FP8 (W8A8) * INT8 SmoothQuant (W8A8) * INT4 Weight-Only (W4A16 AWQ / GPTQ) (Planned in release 2024.2) Furiosa LLM [#](#furiosa-llm ""Link to this heading"") ---------------------------------------------------- Furiosa LLM provides a high-performance inference engine for LLM models, such as Llama 3.1 70B, 8B, GPT-J, and Bert. Furiosa LLM is designed to provide the state-of-the-art serving optimization for LLM models. The key features of Furiosa LLM include vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and OpenAI-compatible API server. You can find further information at [Furiosa LLM](../furiosa_llm/intro.html#furiosallm) . Kubernetes Support [#](#kubernetes-support ""Link to this heading"") ------------------------------------------------------------------ Kubernetes, an open-source platform designed to manage containerized applications and services, is extensively adopted by various companies due to its robust capabilities for deploying, scaling, and automating containerized workloads. FuriosaAI software stack also offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment. FuriosaAI’s device plugin enables Kubernetes clusters to recognize FuriosaAI’s NPUs and allows NPUs to be scheduled for workloads and services that require them. This feature allows users to easily deploy AI workloads with FuriosaAI NPUs on Kubernetes, enabling efficient resource utilization and scaling. You can find more information about Kubernetes support in the [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) . [previous FuriosaAI RNGD](rngd.html ""previous page"") [next Supported Models](supported_models.html ""next page"") Contents * [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","The Furiosa LLM enhances AI server capabilities by providing a high-performance inference engine for large language models, such as Llama 3.1 and GPT-J, with features like a vLLM-compatible API and OpenAI-compatible API server. Additionally, FuriosaAI's software stack offers native integration with Kubernetes, allowing seamless deployment and management of AI applications. This integration enables Kubernetes clusters to recognize FuriosaAI's NPUs, facilitating efficient resource utilization and scaling of AI workloads.",multi_hop_specific_query_synthesizer
37,How does the FuriosaAI Software Stack facilitate the benchmarking of LLaMA-3.1-8B using the MLPerf Inference Benchmark?,"['<1-hop>\n\n* [.rst](../_sources/furiosa_llm/furiosa-llm-serve.rst ""Download source file"") * .pdf OpenAI Compatible Server ======================== Contents -------- * [ Preparing Chat Templates](#preparing-chat-templates) * [ Launching the Server](#launching-the-server) + [ Arguments for the serve command](#arguments-for-the-serve-command) * [ Examples](#examples) + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds) + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [', '<2-hop>\n\nSYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1. Installing `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command ""Link to this heading"") -------------------------------------------------------------------------------------------------- To install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following: The minimum requirements for `furiosa-mlperf` are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers ( [Setting up APT server](prerequisites.html#aptsetup) ) * About 100GB storage space (only for the Llama 3.1 70B) Then, please install the `furiosa-mlperf` package as follows: ``` sudo apt update sudo apt install -y furiosa-mlperf ``` This command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` . Running MLPerf Inference Benchmark [#](#id1 ""Link to this heading"") ------------------------------------------------------------------- ### SYNOPSIS [#](#synopsis ""Link to this heading"") The `furiosa-mlperf` command provides the following subcommands: ``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z) Usage: furiosa-mlperf <COMMAND> Commands: bert-offline Run BERT benchmark with offline scenario bert-server Run BERT benchmark with server scenario gpt-j-offline Run GPT-J benchmark with offline scenario gpt-j-server Run GPT-J benchmark with server scenario llama-3.1-offline Run Llama 3.1 benchmark with offline scenario llama-3.1-server Run Llama 3.1 benchmark with server scenario help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version ``` ### Examples [#](#examples ""Link to this heading"") * BERT benchmark The BERT benchmark is based on running with a single RNGD. + Server Scenario To run BERT Large serving inference benchmark, you can use the following command: ``` furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh ""npu:0:*"" ``` + Offline Scenario To run BERT Large offline inference benchmark, you can use the following command: ``` furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh ""npu:0:*"" ``` * GPT-J benchmark The GPT-J benchmark is based on running with a single RNGD. + Server Scenario To run GPT-J 6B serving inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result ``` + Offline Scenario To run GPT-J 6B offline inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result ``` * Llama 3.1 benchmark The Llama 3.1 benchmark is based on running with four RNGDs. + Server Scenario To run Llama 3.1 70B serving inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result ``` + Offline Scenario To run Llama 3.1 70B offline inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result ``` * Common Once the process completes, it writes the results to a file in the specified results directory. You can open this file to view a summary of the results. ``` cat gpt-j-offline-result/mlperf_log_summary.txt ``` ``` ================================================ MLPerf Results Summary ================================================ SUT name : GPT-J SUT Scenario : Offline Mode : PerformanceOnly Samples per second: 11.842 Tokens per second (inferred): 817.095 Result is : VALID Min duration satisfied : Yes Min queries satisfied : Yes Early stopping satisfied: Yes ```']","The FuriosaAI Software Stack facilitates the benchmarking of LLaMA-3.1-8B using the MLPerf Inference Benchmark by providing the `furiosa-mlperf` command. This command allows users to run the LLaMA-3.1 benchmark in both server and offline scenarios. Specifically, for the LLaMA-3.1-8B model, the benchmark is executed with a single RNGD, as mentioned in the context. The `furiosa-mlperf` command is part of a suite that evaluates the performance of machine learning software, hardware, and cloud platforms, and it is based on MLPerf Inference Benchmark v4.1, with the Llama2 benchmark replaced by Llama 3.1.",multi_hop_specific_query_synthesizer
38,"How does the FuriosaAI software stack utilize Linux systems to enhance AI server performance, particularly with the integration of NPUs and Kubernetes support?","['<1-hop>\n\nInstalling Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware) Installing Prerequisites [#](#installing-prerequisites ""Link to this heading"") ============================================================================== We will explain how to install the prerequisite packages necessary for FuriosaAI software stack. The prerequisite packages include device driver, firmware, and PE Runtime. These packages are available in package format for installation on Debian and Ubuntu systems. Requirements [#](#requirements ""Link to this heading"") ------------------------------------------------------ The minimum requirements are as follows: * Ubuntu 20.04 LTS (or Debian bullseye) or later * Administrator privileges on system (root) Verifying if the system has devices [#](#verifying-if-the-system-has-devices ""Link to this heading"") ---------------------------------------------------------------------------------------------------- You can verify the proper installation of FuriosaAI’s devices on your machine by running the following commands: ``` lspci -nn | grep FuriosaAI ``` If the device is properly installed, you should see the PCI information as shown below. ``` 4e:00.0 Processing accelerators [1200]: FuriosaAI, Inc. Device [1ed2:0001] (rev 01) ``` If the `lspci` command is not available, please install the following packages and run the commands to update PCIe ID database: ``` sudo apt update sudo apt install -y pciutils sudo update-pciids ``` Setting up APT server [#](#setting-up-apt-server ""Link to this heading"") ------------------------------------------------------------------------ To use the APT server provided by FuriosaAI, you must configure it on Ubuntu or Debian Linux as outlined below. 1. Install the required packages and register the signing key. ``` sudo apt update sudo apt install -y curl gnupg curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/cloud.google.gpg ``` 2. Configure the APT server according to the instructions provided for the Linux distribution versions. > ``` > echo ""deb [arch=$(dpkg --print-architecture)] http://asia-northeast3-apt.pkg.dev/projects/furiosa-ai $(. /etc/os-release && echo ""$VERSION_CODENAME"") main"" | sudo tee /etc/apt/sources.list.d/furiosa.list > > ``` Installing Pre-requisite Packages [#](#installing-pre-requisite-packages ""Link to this heading"") ------------------------------------------------------------------------------------------------ If you have registered the APT server as described above, you will be able to install the required packages: the device driver and PE Runtime. ``` sudo apt update sudo apt install furiosa-pert-rngd furiosa-driver-rngd ``` [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) is an useful CLI tool for listing and managing FuriosaAI NPUs. ``` sudo apt install furiosa-smi ``` Checking NPU devices [#](#checking-npu-devices ""Link to this heading"") ---------------------------------------------------------------------- Once the device driver and [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) are successfully installed, you can check the list of NPU devices as following command: ``` furiosa-smi info ``` Output: ``` +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware | Temp. | Power | PCI-BDF | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0 | 0.0.16+b4a67ca | 28.88°C | 38.00 W | 0000:4e:00.0 | +------+--------+----------------+---------+---------+--------------+ ``` Please refer to [furiosa-smi](../device_management/furiosa_smi.html#furiosasmi) to learn more about `furiosa-smi` command. Upgrading Device Firmware [#](#upgrading-device-firmware ""Link to this heading"") -------------------------------------------------------------------------------- Upgrading firmware versions can improve the performance and stability of the devices. If there is newer firmware in the latest release, you can upgrade them using the following methods: ``` sudo apt install furiosa-firmware-tools-rngd sudo apt install furiosa-firmware-image-rngd ``` Installing the `furiosa-firmware-image-rngd` package will automatically upgrade the firmware. The process takes approximately 3 to 5 minutes per device to complete. [previous Roadmap](../overview/roadmap.html ""previous page"") [next Quick Start with Furiosa LLM](furiosa_llm.html ""next page"") Contents * [Requirements](#requirements) * [Verifying if the system has devices](#verifying-if-the-system-has-devices) * [Setting up APT server](#setting-up-apt-server) * [Installing Pre-requisite Packages](#installing-pre-requisite-packages) * [Checking NPU devices](#checking-npu-devices) * [Upgrading Device Firmware](#upgrading-device-firmware) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..', '<2-hop>\n\nFuriosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) FuriosaAI’s Software Stack [#](#furiosaai-s-software-stack ""Link to this heading"") ================================================================================== FuriosaAI provides the streamlined software stack to allow FuriosaAI NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials. The above diagram demonstrates the SW stack provided by FuriosaAI, by layers. The following outlines the key components. Kernel Driver, Firmware, and PE Runtime [#](#kernel-driver-firmware-and-pe-runtime ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- The kernel device driver enables the Linux operating system to recognize NPU devices and expose them as Linux device files. The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE). PERT is responsible for communicating with the host’s runtime and scheduling, managing the resources of PEs to execute NPU tasks. Furiosa Compiler [#](#furiosa-compiler ""Link to this heading"") -------------------------------------------------------------- Furiosa compiler analyzes, optimizes a model graph, and generates a NPU executable program. The operation passes involve graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. When `torch.compile()` backend, `FuriosaBackend` is used or `furiosa-llm` is used, the Furiosa Compiler is transparently used to generate NPU executable programs for Runtime. Furiosa Runtime [#](#furiosa-runtime ""Link to this heading"") ------------------------------------------------------------ Runtime loads multiple executable NPU programs generated by Furiosa compiler, and run them on the NPU. A single model can be compiled into multiple executable programs according to model architectures and applications. Runtime is responsible for scheduling NPU programs and managing computation and memory resource on NPUs and CPUs. Also, Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs. Furiosa Model Compressor (Quantizer) [#](#furiosa-model-compressor-quantizer ""Link to this heading"") ---------------------------------------------------------------------------------------------------- Furiosa Model Compressor is a library as well as toolkit for model calibration and quantization. Model quantization is a powerful technique to reduce memory footprint, computation cost, inference latency and power consumption. Furiosa Model Compressor provides post-training quantization methods, such as * BF16 (W16A16) * INT8 Weight-Only (W8A16) * FP8 (W8A8) * INT8 SmoothQuant (W8A8) * INT4 Weight-Only (W4A16 AWQ / GPTQ) (Planned in release 2024.2) Furiosa LLM [#](#furiosa-llm ""Link to this heading"") ---------------------------------------------------- Furiosa LLM provides a high-performance inference engine for LLM models, such as Llama 3.1 70B, 8B, GPT-J, and Bert. Furiosa LLM is designed to provide the state-of-the-art serving optimization for LLM models. The key features of Furiosa LLM include vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and OpenAI-compatible API server. You can find further information at [Furiosa LLM](../furiosa_llm/intro.html#furiosallm) . Kubernetes Support [#](#kubernetes-support ""Link to this heading"") ------------------------------------------------------------------ Kubernetes, an open-source platform designed to manage containerized applications and services, is extensively adopted by various companies due to its robust capabilities for deploying, scaling, and automating containerized workloads. FuriosaAI software stack also offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment. FuriosaAI’s device plugin enables Kubernetes clusters to recognize FuriosaAI’s NPUs and allows NPUs to be scheduled for workloads and services that require them. This feature allows users to easily deploy AI workloads with FuriosaAI NPUs on Kubernetes, enabling efficient resource utilization and scaling. You can find more information about Kubernetes support in the [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) . [previous FuriosaAI RNGD](rngd.html ""previous page"") [next Supported Models](supported_models.html ""next page"") Contents * [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","The FuriosaAI software stack enhances AI server performance on Linux systems by utilizing a comprehensive set of components that include kernel drivers, firmware, and PE Runtime. These components enable the Linux operating system to recognize FuriosaAI NPU devices and manage them effectively. The kernel device driver exposes NPUs as Linux device files, while the firmware provides low-level APIs to the PE Runtime, which is responsible for scheduling and managing resources for executing NPU tasks. Additionally, FuriosaAI offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment. This integration enables Kubernetes clusters to recognize FuriosaAI’s NPUs, facilitating efficient resource utilization and scaling of AI workloads.",multi_hop_specific_query_synthesizer
39,How does the FuriosaAI Software Stack integrate Llama2 and Llama models for running MLPerf Inference Benchmark?,"['<1-hop>\n\nUsing OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api) OpenAI Compatible Server [#](#openai-compatible-server ""Link to this heading"") ============================================================================== The `furiosa-llm` package includes an OpenAI-compatible server that can interact with OpenAI clients (and, of course, HTTP clients as well), supporting the `/v1/chat/completions` and `/v1/completions` APIs. In this section, we will explain how to launch the OpenAI-compatible furiosa-llm server. Tip You can learn more about the OpenAI API at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) . To launch the server, you must prepare: (1) the FuriosaAI LLM Engine artifact and (2) a chat template for the model. To download the FuriosaAI LLM Engine artifact, follow the instructions provided through our distribution channels or contact our sales team. To prepare the chat template, follow the instructions in the following section. Preparing Chat Templates [#](#preparing-chat-templates ""Link to this heading"") ------------------------------------------------------------------------------ Furiosa SDK 2024.1.0 (alpha) uses Transformers v4.31.0, which does not include a chat template. Therefore, to support `/v1/chat/completions` , you must provide a chat template yourself. This constraint will be removed in future releases. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. If you have access to the Llama repositories on Hugging Face, you can obtain the chat template for Llama using the following commands: ``` # Prerequisite: create a separate environment to install the latest Transformers version pip install ""transformers>=4.34.0"" python - <<EOF from transformers import AutoTokenizer tok = AutoTokenizer.from_pretrained(\'meta-llama/Meta-Llama-3.1-70B-Instruct\') with open(\'chat_template.tpl\', \'w\') as f: f.write(tok.chat_template) EOF ``` Launching the Server [#](#launching-the-server ""Link to this heading"") ---------------------------------------------------------------------- You can launch the server using the furiosa-llm serve command. ### Arguments for the serve command [#](#arguments-for-the-serve-command ""Link to this heading"") ``` usage: furiosa-llm serve [-h] --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} --artifact ARTIFACT [--host HOST] [--port PORT] --chat-template CHAT_TEMPLATE [--response-role RESPONSE_ROLE] [-pp PIPELINE_PARALLEL_SIZE] [-tp TENSOR_PARALLEL_SIZE] [--devices DEVICES] options: -h, --help show this help message and exit --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} The model to use. Currently only one model is supported per server. --artifact ARTIFACT Path to Furiosa LLM Engine artifact --host HOST Host to bind the server to --port PORT Port to bind the server to --chat-template CHAT_TEMPLATE Path to chat template file (must be a jinja2 template) --response-role RESPONSE_ROLE Response role for /v1/chat/completions API (default: \'assistant\') -pp PIPELINE_PARALLEL_SIZE, --pipeline-parallel-size PIPELINE_PARALLEL_SIZE Number of pipeline stages. -tp TENSOR_PARALLEL_SIZE, --tensor-parallel-size TENSOR_PARALLEL_SIZE Number of tensor parallel replicas. --devices DEVICES Devices to use (e.g. ""npu:0:*,npu:1:*""). If unspecified, all available devices from the host will be used. ``` Examples [#](#examples ""Link to this heading"") ---------------------------------------------- ### LLaMA-3.1-70B with 4 RNGDs [#](#llama-3-1-70b-with-4-rngds ""Link to this heading"") ``` furiosa-llm serve \\ --model {path to mlperf-llama-3-1-fp8-pp4} \\ -tp 4 -pp 4 --devices ""npu:0:*,npu:1:*,npu:2:*,npu:3:*"" \\ --chat-template {path to chat template} ``` ### LLaMA-3.1-8B with Single RNGD [#](#llama-3-1-8b-with-single-rngd ""Link to this heading"") ``` furiosa-llm serve \\ --model {path to mlperf-llama-3-1-8b-fp8} \\ -tp 4 -pp 1 --devices ""npu:0:*"" \\ # You may choose arbitrary device index, if multiple devices are on host --chat-template {path to chat template} ``` Using OpenAI Client [#](#using-openai-client ""Link to this heading"") -------------------------------------------------------------------- You can use two APIs: `client.chat.completions` and `client.completions` . You can also set `stream=True` to receive a streaming response. Tip You can install the OpenAI Python client using the following command: ``` pip install openai ``` ``` import openai HOST = ""localhost:8000"" openai.api_base = f""http://{HOST}/v1"" openai.api_key = ""0000"" stream_chat_completion = openai.ChatCompletion.create( model="""", messages=[ {""role"": ""system"", ""content"": ""You are a helpful assistant.""}, {""role"": ""user"", ""content"": ""What is the largest animal in the world?""}, ], stream=True, ) for completion in stream_chat_completion: content = completion.choices[0].delta.get(""content"") if content: print(content, end="""") ``` The compatibility with OpenAI API [#](#the-compatibility-with-openai-api ""Link to this heading"") ------------------------------------------------------------------------------------------------ Currently, `furiosa serve` supports the following OpenAI API parameters: You can find more about each parameter at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) . Warning Please note that using `use_beam_search` with `stream` is not allowed because the beam search cannot determine the tokens until the end of the sequence. In 2024.1 release, `n` works only for beam search and it will be fixed in the next release. * `n` * `temperature` * `top_p` * `top_k` * `early_stopping` * `length_penalty` * `max_tokens` * `min_tokens` * `use_beam_search` * `best_of` [previous Furiosa LLM](intro.html ""previous page"") [next References](references.html ""next page"") Contents * [Preparing Chat Templates](#preparing-chat-templates) * [Launching the Server](#launching-the-server) + [Arguments for the serve command](#arguments-for-the-serve-command) * [Examples](#examples) + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds) + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..', '<2-hop>\n\n* [.rst](../_sources/getting_started/furiosa_mlperf.rst ""Download source file"") * .pdf Running MLPerf™ Inference Benchmark =================================== Contents -------- * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1.  Running MLPerf Inference Benchmark](#id1) + [']","The FuriosaAI Software Stack integrates Llama models by providing the `furiosa-mlperf` command to run the MLPerf Inference Benchmark. While the benchmark is based on MLPerf Inference Benchmark v4.1, it replaces the Llama2 benchmark with one using Llama 3.1. Additionally, the `furiosa-llm` package includes an OpenAI-compatible server that supports models like Llama-3.1-70B and Llama-3.1-8B, allowing for efficient API interaction and integration into existing infrastructure.",multi_hop_specific_query_synthesizer
40,"How can the Llama 3.1 70B model be integrated into a container environment using Furiosa LLM, and how is it benchmarked using MLPerf?","['<1-hop>\n\nRunning Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) Quick Start with Furiosa LLM [#](#quick-start-with-furiosa-llm ""Link to this heading"") ====================================================================================== Furiosa LLM is a serving framework for LLM models that utilizes FuriosaAI’s NPU. It provides a Python API compatible with vLLM and a server compatible with the OpenAI API. This document explains how to install and use Furiosa LLM. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. Installing Furiosa LLM [#](#installing-furiosa-llm ""Link to this heading"") -------------------------------------------------------------------------- The minimum requirements for Furiosa LLM are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Administrator privileges on system (root) * [Setting up APT server](prerequisites.html#aptsetup) and [Installing Prerequisites](prerequisites.html#installingprerequisites) * Python 3.8, 3.9, or 3.10 * Enough storage space for model weights; e.g., about 100GB for Llama 3.1 70B model Then, please install the `furiosa-compiler` package as follows: ``` sudo apt install -y furiosa-compiler ``` Also, you need to create a Python virtual environment depending on your environment. Note Note that some models, such as meta-llama/Meta-Llama-3.1-8B require you to accept their license, hence, you need to create a HuggingFace account, accept the model’s license, and generate a token. Usually, you can create your token at <https://huggingface.co/settings/tokens> . Once you get a token, you can authenticate on the HuggingFace Hub as following: ``` huggingface-cli login --token $HF_TOKEN ``` Then, you can install the Furiosa LLM with the following command: ``` pip install furiosa-llm ``` ### Offline Batch Inference with Furiosa LLM [#](#offline-batch-inference-with-furiosa-llm ""Link to this heading"") In this section, we will explain how to perform offline LLM inference using the Python API of Furiosa LLM. First, import the `LLM` class and `SamplingParams` from the furiosa\\_llm module. `LLM` class is used to load LLM models and provides the core API for LLM inference. `SamplingParams` allows to specify various parameters for text generation. ``` from furiosa_llm import LLM, SamplingParams ``` Next, we will load an LLM model using the LLM class. The following example loads the meta-llama/Meta-Llama-3.1-8B model from the HuggingFace Hub and performs quantization. The model will be loaded into the memory and ready for inference. ``` llm = LLM(model=""meta-llama/Llama-3.1-8B-Instruct"") ``` After loading the model, you can perform LLM inference by calling the `generate` method. ``` prompts = [ """" ] sampling_params = SamplingParams(temperature=0.0) ``` Launching the OpenAI-Compatible Server [#](#launching-the-openai-compatible-server ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- You can find more details at [OpenAI Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) . Running Furiosa LLM in container environment [#](#running-furiosa-llm-in-container-environment ""Link to this heading"") ---------------------------------------------------------------------------------------------------------------------- FuriosaAI provides an image for running `furiosa-llm` in a containerized environment. By using the containerized version, you can run environments that utilize `furiosa-llm` without installing the FuriosaAI Software Stack on your host system, or you can execute it within a Kubernetes environment. To run the `furiosa-llm` container, you can use the following command: ``` $ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash (container) # python ``` Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Installing Prerequisites](prerequisites.html ""previous page"") [next Running MLPerf™ Inference Benchmark](furiosa_mlperf.html ""next page"") Contents * [Installing Furiosa LLM](#installing-furiosa-llm) + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..', '<2-hop>\n\nSYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1. Installing `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command ""Link to this heading"") -------------------------------------------------------------------------------------------------- To install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following: The minimum requirements for `furiosa-mlperf` are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers ( [Setting up APT server](prerequisites.html#aptsetup) ) * About 100GB storage space (only for the Llama 3.1 70B) Then, please install the `furiosa-mlperf` package as follows: ``` sudo apt update sudo apt install -y furiosa-mlperf ``` This command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` . Running MLPerf Inference Benchmark [#](#id1 ""Link to this heading"") ------------------------------------------------------------------- ### SYNOPSIS [#](#synopsis ""Link to this heading"") The `furiosa-mlperf` command provides the following subcommands: ``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z) Usage: furiosa-mlperf <COMMAND> Commands: bert-offline Run BERT benchmark with offline scenario bert-server Run BERT benchmark with server scenario gpt-j-offline Run GPT-J benchmark with offline scenario gpt-j-server Run GPT-J benchmark with server scenario llama-3.1-offline Run Llama 3.1 benchmark with offline scenario llama-3.1-server Run Llama 3.1 benchmark with server scenario help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version ``` ### Examples [#](#examples ""Link to this heading"") * BERT benchmark The BERT benchmark is based on running with a single RNGD. + Server Scenario To run BERT Large serving inference benchmark, you can use the following command: ``` furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh ""npu:0:*"" ``` + Offline Scenario To run BERT Large offline inference benchmark, you can use the following command: ``` furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh ""npu:0:*"" ``` * GPT-J benchmark The GPT-J benchmark is based on running with a single RNGD. + Server Scenario To run GPT-J 6B serving inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result ``` + Offline Scenario To run GPT-J 6B offline inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result ``` * Llama 3.1 benchmark The Llama 3.1 benchmark is based on running with four RNGDs. + Server Scenario To run Llama 3.1 70B serving inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result ``` + Offline Scenario To run Llama 3.1 70B offline inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result ``` * Common Once the process completes, it writes the results to a file in the specified results directory. You can open this file to view a summary of the results. ``` cat gpt-j-offline-result/mlperf_log_summary.txt ``` ``` ================================================ MLPerf Results Summary ================================================ SUT name : GPT-J SUT Scenario : Offline Mode : PerformanceOnly Samples per second: 11.842 Tokens per second (inferred): 817.095 Result is : VALID Min duration satisfied : Yes Min queries satisfied : Yes Early stopping satisfied: Yes ```']","The Llama 3.1 70B model can be integrated into a container environment using Furiosa LLM by utilizing FuriosaAI's provided image for running `furiosa-llm` in a containerized setup. This allows the model to be executed without installing the FuriosaAI Software Stack on the host system, or within a Kubernetes environment. The command to run the `furiosa-llm` container is: `docker run -it --rm --privileged furiosa-llm:2024.1.0 bash`. For benchmarking, the Llama 3.1 70B model is evaluated using the MLPerf Inference Benchmark. The `furiosa-mlperf` command is used to run the benchmark, with specific commands for both server and offline scenarios. For the server scenario, the command is: `furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result`, and for the offline scenario, it is: `furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result`.",multi_hop_specific_query_synthesizer
41,"How can the FuriosaAI Software Stack be used to run the MLPerf Inference Benchmark, and what are the specific requirements and commands for executing the Llama 3.1 benchmark?","['<1-hop>\n\n* [.rst](../_sources/getting_started/furiosa_mlperf.rst ""Download source file"") * .pdf Running MLPerf™ Inference Benchmark =================================== Contents -------- * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1.  Running MLPerf Inference Benchmark](#id1) + [', '<2-hop>\n\nSYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1. Installing `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command ""Link to this heading"") -------------------------------------------------------------------------------------------------- To install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following: The minimum requirements for `furiosa-mlperf` are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers ( [Setting up APT server](prerequisites.html#aptsetup) ) * About 100GB storage space (only for the Llama 3.1 70B) Then, please install the `furiosa-mlperf` package as follows: ``` sudo apt update sudo apt install -y furiosa-mlperf ``` This command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` . Running MLPerf Inference Benchmark [#](#id1 ""Link to this heading"") ------------------------------------------------------------------- ### SYNOPSIS [#](#synopsis ""Link to this heading"") The `furiosa-mlperf` command provides the following subcommands: ``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z) Usage: furiosa-mlperf <COMMAND> Commands: bert-offline Run BERT benchmark with offline scenario bert-server Run BERT benchmark with server scenario gpt-j-offline Run GPT-J benchmark with offline scenario gpt-j-server Run GPT-J benchmark with server scenario llama-3.1-offline Run Llama 3.1 benchmark with offline scenario llama-3.1-server Run Llama 3.1 benchmark with server scenario help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version ``` ### Examples [#](#examples ""Link to this heading"") * BERT benchmark The BERT benchmark is based on running with a single RNGD. + Server Scenario To run BERT Large serving inference benchmark, you can use the following command: ``` furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh ""npu:0:*"" ``` + Offline Scenario To run BERT Large offline inference benchmark, you can use the following command: ``` furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh ""npu:0:*"" ``` * GPT-J benchmark The GPT-J benchmark is based on running with a single RNGD. + Server Scenario To run GPT-J 6B serving inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result ``` + Offline Scenario To run GPT-J 6B offline inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result ``` * Llama 3.1 benchmark The Llama 3.1 benchmark is based on running with four RNGDs. + Server Scenario To run Llama 3.1 70B serving inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result ``` + Offline Scenario To run Llama 3.1 70B offline inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result ``` * Common Once the process completes, it writes the results to a file in the specified results directory. You can open this file to view a summary of the results. ``` cat gpt-j-offline-result/mlperf_log_summary.txt ``` ``` ================================================ MLPerf Results Summary ================================================ SUT name : GPT-J SUT Scenario : Offline Mode : PerformanceOnly Samples per second: 11.842 Tokens per second (inferred): 817.095 Result is : VALID Min duration satisfied : Yes Min queries satisfied : Yes Early stopping satisfied: Yes ```']","The FuriosaAI Software Stack provides the `furiosa-mlperf` command to easily run the MLPerf Inference Benchmark. To use this stack, you need to install the `furiosa-mlperf` package, which requires Ubuntu 20.04 LTS or later, root or sudo permissions, configuring the APT server, installing device drivers, and about 100GB of storage space for the Llama 3.1 70B benchmark. The installation can be done using the command: `sudo apt update` followed by `sudo apt install -y furiosa-mlperf`. Once installed, the `furiosa-mlperf` command offers subcommands for different benchmarks. For the Llama 3.1 benchmark, you can run it in a server scenario using the command: `furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result`, or in an offline scenario using: `furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result`. The results are written to a file in the specified results directory, which can be viewed for a summary of the performance.",multi_hop_specific_query_synthesizer
42,How to run Furiosa LLM in a container environment using Furiosa SDK 2024.1.0 and ensure compatibility with OpenAI API?,"['<1-hop>\n\nRunning Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) Quick Start with Furiosa LLM [#](#quick-start-with-furiosa-llm ""Link to this heading"") ====================================================================================== Furiosa LLM is a serving framework for LLM models that utilizes FuriosaAI’s NPU. It provides a Python API compatible with vLLM and a server compatible with the OpenAI API. This document explains how to install and use Furiosa LLM. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. Installing Furiosa LLM [#](#installing-furiosa-llm ""Link to this heading"") -------------------------------------------------------------------------- The minimum requirements for Furiosa LLM are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Administrator privileges on system (root) * [Setting up APT server](prerequisites.html#aptsetup) and [Installing Prerequisites](prerequisites.html#installingprerequisites) * Python 3.8, 3.9, or 3.10 * Enough storage space for model weights; e.g., about 100GB for Llama 3.1 70B model Then, please install the `furiosa-compiler` package as follows: ``` sudo apt install -y furiosa-compiler ``` Also, you need to create a Python virtual environment depending on your environment. Note Note that some models, such as meta-llama/Meta-Llama-3.1-8B require you to accept their license, hence, you need to create a HuggingFace account, accept the model’s license, and generate a token. Usually, you can create your token at <https://huggingface.co/settings/tokens> . Once you get a token, you can authenticate on the HuggingFace Hub as following: ``` huggingface-cli login --token $HF_TOKEN ``` Then, you can install the Furiosa LLM with the following command: ``` pip install furiosa-llm ``` ### Offline Batch Inference with Furiosa LLM [#](#offline-batch-inference-with-furiosa-llm ""Link to this heading"") In this section, we will explain how to perform offline LLM inference using the Python API of Furiosa LLM. First, import the `LLM` class and `SamplingParams` from the furiosa\\_llm module. `LLM` class is used to load LLM models and provides the core API for LLM inference. `SamplingParams` allows to specify various parameters for text generation. ``` from furiosa_llm import LLM, SamplingParams ``` Next, we will load an LLM model using the LLM class. The following example loads the meta-llama/Meta-Llama-3.1-8B model from the HuggingFace Hub and performs quantization. The model will be loaded into the memory and ready for inference. ``` llm = LLM(model=""meta-llama/Llama-3.1-8B-Instruct"") ``` After loading the model, you can perform LLM inference by calling the `generate` method. ``` prompts = [ """" ] sampling_params = SamplingParams(temperature=0.0) ``` Launching the OpenAI-Compatible Server [#](#launching-the-openai-compatible-server ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- You can find more details at [OpenAI Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) . Running Furiosa LLM in container environment [#](#running-furiosa-llm-in-container-environment ""Link to this heading"") ---------------------------------------------------------------------------------------------------------------------- FuriosaAI provides an image for running `furiosa-llm` in a containerized environment. By using the containerized version, you can run environments that utilize `furiosa-llm` without installing the FuriosaAI Software Stack on your host system, or you can execute it within a Kubernetes environment. To run the `furiosa-llm` container, you can use the following command: ``` $ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash (container) # python ``` Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Installing Prerequisites](prerequisites.html ""previous page"") [next Running MLPerf™ Inference Benchmark](furiosa_mlperf.html ""next page"") Contents * [Installing Furiosa LLM](#installing-furiosa-llm) + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..', '<2-hop>\n\nUsing OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api) OpenAI Compatible Server [#](#openai-compatible-server ""Link to this heading"") ============================================================================== The `furiosa-llm` package includes an OpenAI-compatible server that can interact with OpenAI clients (and, of course, HTTP clients as well), supporting the `/v1/chat/completions` and `/v1/completions` APIs. In this section, we will explain how to launch the OpenAI-compatible furiosa-llm server. Tip You can learn more about the OpenAI API at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) . To launch the server, you must prepare: (1) the FuriosaAI LLM Engine artifact and (2) a chat template for the model. To download the FuriosaAI LLM Engine artifact, follow the instructions provided through our distribution channels or contact our sales team. To prepare the chat template, follow the instructions in the following section. Preparing Chat Templates [#](#preparing-chat-templates ""Link to this heading"") ------------------------------------------------------------------------------ Furiosa SDK 2024.1.0 (alpha) uses Transformers v4.31.0, which does not include a chat template. Therefore, to support `/v1/chat/completions` , you must provide a chat template yourself. This constraint will be removed in future releases. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. If you have access to the Llama repositories on Hugging Face, you can obtain the chat template for Llama using the following commands: ``` # Prerequisite: create a separate environment to install the latest Transformers version pip install ""transformers>=4.34.0"" python - <<EOF from transformers import AutoTokenizer tok = AutoTokenizer.from_pretrained(\'meta-llama/Meta-Llama-3.1-70B-Instruct\') with open(\'chat_template.tpl\', \'w\') as f: f.write(tok.chat_template) EOF ``` Launching the Server [#](#launching-the-server ""Link to this heading"") ---------------------------------------------------------------------- You can launch the server using the furiosa-llm serve command. ### Arguments for the serve command [#](#arguments-for-the-serve-command ""Link to this heading"") ``` usage: furiosa-llm serve [-h] --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} --artifact ARTIFACT [--host HOST] [--port PORT] --chat-template CHAT_TEMPLATE [--response-role RESPONSE_ROLE] [-pp PIPELINE_PARALLEL_SIZE] [-tp TENSOR_PARALLEL_SIZE] [--devices DEVICES] options: -h, --help show this help message and exit --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} The model to use. Currently only one model is supported per server. --artifact ARTIFACT Path to Furiosa LLM Engine artifact --host HOST Host to bind the server to --port PORT Port to bind the server to --chat-template CHAT_TEMPLATE Path to chat template file (must be a jinja2 template) --response-role RESPONSE_ROLE Response role for /v1/chat/completions API (default: \'assistant\') -pp PIPELINE_PARALLEL_SIZE, --pipeline-parallel-size PIPELINE_PARALLEL_SIZE Number of pipeline stages. -tp TENSOR_PARALLEL_SIZE, --tensor-parallel-size TENSOR_PARALLEL_SIZE Number of tensor parallel replicas. --devices DEVICES Devices to use (e.g. ""npu:0:*,npu:1:*""). If unspecified, all available devices from the host will be used. ``` Examples [#](#examples ""Link to this heading"") ---------------------------------------------- ### LLaMA-3.1-70B with 4 RNGDs [#](#llama-3-1-70b-with-4-rngds ""Link to this heading"") ``` furiosa-llm serve \\ --model {path to mlperf-llama-3-1-fp8-pp4} \\ -tp 4 -pp 4 --devices ""npu:0:*,npu:1:*,npu:2:*,npu:3:*"" \\ --chat-template {path to chat template} ``` ### LLaMA-3.1-8B with Single RNGD [#](#llama-3-1-8b-with-single-rngd ""Link to this heading"") ``` furiosa-llm serve \\ --model {path to mlperf-llama-3-1-8b-fp8} \\ -tp 4 -pp 1 --devices ""npu:0:*"" \\ # You may choose arbitrary device index, if multiple devices are on host --chat-template {path to chat template} ``` Using OpenAI Client [#](#using-openai-client ""Link to this heading"") -------------------------------------------------------------------- You can use two APIs: `client.chat.completions` and `client.completions` . You can also set `stream=True` to receive a streaming response. Tip You can install the OpenAI Python client using the following command: ``` pip install openai ``` ``` import openai HOST = ""localhost:8000"" openai.api_base = f""http://{HOST}/v1"" openai.api_key = ""0000"" stream_chat_completion = openai.ChatCompletion.create( model="""", messages=[ {""role"": ""system"", ""content"": ""You are a helpful assistant.""}, {""role"": ""user"", ""content"": ""What is the largest animal in the world?""}, ], stream=True, ) for completion in stream_chat_completion: content = completion.choices[0].delta.get(""content"") if content: print(content, end="""") ``` The compatibility with OpenAI API [#](#the-compatibility-with-openai-api ""Link to this heading"") ------------------------------------------------------------------------------------------------ Currently, `furiosa serve` supports the following OpenAI API parameters: You can find more about each parameter at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) . Warning Please note that using `use_beam_search` with `stream` is not allowed because the beam search cannot determine the tokens until the end of the sequence. In 2024.1 release, `n` works only for beam search and it will be fixed in the next release. * `n` * `temperature` * `top_p` * `top_k` * `early_stopping` * `length_penalty` * `max_tokens` * `min_tokens` * `use_beam_search` * `best_of` [previous Furiosa LLM](intro.html ""previous page"") [next References](references.html ""next page"") Contents * [Preparing Chat Templates](#preparing-chat-templates) * [Launching the Server](#launching-the-server) + [Arguments for the serve command](#arguments-for-the-serve-command) * [Examples](#examples) + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds) + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","To run Furiosa LLM in a container environment using Furiosa SDK 2024.1.0, you can use the provided Docker image by executing the command `docker run -it --rm --privileged furiosa-llm:2024.1.0 bash`. This allows you to run environments utilizing `furiosa-llm` without installing the FuriosaAI Software Stack on your host system. For compatibility with the OpenAI API, the `furiosa-llm` package includes an OpenAI-compatible server that supports the `/v1/chat/completions` and `/v1/completions` APIs. You need to prepare the FuriosaAI LLM Engine artifact and a chat template for the model to launch the server. The server can be launched using the `furiosa-llm serve` command with appropriate arguments for model, artifact, and chat template.",multi_hop_specific_query_synthesizer
43,How Kubernetes support in FuriosaAI's software stack helps in deploying Furiosa Metrics Exporter for monitoring NPU devices?,"['<1-hop>\n\nDeploying Furiosa Metrics Exporter with Helm](#deploying-furiosa-metrics-exporter-with-helm) Installing Furiosa Metrics Exporter [#](#installing-furiosa-metrics-exporter ""Link to this heading"") ==================================================================================================== Furiosa Metrics Exporter [#](#furiosa-metrics-exporter ""Link to this heading"") ------------------------------------------------------------------------------ The Furiosa metrics exporter exposes collection of metrics related to FuriosaAI NPU devices in [Prometheus](https://prometheus.io/) format. In a Kubernetes cluster, you can scrape the metrics provided by furiosa-metrics-exporter using Prometheus and visualize them with a Grafana dashboard. This can be easily set up using the [Prometheus Chart](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus) and [Grafana](https://github.com/grafana/helm-charts/tree/main/charts/grafana) Helm charts, along with the furiosa-metrics-exporter Helm chart. ### Metrics [#](#metrics ""Link to this heading"") The exporter is composed of chain of collectors, each collector is responsible for collecting specific metrics from the Furiosa NPU devices. The following table shows the available collectors and metrics: NPU Metrics [#](#id1 ""Link to this table"") | Collector Name | Metric | Type | Metric Labels | Description | | --- | --- | --- | --- | --- | | Liveness | furiosa\\_npu\\_alive | guage | arch, core, device, uuid, kubernetes\\_node\\_name | The liveness of the Furiosa NPU device. | | Error | furiosa\\_npu\\_error | guage | arch, core, device, uuid, kubernetes\\_node\\_name, label | The error count of the Furiosa NPU device. | | Temperature | furiosa\\_npu\\_hw\\_temperature | guage | arch, core, device, uuid, kubernetes\\_node\\_name, label | The temperature of the Furiosa NPU device. | | Power | furiosa\\_npu\\_hw\\_power | guage | arch, core, device, uuid, kubernetes\\_node\\_name, label | The power consumption of the Furiosa NPU device. | | Core Utilization | furiosa\\_npu\\_core\\_utilization | guage | arch, core, device, uuid, kubernetes\\_node\\_name | The core utilization of the Furiosa NPU device. | All metrics share common metric labels such as arch, core, device, kubernetes\\_node\\_name, and uuid. The following table describes the common metric labels: Common NPU Metrics Label [#](#id2 ""Link to this table"") | Common Metric Label | Description | | --- | --- | | arch | The architecture of the Furiosa NPU device. e.g. warboy, rngd | | core | The core number of the Furiosa NPU device. e.g. 0, 1, 2, 3, 4, 5, 6, 7, 0-1, 2-3, 0-3, 4-5, 6-7, 4-7, 0-7 | | device | The device name of the Furiosa NPU device. e.g. npu0 | | kubernetes\\_node\\_name | The name of the Kubernetes node where the exporter is running, this attribute can be missing if the exporter is running on the host machine or in a naked container. | | uuid | The UUID of the Furiosa NPU device. | The metric label “label” is used to describe additional attributes specific to each metric. This approach helps avoid having too many metric definitions and effectively aggregates metrics that share common characteristics. NPU Metrics Type [#](#id3 ""Link to this table"") | Metric Type | Label Attribute | Description | | --- | --- | --- | | Error | axi\\_post\\_error | Indicates count of axi post error. | | Error | axi\\_fetch\\_error | Indicates count of axi fetch error. | | Error | axi\\_discard\\_error | Indicates count of axi discard error. | | Error | axi\\_doorbell\\_done | Indicates count of axi doorbell done error. | | Error | pcie\\_post\\_error | Indicates count of PCIe post error. | | Error | pcie\\_fetch\\_error | Indicates count of PCIe fetch error. | | Error | pcie\\_discard\\_error | Indicates count of PCIe discard error. | | Error | pcie\\_doorbell\\_done | Indicates count of PCIe doorbell done error. | | Error | device\\_error | Total count of device error. | | Temperature | peak | The highest temperature observed from SoC sensors | | Temperature | ambient | The temperature observed from sensors attached to the board | | Power | rms | Root Mean Square (RMS) value of the power consumed by the device, providing an average power consumption metric over a period of time. | The following shows real-world example of the metrics: ``` #liveness furiosa_npu_alive{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 1 #error furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_post_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_fetch_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_discard_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_doorbell_done"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_post_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_fetch_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_discard_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_doorbell_done"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""device_error"",uuid=""uuid""} 0 #temperature furiosa_npu_hw_temperature{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""peak"",uuid=""uuid""} 39 furiosa_npu_hw_temperature{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""ambient"",uuid=""uuid""} 35 #power furiosa_npu_hw_power{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""rms"",uuid=""uuid""} 4795000 #core utilization furiosa_npu_core_utilization{arch=""rngd"",core=""0"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""1"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""2"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""3"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""4"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""5"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""6"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""7"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 ``` ### Deploying Furiosa Metrics Exporter with Helm [#](#deploying-furiosa-metrics-exporter-with-helm ""Link to this heading"") The Furiosa metrics exporter helm chart is available at [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) . To configure deployment as you need, you can modify `charts/furiosa-metrics-exporter/values.yaml` . For example, the Furiosa metrics exporter Helm chart automatically creates a Service Object with Prometheus annotations to enable metric scraping automatically. You can modify the values.yaml to change the port or disable the Prometheus annotations if needed. You can deploy the Furiosa Metrics Exporter by running the following commands: ``` helm repo add furiosa https://furiosa-ai.github.io/helm-charts helm repo update helm install furiosa-metrics-exporter furiosa/furiosa-metrics-exporter -n kube-system ``` [previous Installing Furiosa Device Plugin](device_plugin.html ""previous page"") [next Scheduling NPUs](scheduling_npus.html ""next page"") Contents * [Furiosa Metrics Exporter](#furiosa-metrics-exporter) + [Metrics](#metrics) + [Deploying Furiosa Metrics Exporter with Helm](#deploying-furiosa-metrics-exporter-with-helm) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..', '<2-hop>\n\nFuriosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) FuriosaAI’s Software Stack [#](#furiosaai-s-software-stack ""Link to this heading"") ================================================================================== FuriosaAI provides the streamlined software stack to allow FuriosaAI NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials. The above diagram demonstrates the SW stack provided by FuriosaAI, by layers. The following outlines the key components. Kernel Driver, Firmware, and PE Runtime [#](#kernel-driver-firmware-and-pe-runtime ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- The kernel device driver enables the Linux operating system to recognize NPU devices and expose them as Linux device files. The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE). PERT is responsible for communicating with the host’s runtime and scheduling, managing the resources of PEs to execute NPU tasks. Furiosa Compiler [#](#furiosa-compiler ""Link to this heading"") -------------------------------------------------------------- Furiosa compiler analyzes, optimizes a model graph, and generates a NPU executable program. The operation passes involve graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. When `torch.compile()` backend, `FuriosaBackend` is used or `furiosa-llm` is used, the Furiosa Compiler is transparently used to generate NPU executable programs for Runtime. Furiosa Runtime [#](#furiosa-runtime ""Link to this heading"") ------------------------------------------------------------ Runtime loads multiple executable NPU programs generated by Furiosa compiler, and run them on the NPU. A single model can be compiled into multiple executable programs according to model architectures and applications. Runtime is responsible for scheduling NPU programs and managing computation and memory resource on NPUs and CPUs. Also, Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs. Furiosa Model Compressor (Quantizer) [#](#furiosa-model-compressor-quantizer ""Link to this heading"") ---------------------------------------------------------------------------------------------------- Furiosa Model Compressor is a library as well as toolkit for model calibration and quantization. Model quantization is a powerful technique to reduce memory footprint, computation cost, inference latency and power consumption. Furiosa Model Compressor provides post-training quantization methods, such as * BF16 (W16A16) * INT8 Weight-Only (W8A16) * FP8 (W8A8) * INT8 SmoothQuant (W8A8) * INT4 Weight-Only (W4A16 AWQ / GPTQ) (Planned in release 2024.2) Furiosa LLM [#](#furiosa-llm ""Link to this heading"") ---------------------------------------------------- Furiosa LLM provides a high-performance inference engine for LLM models, such as Llama 3.1 70B, 8B, GPT-J, and Bert. Furiosa LLM is designed to provide the state-of-the-art serving optimization for LLM models. The key features of Furiosa LLM include vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and OpenAI-compatible API server. You can find further information at [Furiosa LLM](../furiosa_llm/intro.html#furiosallm) . Kubernetes Support [#](#kubernetes-support ""Link to this heading"") ------------------------------------------------------------------ Kubernetes, an open-source platform designed to manage containerized applications and services, is extensively adopted by various companies due to its robust capabilities for deploying, scaling, and automating containerized workloads. FuriosaAI software stack also offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment. FuriosaAI’s device plugin enables Kubernetes clusters to recognize FuriosaAI’s NPUs and allows NPUs to be scheduled for workloads and services that require them. This feature allows users to easily deploy AI workloads with FuriosaAI NPUs on Kubernetes, enabling efficient resource utilization and scaling. You can find more information about Kubernetes support in the [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) . [previous FuriosaAI RNGD](rngd.html ""previous page"") [next Supported Models](supported_models.html ""next page"") Contents * [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","Kubernetes support in FuriosaAI's software stack allows seamless deployment and management of AI applications within a Kubernetes environment. This integration enables Kubernetes clusters to recognize FuriosaAI’s NPUs, allowing them to be scheduled for workloads and services that require them. For deploying the Furiosa Metrics Exporter, this means that the metrics related to FuriosaAI NPU devices can be easily scraped using Prometheus and visualized with a Grafana dashboard. The Furiosa metrics exporter Helm chart, which is part of this setup, automatically creates a Service Object with Prometheus annotations to enable metric scraping, facilitating efficient monitoring of NPU devices in a Kubernetes cluster.",multi_hop_specific_query_synthesizer
44,"How can the LLaMA-3.1-70B model be integrated into a containerized environment using Furiosa LLM, and what are the necessary steps to ensure compatibility with the OpenAI API?","['<1-hop>\n\nRunning Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) Quick Start with Furiosa LLM [#](#quick-start-with-furiosa-llm ""Link to this heading"") ====================================================================================== Furiosa LLM is a serving framework for LLM models that utilizes FuriosaAI’s NPU. It provides a Python API compatible with vLLM and a server compatible with the OpenAI API. This document explains how to install and use Furiosa LLM. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. Installing Furiosa LLM [#](#installing-furiosa-llm ""Link to this heading"") -------------------------------------------------------------------------- The minimum requirements for Furiosa LLM are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Administrator privileges on system (root) * [Setting up APT server](prerequisites.html#aptsetup) and [Installing Prerequisites](prerequisites.html#installingprerequisites) * Python 3.8, 3.9, or 3.10 * Enough storage space for model weights; e.g., about 100GB for Llama 3.1 70B model Then, please install the `furiosa-compiler` package as follows: ``` sudo apt install -y furiosa-compiler ``` Also, you need to create a Python virtual environment depending on your environment. Note Note that some models, such as meta-llama/Meta-Llama-3.1-8B require you to accept their license, hence, you need to create a HuggingFace account, accept the model’s license, and generate a token. Usually, you can create your token at <https://huggingface.co/settings/tokens> . Once you get a token, you can authenticate on the HuggingFace Hub as following: ``` huggingface-cli login --token $HF_TOKEN ``` Then, you can install the Furiosa LLM with the following command: ``` pip install furiosa-llm ``` ### Offline Batch Inference with Furiosa LLM [#](#offline-batch-inference-with-furiosa-llm ""Link to this heading"") In this section, we will explain how to perform offline LLM inference using the Python API of Furiosa LLM. First, import the `LLM` class and `SamplingParams` from the furiosa\\_llm module. `LLM` class is used to load LLM models and provides the core API for LLM inference. `SamplingParams` allows to specify various parameters for text generation. ``` from furiosa_llm import LLM, SamplingParams ``` Next, we will load an LLM model using the LLM class. The following example loads the meta-llama/Meta-Llama-3.1-8B model from the HuggingFace Hub and performs quantization. The model will be loaded into the memory and ready for inference. ``` llm = LLM(model=""meta-llama/Llama-3.1-8B-Instruct"") ``` After loading the model, you can perform LLM inference by calling the `generate` method. ``` prompts = [ """" ] sampling_params = SamplingParams(temperature=0.0) ``` Launching the OpenAI-Compatible Server [#](#launching-the-openai-compatible-server ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- You can find more details at [OpenAI Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) . Running Furiosa LLM in container environment [#](#running-furiosa-llm-in-container-environment ""Link to this heading"") ---------------------------------------------------------------------------------------------------------------------- FuriosaAI provides an image for running `furiosa-llm` in a containerized environment. By using the containerized version, you can run environments that utilize `furiosa-llm` without installing the FuriosaAI Software Stack on your host system, or you can execute it within a Kubernetes environment. To run the `furiosa-llm` container, you can use the following command: ``` $ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash (container) # python ``` Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Installing Prerequisites](prerequisites.html ""previous page"") [next Running MLPerf™ Inference Benchmark](furiosa_mlperf.html ""next page"") Contents * [Installing Furiosa LLM](#installing-furiosa-llm) + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..', '<2-hop>\n\n* [.rst](../_sources/furiosa_llm/furiosa-llm-serve.rst ""Download source file"") * .pdf OpenAI Compatible Server ======================== Contents -------- * [ Preparing Chat Templates](#preparing-chat-templates) * [ Launching the Server](#launching-the-server) + [ Arguments for the serve command](#arguments-for-the-serve-command) * [ Examples](#examples) + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds) + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [']","To integrate the LLaMA-3.1-70B model into a containerized environment using Furiosa LLM, you need to follow several steps. First, ensure that your system meets the minimum requirements, such as running Ubuntu 20.04 LTS or later, having administrator privileges, and sufficient storage space for the model weights, approximately 100GB for the LLaMA-3.1-70B model. Install the Furiosa LLM by setting up the APT server, installing prerequisites, and using the command `pip install furiosa-llm`. For containerized deployment, FuriosaAI provides an image that allows you to run `furiosa-llm` without installing the software stack on your host system. Use the command `docker run -it --rm --privileged furiosa-llm:2024.1.0 bash` to run the container, although using the `--privileged` option is not recommended for security reasons. For compatibility with the OpenAI API, Furiosa LLM offers a server that is compatible with the OpenAI API, allowing seamless integration and interaction with existing infrastructure.",multi_hop_specific_query_synthesizer
45,"How does the Furiosa Compiler optimize model graphs and generate NPU executable programs, and what role does it play in the FuriosaAI software stack for efficient deployment and performance?","['<1-hop>\n\n* [.rst](../_sources/overview/software_stack.rst ""Download source file"") * .pdf FuriosaAI’s Software Stack ========================== Contents -------- * [ Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [ Furiosa Compiler](#furiosa-compiler) * [ Furiosa Runtime](#furiosa-runtime) * [ Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [', '<2-hop>\n\nFuriosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) FuriosaAI’s Software Stack [#](#furiosaai-s-software-stack ""Link to this heading"") ================================================================================== FuriosaAI provides the streamlined software stack to allow FuriosaAI NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials. The above diagram demonstrates the SW stack provided by FuriosaAI, by layers. The following outlines the key components. Kernel Driver, Firmware, and PE Runtime [#](#kernel-driver-firmware-and-pe-runtime ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- The kernel device driver enables the Linux operating system to recognize NPU devices and expose them as Linux device files. The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE). PERT is responsible for communicating with the host’s runtime and scheduling, managing the resources of PEs to execute NPU tasks. Furiosa Compiler [#](#furiosa-compiler ""Link to this heading"") -------------------------------------------------------------- Furiosa compiler analyzes, optimizes a model graph, and generates a NPU executable program. The operation passes involve graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. When `torch.compile()` backend, `FuriosaBackend` is used or `furiosa-llm` is used, the Furiosa Compiler is transparently used to generate NPU executable programs for Runtime. Furiosa Runtime [#](#furiosa-runtime ""Link to this heading"") ------------------------------------------------------------ Runtime loads multiple executable NPU programs generated by Furiosa compiler, and run them on the NPU. A single model can be compiled into multiple executable programs according to model architectures and applications. Runtime is responsible for scheduling NPU programs and managing computation and memory resource on NPUs and CPUs. Also, Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs. Furiosa Model Compressor (Quantizer) [#](#furiosa-model-compressor-quantizer ""Link to this heading"") ---------------------------------------------------------------------------------------------------- Furiosa Model Compressor is a library as well as toolkit for model calibration and quantization. Model quantization is a powerful technique to reduce memory footprint, computation cost, inference latency and power consumption. Furiosa Model Compressor provides post-training quantization methods, such as * BF16 (W16A16) * INT8 Weight-Only (W8A16) * FP8 (W8A8) * INT8 SmoothQuant (W8A8) * INT4 Weight-Only (W4A16 AWQ / GPTQ) (Planned in release 2024.2) Furiosa LLM [#](#furiosa-llm ""Link to this heading"") ---------------------------------------------------- Furiosa LLM provides a high-performance inference engine for LLM models, such as Llama 3.1 70B, 8B, GPT-J, and Bert. Furiosa LLM is designed to provide the state-of-the-art serving optimization for LLM models. The key features of Furiosa LLM include vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and OpenAI-compatible API server. You can find further information at [Furiosa LLM](../furiosa_llm/intro.html#furiosallm) . Kubernetes Support [#](#kubernetes-support ""Link to this heading"") ------------------------------------------------------------------ Kubernetes, an open-source platform designed to manage containerized applications and services, is extensively adopted by various companies due to its robust capabilities for deploying, scaling, and automating containerized workloads. FuriosaAI software stack also offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment. FuriosaAI’s device plugin enables Kubernetes clusters to recognize FuriosaAI’s NPUs and allows NPUs to be scheduled for workloads and services that require them. This feature allows users to easily deploy AI workloads with FuriosaAI NPUs on Kubernetes, enabling efficient resource utilization and scaling. You can find more information about Kubernetes support in the [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) . [previous FuriosaAI RNGD](rngd.html ""previous page"") [next Supported Models](supported_models.html ""next page"") Contents * [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","The Furiosa Compiler plays a crucial role in the FuriosaAI software stack by analyzing and optimizing model graphs to generate NPU executable programs. It performs graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. This process ensures that the models are efficiently compiled for execution on NPUs. The compiler is transparently used when the `torch.compile()` backend or `furiosa-llm` is employed, facilitating the generation of NPU executable programs for runtime. This optimization is essential for the efficient deployment and performance of AI applications using FuriosaAI's NPUs.",multi_hop_specific_query_synthesizer
46,How does the FuriosaAI LLM Engine integrate with existing infrastructure to support OpenAI-compatible APIs and Kubernetes environments?,"['<1-hop>\n\nUsing OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api) OpenAI Compatible Server [#](#openai-compatible-server ""Link to this heading"") ============================================================================== The `furiosa-llm` package includes an OpenAI-compatible server that can interact with OpenAI clients (and, of course, HTTP clients as well), supporting the `/v1/chat/completions` and `/v1/completions` APIs. In this section, we will explain how to launch the OpenAI-compatible furiosa-llm server. Tip You can learn more about the OpenAI API at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) . To launch the server, you must prepare: (1) the FuriosaAI LLM Engine artifact and (2) a chat template for the model. To download the FuriosaAI LLM Engine artifact, follow the instructions provided through our distribution channels or contact our sales team. To prepare the chat template, follow the instructions in the following section. Preparing Chat Templates [#](#preparing-chat-templates ""Link to this heading"") ------------------------------------------------------------------------------ Furiosa SDK 2024.1.0 (alpha) uses Transformers v4.31.0, which does not include a chat template. Therefore, to support `/v1/chat/completions` , you must provide a chat template yourself. This constraint will be removed in future releases. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. If you have access to the Llama repositories on Hugging Face, you can obtain the chat template for Llama using the following commands: ``` # Prerequisite: create a separate environment to install the latest Transformers version pip install ""transformers>=4.34.0"" python - <<EOF from transformers import AutoTokenizer tok = AutoTokenizer.from_pretrained(\'meta-llama/Meta-Llama-3.1-70B-Instruct\') with open(\'chat_template.tpl\', \'w\') as f: f.write(tok.chat_template) EOF ``` Launching the Server [#](#launching-the-server ""Link to this heading"") ---------------------------------------------------------------------- You can launch the server using the furiosa-llm serve command. ### Arguments for the serve command [#](#arguments-for-the-serve-command ""Link to this heading"") ``` usage: furiosa-llm serve [-h] --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} --artifact ARTIFACT [--host HOST] [--port PORT] --chat-template CHAT_TEMPLATE [--response-role RESPONSE_ROLE] [-pp PIPELINE_PARALLEL_SIZE] [-tp TENSOR_PARALLEL_SIZE] [--devices DEVICES] options: -h, --help show this help message and exit --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} The model to use. Currently only one model is supported per server. --artifact ARTIFACT Path to Furiosa LLM Engine artifact --host HOST Host to bind the server to --port PORT Port to bind the server to --chat-template CHAT_TEMPLATE Path to chat template file (must be a jinja2 template) --response-role RESPONSE_ROLE Response role for /v1/chat/completions API (default: \'assistant\') -pp PIPELINE_PARALLEL_SIZE, --pipeline-parallel-size PIPELINE_PARALLEL_SIZE Number of pipeline stages. -tp TENSOR_PARALLEL_SIZE, --tensor-parallel-size TENSOR_PARALLEL_SIZE Number of tensor parallel replicas. --devices DEVICES Devices to use (e.g. ""npu:0:*,npu:1:*""). If unspecified, all available devices from the host will be used. ``` Examples [#](#examples ""Link to this heading"") ---------------------------------------------- ### LLaMA-3.1-70B with 4 RNGDs [#](#llama-3-1-70b-with-4-rngds ""Link to this heading"") ``` furiosa-llm serve \\ --model {path to mlperf-llama-3-1-fp8-pp4} \\ -tp 4 -pp 4 --devices ""npu:0:*,npu:1:*,npu:2:*,npu:3:*"" \\ --chat-template {path to chat template} ``` ### LLaMA-3.1-8B with Single RNGD [#](#llama-3-1-8b-with-single-rngd ""Link to this heading"") ``` furiosa-llm serve \\ --model {path to mlperf-llama-3-1-8b-fp8} \\ -tp 4 -pp 1 --devices ""npu:0:*"" \\ # You may choose arbitrary device index, if multiple devices are on host --chat-template {path to chat template} ``` Using OpenAI Client [#](#using-openai-client ""Link to this heading"") -------------------------------------------------------------------- You can use two APIs: `client.chat.completions` and `client.completions` . You can also set `stream=True` to receive a streaming response. Tip You can install the OpenAI Python client using the following command: ``` pip install openai ``` ``` import openai HOST = ""localhost:8000"" openai.api_base = f""http://{HOST}/v1"" openai.api_key = ""0000"" stream_chat_completion = openai.ChatCompletion.create( model="""", messages=[ {""role"": ""system"", ""content"": ""You are a helpful assistant.""}, {""role"": ""user"", ""content"": ""What is the largest animal in the world?""}, ], stream=True, ) for completion in stream_chat_completion: content = completion.choices[0].delta.get(""content"") if content: print(content, end="""") ``` The compatibility with OpenAI API [#](#the-compatibility-with-openai-api ""Link to this heading"") ------------------------------------------------------------------------------------------------ Currently, `furiosa serve` supports the following OpenAI API parameters: You can find more about each parameter at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) . Warning Please note that using `use_beam_search` with `stream` is not allowed because the beam search cannot determine the tokens until the end of the sequence. In 2024.1 release, `n` works only for beam search and it will be fixed in the next release. * `n` * `temperature` * `top_p` * `top_k` * `early_stopping` * `length_penalty` * `max_tokens` * `min_tokens` * `use_beam_search` * `best_of` [previous Furiosa LLM](intro.html ""previous page"") [next References](references.html ""next page"") Contents * [Preparing Chat Templates](#preparing-chat-templates) * [Launching the Server](#launching-the-server) + [Arguments for the serve command](#arguments-for-the-serve-command) * [Examples](#examples) + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds) + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..', '<2-hop>\n\nFuriosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) FuriosaAI’s Software Stack [#](#furiosaai-s-software-stack ""Link to this heading"") ================================================================================== FuriosaAI provides the streamlined software stack to allow FuriosaAI NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials. The above diagram demonstrates the SW stack provided by FuriosaAI, by layers. The following outlines the key components. Kernel Driver, Firmware, and PE Runtime [#](#kernel-driver-firmware-and-pe-runtime ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- The kernel device driver enables the Linux operating system to recognize NPU devices and expose them as Linux device files. The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE). PERT is responsible for communicating with the host’s runtime and scheduling, managing the resources of PEs to execute NPU tasks. Furiosa Compiler [#](#furiosa-compiler ""Link to this heading"") -------------------------------------------------------------- Furiosa compiler analyzes, optimizes a model graph, and generates a NPU executable program. The operation passes involve graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. When `torch.compile()` backend, `FuriosaBackend` is used or `furiosa-llm` is used, the Furiosa Compiler is transparently used to generate NPU executable programs for Runtime. Furiosa Runtime [#](#furiosa-runtime ""Link to this heading"") ------------------------------------------------------------ Runtime loads multiple executable NPU programs generated by Furiosa compiler, and run them on the NPU. A single model can be compiled into multiple executable programs according to model architectures and applications. Runtime is responsible for scheduling NPU programs and managing computation and memory resource on NPUs and CPUs. Also, Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs. Furiosa Model Compressor (Quantizer) [#](#furiosa-model-compressor-quantizer ""Link to this heading"") ---------------------------------------------------------------------------------------------------- Furiosa Model Compressor is a library as well as toolkit for model calibration and quantization. Model quantization is a powerful technique to reduce memory footprint, computation cost, inference latency and power consumption. Furiosa Model Compressor provides post-training quantization methods, such as * BF16 (W16A16) * INT8 Weight-Only (W8A16) * FP8 (W8A8) * INT8 SmoothQuant (W8A8) * INT4 Weight-Only (W4A16 AWQ / GPTQ) (Planned in release 2024.2) Furiosa LLM [#](#furiosa-llm ""Link to this heading"") ---------------------------------------------------- Furiosa LLM provides a high-performance inference engine for LLM models, such as Llama 3.1 70B, 8B, GPT-J, and Bert. Furiosa LLM is designed to provide the state-of-the-art serving optimization for LLM models. The key features of Furiosa LLM include vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and OpenAI-compatible API server. You can find further information at [Furiosa LLM](../furiosa_llm/intro.html#furiosallm) . Kubernetes Support [#](#kubernetes-support ""Link to this heading"") ------------------------------------------------------------------ Kubernetes, an open-source platform designed to manage containerized applications and services, is extensively adopted by various companies due to its robust capabilities for deploying, scaling, and automating containerized workloads. FuriosaAI software stack also offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment. FuriosaAI’s device plugin enables Kubernetes clusters to recognize FuriosaAI’s NPUs and allows NPUs to be scheduled for workloads and services that require them. This feature allows users to easily deploy AI workloads with FuriosaAI NPUs on Kubernetes, enabling efficient resource utilization and scaling. You can find more information about Kubernetes support in the [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) . [previous FuriosaAI RNGD](rngd.html ""previous page"") [next Supported Models](supported_models.html ""next page"") Contents * [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","The FuriosaAI LLM Engine integrates with existing infrastructure by providing an OpenAI-compatible server that supports the `/v1/chat/completions` and `/v1/completions` APIs, allowing interaction with OpenAI clients and HTTP clients. To launch this server, the FuriosaAI LLM Engine artifact and a chat template are required. Additionally, FuriosaAI's software stack offers native integration with Kubernetes, enabling seamless deployment and management of AI applications within a Kubernetes environment. This integration allows Kubernetes clusters to recognize FuriosaAI's NPUs, facilitating efficient resource utilization and scaling for AI workloads.",multi_hop_specific_query_synthesizer
47,"How does the FuriosaAI software stack utilize RNGD for optimizing LLM benchmarks like Llama 3.1, and what role does Kubernetes play in this setup?","['<1-hop>\n\nSYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1. Installing `furiosa-mlperf` command [#](#installing-furiosa-mlperf-command ""Link to this heading"") -------------------------------------------------------------------------------------------------- To install the `furiosa-mlperf` command, you need to install `furiosa-mlperf` as following: The minimum requirements for `furiosa-mlperf` are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers ( [Setting up APT server](prerequisites.html#aptsetup) ) * About 100GB storage space (only for the Llama 3.1 70B) Then, please install the `furiosa-mlperf` package as follows: ``` sudo apt update sudo apt install -y furiosa-mlperf ``` This command installs packages `furiosa-compiler` , `furiosa-mlperf` and `furiosa-mlperf-resources` . Running MLPerf Inference Benchmark [#](#id1 ""Link to this heading"") ------------------------------------------------------------------- ### SYNOPSIS [#](#synopsis ""Link to this heading"") The `furiosa-mlperf` command provides the following subcommands: ``` FuriosaAI MLPerf Inference Benchmark Launcher v2024.1.0 (2024-10-04T13:45:37Z) Usage: furiosa-mlperf <COMMAND> Commands: bert-offline Run BERT benchmark with offline scenario bert-server Run BERT benchmark with server scenario gpt-j-offline Run GPT-J benchmark with offline scenario gpt-j-server Run GPT-J benchmark with server scenario llama-3.1-offline Run Llama 3.1 benchmark with offline scenario llama-3.1-server Run Llama 3.1 benchmark with server scenario help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version ``` ### Examples [#](#examples ""Link to this heading"") * BERT benchmark The BERT benchmark is based on running with a single RNGD. + Server Scenario To run BERT Large serving inference benchmark, you can use the following command: ``` furiosa-mlperf bert-server ./bert-large ./bert-server-result --device-mesh ""npu:0:*"" ``` + Offline Scenario To run BERT Large offline inference benchmark, you can use the following command: ``` furiosa-mlperf bert-offline ./bert-large ./bert-offline-result --device-mesh ""npu:0:*"" ``` * GPT-J benchmark The GPT-J benchmark is based on running with a single RNGD. + Server Scenario To run GPT-J 6B serving inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-server ./gpt-j-6b ./gpt-j-server-result ``` + Offline Scenario To run GPT-J 6B offline inference benchmark, you can use the following command: ``` furiosa-mlperf gpt-j-offline ./gpt-j-6b ./gpt-j-offline-result ``` * Llama 3.1 benchmark The Llama 3.1 benchmark is based on running with four RNGDs. + Server Scenario To run Llama 3.1 70B serving inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-server ./llama-3.1-70b ./llama-3.1-server-result ``` + Offline Scenario To run Llama 3.1 70B offline inference benchmark, you can use the following command: ``` furiosa-mlperf llama-3.1-offline ./llama-3.1-70b ./llama-3.1-offline-result ``` * Common Once the process completes, it writes the results to a file in the specified results directory. You can open this file to view a summary of the results. ``` cat gpt-j-offline-result/mlperf_log_summary.txt ``` ``` ================================================ MLPerf Results Summary ================================================ SUT name : GPT-J SUT Scenario : Offline Mode : PerformanceOnly Samples per second: 11.842 Tokens per second (inferred): 817.095 Result is : VALID Min duration satisfied : Yes Min queries satisfied : Yes Early stopping satisfied: Yes ```', '<2-hop>\n\nFuriosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) FuriosaAI’s Software Stack [#](#furiosaai-s-software-stack ""Link to this heading"") ================================================================================== FuriosaAI provides the streamlined software stack to allow FuriosaAI NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials. The above diagram demonstrates the SW stack provided by FuriosaAI, by layers. The following outlines the key components. Kernel Driver, Firmware, and PE Runtime [#](#kernel-driver-firmware-and-pe-runtime ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- The kernel device driver enables the Linux operating system to recognize NPU devices and expose them as Linux device files. The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE). PERT is responsible for communicating with the host’s runtime and scheduling, managing the resources of PEs to execute NPU tasks. Furiosa Compiler [#](#furiosa-compiler ""Link to this heading"") -------------------------------------------------------------- Furiosa compiler analyzes, optimizes a model graph, and generates a NPU executable program. The operation passes involve graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. When `torch.compile()` backend, `FuriosaBackend` is used or `furiosa-llm` is used, the Furiosa Compiler is transparently used to generate NPU executable programs for Runtime. Furiosa Runtime [#](#furiosa-runtime ""Link to this heading"") ------------------------------------------------------------ Runtime loads multiple executable NPU programs generated by Furiosa compiler, and run them on the NPU. A single model can be compiled into multiple executable programs according to model architectures and applications. Runtime is responsible for scheduling NPU programs and managing computation and memory resource on NPUs and CPUs. Also, Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs. Furiosa Model Compressor (Quantizer) [#](#furiosa-model-compressor-quantizer ""Link to this heading"") ---------------------------------------------------------------------------------------------------- Furiosa Model Compressor is a library as well as toolkit for model calibration and quantization. Model quantization is a powerful technique to reduce memory footprint, computation cost, inference latency and power consumption. Furiosa Model Compressor provides post-training quantization methods, such as * BF16 (W16A16) * INT8 Weight-Only (W8A16) * FP8 (W8A8) * INT8 SmoothQuant (W8A8) * INT4 Weight-Only (W4A16 AWQ / GPTQ) (Planned in release 2024.2) Furiosa LLM [#](#furiosa-llm ""Link to this heading"") ---------------------------------------------------- Furiosa LLM provides a high-performance inference engine for LLM models, such as Llama 3.1 70B, 8B, GPT-J, and Bert. Furiosa LLM is designed to provide the state-of-the-art serving optimization for LLM models. The key features of Furiosa LLM include vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and OpenAI-compatible API server. You can find further information at [Furiosa LLM](../furiosa_llm/intro.html#furiosallm) . Kubernetes Support [#](#kubernetes-support ""Link to this heading"") ------------------------------------------------------------------ Kubernetes, an open-source platform designed to manage containerized applications and services, is extensively adopted by various companies due to its robust capabilities for deploying, scaling, and automating containerized workloads. FuriosaAI software stack also offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment. FuriosaAI’s device plugin enables Kubernetes clusters to recognize FuriosaAI’s NPUs and allows NPUs to be scheduled for workloads and services that require them. This feature allows users to easily deploy AI workloads with FuriosaAI NPUs on Kubernetes, enabling efficient resource utilization and scaling. You can find more information about Kubernetes support in the [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) . [previous FuriosaAI RNGD](rngd.html ""previous page"") [next Supported Models](supported_models.html ""next page"") Contents * [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","The FuriosaAI software stack utilizes RNGD by running the firmware on the SoC within the RNGD card, which provides low-level APIs to the PE Runtime (PERT) that manages the resources of Processing Elements (PEs) to execute NPU tasks. This setup is crucial for optimizing LLM benchmarks like Llama 3.1, as it allows the Furiosa Compiler to analyze and optimize model graphs, generating NPU executable programs that are efficiently run by the Furiosa Runtime. The Furiosa LLM component further enhances performance by providing a high-performance inference engine for LLM models, including Llama 3.1. Kubernetes plays a significant role by offering native integration with the FuriosaAI software stack, allowing seamless deployment and management of AI applications within a Kubernetes environment. This integration enables Kubernetes clusters to recognize FuriosaAI’s NPUs, facilitating efficient resource utilization and scaling of AI workloads.",multi_hop_specific_query_synthesizer
48,How can Furiosa LLM be set up to run in a container environment and be compatible with the OpenAI API?,"['<1-hop>\n\nRunning Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) Quick Start with Furiosa LLM [#](#quick-start-with-furiosa-llm ""Link to this heading"") ====================================================================================== Furiosa LLM is a serving framework for LLM models that utilizes FuriosaAI’s NPU. It provides a Python API compatible with vLLM and a server compatible with the OpenAI API. This document explains how to install and use Furiosa LLM. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. Installing Furiosa LLM [#](#installing-furiosa-llm ""Link to this heading"") -------------------------------------------------------------------------- The minimum requirements for Furiosa LLM are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Administrator privileges on system (root) * [Setting up APT server](prerequisites.html#aptsetup) and [Installing Prerequisites](prerequisites.html#installingprerequisites) * Python 3.8, 3.9, or 3.10 * Enough storage space for model weights; e.g., about 100GB for Llama 3.1 70B model Then, please install the `furiosa-compiler` package as follows: ``` sudo apt install -y furiosa-compiler ``` Also, you need to create a Python virtual environment depending on your environment. Note Note that some models, such as meta-llama/Meta-Llama-3.1-8B require you to accept their license, hence, you need to create a HuggingFace account, accept the model’s license, and generate a token. Usually, you can create your token at <https://huggingface.co/settings/tokens> . Once you get a token, you can authenticate on the HuggingFace Hub as following: ``` huggingface-cli login --token $HF_TOKEN ``` Then, you can install the Furiosa LLM with the following command: ``` pip install furiosa-llm ``` ### Offline Batch Inference with Furiosa LLM [#](#offline-batch-inference-with-furiosa-llm ""Link to this heading"") In this section, we will explain how to perform offline LLM inference using the Python API of Furiosa LLM. First, import the `LLM` class and `SamplingParams` from the furiosa\\_llm module. `LLM` class is used to load LLM models and provides the core API for LLM inference. `SamplingParams` allows to specify various parameters for text generation. ``` from furiosa_llm import LLM, SamplingParams ``` Next, we will load an LLM model using the LLM class. The following example loads the meta-llama/Meta-Llama-3.1-8B model from the HuggingFace Hub and performs quantization. The model will be loaded into the memory and ready for inference. ``` llm = LLM(model=""meta-llama/Llama-3.1-8B-Instruct"") ``` After loading the model, you can perform LLM inference by calling the `generate` method. ``` prompts = [ """" ] sampling_params = SamplingParams(temperature=0.0) ``` Launching the OpenAI-Compatible Server [#](#launching-the-openai-compatible-server ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- You can find more details at [OpenAI Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) . Running Furiosa LLM in container environment [#](#running-furiosa-llm-in-container-environment ""Link to this heading"") ---------------------------------------------------------------------------------------------------------------------- FuriosaAI provides an image for running `furiosa-llm` in a containerized environment. By using the containerized version, you can run environments that utilize `furiosa-llm` without installing the FuriosaAI Software Stack on your host system, or you can execute it within a Kubernetes environment. To run the `furiosa-llm` container, you can use the following command: ``` $ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash (container) # python ``` Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Installing Prerequisites](prerequisites.html ""previous page"") [next Running MLPerf™ Inference Benchmark](furiosa_mlperf.html ""next page"") Contents * [Installing Furiosa LLM](#installing-furiosa-llm) + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..', '<2-hop>\n\n* [.rst](../_sources/getting_started/furiosa_llm.rst ""Download source file"") * .pdf Quick Start with Furiosa LLM ============================ Contents -------- * [ Installing Furiosa LLM](#installing-furiosa-llm) + [ Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [ Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [']","Furiosa LLM can be set up to run in a container environment by using the containerized version provided by FuriosaAI. This allows you to run environments utilizing Furiosa LLM without installing the FuriosaAI Software Stack on your host system, or you can execute it within a Kubernetes environment. To run the Furiosa LLM container, you can use the command: `docker run -it --rm --privileged furiosa-llm:2024.1.0 bash`. However, using the `--privileged` option is not recommended for security reasons. For OpenAI API compatibility, Furiosa LLM provides a server that is compatible with the OpenAI API, allowing for enhanced client interaction.",multi_hop_specific_query_synthesizer
49,How does FuriosaAI's NPU integration enhance the deployment of AI models in a Kubernetes environment?,"['<1-hop>\n\nRunning Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) Quick Start with Furiosa LLM [#](#quick-start-with-furiosa-llm ""Link to this heading"") ====================================================================================== Furiosa LLM is a serving framework for LLM models that utilizes FuriosaAI’s NPU. It provides a Python API compatible with vLLM and a server compatible with the OpenAI API. This document explains how to install and use Furiosa LLM. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. Installing Furiosa LLM [#](#installing-furiosa-llm ""Link to this heading"") -------------------------------------------------------------------------- The minimum requirements for Furiosa LLM are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Administrator privileges on system (root) * [Setting up APT server](prerequisites.html#aptsetup) and [Installing Prerequisites](prerequisites.html#installingprerequisites) * Python 3.8, 3.9, or 3.10 * Enough storage space for model weights; e.g., about 100GB for Llama 3.1 70B model Then, please install the `furiosa-compiler` package as follows: ``` sudo apt install -y furiosa-compiler ``` Also, you need to create a Python virtual environment depending on your environment. Note Note that some models, such as meta-llama/Meta-Llama-3.1-8B require you to accept their license, hence, you need to create a HuggingFace account, accept the model’s license, and generate a token. Usually, you can create your token at <https://huggingface.co/settings/tokens> . Once you get a token, you can authenticate on the HuggingFace Hub as following: ``` huggingface-cli login --token $HF_TOKEN ``` Then, you can install the Furiosa LLM with the following command: ``` pip install furiosa-llm ``` ### Offline Batch Inference with Furiosa LLM [#](#offline-batch-inference-with-furiosa-llm ""Link to this heading"") In this section, we will explain how to perform offline LLM inference using the Python API of Furiosa LLM. First, import the `LLM` class and `SamplingParams` from the furiosa\\_llm module. `LLM` class is used to load LLM models and provides the core API for LLM inference. `SamplingParams` allows to specify various parameters for text generation. ``` from furiosa_llm import LLM, SamplingParams ``` Next, we will load an LLM model using the LLM class. The following example loads the meta-llama/Meta-Llama-3.1-8B model from the HuggingFace Hub and performs quantization. The model will be loaded into the memory and ready for inference. ``` llm = LLM(model=""meta-llama/Llama-3.1-8B-Instruct"") ``` After loading the model, you can perform LLM inference by calling the `generate` method. ``` prompts = [ """" ] sampling_params = SamplingParams(temperature=0.0) ``` Launching the OpenAI-Compatible Server [#](#launching-the-openai-compatible-server ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- You can find more details at [OpenAI Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) . Running Furiosa LLM in container environment [#](#running-furiosa-llm-in-container-environment ""Link to this heading"") ---------------------------------------------------------------------------------------------------------------------- FuriosaAI provides an image for running `furiosa-llm` in a containerized environment. By using the containerized version, you can run environments that utilize `furiosa-llm` without installing the FuriosaAI Software Stack on your host system, or you can execute it within a Kubernetes environment. To run the `furiosa-llm` container, you can use the following command: ``` $ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash (container) # python ``` Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Installing Prerequisites](prerequisites.html ""previous page"") [next Running MLPerf™ Inference Benchmark](furiosa_mlperf.html ""next page"") Contents * [Installing Furiosa LLM](#installing-furiosa-llm) + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..', '<2-hop>\n\nFuriosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) FuriosaAI’s Software Stack [#](#furiosaai-s-software-stack ""Link to this heading"") ================================================================================== FuriosaAI provides the streamlined software stack to allow FuriosaAI NPU to be used in various applications and environments. Here, we outline the SW stack provided by FuriosaAI, explaining the roles of each component, together with guidelines and tutorials. The above diagram demonstrates the SW stack provided by FuriosaAI, by layers. The following outlines the key components. Kernel Driver, Firmware, and PE Runtime [#](#kernel-driver-firmware-and-pe-runtime ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- The kernel device driver enables the Linux operating system to recognize NPU devices and expose them as Linux device files. The firmware runs on the SoC within the RNGD card and provides low-level APIs to the PE Runtime (PERT) that runs on the Processing Element (PE). PERT is responsible for communicating with the host’s runtime and scheduling, managing the resources of PEs to execute NPU tasks. Furiosa Compiler [#](#furiosa-compiler ""Link to this heading"") -------------------------------------------------------------- Furiosa compiler analyzes, optimizes a model graph, and generates a NPU executable program. The operation passes involve graph-level optimization, operator fusion, memory allocation optimization, scheduling, and data movement minimization across layers. When `torch.compile()` backend, `FuriosaBackend` is used or `furiosa-llm` is used, the Furiosa Compiler is transparently used to generate NPU executable programs for Runtime. Furiosa Runtime [#](#furiosa-runtime ""Link to this heading"") ------------------------------------------------------------ Runtime loads multiple executable NPU programs generated by Furiosa compiler, and run them on the NPU. A single model can be compiled into multiple executable programs according to model architectures and applications. Runtime is responsible for scheduling NPU programs and managing computation and memory resource on NPUs and CPUs. Also, Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs. Furiosa Model Compressor (Quantizer) [#](#furiosa-model-compressor-quantizer ""Link to this heading"") ---------------------------------------------------------------------------------------------------- Furiosa Model Compressor is a library as well as toolkit for model calibration and quantization. Model quantization is a powerful technique to reduce memory footprint, computation cost, inference latency and power consumption. Furiosa Model Compressor provides post-training quantization methods, such as * BF16 (W16A16) * INT8 Weight-Only (W8A16) * FP8 (W8A8) * INT8 SmoothQuant (W8A8) * INT4 Weight-Only (W4A16 AWQ / GPTQ) (Planned in release 2024.2) Furiosa LLM [#](#furiosa-llm ""Link to this heading"") ---------------------------------------------------- Furiosa LLM provides a high-performance inference engine for LLM models, such as Llama 3.1 70B, 8B, GPT-J, and Bert. Furiosa LLM is designed to provide the state-of-the-art serving optimization for LLM models. The key features of Furiosa LLM include vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and OpenAI-compatible API server. You can find further information at [Furiosa LLM](../furiosa_llm/intro.html#furiosallm) . Kubernetes Support [#](#kubernetes-support ""Link to this heading"") ------------------------------------------------------------------ Kubernetes, an open-source platform designed to manage containerized applications and services, is extensively adopted by various companies due to its robust capabilities for deploying, scaling, and automating containerized workloads. FuriosaAI software stack also offers native integration with Kubernetes, allowing seamless deployment and management of AI applications within a Kubernetes environment. FuriosaAI’s device plugin enables Kubernetes clusters to recognize FuriosaAI’s NPUs and allows NPUs to be scheduled for workloads and services that require them. This feature allows users to easily deploy AI workloads with FuriosaAI NPUs on Kubernetes, enabling efficient resource utilization and scaling. You can find more information about Kubernetes support in the [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) . [previous FuriosaAI RNGD](rngd.html ""previous page"") [next Supported Models](supported_models.html ""next page"") Contents * [Kernel Driver, Firmware, and PE Runtime](#kernel-driver-firmware-and-pe-runtime) * [Furiosa Compiler](#furiosa-compiler) * [Furiosa Runtime](#furiosa-runtime) * [Furiosa Model Compressor (Quantizer)](#furiosa-model-compressor-quantizer) * [Furiosa LLM](#furiosa-llm) * [Kubernetes Support](#kubernetes-support) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","FuriosaAI's NPU integration enhances the deployment of AI models in a Kubernetes environment by providing a native integration that allows Kubernetes clusters to recognize FuriosaAI's NPUs. This enables NPUs to be scheduled for workloads and services that require them, facilitating efficient resource utilization and scaling. Additionally, FuriosaAI's software stack, which includes components like the Furiosa Compiler and Runtime, optimizes model execution on NPUs, further improving performance and efficiency in AI applications deployed within Kubernetes.",multi_hop_specific_query_synthesizer
50,how deploy furiosa metrics exporter in kubernetes and run furiosa-mlperf in container?,"['<1-hop>\n\nDeploying Furiosa Metrics Exporter with Helm](#deploying-furiosa-metrics-exporter-with-helm) Installing Furiosa Metrics Exporter [#](#installing-furiosa-metrics-exporter ""Link to this heading"") ==================================================================================================== Furiosa Metrics Exporter [#](#furiosa-metrics-exporter ""Link to this heading"") ------------------------------------------------------------------------------ The Furiosa metrics exporter exposes collection of metrics related to FuriosaAI NPU devices in [Prometheus](https://prometheus.io/) format. In a Kubernetes cluster, you can scrape the metrics provided by furiosa-metrics-exporter using Prometheus and visualize them with a Grafana dashboard. This can be easily set up using the [Prometheus Chart](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus) and [Grafana](https://github.com/grafana/helm-charts/tree/main/charts/grafana) Helm charts, along with the furiosa-metrics-exporter Helm chart. ### Metrics [#](#metrics ""Link to this heading"") The exporter is composed of chain of collectors, each collector is responsible for collecting specific metrics from the Furiosa NPU devices. The following table shows the available collectors and metrics: NPU Metrics [#](#id1 ""Link to this table"") | Collector Name | Metric | Type | Metric Labels | Description | | --- | --- | --- | --- | --- | | Liveness | furiosa\\_npu\\_alive | guage | arch, core, device, uuid, kubernetes\\_node\\_name | The liveness of the Furiosa NPU device. | | Error | furiosa\\_npu\\_error | guage | arch, core, device, uuid, kubernetes\\_node\\_name, label | The error count of the Furiosa NPU device. | | Temperature | furiosa\\_npu\\_hw\\_temperature | guage | arch, core, device, uuid, kubernetes\\_node\\_name, label | The temperature of the Furiosa NPU device. | | Power | furiosa\\_npu\\_hw\\_power | guage | arch, core, device, uuid, kubernetes\\_node\\_name, label | The power consumption of the Furiosa NPU device. | | Core Utilization | furiosa\\_npu\\_core\\_utilization | guage | arch, core, device, uuid, kubernetes\\_node\\_name | The core utilization of the Furiosa NPU device. | All metrics share common metric labels such as arch, core, device, kubernetes\\_node\\_name, and uuid. The following table describes the common metric labels: Common NPU Metrics Label [#](#id2 ""Link to this table"") | Common Metric Label | Description | | --- | --- | | arch | The architecture of the Furiosa NPU device. e.g. warboy, rngd | | core | The core number of the Furiosa NPU device. e.g. 0, 1, 2, 3, 4, 5, 6, 7, 0-1, 2-3, 0-3, 4-5, 6-7, 4-7, 0-7 | | device | The device name of the Furiosa NPU device. e.g. npu0 | | kubernetes\\_node\\_name | The name of the Kubernetes node where the exporter is running, this attribute can be missing if the exporter is running on the host machine or in a naked container. | | uuid | The UUID of the Furiosa NPU device. | The metric label “label” is used to describe additional attributes specific to each metric. This approach helps avoid having too many metric definitions and effectively aggregates metrics that share common characteristics. NPU Metrics Type [#](#id3 ""Link to this table"") | Metric Type | Label Attribute | Description | | --- | --- | --- | | Error | axi\\_post\\_error | Indicates count of axi post error. | | Error | axi\\_fetch\\_error | Indicates count of axi fetch error. | | Error | axi\\_discard\\_error | Indicates count of axi discard error. | | Error | axi\\_doorbell\\_done | Indicates count of axi doorbell done error. | | Error | pcie\\_post\\_error | Indicates count of PCIe post error. | | Error | pcie\\_fetch\\_error | Indicates count of PCIe fetch error. | | Error | pcie\\_discard\\_error | Indicates count of PCIe discard error. | | Error | pcie\\_doorbell\\_done | Indicates count of PCIe doorbell done error. | | Error | device\\_error | Total count of device error. | | Temperature | peak | The highest temperature observed from SoC sensors | | Temperature | ambient | The temperature observed from sensors attached to the board | | Power | rms | Root Mean Square (RMS) value of the power consumed by the device, providing an average power consumption metric over a period of time. | The following shows real-world example of the metrics: ``` #liveness furiosa_npu_alive{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 1 #error furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_post_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_fetch_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_discard_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""axi_doorbell_done"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_post_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_fetch_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_discard_error"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""pcie_doorbell_done"",uuid=""uuid""} 0 furiosa_npu_error{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""device_error"",uuid=""uuid""} 0 #temperature furiosa_npu_hw_temperature{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""peak"",uuid=""uuid""} 39 furiosa_npu_hw_temperature{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""ambient"",uuid=""uuid""} 35 #power furiosa_npu_hw_power{arch=""rngd"",core=""0-7"",device=""npu0"",kubernetes_node_name=""node"",label=""rms"",uuid=""uuid""} 4795000 #core utilization furiosa_npu_core_utilization{arch=""rngd"",core=""0"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""1"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""2"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""3"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""4"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""5"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""6"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 furiosa_npu_core_utilization{arch=""rngd"",core=""7"",device=""npu0"",kubernetes_node_name=""node"",uuid=""uuid""} 90 ``` ### Deploying Furiosa Metrics Exporter with Helm [#](#deploying-furiosa-metrics-exporter-with-helm ""Link to this heading"") The Furiosa metrics exporter helm chart is available at [furiosa-ai/helm-charts](https://github.com/furiosa-ai/helm-charts) . To configure deployment as you need, you can modify `charts/furiosa-metrics-exporter/values.yaml` . For example, the Furiosa metrics exporter Helm chart automatically creates a Service Object with Prometheus annotations to enable metric scraping automatically. You can modify the values.yaml to change the port or disable the Prometheus annotations if needed. You can deploy the Furiosa Metrics Exporter by running the following commands: ``` helm repo add furiosa https://furiosa-ai.github.io/helm-charts helm repo update helm install furiosa-metrics-exporter furiosa/furiosa-metrics-exporter -n kube-system ``` [previous Installing Furiosa Device Plugin](device_plugin.html ""previous page"") [next Scheduling NPUs](scheduling_npus.html ""next page"") Contents * [Furiosa Metrics Exporter](#furiosa-metrics-exporter) + [Metrics](#metrics) + [Deploying Furiosa Metrics Exporter with Helm](#deploying-furiosa-metrics-exporter-with-helm) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..', '<2-hop>\n\nRunning `furiosa-mlperf` in container environment [#](#running-furiosa-mlperf-in-container-environment ""Link to this heading"") ------------------------------------------------------------------------------------------------------------------------------ FuriosaAI provides a containerized version of the `furiosa-mlperf` command. If you use the containerized version, you can run the `furiosa-mlperf` command without installing the FuriosaAI Software Stack on your host system or you can run the command on Kubernetes environment. To run the `furiosa-mlperf` container, you can use the following command: (Assumes model artifacts exist in `/opt/gpt-j-6b` directory) ``` $ docker run -it --rm --privileged \\ -v /opt/gpt-j-6b:gpt-j-6b furiosa-mlperf:2024.1.0 bash (container) # furiosa-mlperf gpt-j-offline /gpt-j-6b /gpt-j-offline-result ``` To run in a containerized environment, refer to the examples provided in this document. Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Quick Start with Furiosa LLM](furiosa_llm.html ""previous page"") [next Furiosa LLM](../furiosa_llm/intro.html ""next page"") Contents * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","To deploy the Furiosa Metrics Exporter in a Kubernetes environment, you can use Helm. First, add the Furiosa Helm repository with the command `helm repo add furiosa https://furiosa-ai.github.io/helm-charts` and update it using `helm repo update`. Then, install the Furiosa Metrics Exporter by executing `helm install furiosa-metrics-exporter furiosa/furiosa-metrics-exporter -n kube-system`. For running `furiosa-mlperf` in a container, you can use Docker. Execute the command `docker run -it --rm --privileged -v /opt/gpt-j-6b:gpt-j-6b furiosa-mlperf:2024.1.0 bash` to start the container and run the `furiosa-mlperf` command inside it. Note that using the `--privileged` option is not recommended for security reasons.",multi_hop_specific_query_synthesizer
51,How do you launch the FuriosaAI LLM Engine using the OpenAI-compatible server and what are the necessary components?,"['<1-hop>\n\nUsing OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api) OpenAI Compatible Server [#](#openai-compatible-server ""Link to this heading"") ============================================================================== The `furiosa-llm` package includes an OpenAI-compatible server that can interact with OpenAI clients (and, of course, HTTP clients as well), supporting the `/v1/chat/completions` and `/v1/completions` APIs. In this section, we will explain how to launch the OpenAI-compatible furiosa-llm server. Tip You can learn more about the OpenAI API at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) . To launch the server, you must prepare: (1) the FuriosaAI LLM Engine artifact and (2) a chat template for the model. To download the FuriosaAI LLM Engine artifact, follow the instructions provided through our distribution channels or contact our sales team. To prepare the chat template, follow the instructions in the following section. Preparing Chat Templates [#](#preparing-chat-templates ""Link to this heading"") ------------------------------------------------------------------------------ Furiosa SDK 2024.1.0 (alpha) uses Transformers v4.31.0, which does not include a chat template. Therefore, to support `/v1/chat/completions` , you must provide a chat template yourself. This constraint will be removed in future releases. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. If you have access to the Llama repositories on Hugging Face, you can obtain the chat template for Llama using the following commands: ``` # Prerequisite: create a separate environment to install the latest Transformers version pip install ""transformers>=4.34.0"" python - <<EOF from transformers import AutoTokenizer tok = AutoTokenizer.from_pretrained(\'meta-llama/Meta-Llama-3.1-70B-Instruct\') with open(\'chat_template.tpl\', \'w\') as f: f.write(tok.chat_template) EOF ``` Launching the Server [#](#launching-the-server ""Link to this heading"") ---------------------------------------------------------------------- You can launch the server using the furiosa-llm serve command. ### Arguments for the serve command [#](#arguments-for-the-serve-command ""Link to this heading"") ``` usage: furiosa-llm serve [-h] --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} --artifact ARTIFACT [--host HOST] [--port PORT] --chat-template CHAT_TEMPLATE [--response-role RESPONSE_ROLE] [-pp PIPELINE_PARALLEL_SIZE] [-tp TENSOR_PARALLEL_SIZE] [--devices DEVICES] options: -h, --help show this help message and exit --model {furiosa-ai/llama-3-1-70b,furiosa-ai/llama-3-1-8b,furiosa-ai/fake-llm} The model to use. Currently only one model is supported per server. --artifact ARTIFACT Path to Furiosa LLM Engine artifact --host HOST Host to bind the server to --port PORT Port to bind the server to --chat-template CHAT_TEMPLATE Path to chat template file (must be a jinja2 template) --response-role RESPONSE_ROLE Response role for /v1/chat/completions API (default: \'assistant\') -pp PIPELINE_PARALLEL_SIZE, --pipeline-parallel-size PIPELINE_PARALLEL_SIZE Number of pipeline stages. -tp TENSOR_PARALLEL_SIZE, --tensor-parallel-size TENSOR_PARALLEL_SIZE Number of tensor parallel replicas. --devices DEVICES Devices to use (e.g. ""npu:0:*,npu:1:*""). If unspecified, all available devices from the host will be used. ``` Examples [#](#examples ""Link to this heading"") ---------------------------------------------- ### LLaMA-3.1-70B with 4 RNGDs [#](#llama-3-1-70b-with-4-rngds ""Link to this heading"") ``` furiosa-llm serve \\ --model {path to mlperf-llama-3-1-fp8-pp4} \\ -tp 4 -pp 4 --devices ""npu:0:*,npu:1:*,npu:2:*,npu:3:*"" \\ --chat-template {path to chat template} ``` ### LLaMA-3.1-8B with Single RNGD [#](#llama-3-1-8b-with-single-rngd ""Link to this heading"") ``` furiosa-llm serve \\ --model {path to mlperf-llama-3-1-8b-fp8} \\ -tp 4 -pp 1 --devices ""npu:0:*"" \\ # You may choose arbitrary device index, if multiple devices are on host --chat-template {path to chat template} ``` Using OpenAI Client [#](#using-openai-client ""Link to this heading"") -------------------------------------------------------------------- You can use two APIs: `client.chat.completions` and `client.completions` . You can also set `stream=True` to receive a streaming response. Tip You can install the OpenAI Python client using the following command: ``` pip install openai ``` ``` import openai HOST = ""localhost:8000"" openai.api_base = f""http://{HOST}/v1"" openai.api_key = ""0000"" stream_chat_completion = openai.ChatCompletion.create( model="""", messages=[ {""role"": ""system"", ""content"": ""You are a helpful assistant.""}, {""role"": ""user"", ""content"": ""What is the largest animal in the world?""}, ], stream=True, ) for completion in stream_chat_completion: content = completion.choices[0].delta.get(""content"") if content: print(content, end="""") ``` The compatibility with OpenAI API [#](#the-compatibility-with-openai-api ""Link to this heading"") ------------------------------------------------------------------------------------------------ Currently, `furiosa serve` supports the following OpenAI API parameters: You can find more about each parameter at [Completions API](https://platform.openai.com/docs/api-reference/completions) and [Chat API](https://platform.openai.com/docs/api-reference/chat) . Warning Please note that using `use_beam_search` with `stream` is not allowed because the beam search cannot determine the tokens until the end of the sequence. In 2024.1 release, `n` works only for beam search and it will be fixed in the next release. * `n` * `temperature` * `top_p` * `top_k` * `early_stopping` * `length_penalty` * `max_tokens` * `min_tokens` * `use_beam_search` * `best_of` [previous Furiosa LLM](intro.html ""previous page"") [next References](references.html ""next page"") Contents * [Preparing Chat Templates](#preparing-chat-templates) * [Launching the Server](#launching-the-server) + [Arguments for the serve command](#arguments-for-the-serve-command) * [Examples](#examples) + [LLaMA-3.1-70B with 4 RNGDs](#llama-3-1-70b-with-4-rngds) + [LLaMA-3.1-8B with Single RNGD](#llama-3-1-8b-with-single-rngd) * [Using OpenAI Client](#using-openai-client) * [The compatibility with OpenAI API](#the-compatibility-with-openai-api) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..', '<2-hop>\n\n* [.rst](../_sources/getting_started/furiosa_llm.rst ""Download source file"") * .pdf Quick Start with Furiosa LLM ============================ Contents -------- * [ Installing Furiosa LLM](#installing-furiosa-llm) + [ Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [ Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [']","To launch the FuriosaAI LLM Engine using the OpenAI-compatible server, you need to prepare two main components: the FuriosaAI LLM Engine artifact and a chat template for the model. The server can be launched using the 'furiosa-llm serve' command, which requires specifying the model, the path to the Furiosa LLM Engine artifact, and the path to the chat template file. The server supports the '/v1/chat/completions' and '/v1/completions' APIs, making it compatible with OpenAI clients.",multi_hop_specific_query_synthesizer