,page_id,link,question,answer
0,cf227685-cc4e-420e-b21a-e7da166093e5,https://furiosa-ai.github.io/docs/latest/en/software/vm_support.html,What steps are necessary to ensure that a Warboy device is recognized and available within a virtual machine using QEMU-KVM?,"First, enable IOMMU in both BIOS and Linux OS. Verify IOMMU is enabled by checking for DMAR or IOMMU messages using 'dmesg' and 'grep' commands. Load the 'vfio-pci' kernel module if not already loaded. Ensure the virtual machine tool is ready by running 'virt-host-validate' and confirming all checks pass. Identify the PCI BDF of the Warboy card using 'lspci' and find the PCIe device name with 'virsh nodedev-list'. Create the virtual machine with 'virt-install', specifying the Warboy device using the '--host-device' option. Finally, verify the Warboy device is available in the VM by using 'lspci' and 'update-pciids' commands."
1,cf227685-cc4e-420e-b21a-e7da166093e5,https://furiosa-ai.github.io/docs/latest/en/software/vm_support.html,"What is the significance of the PCI BDF in configuring Warboy pass-through for a virtual machine, and how can it be identified?","The PCI BDF (Bus, Device, Function) is a unique identifier assigned to every PCIe device connected to a machine, crucial for identifying the Warboy card to pass through to a virtual machine. It can be identified using the command 'lspci -nD | grep 1ed2', where '1ed2' is the PCI vendor ID of FuriosaAI Inc. The output includes the PCI BDF, such as '01:00.0', which is then used to form the PCIe device name for the virtual machine tool."
2,cf227685-cc4e-420e-b21a-e7da166093e5,https://furiosa-ai.github.io/docs/latest/en/software/vm_support.html,What are the specific BIOS and Linux OS configurations required to enable IOMMU for Warboy pass-through in a virtual machine setup?,"In BIOS, IOMMU and VT-x should be enabled. In Linux OS, check for IOMMU messages using 'dmesg | grep -e DMAR -e IOMMU'. If not found, add 'intel_iommu=on' for Intel CPU or 'amd_iommu=on' for AMD CPU to 'GRUB_CMDLINE_LINUX' in '/etc/default/grub' and apply changes by rebooting. For legacy BIOS boot mode, use 'grub2-mkconfig -o /boot/grub2/grub.cfg', and for UEFI boot mode, use 'grub2-mkconfig -o /boot/efi/EFI/{linux_distrib}/grub.cfg', replacing '{linux_distrib}' with the Linux OS name."
3,333851a4-2ea4-4903-87a2-0d50943faf1f,https://furiosa-ai.github.io/docs/latest/en/software/performance.html,"How does the optimization of the 'Quantize' operator using the 'ModelEditor' API improve inference latency, and what are the specific changes made to the input tensor data type?","The optimization of the 'Quantize' operator using the 'ModelEditor' API improves inference latency by changing the data type of the input tensor from 'float32' to 'uint8', allowing the 'quantize' operator to convert 'uint8' values to 'int8' values instead of 'float32' to 'int8'. This conversion is much faster, reducing the 'Quantize' execution time from 60.639 ms to 0.277 ms."
4,333851a4-2ea4-4903-87a2-0d50943faf1f,https://furiosa-ai.github.io/docs/latest/en/software/performance.html,"What are the key considerations and strategies for optimizing inference performance in a production environment using Furiosa SDK, particularly in relation to the interplay between NPU execution, CPU computation, and I/O operations?","Optimizing inference performance in a production environment using Furiosa SDK involves focusing on throughput and latency as key metrics. The interplay between NPU execution, CPU computation, and I/O operations is crucial, as these operations run independently and can overlap when multiple inferences are processed simultaneously. The longest operation among these determines the inference time, as shorter operations are hidden by longer ones. Strategies include model optimization to reduce bottlenecks, runtime optimization to leverage concurrency, and using tools like furiosa-bench for performance profiling. Additionally, optimizing NPU utilization by adjusting batch sizes and using multiple workers can enhance performance."
5,333851a4-2ea4-4903-87a2-0d50943faf1f,https://furiosa-ai.github.io/docs/latest/en/software/performance.html,"How does the Furiosa Compiler optimize models with large input and output tensors to improve performance, and what is the role of patch size in this optimization?","The Furiosa Compiler optimizes models with large input and output tensors by splitting them into smaller tensors, known as patches, and merging the results. This approach hides I/O operations between DRAM and SRAM by overlapping them with NPU executions. The patch size is crucial as it determines the balance between time spent on NPU computation and I/O operations, with smaller patches increasing NPU computation time and larger patches increasing I/O operation time."
6,13973d88-9f7d-49e4-8019-68c7f29141b3,https://furiosa-ai.github.io/docs/latest/en/releases/0.9.0.html,How does the percentile calibration method impact the accuracy of EfficientNet-B0 compared to the min-max calibration method in the Furiosa SDK 0.9.0 release?,The accuracy of EfficientNet-B0 increased from 16.104% with the min-max calibration method to 73.556% with the percentile calibration method.
7,13973d88-9f7d-49e4-8019-68c7f29141b3,https://furiosa-ai.github.io/docs/latest/en/releases/0.9.0.html,What are the new features introduced in the quantization tool of Furiosa SDK 0.9.0 that enhance model accuracy?,"The quantization tool in Furiosa SDK 0.9.0 introduces new APIs for flexibility, new calibration methods, an option to perform quantization at the model's beginning, and the ability to convert model output to uint8 using the 'normalized_pixel_outputs' argument. These enhancements contribute to improved model accuracy."
8,13973d88-9f7d-49e4-8019-68c7f29141b3,https://furiosa-ai.github.io/docs/latest/en/releases/0.9.0.html,What improvements have been made to the 'furiosactl' command-line tool in the Furiosa SDK 0.9.0 release?,"The 'furiosactl' command-line tool in the Furiosa SDK 0.9.0 release includes the addition of the 'furiosactl top' command to view NPU device utilization over time and improvements to the 'furiosactl info' command for displaying concise device information, with an option for more detailed output using '--full'."
9,64f0ffea-6087-4f27-8889-83479e61e89e,https://furiosa-ai.github.io/docs/latest/en/software/profiler.html,What are the advantages of using a Profiler Context for tracing model inference performance over tracing via environment variables?,"Using a Profiler Context allows for immediate trace enabling in interactive environments, the specification of labels for certain inference runs, and selective measurement of specified operator categories."
10,64f0ffea-6087-4f27-8889-83479e61e89e,https://furiosa-ai.github.io/docs/latest/en/software/profiler.html,How can the Furiosa SDK's profiling tool help mitigate the challenges associated with tracing long-running jobs?,"The Furiosa SDK's profiling tool provides an API to pause and resume the profiler within the context, allowing users to exclude certain executions from profiling. This reduces profiling overhead, decreases trace file size, and makes it easier to identify interesting sections in the trace data."
11,64f0ffea-6087-4f27-8889-83479e61e89e,https://furiosa-ai.github.io/docs/latest/en/software/profiler.html,What are the potential drawbacks of enabling trace generation by default in the Furiosa SDK's profiling tool?,"Enabling trace generation by default can introduce temporal overheads as it measures time for each step and writes the results to a file, which can affect performance."
12,65a65779-8c0e-48f0-890a-f6ecc20a9f41,https://furiosa-ai.github.io/docs/latest/en/releases/0.5.0.html,"What improvements were made to the Session API in the FuriosaAI SDK 0.5.0 release, and how do these changes enhance the functionality of the API?","The Session API improvements in the FuriosaAI SDK 0.5.0 release include the ability to designate NPU devices directly in the Session API, rather than only through the environment variable 'NPU_DEVNAME'. This allows for more flexible device management when generating sessions. Additionally, support for tensor names was added, enabling explicit designation of input and output tensor names, which enhances the clarity and specificity of session.run() operations."
13,65a65779-8c0e-48f0-890a-f6ecc20a9f41,https://furiosa-ai.github.io/docs/latest/en/releases/0.5.0.html,"How does the FuriosaAI SDK 0.5.0 release enhance error diagnosis and handling, and what specific improvements have been introduced in this area?","The 0.5.0 release improves error diagnosis by outputting version information and compiler logs independently, facilitating easier bug reporting. Specific improvements include error fixes for duplicate device usage, added timeout for CompletionQueue, error handling for session connection termination, and a fix for hanging issues during compilation interruptions."
14,65a65779-8c0e-48f0-890a-f6ecc20a9f41,https://furiosa-ai.github.io/docs/latest/en/releases/0.5.0.html,"What new capabilities does the FuriosaAI SDK 0.5.0 release introduce for model serving, and how can users quickly deploy a model using these features?","The FuriosaAI SDK 0.5.0 release introduces the Furiosa Server, a serving framework that supports GRPC and REST API. Users can quickly deploy a model by installing the server with 'pip install furiosa-sdk[server]' and running it with the command 'furiosa server --model-path MNISTnet_uint8_quant_without_softmax.tflite --model-name mnist'."
15,b6c45005-84e1-46e6-a185-42be1be00b6e,https://furiosa-ai.github.io/docs/latest/en/software/compiler.html,What are the potential consequences of setting an inappropriate batch size when using the FuriosaAI compiler for model inference?,"Setting an inappropriate batch size can lead to increased memory I/O costs between the host and the NPU if the necessary memory size exceeds the NPU DRAM size, resulting in significant performance degradation."
16,b6c45005-84e1-46e6-a185-42be1be00b6e,https://furiosa-ai.github.io/docs/latest/en/software/compiler.html,"How can the FuriosaAI compiler cache be configured to use a Redis cluster as storage, and what are the implications of setting a negative value for FC_CACHE_LIFETIME?","To configure the FuriosaAI compiler cache to use a Redis cluster as storage, set the environment variable FC_CACHE_STORE_URL to a Redis URL, such as redis://:<PASSWORD>@127.0.0.1:6379. Setting a negative value for FC_CACHE_LIFETIME means the cache will be alive forever without expiration, which can be useful for a read-only cache."
17,b6c45005-84e1-46e6-a185-42be1be00b6e,https://furiosa-ai.github.io/docs/latest/en/software/compiler.html,"What is the significance of the 'output.enf' file generated by the FuriosaAI compiler, and how can it be utilized in an operational environment?","The 'output.enf' file is the final output of the FuriosaAI compiler, representing the Executable NPU Format. It can be reused to bypass the compilation process, which is beneficial for creating sessions or serving a model across multiple machines in an operational environment. This reuse is facilitated by the Python SDK, where the ENF file is passed to the 'create_runner()' function to instantly create a runner without recompilation."
18,a50ce0f1-25f2-4882-a3e5-8da4cb7215b7,https://furiosa-ai.github.io/docs/latest/en/software/references.html,What resources are available for developers seeking to integrate Furiosa AI's technology using different programming languages?,"Developers can refer to the C Language SDK Reference and the Python SDK Reference for integrating Furiosa AI's technology using C and Python, respectively."
19,a50ce0f1-25f2-4882-a3e5-8da4cb7215b7,https://furiosa-ai.github.io/docs/latest/en/software/references.html,What tools and platforms were utilized to construct the documentation page for FuriosaAI?,The documentation page was built with Sphinx using a theme provided by Read the Docs.
20,a50ce0f1-25f2-4882-a3e5-8da4cb7215b7,https://furiosa-ai.github.io/docs/latest/en/software/references.html,What version of the Furiosa Model Zoo Reference is linked in the markdown content?,v0.10.0
21,6c328d98-54c8-4c8e-bea7-5927c3921609,https://furiosa-ai.github.io/docs/latest/en/software/serving.html,What are the specific steps and commands required to run a model server using a configuration file in the Furiosa Model Server framework?,"To run a model server using a configuration file, navigate to the 'furiosa-sdk/python/furiosa-server' directory and execute the command 'furiosa server --model-config samples/model_config_example.yaml'. This will start the server with the configurations specified in the YAML file, such as model paths, versions, and NPU device settings."
22,6c328d98-54c8-4c8e-bea7-5927c3921609,https://furiosa-ai.github.io/docs/latest/en/software/serving.html,"How does the Furiosa Model Server ensure compatibility with the KServe Predict Protocol Version 2, and what are the key endpoints provided for model inference?","The Furiosa Model Server provides REST/GRPC endpoints that are compatible with the KServe Predict Protocol Version 2. Key endpoints include GET /v2/health/live for server liveness, GET /v2/health/ready for model readiness, GET /v2/models/${MODEL_NAME}/versions/${MODEL_VERSION} for model metadata, GET /v2/models/${MODEL_NAME}/versions/${MODEL_VERSION}/ready for specific model readiness, and POST /v2/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/infer for inference requests."
23,6c328d98-54c8-4c8e-bea7-5927c3921609,https://furiosa-ai.github.io/docs/latest/en/software/serving.html,What are the requirements and steps for installing the Furiosa Model Server on a compatible system?,"The requirements for installing the Furiosa Model Server include Ubuntu 20.04 LTS (Debian bullseye) or higher, Driver, Firmware, and Runtime Installation, and Python 3.8 or higher. To install using PIP, run the command 'pip install 'furiosa-sdk[server]''. To install from source code, clone the repository with 'git clone https://github.com/furiosa-ai/furiosa-sdk.git', navigate to 'furiosa-sdk/python/furiosa-server', and run 'pip install .'."
24,4336ecbc-28cf-41e6-863f-25212b860e38,https://furiosa-ai.github.io/docs/latest/en/software/tutorials.html,Which tutorial would you refer to for learning about advanced inference APIs using the Furiosa SDK?,[Advanced Inference API](https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/AdvancedTopicsInInferenceAPIs.ipynb)
25,4336ecbc-28cf-41e6-863f-25212b860e38,https://furiosa-ai.github.io/docs/latest/en/software/tutorials.html,What resource would you consult to compare the accuracy of CPU-based inference with Furiosa SDK?,[Comparing Accuracy with CPU-based inference](https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/InferenceAccuracyCheck.ipynb)
26,4336ecbc-28cf-41e6-863f-25212b860e38,https://furiosa-ai.github.io/docs/latest/en/software/tutorials.html,What resource should you explore to learn about performing image classification model inference using the Furiosa SDK?,The 'Image Classification Model Inference' notebook available at https://github.com/furiosa-ai/furiosa-sdk/blob/branch-0.10.0/examples/notebooks/Image_Classification.ipynb.
27,e527d72f-b0b3-4132-b935-b89558bc7add,https://furiosa-ai.github.io/docs/latest/en/releases/0.8.0.html,"What are the key improvements introduced in the Furiosa SDK 0.8.0 release for the serving framework API, and how do they enhance model serving capabilities?","The Furiosa SDK 0.8.0 release introduces several key improvements to the serving framework API, including the implementation of a session pool that enables model serving with multiple NPUs, significantly improving throughput and reducing latency for applications with large inputs. It also shifts from thread-based to asyncio-based NPU query processing, allowing for lower latency in handling small and frequent inference queries. Additionally, the release expands support for external devices and runtimes, such as OpenVINO, and introduces support for S3 cloud storage repositories and OpenTelemetry-compatible tracing for performance monitoring."
28,e527d72f-b0b3-4132-b935-b89558bc7add,https://furiosa-ai.github.io/docs/latest/en/releases/0.8.0.html,"How does the 0.8.0 release of Furiosa SDK enhance the model quantization process, and what specific improvements have been made to operator processing?","The 0.8.0 release improves the model quantization process by enhancing accuracy when processing SiLU, MatMul/Gemm, and Add/Sub/Mul/Div operators. It also adds NPU acceleration for more auto_pad properties in Conv/ConvTranspose/MaxPool operators and supports the PRelu operator."
29,e527d72f-b0b3-4132-b935-b89558bc7add,https://furiosa-ai.github.io/docs/latest/en/releases/0.8.0.html,"What new functionalities does the 'furiosactl' command line tool offer in the Furiosa SDK 0.8.0 release, and how can these functionalities assist users in managing NPU devices?","The 'furiosactl' command line tool in the Furiosa SDK 0.8.0 release includes the 'furiosactl ps' command, which allows users to print the OS processes occupying the NPU device, and the 'furiosactl info' command, which now prints the unique UUID for each device. These functionalities assist users in managing NPU devices by providing detailed information about device usage and unique identification."
30,a643c5bc-d22e-493d-bee8-90e6b58fc9dd,https://furiosa-ai.github.io/docs/latest/en/software/kubernetes_support.html,What are the specific steps and configurations required to enable Kubernetes to recognize and utilize FuriosaAI NPUs within a cluster?,"To enable Kubernetes to recognize and utilize FuriosaAI NPUs, follow these steps: 1) Prepare NPU nodes by ensuring they run Ubuntu 20.04 or higher with an Intel compatible CPU, and install the NPU driver and toolkit using 'apt-get update && apt install -y furiosa-driver-warboy furiosa-toolkit'. 2) Install Node Feature Discovery to label nodes with NPUs, ensuring 'beta.furiosa.ai' is included in the '--extra-label-ns' option of 'nfd-master'. 3) Deploy the Device Plugin and NPU Feature Discovery DaemonSet using 'kubectl apply' commands for the respective YAML files. 4) Configure the Device Plugin via command line arguments or a configuration file, setting options like 'interval', 'defaultPe', and 'resourceName'. 5) Create a Pod with NPUs by specifying 'beta.furiosa.ai/npu: ""1""' in 'spec.containers[].resources.limits'."
31,a643c5bc-d22e-493d-bee8-90e6b58fc9dd,https://furiosa-ai.github.io/docs/latest/en/software/kubernetes_support.html,How does the Node Feature Discovery DaemonSet contribute to the management of FuriosaAI NPUs in a Kubernetes cluster?,"The Node Feature Discovery DaemonSet identifies NPU information on NPU-equipped machines and registers it as node labels, allowing for selective scheduling of Pods using nodeSelector. It attaches metadata such as 'feature.node.kubernetes.io/pci-1ed2.present=true' to nodes, which is used by the FuriosaAI Device Plugin and NPU Feature Discovery DaemonSet to ensure they are only distributed to nodes with NPUs."
32,a643c5bc-d22e-493d-bee8-90e6b58fc9dd,https://furiosa-ai.github.io/docs/latest/en/software/kubernetes_support.html,"What role does the FuriosaAI NPU Device Plugin play in the Kubernetes environment, and how is it configured?","The FuriosaAI NPU Device Plugin allows Kubernetes to schedule NPUs for Pod workloads by making the cluster aware of available NPUs on the nodes. It can be configured using command line arguments or a configuration file, specifying options like the interval for searching devices, default core type, and resource name. The configuration can be applied via a ConfigMap in Kubernetes."
33,3e1275e1-f809-4d7a-beb7-ec8b8b5de8f5,https://furiosa-ai.github.io/docs/latest/en/software/quantization.html,"What are the three steps involved in the quantization process of an ONNX model using Furiosa SDK, and how does each step contribute to the overall quantization?","The three steps involved in the quantization process using Furiosa SDK are Graph Optimization, Calibration, and Quantization. In Graph Optimization, the model's graph structure is modified by adding or replacing operators to enable processing of quantized data with minimal accuracy loss. During Calibration, the model's weights are adjusted using the data that trained the model. Finally, Quantization converts the model to an 8-bit representation, following the Tensorflow Lite 8-bit quantization specification."
34,3e1275e1-f809-4d7a-beb7-ec8b8b5de8f5,https://furiosa-ai.github.io/docs/latest/en/software/quantization.html,"How does the accuracy of quantized models using Furiosa SDK compare to their original floating-point counterparts, and what calibration methods are employed?","The accuracy of quantized models using Furiosa SDK is slightly lower than their original floating-point models, with the INT8 accuracy ranging from approximately 93.678% to 99.702% of the FP accuracy. Calibration methods used include Asymmetric MSE, Asymmetric 99.99%-Percentile, Symmetric Entropy, and Asymmetric SQNR."
35,3e1275e1-f809-4d7a-beb7-ec8b8b5de8f5,https://furiosa-ai.github.io/docs/latest/en/software/quantization.html,"What role does graph optimization play in the quantization process of ONNX models using Furiosa SDK, and how does it affect model accuracy?","Graph optimization changes the topological structure of the graph by adding or replacing operators in the model through analysis of the original model network structure, allowing the model to process quantized data with minimal accuracy loss."
36,2114c344-3971-4d26-8c09-a001327ce7bb,https://furiosa-ai.github.io/docs/latest/en/software/c-sdk.html,What are the system requirements and steps needed to install the FuriosaAI C SDK on an Ubuntu system?,"The minimum requirements for installing the FuriosaAI C SDK are Ubuntu 20.04 LTS (Debian bullseye) or higher, system administrator privileges, and the FuriosaAI SDK required packages. To install, you must first install the driver, firmware, and runtime library as per the Required Package Installation guide. Then, configure the APT server for FuriosaAI, authenticate the server connection, and run 'apt-get update && apt-get install -y furiosa-libnux-dev'."
37,2114c344-3971-4d26-8c09-a001327ce7bb,https://furiosa-ai.github.io/docs/latest/en/software/c-sdk.html,What are the implications of the deprecation of 'furiosa-libnux-dev' and the current C API for developers using the FuriosaAI C SDK?,"The deprecation of 'furiosa-libnux-dev' and the current C API means that developers will need to transition to a new C API based on the next-generation runtime called FuriosaRT, which will offer more features in future releases."
38,2114c344-3971-4d26-8c09-a001327ce7bb,https://furiosa-ai.github.io/docs/latest/en/software/c-sdk.html,How does the C SDK's approach to application development differ from the Python SDK in terms of performance and use cases?,"The C SDK is lower-level than the Python SDK, offering lower latency and higher performance, making it suitable for scenarios where the Python runtime cannot be used."
39,2a739e9e-c42e-4c07-82d5-41cf614dd6a9,https://furiosa-ai.github.io/docs/latest/en/software/installation.html,What steps are necessary to configure the APT server for installing FuriosaAI packages on an Ubuntu or Debian system?,"First, install the necessary packages for accessing an HTTPS-based APT server using 'sudo apt update' and 'sudo apt install -y ca-certificates apt-transport-https gnupg wget'. Next, register the FuriosaAI public signing key with 'mkdir -p /etc/apt/keyrings && wget -q -O- https://archive.furiosa.ai/furiosa-apt-key.gpg | gpg --dearmor | sudo tee /etc/apt/keyrings/furiosa-apt-key.gpg > /dev/null'. Then, generate a new API key from FuriosaAI IAM and configure it in '/etc/apt/auth.conf.d/furiosa.conf'. Finally, register the APT server by adding the appropriate line to '/etc/apt/sources.list.d/furiosa.list' depending on the Ubuntu or Debian version."
40,2a739e9e-c42e-4c07-82d5-41cf614dd6a9,https://furiosa-ai.github.io/docs/latest/en/software/installation.html,What is the purpose of adding a user to the 'furiosa' group during the installation of FuriosaAI components?,"Adding a user to the 'furiosa' group allows them access to NPU devices, as the NPU device driver restricts access exclusively to users within this group."
41,2a739e9e-c42e-4c07-82d5-41cf614dd6a9,https://furiosa-ai.github.io/docs/latest/en/software/installation.html,How can you ensure that the installed versions of FuriosaAI packages remain stable and are not automatically updated?,Use the command 'sudo apt-mark hold' followed by the package names to hold the currently installed versions and prevent them from being automatically updated.
42,8bcf3c20-b1d0-44c9-bedc-9e24a3ce9586,https://furiosa-ai.github.io/docs/latest/en/software/cli.html,"What steps does the 'furiosa litmus' command execute to verify the compatibility of an ONNX model with Furiosa SDK, and how can it assist in bug reporting?","The 'furiosa litmus' command executes the following steps: 1) Loads and checks if the model is valid, 2) Quantizes the model with random calibration, 3) Compiles the quantized model, and 4) Performs inference using 'furiosa-bench'. For bug reporting, it can collect logs and environment information into an archive file using the '--dump' option."
43,8bcf3c20-b1d0-44c9-bedc-9e24a3ce9586,https://furiosa-ai.github.io/docs/latest/en/software/cli.html,"What information can be obtained by using the 'furiosactl info --full' command, and how does it differ from the basic 'furiosactl info' command?","The 'furiosactl info --full' command provides detailed information about the NPU device, including the device's UUID and serial number, in addition to the basic information such as temperature, power consumption, and PCI information. The basic 'furiosactl info' command does not include the UUID and serial number."
44,8bcf3c20-b1d0-44c9-bedc-9e24a3ce9586,https://furiosa-ai.github.io/docs/latest/en/software/cli.html,"How does the 'furiosa-bench' command facilitate performance evaluation of models, and what specific metrics does it provide in its output?","The 'furiosa-bench' command performs a benchmark on ONNX or TFLite models using furiosa-runtime, providing metrics such as tail latency and queries per second (QPS). It outputs detailed latency statistics, including minimum, maximum, mean, and various percentile latencies for individual queries."
45,aa27e152-d0c0-4c00-99e9-54d1516e3a5b,https://furiosa-ai.github.io/docs/latest/en/releases/0.6.0.html,"What major changes were introduced in the FuriosaAI SDK 0.6.0 release to enhance the performance and functionality of the NPU, and how do these changes impact the model execution process?","The kernel driver was upgraded to 1.2.2 and the user-level driver to 0.5.2, providing more stable and higher NPU performance. New NPU accelerated operators were added, such as Space-to-depth (CRD mode), Transpose, Slice (height axis only), Concat (height axis only), and Grouped Convolution (if groups <= 128). These changes reduce the frequency of CPU tasks in models with operators requiring large memory usage, thereby reducing execution time."
46,aa27e152-d0c0-4c00-99e9-54d1516e3a5b,https://furiosa-ai.github.io/docs/latest/en/releases/0.6.0.html,"In the FuriosaAI SDK 0.6.0 release, what modifications were made to the Python SDK project structure, and how do these changes affect the installation and usage of the SDK components?","The Python SDK project structure was changed by renaming furiosa-sdk-runtime to furiosa-sdk, furiosa-sdk-quantizer to furiosa-quantizer, and furiosa-sdk-validator to furiosa-litmus. The validator package, now called litmus, checks model compatibility with the Furiosa SDK, and its installation instructions have been updated to 'pip install furiosa-sdk[litmus]'."
47,aa27e152-d0c0-4c00-99e9-54d1516e3a5b,https://furiosa-ai.github.io/docs/latest/en/releases/0.6.0.html,"What are the specific improvements made to the Quantizer in the FuriosaAI SDK 0.6.0 release, and how do these changes contribute to the overall model optimization process?","The improvements to the Quantizer include ensuring the model quantization process is idempotent, removing reliance on PyTorch, enhancing code quality by eliminating multiple Pylint warnings, and upgrading several library dependencies such as Numpy to 1.21.5 and Pyyaml to 6.0.0. These changes contribute to a more reliable and efficient model optimization process by streamlining dependencies and improving code maintainability."
48,ab2af890-296a-47f5-89d9-ffc5cf4a924f,https://furiosa-ai.github.io/docs/latest/en/software/python-sdk.html,What are the steps to create and activate an isolated Python execution environment using Conda for the FuriosaAI Python SDK?,"First, install Anaconda and then create an execution environment with the command 'conda create -n furiosa-3.8 python=3.8'. Activate the environment using 'conda activate furiosa-3.8'. To deactivate, use 'conda deactivate'."
49,ab2af890-296a-47f5-89d9-ffc5cf4a924f,https://furiosa-ai.github.io/docs/latest/en/software/python-sdk.html,What potential error might occur if you attempt to install the FuriosaAI Python SDK without updating pip to the latest version?,You may encounter an error stating that no matching distribution was found for furiosa-quantizer-impl==0.10.* (from furiosa-quantizer==0.10.*->furiosa-sdk).
50,ab2af890-296a-47f5-89d9-ffc5cf4a924f,https://furiosa-ai.github.io/docs/latest/en/software/python-sdk.html,"What are the necessary steps to install the FuriosaAI Python SDK from the source code, and how can you include additional packages like a model server?","Clone the repository from GitHub using 'git clone https://github.com/furiosa-ai/furiosa-sdk', navigate to the 'furiosa-sdk/python' directory, and install the packages in order: 'pip install furiosa-runtime', 'pip install furiosa-tools', and 'pip install furiosa-sdk'. To include additional packages like a model server, navigate to the same directory and run 'pip install furiosa-server'."
51,bff490c4-d18d-40e2-8cdc-089ac8f529ea,https://furiosa-ai.github.io/docs/latest/en/software/intro.html,"How does the FuriosaAI SW stack facilitate the execution of DNN model inference tasks on NPUs, and what role does the runtime play in this process?","The FuriosaAI SW stack facilitates the execution of DNN model inference tasks on NPUs by using a compiler to optimize DNN models and generate executable code for the NPU. The runtime then analyzes this executable program, executing the DNN model inference task by splitting it into smaller tasks that run on both the NPU and CPU. The runtime is responsible for balancing resources, scheduling tasks according to workload, and controlling the NPU via firmware for tasks executed on the NPU."
52,bff490c4-d18d-40e2-8cdc-089ac8f529ea,https://furiosa-ai.github.io/docs/latest/en/software/intro.html,What mechanisms does the FuriosaAI SW stack provide to ensure efficient resource allocation and workload distribution in a Kubernetes environment?,"FuriosaAI SW stack provides Kubernetes Device Plugin and Kubernetes Node Labeller to ensure efficient resource allocation and workload distribution. The Device Plugin enables the Kubernetes cluster to recognize FuriosaAI’s NPUs and schedule them for workloads requiring the NPU, facilitating resource allocation in multi-tenant environments. The Node Labeller adds metadata about the physical NPU to the Kubernetes node object, allowing users to distribute workloads to nodes meeting specific conditions using `spec.nodeSelector` or `spec.nodeAffinity`."
53,bff490c4-d18d-40e2-8cdc-089ac8f529ea,https://furiosa-ai.github.io/docs/latest/en/software/intro.html,"In the FuriosaAI SW stack, how does the compiler optimize DNN models for NPU acceleration, and what happens to operators that are not supported by the NPU?","The compiler optimizes DNN models by introducing various latest research work and methods, and accelerates major vision models such as ResNet50, SSD-MobileNet, and EfficientNet for NPU execution. For operators not supported by NPU acceleration, the compiler compiles them to be executed on the CPU."
54,c40d4e1b-c960-4ffb-883f-e35d53def7a1,https://furiosa-ai.github.io/docs/latest/en/npu/warboy.html,"How does the FuriosaAI Warboy NPU optimize deep learning inference for low batch sizes, and what architectural features support this optimization?","FuriosaAI Warboy optimizes deep learning inference for low batch sizes by ensuring all of the chip’s resources are maximally utilized to achieve low latency. The architecture includes a large on-chip memory that retains most major CNN models, eliminating memory bottlenecks and achieving high energy efficiency. Additionally, the chip's two processing elements can be fused to minimize response time or used independently to maximize throughput, depending on the performance requirements."
55,c40d4e1b-c960-4ffb-883f-e35d53def7a1,https://furiosa-ai.github.io/docs/latest/en/npu/warboy.html,What are the specific conditions under which the FuriosaAI Warboy NPU supports acceleration for the 'Conv' and 'LpNormalization' operators?,The 'Conv' operator is supported for acceleration only when the group is less than or equal to 128 and the dilation is less than or equal to 12. The 'LpNormalization' operator is supported for acceleration only when p equals 2 and the batch size is less than or equal to 2.
56,c40d4e1b-c960-4ffb-883f-e35d53def7a1,https://furiosa-ai.github.io/docs/latest/en/npu/warboy.html,In what ways can the two processing elements (PEs) of the FuriosaAI Warboy NPU be utilized to adapt to different model size or performance requirements?,"The two processing elements (PEs) of the FuriosaAI Warboy NPU can be utilized independently to maximize throughput or fused to minimize response time, depending on the user's model size or performance requirements."
57,9197c2b9-4a70-427e-910e-e57d9f467929,https://furiosa-ai.github.io/docs/latest/en/releases/0.10.0.html,"What are the key improvements introduced in FuriosaRT with the release of Furiosa SDK 0.10.0, and how do they enhance performance and usability?","FuriosaRT introduces a native asynchronous API based on Python's asyncio, which supports high-performance serving applications by handling many concurrent requests. It also provides multi-device support, allowing high-performance inference using multiple devices without extra implementations. The device configuration is more abstracted, using the FURIOSA_DEVICES environment variable for specifying NPU architecture and count. Additionally, FuriosaRT shows significantly improved throughput, with performance improvements observed even with worker_num >= 10. These enhancements make FuriosaRT more efficient and user-friendly for various workloads."
58,9197c2b9-4a70-427e-910e-e57d9f467929,https://furiosa-ai.github.io/docs/latest/en/releases/0.10.0.html,How does the new ModelEditor class in Furiosa SDK 0.10.0 improve the model quantization process compared to previous versions?,"The ModelEditor class allows more options for model input/output types by providing methods like convert_input_type and convert_output_type, which remove quantize and dequantize operators and change tensor types. This replaces the previous with_quantize and normalized_pixel_outputs parameters, offering more flexibility and supporting both uint8 and int8 types."
59,9197c2b9-4a70-427e-910e-e57d9f467929,https://furiosa-ai.github.io/docs/latest/en/releases/0.10.0.html,"What changes have been made to the device configuration process in FuriosaRT with the release of SDK 0.10.0, and how do these changes impact the specification of NPU devices?","FuriosaRT now allows users to specify NPU architecture and count in a textual representation using the new environment variable 'FURIOSA_DEVICES', simplifying the process by eliminating the need to manually specify device file names. This change enhances usability by automatically finding available devices, while still maintaining backward compatibility with the 'NPU_DEVNAME' variable, which will be deprecated in future releases."
60,180d17f3-caa8-46a6-87f5-74a1c03de3a5,https://furiosa-ai.github.io/docs/latest/en/,"What are the prerequisites for accessing FuriosaAI's software components, and how can one initiate the process?","Access to FuriosaAI's software components requires registration in the Early Access Program (EAP) and agreement to the End User Licence Agreement (EULA). To start the EAP, one must contact FuriosaAI via email at contact@furiosa.ai."
61,180d17f3-caa8-46a6-87f5-74a1c03de3a5,https://furiosa-ai.github.io/docs/latest/en/,What resources are available for learning how to deploy models using FuriosaAI's SDK and tools?,"Resources for learning model deployment using FuriosaAI's SDK include the 'Model Server (Serving Framework)' documentation and 'Kubernetes Support' guide under the 'Serving, Model Deployment, MLOps' section."
62,180d17f3-caa8-46a6-87f5-74a1c03de3a5,https://furiosa-ai.github.io/docs/latest/en/,What specific resources are provided for understanding the installation and usage of FuriosaAI's Python SDK?,The resources provided include the 'Python SDK installation and user guide' and tutorials such as 'Tutorial: Basic Inference API' and 'Tutorial: Advanced Inference API'.
63,f6a8c152-6f4c-4ac0-98ae-fbe3093522c6,https://furiosa-ai.github.io/docs/latest/en/customer-support/bugs.html,"What specific information should be included in a bug report when an error occurs with the Furiosa SDK, and how can one verify the SDK version being used?","A bug report should include how to reproduce the bug, a log or screenshot of the bug, SDK version information, and the compilation log if model compilation failed. To verify the SDK version, use the command: `python -c ""from furiosa import runtime;print(runtime.__full_version__)""`."
64,f6a8c152-6f4c-4ac0-98ae-fbe3093522c6,https://furiosa-ai.github.io/docs/latest/en/customer-support/bugs.html,What steps should be taken if a model compilation fails due to an unsupported dimension value in the Furiosa SDK?,"File a bug report at the FuriosaAI customer service center with the information given below the 'Information Dump' and the compilation log file, such as '/home/furiosa/.local/state/furiosa/logs/compile-20211121223028-l5w4g6.log'."
65,f6a8c152-6f4c-4ac0-98ae-fbe3093522c6,https://furiosa-ai.github.io/docs/latest/en/customer-support/bugs.html,What is the default action recommended by FuriosaAI when an error message is displayed during SDK usage?,File a report to the Bug Report section of the FuriosaAI customer service center with the information given below the 'Information Dump' and the compilation log file outputted in the message.
66,ca538c67-ce79-412f-bd1b-c0f73dd818e7,https://furiosa-ai.github.io/docs/latest/en/releases/0.7.0.html,What new support features were added to the Furiosa Compiler in the 0.7.0 release to enhance NPU acceleration?,"The 0.7.0 release added Linear and Nearest mode support for the Resize operator, DCR mode support for the SpaceToDepth and DepthToSpace operators, CHW axis support for the Pad operator, C axis support for the Slice and Concat operators, increased Dilation support up to x12, and acceleration support for operators Tanh, Exp, Log, Gelu, Erf, and Elu."
67,ca538c67-ce79-412f-bd1b-c0f73dd818e7,https://furiosa-ai.github.io/docs/latest/en/releases/0.7.0.html,"What functionality does the compiler cache introduced in Furiosa SDK 0.7.0 provide, and how can it be utilized?",The compiler cache stores compiled binaries into a cache directory and reuses them when the same model is compiled again. Redis can also be used as the compiler cache storage.
68,ca538c67-ce79-412f-bd1b-c0f73dd818e7,https://furiosa-ai.github.io/docs/latest/en/releases/0.7.0.html,"What improvements have been made to the quantization tools in Furiosa SDK 0.7.0, and how do they affect the handling of ONNX models?","The quantization tools now allow inference of tensor shapes without designated axes in ONNX Squeeze operators below OpSet 12, support Conv operators receiving tensors with NxCxD shapes, and modify the 'Conv - BatchNormalization' subgraph to fuse to Conv without bias. They ensure consistent post-quantization processing by always quantizing Sub, Concat, and Pow operators in QDQ format, prevent ONNX Runtime warnings, and reinforce inspection conditions for tensor shape inference. Additionally, they support random calibration for various input types, stabilize termination for already quantized models, adjust weight scale to prevent zero bias scale, and reinforce 'Gather - MatMul' subgraph fusion."
