,user_input,reference_contexts,reference,synthesizer_name
0,What is FuriosaAI's role in running MLPerf Inference Benchmark?,"['* [.rst](../_sources/getting_started/furiosa_mlperf.rst ""Download source file"") * .pdf Running MLPerf™ Inference Benchmark =================================== Contents -------- * [Installing `furiosa-mlperf` command](#installing-furiosa-mlperf-command) * [Running MLPerf Inference Benchmark](#id1) + [SYNOPSIS](#synopsis) + [Examples](#examples) * [Running `furiosa-mlperf` in container environment](#running-furiosa-mlperf-in-container-environment) Running MLPerf™ Inference Benchmark [#](#running-mlperf-inference-benchmark ""Link to this heading"") =================================================================================================== MLPerf™ is a benchmark suite that evaluates the performance of machine learning (ML) software, hardware, and cloud platforms. It is generally used to compare the performance of different systems, and to help developers and end users make decisions about AI systems. FuriosaAI Software Stack provides `furiosa-mlperf` command to run easily the MLPerf Inference Benchmark. This section describes how to reproduce the MLPerf™ Inference Benchmark using the FuriosaAI Software Stack. Note `furiosa-mlperf` is based on MLPerf™ Inference Benchmark v4.1. The only exception is that we replaced the Llama2 benchmark with one using Llama 3.1.  Running MLPerf Inference Benchmark](#id1) + [']","FuriosaAI Software Stack provides the `furiosa-mlperf` command to easily run the MLPerf Inference Benchmark, which is used to evaluate the performance of machine learning software, hardware, and cloud platforms.",single_hop_specifc_query_synthesizer
1,Howw doo youu usee Furiosa LLM withh FuriosaAI NPU in a containerr environmentt?,"['<1-hop>\n\n* [.rst](../_sources/getting_started/furiosa_llm.rst ""Download source file"") * .pdf Quick Start with Furiosa LLM ============================ Contents -------- * [ Installing Furiosa LLM](#installing-furiosa-llm) + [ Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [ Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [', '<2-hop>\n\nRunning Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) Quick Start with Furiosa LLM [#](#quick-start-with-furiosa-llm ""Link to this heading"") ====================================================================================== Furiosa LLM is a serving framework for LLM models that utilizes FuriosaAI’s NPU. It provides a Python API compatible with vLLM and a server compatible with the OpenAI API. This document explains how to install and use Furiosa LLM. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. Installing Furiosa LLM [#](#installing-furiosa-llm ""Link to this heading"") -------------------------------------------------------------------------- The minimum requirements for Furiosa LLM are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Administrator privileges on system (root) * [Setting up APT server](prerequisites.html#aptsetup) and [Installing Prerequisites](prerequisites.html#installingprerequisites) * Python 3.8, 3.9, or 3.10 * Enough storage space for model weights; e.g., about 100GB for Llama 3.1 70B model Then, please install the `furiosa-compiler` package as follows: ``` sudo apt install -y furiosa-compiler ``` Also, you need to create a Python virtual environment depending on your environment. Note Note that some models, such as meta-llama/Meta-Llama-3.1-8B require you to accept their license, hence, you need to create a HuggingFace account, accept the model’s license, and generate a token. Usually, you can create your token at <https://huggingface.co/settings/tokens> . Once you get a token, you can authenticate on the HuggingFace Hub as following: ``` huggingface-cli login --token $HF_TOKEN ``` Then, you can install the Furiosa LLM with the following command: ``` pip install furiosa-llm ``` ### Offline Batch Inference with Furiosa LLM [#](#offline-batch-inference-with-furiosa-llm ""Link to this heading"") In this section, we will explain how to perform offline LLM inference using the Python API of Furiosa LLM. First, import the `LLM` class and `SamplingParams` from the furiosa\\_llm module. `LLM` class is used to load LLM models and provides the core API for LLM inference. `SamplingParams` allows to specify various parameters for text generation. ``` from furiosa_llm import LLM, SamplingParams ``` Next, we will load an LLM model using the LLM class. The following example loads the meta-llama/Meta-Llama-3.1-8B model from the HuggingFace Hub and performs quantization. The model will be loaded into the memory and ready for inference. ``` llm = LLM(model=""meta-llama/Llama-3.1-8B-Instruct"") ``` After loading the model, you can perform LLM inference by calling the `generate` method. ``` prompts = [ """" ] sampling_params = SamplingParams(temperature=0.0) ``` Launching the OpenAI-Compatible Server [#](#launching-the-openai-compatible-server ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- You can find more details at [OpenAI Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) . Running Furiosa LLM in container environment [#](#running-furiosa-llm-in-container-environment ""Link to this heading"") ---------------------------------------------------------------------------------------------------------------------- FuriosaAI provides an image for running `furiosa-llm` in a containerized environment. By using the containerized version, you can run environments that utilize `furiosa-llm` without installing the FuriosaAI Software Stack on your host system, or you can execute it within a Kubernetes environment. To run the `furiosa-llm` container, you can use the following command: ``` $ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash (container) # python ``` Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Installing Prerequisites](prerequisites.html ""previous page"") [next Running MLPerf™ Inference Benchmark](furiosa_mlperf.html ""next page"") Contents * [Installing Furiosa LLM](#installing-furiosa-llm) + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..']","To use Furiosa LLM with FuriosaAI NPU in a container environment, FuriosaAI provides an image for running `furiosa-llm` in a containerized setup. This allows you to run environments utilizing `furiosa-llm` without installing the FuriosaAI Software Stack on your host system. You can execute it within a Kubernetes environment by using the provided Docker command: `$ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash`. However, it is advised not to use the `--privileged` option for security reasons. For Kubernetes, refer to the recommended method in the Cloud Native Toolkit documentation.",multi_hop_abstract_query_synthesizer
2,What are the installation requirements for Furiosa LLM and furiosa-smi on Ubuntu 20.04 LTS?,"['<1-hop>\n\nRunning Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) Quick Start with Furiosa LLM [#](#quick-start-with-furiosa-llm ""Link to this heading"") ====================================================================================== Furiosa LLM is a serving framework for LLM models that utilizes FuriosaAI’s NPU. It provides a Python API compatible with vLLM and a server compatible with the OpenAI API. This document explains how to install and use Furiosa LLM. Warning This document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described in this document may change in the future. Installing Furiosa LLM [#](#installing-furiosa-llm ""Link to this heading"") -------------------------------------------------------------------------- The minimum requirements for Furiosa LLM are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Administrator privileges on system (root) * [Setting up APT server](prerequisites.html#aptsetup) and [Installing Prerequisites](prerequisites.html#installingprerequisites) * Python 3.8, 3.9, or 3.10 * Enough storage space for model weights; e.g., about 100GB for Llama 3.1 70B model Then, please install the `furiosa-compiler` package as follows: ``` sudo apt install -y furiosa-compiler ``` Also, you need to create a Python virtual environment depending on your environment. Note Note that some models, such as meta-llama/Meta-Llama-3.1-8B require you to accept their license, hence, you need to create a HuggingFace account, accept the model’s license, and generate a token. Usually, you can create your token at <https://huggingface.co/settings/tokens> . Once you get a token, you can authenticate on the HuggingFace Hub as following: ``` huggingface-cli login --token $HF_TOKEN ``` Then, you can install the Furiosa LLM with the following command: ``` pip install furiosa-llm ``` ### Offline Batch Inference with Furiosa LLM [#](#offline-batch-inference-with-furiosa-llm ""Link to this heading"") In this section, we will explain how to perform offline LLM inference using the Python API of Furiosa LLM. First, import the `LLM` class and `SamplingParams` from the furiosa\\_llm module. `LLM` class is used to load LLM models and provides the core API for LLM inference. `SamplingParams` allows to specify various parameters for text generation. ``` from furiosa_llm import LLM, SamplingParams ``` Next, we will load an LLM model using the LLM class. The following example loads the meta-llama/Meta-Llama-3.1-8B model from the HuggingFace Hub and performs quantization. The model will be loaded into the memory and ready for inference. ``` llm = LLM(model=""meta-llama/Llama-3.1-8B-Instruct"") ``` After loading the model, you can perform LLM inference by calling the `generate` method. ``` prompts = [ """" ] sampling_params = SamplingParams(temperature=0.0) ``` Launching the OpenAI-Compatible Server [#](#launching-the-openai-compatible-server ""Link to this heading"") ---------------------------------------------------------------------------------------------------------- You can find more details at [OpenAI Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) . Running Furiosa LLM in container environment [#](#running-furiosa-llm-in-container-environment ""Link to this heading"") ---------------------------------------------------------------------------------------------------------------------- FuriosaAI provides an image for running `furiosa-llm` in a containerized environment. By using the containerized version, you can run environments that utilize `furiosa-llm` without installing the FuriosaAI Software Stack on your host system, or you can execute it within a Kubernetes environment. To run the `furiosa-llm` container, you can use the following command: ``` $ docker run -it --rm --privileged furiosa-llm:2024.1.0 bash (container) # python ``` Warning The example above uses the `--privileged` option for simplicity, but it is not recommended for security reasons. If you are using Kubernetes, please refer to the following page for the recommended method: [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) [previous Installing Prerequisites](prerequisites.html ""previous page"") [next Running MLPerf™ Inference Benchmark](furiosa_mlperf.html ""next page"") Contents * [Installing Furiosa LLM](#installing-furiosa-llm) + [Offline Batch Inference with Furiosa LLM](#offline-batch-inference-with-furiosa-llm) * [Launching the OpenAI-Compatible Server](#launching-the-openai-compatible-server) * [Running Furiosa LLM in container environment](#running-furiosa-llm-in-container-environment) By FuriosaAI, Inc. © Copyright 2024, FuriosaAI, Inc..', '<2-hop>\n\n* [.rst](../_sources/device_management/furiosa_smi.rst ""Download source file"") * .pdf furiosa-smi =========== Contents -------- * [Installing `furiosa-smi` command](#installing-furiosa-smi-command) + [Synopsis](#synopsis) + [`furiosa-smi info`](#furiosa-smi-info) + [`furiosa-smi status`](#furiosa-smi-status) + [`furiosa-smi ps`](#furiosa-smi-ps) + [`furiosa-smi topo`](#furiosa-smi-topo) furiosa-smi [#](#furiosa-smi ""Link to this heading"") ==================================================== The `furiosa-smi` command provides a variety of subcommands and has the ability to obtain information or control the device.  Synopsis](#synopsis) + [`furiosa-smi info`](#furiosa-smi-info) + [`furiosa-smi status`](#furiosa-smi-status) + [`furiosa-smi ps`](#furiosa-smi-ps) + [`furiosa-smi topo`](#furiosa-smi-topo) furiosa-smi [#](#furiosa-smi ""Link to this heading"") ==================================================== The `furiosa-smi` command provides a variety of subcommands and has the ability to obtain information or control the device. Installing `furiosa-smi` command [#](#installing-furiosa-smi-command ""Link to this heading"") -------------------------------------------------------------------------------------------- To install the `furiosa-smi` command, you need to install `furiosa-smi` as following: The minimum requirements for `furiosa-smi` are as follows: * Ubuntu 20.04 LTS (Debian bullseye) or later * Root permission or sudo permission * Configuring the APT server and installing device drivers ( [Setting up APT server](../getting_started/prerequisites.html#aptsetup) ) Then, please install the `furiosa-smi` package as follows: ``` sudo apt update sudo apt install -y furiosa-smi ``` This command installs packages `furiosa-libsmi` and `furiosa-smi` . ### Synopsis [#](#synopsis ""Link to this heading"") ``` furiosa-smi <sub command> [option] .. ``` ### `furiosa-smi info` [#](#furiosa-smi-info ""Link to this heading"") After installing the kernel driver, you can use the `furiosa-smi` command to check whether the NPU device is recognized. Currently, this command provides the `furiosa-smi info` command to output temperature, power consumption and PCI information of the NPU device. If the device is not visible with this command after mounting it on the machine, please install the driver. If you add the `--full` option to the `info` command, you can see the device’s UUID and serial number information together. ``` $ furiosa-smi info +------+--------+----------------+---------+---------+--------------+ | Arch | Device | Firmware | Temp. | Power | PCI-BDF | +------+--------+----------------+---------+---------+--------------+ | rngd | npu0 | 0.0.15+af1daaa | 30.18°C | 53.00 W | 0000:17:00.0 | +------+--------+----------------+---------+---------+--------------+ | rngd | npu1 | 0.0.15+af1daaa | 29.25°C | 53.00 W | 0000:2a:00.0 | +------+--------+----------------+---------+---------+--------------+ $ furiosa-smi info --format full +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ | Arch | Device | UUID | S/N | Firmware | Temp. | Power | Clock | PCI-BDF | PCI-DEV | +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ | rngd | npu0 | 3E93AE7C-E8EA-4C62-BED6-AD2EC0461AE8 | RNGDXXXXXX | 0.0.15+af1daaa | 30.18°C | 53.00 W | N/A | 0000:17:00.0 | 508:0 | +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ | rngd | npu1 | 176DAD0F-1510-475C-91D8-5F79551CF718 | RNGDXXXXXY | 0.0.15+af1daaa | 29.44°C | 53.00 W | N/A | 0000:2a:00.0 | 506:0 | +------+--------+--------------------------------------+------------+----------------+---------+---------+-------+--------------+---------+ ``` ### `furiosa-smi status` [#](#furiosa-smi-status ""Link to this heading"") The `status` subcommand provides information about the device files available on the NPU device. You can also check whether each core present in the NPU is in use or idle. ``` $ furiosa-smi status +------+--------+---------------+------------------+ | Arch | Device | Cores | Core Utilization | +------+--------+---------------+------------------+ | | | 0 (occupied), | Core 0: 0.00%, | | | | 1 (occupied), | Core 1: 0.00%, | | | | 2 (occupied), | Core 2: 0.00%, | | rngd | npu0 | 3 (occupied), | Core 3: 0.00%, | | | | 4 (occupied), | Core 4: 0.00%, | | | | 5 (occupied), | Core 5: 0.00%, | | | | 6 (occupied), | Core 6: 0.00%, | | | | 7 (occupied) | Core 7: 0.00% | +------+--------+---------------+------------------+ | | | 0 (occupied), | Core 0: 0.00%, | | | | 1 (occupied), | Core 1: 0.00%, | | | | 2 (occupied), | Core 2: 0.00%, | | rngd | npu1 | 3 (occupied), | Core 3: 0.00%, | | | | 4 (occupied), | Core 4: 0.00%, | | | | 5 (occupied), | Core 5: 0.00%, | | | | 6 (occupied), | Core 6: 0.00%, | | | | 7 (occupied) | Core 7: 0.00% | +------+--------+---------------+------------------+ ``` ###']","The installation requirements for Furiosa LLM on Ubuntu 20.04 LTS include having administrator privileges, setting up an APT server, installing prerequisites, and having Python 3.8, 3.9, or 3.10. Additionally, enough storage space is needed for model weights, such as about 100GB for the Llama 3.1 70B model. For furiosa-smi, the requirements include having root or sudo permission, configuring the APT server, and installing device drivers. Both require Ubuntu 20.04 LTS (Debian bullseye) or later.",multi_hop_specific_query_synthesizer
