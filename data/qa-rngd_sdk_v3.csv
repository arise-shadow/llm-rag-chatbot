,page_id,link,question,answer
0,3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/intro.html,"What are the planned features for Furiosa LLM's future releases, and how do they enhance its capabilities?",Planned features for Furiosa LLM include Tensor Parallelism across multiple NPUs and speculative decoding. These enhancements aim to improve parallel processing capabilities and offer more efficient decoding strategies.
1,3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/intro.html,What are the key features of Furiosa LLM that enable it to optimize the serving of LLM models?,"Furiosa LLM features include a vLLM-compatible API, efficient KV cache management with PagedAttention, continuous batching of incoming requests, quantization options (INT4, INT8, FP8, GPTQ, AWQ), data and pipeline parallelism across multiple NPUs, an OpenAI-compatible API server, various decoding algorithms, and integration with HuggingFace models and hub support."
2,3e9b309f-d9d2-4ee7-be9e-9ffe421d4cfa,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/intro.html,How does Furiosa LLM manage efficient KV cache and what role does PagedAttention play in this process?,"Furiosa LLM manages efficient KV cache through the use of PagedAttention, which is one of its key features for optimizing inference performance."
3,dcd59fbc-fb76-4f34-b6ec-ea88a833b047,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/device_plugin.html,"What are the default configuration values for deploying the Furiosa Device Plugin using Helm, and how can they be modified?","The default configuration values for deploying the Furiosa Device Plugin using Helm are: resourceStrategy set to 'generic', debugMode set to false, and disabledDeviceUUIDListMap set to an empty list []. These can be modified by changing the values in the 'charts/furiosa-device-plugin/values.yaml' file."
4,dcd59fbc-fb76-4f34-b6ec-ea88a833b047,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/device_plugin.html,What are the functionalities provided by the Furiosa device plugin when integrated into a Kubernetes cluster?,"The Furiosa device plugin discovers Furiosa NPU devices, registers them to a Kubernetes cluster, tracks the health of the devices, reports to the cluster, and runs AI workloads on the Furiosa NPU devices within the cluster."
5,dcd59fbc-fb76-4f34-b6ec-ea88a833b047,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/device_plugin.html,"How does the Furiosa Device Plugin manage NPU resources in a Kubernetes cluster, and what are the implications of using different resource strategies?","The Furiosa Device Plugin can manage NPU resources by exposing a single NPU card as either a single resource or partitioning it into multiple resources. The 'legacy' strategy uses the resource name 'beta.furiosa.ai/npu' with one resource per card, while the 'generic' strategy uses 'furiosa.ai/rngd' also with one resource per card. Partitioning allows for more granular control over resource allocation."
6,3bb3bd0a-7bdc-45a7-8e3e-e556a52d7eda,https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/furiosa_mlperf.html,What are the minimum system requirements and installation steps needed to set up the 'furiosa-mlperf' command for running MLPerf Inference Benchmark?,"The minimum system requirements for 'furiosa-mlperf' are Ubuntu 20.04 LTS (Debian bullseye) or later, root or sudo permission, configuring the APT server and installing device drivers, and about 100GB storage space for the Llama 3.1 70B. To install, update the package list with 'sudo apt update' and then install using 'sudo apt install -y furiosa-mlperf'. This installs 'furiosa-compiler', 'furiosa-mlperf', and 'furiosa-mlperf-resources' packages."
7,3bb3bd0a-7bdc-45a7-8e3e-e556a52d7eda,https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/furiosa_mlperf.html,"How does the FuriosaAI Software Stack modify the MLPerf Inference Benchmark for its implementation, and what specific benchmark replacement does it include?",The FuriosaAI Software Stack modifies the MLPerf Inference Benchmark by replacing the Llama2 benchmark with one using Llama 3.1.
8,3bb3bd0a-7bdc-45a7-8e3e-e556a52d7eda,https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/furiosa_mlperf.html,"What are the specific subcommands provided by the 'furiosa-mlperf' command for running different MLPerf Inference Benchmarks, and what scenarios do they cover?","The 'furiosa-mlperf' command provides subcommands for running benchmarks in different scenarios: 'bert-offline' and 'bert-server' for BERT benchmarks, 'gpt-j-offline' and 'gpt-j-server' for GPT-J benchmarks, and 'llama-3.1-offline' and 'llama-3.1-server' for Llama 3.1 benchmarks. Each subcommand corresponds to either an offline or server scenario."
9,a3d94379-304a-4dbc-8300-39169378bfd5,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/furiosa-llm-serve.html,"What are the necessary components and steps required to launch the OpenAI-compatible Furiosa-LLM server, and how does the chat template factor into this process?","To launch the OpenAI-compatible Furiosa-LLM server, you need the FuriosaAI LLM Engine artifact and a chat template for the model. The chat template is crucial because the Furiosa SDK 2024.1.0 (alpha) uses Transformers v4.31.0, which does not include a chat template by default. Therefore, users must provide their own chat template to support the /v1/chat/completions API. The server is launched using the 'furiosa-llm serve' command with specific arguments, including the model, artifact path, host, port, chat template path, and optional parameters like response role, pipeline parallel size, tensor parallel size, and devices."
10,a3d94379-304a-4dbc-8300-39169378bfd5,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/furiosa-llm-serve.html,"What specific constraints are associated with the chat template when using Furiosa SDK 2024.1.0 (alpha) for the OpenAI-compatible server, and how can these be addressed?","Furiosa SDK 2024.1.0 (alpha) uses Transformers v4.31.0, which does not include a chat template. Therefore, users must provide a chat template themselves to support the /v1/chat/completions API. This constraint will be removed in future releases. If users have access to the Llama repositories on Hugging Face, they can obtain the chat template for Llama by installing the latest Transformers version and using the AutoTokenizer from the 'meta-llama/Meta-Llama-3.1-70B-Instruct' model to write the chat template to a file."
11,a3d94379-304a-4dbc-8300-39169378bfd5,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/furiosa-llm-serve.html,"What are the limitations of using the 'use_beam_search' parameter with the 'stream' option in the Furiosa-LLM server, and how is this expected to change in future releases?","Using 'use_beam_search' with 'stream' is not allowed because the beam search cannot determine the tokens until the end of the sequence. In the 2024.1 release, 'n' works only for beam search, and this will be fixed in the next release."
12,13853744-dc18-4c98-b1c2-4f5806b28514,https://furiosa-ai.github.io/docs-dev/2024.1/en/,"What is the primary purpose of the FuriosaAI Developer Center as described in the document, and what key stages of the workflow does it cover?","The primary purpose of the FuriosaAI Developer Center is to provide a guide for performing the entire workflow of writing inference applications using FuriosaAI NPUs. It covers stages from starting with a PyTorch model to model quantization, serving, and production deployment."
13,13853744-dc18-4c98-b1c2-4f5806b28514,https://furiosa-ai.github.io/docs-dev/2024.1/en/,"What version of the Furiosa SDK is the document based on, and what implication does this have for the features and APIs described?","The document is based on Furiosa SDK 2024.1.0 (alpha) version, and the features and APIs described may change in the future."
14,13853744-dc18-4c98-b1c2-4f5806b28514,https://furiosa-ai.github.io/docs-dev/2024.1/en/,"What specific utility is provided for managing FuriosaAI NPUs, and where can detailed information about it be found?","The utility provided for managing FuriosaAI NPUs is 'furiosa-smi', and detailed information about it can be found under the Device Management section."
15,2b294227-f255-492c-b642-46e100e59705,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes.html,"What are the recommended container runtimes for Kubernetes, and why is Docker not advised for use?","The recommended container runtimes for Kubernetes are containerd and CRI-O. Docker is not advised because it is officially deprecated as a container runtime in Kubernetes, and using it may lead to unexpected issues with the device plugin."
16,2b294227-f255-492c-b642-46e100e59705,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes.html,"Which versions of Kubernetes and Helm are supported according to the document, and what might be the implications of using unsupported versions?",The document supports Kubernetes v1.24.0 or later and Helm v3.0.0 or later. Using unsupported versions might lead to compatibility issues and unexpected behavior with the device plugin.
17,2b294227-f255-492c-b642-46e100e59705,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes.html,"What potential issues might arise if Docker is used as a container runtime in Kubernetes, and what alternatives are recommended?",Using Docker as a container runtime in Kubernetes may lead to unexpected issues with the device plugin. It is recommended to use containerd or CRI-O instead.
18,d4927eaf-a1a0-40bd-9624-79db4213c5fc,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/roadmap.html,What are the key features and model supports planned for the FuriosaAI 2024.2.0 (beta 0) release in November 2024?,"The 2024.2.0 (beta 0) release in November 2024 includes support for language models such as CodeLLaaMA2, Vicuna, Solar, and EXAONE-3.0, as well as vision models like MobileNetV1, MobileNetV2, ResNet152, ResNet50, EfficientNet, and YOLOv8m. It also introduces Phase 1 of Tensor Parallelism support for Furiosa LLM with intra-chip capabilities, Torch 2.4.1 support, and CPU memory swapping in Furiosa LLM."
19,d4927eaf-a1a0-40bd-9624-79db4213c5fc,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/roadmap.html,How does FuriosaAI plan to enhance its LLM capabilities in the 2024.3.0 release scheduled for December 2024?,FuriosaAI plans to enhance its LLM capabilities by implementing Tensor Parallelism support Phase 2: Inter-chip and integrating Huggingface Optimum.
20,d4927eaf-a1a0-40bd-9624-79db4213c5fc,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/roadmap.html,What is the significance of the Tensor Parallelism support phases mentioned in the FuriosaAI roadmap for 2024?,"The Tensor Parallelism support phases in the FuriosaAI roadmap signify the planned enhancements in distributing computational workloads across hardware. Phase 1, scheduled for November 2024, focuses on intra-chip parallelism, while Phase 2, set for December 2024, aims to extend this capability to inter-chip parallelism, thereby improving the efficiency and scalability of Furiosa LLM."
21,4f19d9ef-0eb7-40bf-a693-6e2d80ddb54a,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/supported_models.html,"Which specific model architectures are supported by Furiosa SDK for encoder-only tasks, and what are some example models available on HuggingFace Hub?",The encoder-only model architecture supported by Furiosa SDK is `BertForQuestionAnswering`. Example models available on HuggingFace Hub include `google-bert/bert-large-uncased` and `google-bert/bert-base-uncased`.
22,4f19d9ef-0eb7-40bf-a693-6e2d80ddb54a,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/supported_models.html,What capabilities does the Furiosa SDK provide for models based on supported architectures?,"The Furiosa SDK allows users to compile, quantize, and run models based on supported architectures on FuriosaAI RNGD."
23,4f19d9ef-0eb7-40bf-a693-6e2d80ddb54a,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/supported_models.html,What are the specific example models available on HuggingFace Hub for the 'LlamaForCausalLM' architecture supported by Furiosa SDK?,"`meta-llama/Llama-2-70b-hf`, `meta-llama/Meta-Llama-3.1-70B`, `meta-llama/Meta-Llama-3-70B-Instruct`, `meta-llama/Meta-Llama-3.1-8B`, `meta-llama/Llama-3.1-8B-Instruct`"
24,0acdfa06-7dff-4603-9a5c-dbc4e3310580,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/software_stack.html,"How does the Furiosa Runtime manage the execution of NPU programs, and what capabilities does it offer for utilizing multiple NPUs?","The Furiosa Runtime loads multiple executable NPU programs generated by the Furiosa compiler and runs them on the NPU. It is responsible for scheduling NPU programs and managing computation and memory resources on NPUs and CPUs. Additionally, the Runtime can use multiple NPUs and provides a single entry point to run the model on multiple NPUs."
25,0acdfa06-7dff-4603-9a5c-dbc4e3310580,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/software_stack.html,"What role does the Furiosa Model Compressor play in optimizing AI models, and what specific quantization methods does it support?","The Furiosa Model Compressor is a library and toolkit for model calibration and quantization, aimed at reducing memory footprint, computation cost, inference latency, and power consumption. It supports post-training quantization methods such as BF16 (W16A16), INT8 Weight-Only (W8A16), FP8 (W8A8), INT8 SmoothQuant (W8A8), and plans to support INT4 Weight-Only (W4A16 AWQ / GPTQ) in release 2024.2."
26,0acdfa06-7dff-4603-9a5c-dbc4e3310580,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/software_stack.html,What are the key features of Furiosa LLM that enhance its performance for large language model inference?,"Furiosa LLM provides a high-performance inference engine with features such as a vLLM-compatible API, PagedAttention, continuous batching, HuggingFace hub support, and an OpenAI-compatible API server."
27,da255daf-c8d3-430e-b976-ad096f3a9ad7,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/references/sampling_params.html,"How does the 'temperature' parameter influence the text generation process in the SamplingParams class, and what effect does setting it to zero have?","The 'temperature' parameter controls the randomness of the sampling in text generation. Lower values make the model more deterministic, while higher values increase randomness. Setting it to zero results in greedy sampling, where the model always chooses the most likely next token."
28,da255daf-c8d3-430e-b976-ad096f3a9ad7,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/references/sampling_params.html,"In the context of the SamplingParams class, how does the 'early_stopping' parameter affect the beam search process, and what are the implications of setting it to 'never'?","The 'early_stopping' parameter controls when the beam search process stops. Setting it to 'never' means the beam search will only stop when there cannot be better candidates, following the canonical beam search algorithm."
29,da255daf-c8d3-430e-b976-ad096f3a9ad7,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/references/sampling_params.html,"In the SamplingParams class, what is the significance of the 'best_of' parameter when 'use_beam_search' is set to True, and how does it relate to the 'n' parameter?","When 'use_beam_search' is set to True, the 'best_of' parameter acts as the beam width, determining the number of output sequences generated from the prompt. The top 'n' sequences are then selected from these 'best_of' sequences. 'best_of' must be greater than or equal to 'n'."
30,edfa6fb4-e88c-4fa8-a6aa-9986213aa6ab,https://furiosa-ai.github.io/docs-dev/2024.1/en/device_management/furiosa_smi.html,"What information can be obtained using the 'furiosa-smi info --format full' command, and how does it differ from the basic 'furiosa-smi info' command?","The 'furiosa-smi info --format full' command provides detailed information including the device's UUID and serial number, in addition to the basic information such as architecture, device, firmware version, temperature, power consumption, and PCI-BDF. The basic 'furiosa-smi info' command does not include the UUID and serial number."
31,edfa6fb4-e88c-4fa8-a6aa-9986213aa6ab,https://furiosa-ai.github.io/docs-dev/2024.1/en/device_management/furiosa_smi.html,What are the prerequisites and steps required to install the 'furiosa-smi' command on a system?,"The prerequisites for installing 'furiosa-smi' include having Ubuntu 20.04 LTS (Debian bullseye) or later, root or sudo permissions, and configuring the APT server along with installing device drivers. The installation steps involve updating the package list with 'sudo apt update' and then installing the package using 'sudo apt install -y furiosa-smi'."
32,edfa6fb4-e88c-4fa8-a6aa-9986213aa6ab,https://furiosa-ai.github.io/docs-dev/2024.1/en/device_management/furiosa_smi.html,"How does the 'furiosa-smi status' subcommand help in monitoring the utilization of NPU cores, and what specific details does it provide?","The 'furiosa-smi status' subcommand provides information about the device files available on the NPU device and checks whether each core is in use or idle. It details the occupancy status of each core and their utilization percentages, indicating that all cores (0 to 7) are occupied but currently have 0.00% utilization."
33,e08e3845-eb5a-4b6f-8530-fb63f81ef7d0,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/references/llm.html,"What is the significance of the 'use_blockwise_compile' parameter in the LLM class, and how does it affect the compilation process?","The 'use_blockwise_compile' parameter, when set to True, ensures that each task is compiled at the level of a transformer block. This allows the compilation result for a transformer block to be generated once and reused, optimizing the compilation process."
34,e08e3845-eb5a-4b6f-8530-fb63f81ef7d0,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/references/llm.html,How does the 'scheduler_config' parameter influence the task management and resource allocation in the LLM class?,"The 'scheduler_config' parameter configures the scheduler to manage the maximum number of tasks that can be queued to hardware, the maximum number of samples that can be processed, and the ratio of spare blocks reserved by the scheduler."
35,e08e3845-eb5a-4b6f-8530-fb63f81ef7d0,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/references/llm.html,What role does the 'kv_cache_sharing_across_beams_config' parameter play in optimizing the model's performance during the decode phase?,"The 'kv_cache_sharing_across_beams_config' parameter configures the sharing of key-value cache across beams, which optimizes the model's performance by creating decode phase buckets with a batch size of 'batch_size' multiplied by 'kv_cache_sharing_across_beams_config.beam_width'."
36,85efd72a-7fd9-4895-a0c7-090891c1e2cf,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/references.html,"What types of reference materials are provided in the markdown content, and how are they organized?","The markdown content provides references to an LLM class and SamplingParams, organized as links to their respective HTML pages."
37,85efd72a-7fd9-4895-a0c7-090891c1e2cf,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/references.html,"What is the purpose of the links labeled 'previous' and 'next' in the markdown content, and how do they contribute to the navigation structure?","The 'previous' link directs to the 'OpenAI Compatible Server' page, and the 'next' link leads to the 'LLM class' page, facilitating sequential navigation through the documentation."
38,85efd72a-7fd9-4895-a0c7-090891c1e2cf,https://furiosa-ai.github.io/docs-dev/2024.1/en/furiosa_llm/references.html,"What specific resources are linked under the 'References' section, and how might they be relevant to someone exploring FuriosaAI's documentation?","The 'References' section links to the 'LLM class' and 'SamplingParams' resources, which are likely relevant for understanding specific components or functionalities within FuriosaAI's documentation."
39,fe9686e2-f00f-4e68-87fa-80abbaf03e2b,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/metrics_exporter.html,"What are the specific metric labels used by the Furiosa Metrics Exporter to describe the attributes of the NPU devices, and how do they contribute to the aggregation of metrics?","The Furiosa Metrics Exporter uses metric labels such as arch, core, device, kubernetes_node_name, and uuid to describe the attributes of NPU devices. These labels help in aggregating metrics by providing common characteristics across different metrics, allowing for effective grouping and analysis. Additionally, a 'label' metric label is used to describe specific attributes for each metric, further aiding in the aggregation process."
40,fe9686e2-f00f-4e68-87fa-80abbaf03e2b,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/metrics_exporter.html,How does the Furiosa Metrics Exporter facilitate the monitoring of FuriosaAI NPU devices within a Kubernetes cluster using Prometheus and Grafana?,"The Furiosa Metrics Exporter exposes metrics related to FuriosaAI NPU devices in Prometheus format, which can be scraped by Prometheus in a Kubernetes cluster. These metrics can then be visualized using a Grafana dashboard. This setup is facilitated by using the Prometheus and Grafana Helm charts along with the furiosa-metrics-exporter Helm chart."
41,fe9686e2-f00f-4e68-87fa-80abbaf03e2b,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/metrics_exporter.html,What role do the 'label' attributes play in the categorization of error and temperature metrics collected by the Furiosa Metrics Exporter?,"The 'label' attributes in the Furiosa Metrics Exporter are used to describe additional specific attributes for each metric type, such as different types of errors (e.g., axi_post_error, pcie_fetch_error) and temperature readings (e.g., peak, ambient). This helps in categorizing and aggregating metrics that share common characteristics without creating too many separate metric definitions."
42,a1f21dc2-1473-410f-a3ae-e46d451a9c0e,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/feature_discovery.html,"What role does Node Feature Discovery (NFD) play in the deployment of Furiosa Feature Discovery, and why is it recommended to use them together?","Node Feature Discovery (NFD) detects hardware features and labels Kubernetes nodes, which Furiosa Feature Discovery leverages to automatically label nodes with FuriosaAI NPU properties. It is recommended to use them together to ensure that the Cloud Native Toolkit is deployed only on nodes equipped with FuriosaAI NPUs."
43,a1f21dc2-1473-410f-a3ae-e46d451a9c0e,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/feature_discovery.html,What specific information does the Furiosa Feature Discovery label provide about the NPU driver version on Kubernetes nodes?,"The Furiosa Feature Discovery labels provide the NPU driver version in terms of major, minor, and patch parts, as well as additional metadata. Specifically, the labels are: furiosa.ai/npu.driver.version for the full version, furiosa.ai/npu.driver.version.major for the major part, furiosa.ai/npu.driver.version.minor for the minor part, furiosa.ai/npu.driver.version.patch for the patch part, and furiosa.ai/npu.driver.version.metadata for any additional metadata."
44,a1f21dc2-1473-410f-a3ae-e46d451a9c0e,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/feature_discovery.html,How does the use of Helm facilitate the deployment of Furiosa Feature Discovery in a Kubernetes cluster?,Helm allows for the easy installation of Furiosa Feature Discovery and NFD into a Kubernetes cluster by using a helm chart available at furiosa-ai/helm-charts. The deployment can be customized by modifying the values.yaml file in the charts/furiosa-feature-discovery directory.
45,a53038c2-0668-4963-875e-79abe9c99e2c,https://furiosa-ai.github.io/docs-dev/2024.1/en/whatsnew/index.html,"What are the quantization methods supported by the Furiosa Quantizer in the Furiosa SDK 2024.1.0 release, and how do they differ in terms of weight and activation precision?","The Furiosa Quantizer supports BF16 (W16A16), INT8 Weight-Only (W8A16), FP8 (W8A8), and INT8 SmoothQuant (W8A8). BF16 uses 16-bit precision for both weights and activations, INT8 Weight-Only uses 8-bit precision for weights and 16-bit for activations, while FP8 and INT8 SmoothQuant use 8-bit precision for both weights and activations."
46,a53038c2-0668-4963-875e-79abe9c99e2c,https://furiosa-ai.github.io/docs-dev/2024.1/en/whatsnew/index.html,What are the key features of the Furiosa LLM introduced in the Furiosa SDK 2024.1.0 release?,"Efficient KV cache management with PagedAttention, continuous batching support in serving, OpenAI-compatible API server, greedy search and beam search, and pipeline parallelism and data parallelism across multiple NPUs."
47,a53038c2-0668-4963-875e-79abe9c99e2c,https://furiosa-ai.github.io/docs-dev/2024.1/en/whatsnew/index.html,What are the capabilities of the Furiosa SDK 2024.1.0 in terms of model support and system management for NPUs?,"The Furiosa SDK 2024.1.0 supports models such as LLaMA 3.1 8B/70B, BERT Large, and GPT-J 6B. It includes a System Management Interface with a library and CLI for managing the Furiosa NPU family, and offers Kubernetes integration for cloud-native management and monitoring."
48,56ae0c48-fbd4-4d43-b6f7-395542108fa7,https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/furiosa_llm.html,"What are the steps required to perform offline batch inference using Furiosa LLM's Python API, and what specific model is used in the example provided?","To perform offline batch inference using Furiosa LLM's Python API, first import the LLM class and SamplingParams from the furiosa_llm module. Then, load an LLM model using the LLM class, specifically the meta-llama/Meta-Llama-3.1-8B model from the HuggingFace Hub, and perform quantization. After loading the model, perform LLM inference by calling the generate method."
49,56ae0c48-fbd4-4d43-b6f7-395542108fa7,https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/furiosa_llm.html,What are the minimum system requirements and initial setup steps needed to install Furiosa LLM?,"The minimum system requirements for installing Furiosa LLM include Ubuntu 20.04 LTS (Debian bullseye) or later, administrator privileges on the system, setting up an APT server, installing prerequisites, Python 3.8, 3.9, or 3.10, and enough storage space for model weights (e.g., about 100GB for Llama 3.1 70B model). The initial setup steps involve installing the 'furiosa-compiler' package using 'sudo apt install -y furiosa-compiler', creating a Python virtual environment, and installing Furiosa LLM with 'pip install furiosa-llm'."
50,56ae0c48-fbd4-4d43-b6f7-395542108fa7,https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/furiosa_llm.html,"What is the purpose of using the '--privileged' option when running the Furiosa LLM container, and why is it not recommended for security reasons?","The '--privileged' option is used for simplicity when running the Furiosa LLM container, but it is not recommended for security reasons because it grants the container elevated permissions that could pose security risks."
51,79971985-74d0-445e-8c78-a80a93ef215b,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/intro.html,What is the primary purpose of the FuriosaAI Cloud Native Toolkit within the Kubernetes and Container ecosystem?,To enable FuriosaAIâ€™s NPU product.
52,79971985-74d0-445e-8c78-a80a93ef215b,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/intro.html,Who is responsible for the creation and copyright of the Cloud Native Toolkit?,"FuriosaAI, Inc."
53,79971985-74d0-445e-8c78-a80a93ef215b,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/intro.html,What is the specific technological environment that the FuriosaAI Cloud Native Toolkit is designed to integrate with?,Kubernetes and Container ecosystem.
54,ccf0c765-aa95-4b97-bf6b-fe5650ff8858,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/rngd.html,What architectural and technological features enable the FuriosaAI RNGD chip to support high-performance deep learning models in multi-tenant environments?,"The FuriosaAI RNGD chip is based on the Tensor Contraction Processor architecture and utilizes TSMC's 5nm process node, operating at 1.0 GHz. It offers 512 TOPS and 1024 TOPS of INT8 and INT4 performance, respectively, and is configured with two HBM3 modules providing a memory bandwidth of 1.5 TB/s. It supports PCIe Gen5 x16 and can function as 2, 4, or 8 individual NPUs in multi-tenant environments, each isolated with its own cores and memory bandwidth. Additionally, it supports Single Root IO Virtualization (SR-IOV) and virtualization for multi-instance NPUs."
55,ccf0c765-aa95-4b97-bf6b-fe5650ff8858,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/rngd.html,How does the FuriosaAI RNGD chip ensure efficient memory management and data transfer for high-performance AI workloads?,"The RNGD chip is equipped with two HBM3 modules providing a memory bandwidth of 1.5 TB/s and supports PCIe Gen5 x16, facilitating efficient memory management and data transfer for high-performance AI workloads."
56,ccf0c765-aa95-4b97-bf6b-fe5650ff8858,https://furiosa-ai.github.io/docs-dev/2024.1/en/overview/rngd.html,"What are the specific performance capabilities of the FuriosaAI RNGD chip in terms of different data types, and how does its architecture support these capabilities?","The FuriosaAI RNGD chip offers 512 TOPS for INT8 and 1024 TOPS for INT4 performance, with BF16 and FP8 capabilities at 256 TFLOPS and 512 TFLOPS respectively. Its architecture, based on the Tensor Contraction Processor, utilizes TSMC's 5nm process node and operates at 1.0 GHz, supporting these high-performance capabilities."
57,a214fb49-b797-4d38-b877-597b6bb059eb,https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/prerequisites.html,"What steps are involved in configuring the APT server for FuriosaAI on a Debian or Ubuntu system, and why is this configuration necessary?","To configure the APT server for FuriosaAI, first install the required packages and register the signing key using 'sudo apt update', 'sudo apt install -y curl gnupg', and 'curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/cloud.google.gpg'. Then, configure the APT server by adding the FuriosaAI repository to the sources list with 'echo ""deb [arch=$(dpkg --print-architecture)] http://asia-northeast3-apt.pkg.dev/projects/furiosa-ai $(. /etc/os-release && echo ""$VERSION_CODENAME"") main"" | sudo tee /etc/apt/sources.list.d/furiosa.list'. This configuration is necessary to access and install FuriosaAI's software packages on Debian or Ubuntu systems."
58,a214fb49-b797-4d38-b877-597b6bb059eb,https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/prerequisites.html,"What command can be used to verify the installation of FuriosaAI devices on a system, and what output indicates a successful installation?","The command 'lspci -nn | grep FuriosaAI' can be used to verify the installation. A successful installation is indicated by output showing PCI information such as '4e:00.0 Processing accelerators [1200]: FuriosaAI, Inc. Device [1ed2:0001] (rev 01)'."
59,a214fb49-b797-4d38-b877-597b6bb059eb,https://furiosa-ai.github.io/docs-dev/2024.1/en/getting_started/prerequisites.html,"What are the necessary steps to upgrade the firmware of FuriosaAI devices, and how long does the process typically take per device?","To upgrade the firmware of FuriosaAI devices, install the 'furiosa-firmware-tools-rngd' and 'furiosa-firmware-image-rngd' packages using the command 'sudo apt install'. The process takes approximately 3 to 5 minutes per device to complete."
60,1ba93fae-bf2e-42c1-a66d-dabbee880912,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/scheduling_npus.html,"What are the constraints when specifying NPU resources in a Kubernetes Pod manifest, and how does Kubernetes handle unspecified NPU requests?","When specifying NPU resources, you can set NPU limits without requests, as Kubernetes will use the limit as the request if the request is not specified. However, if both limits and requests are specified, they must be equal. You cannot specify an NPU request without a limit."
61,1ba93fae-bf2e-42c1-a66d-dabbee880912,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/scheduling_npus.html,How does the deployment of Furiosa Feature Discovery enhance the scheduling of NPUs in a Kubernetes cluster?,"Furiosa Feature Discovery labels nodes based on their hardware and software configurations, including driver versions, allowing users to schedule Pods on nodes that meet specific requirements."
62,1ba93fae-bf2e-42c1-a66d-dabbee880912,https://furiosa-ai.github.io/docs-dev/2024.1/en/cloud_native_toolkit/kubernetes/scheduling_npus.html,"What steps must an administrator take to prepare a Kubernetes node for scheduling Furiosa NPUs, and how can they verify the node's readiness?","An administrator must install prerequisites like drivers and firmware on the nodes and deploy the Furiosa Device Plugin. To verify readiness, they should check the Capacity and Allocatable fields of the v1.node object, ensuring that resources like furiosa.ai/rngd are listed as schedulable."
