{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번역 성능 평가 \n",
    "\n",
    "Kor - Eng - Jpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번역 시스템 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2024-12-28 23:05:59 WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.1.0+cu121)\n",
      "    Python  3.10.15 (you have 3.10.15)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from furiosa_llm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2024-12-28 23:06:02 Prefill buckets: [Bucket(batch_size=1, attention_size=512), Bucket(batch_size=1, attention_size=1024)]\n",
      "INFO:2024-12-28 23:06:02 Decode buckets: [Bucket(batch_size=64, attention_size=2048), Bucket(batch_size=128, attention_size=2048)]\n",
      "INFO:2024-12-28 23:06:02 For some LLaMA V1 models, initializing the fast tokenizer may take a long time. To reduce the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.\n",
      "/home/elicer/anaconda3/envs/jun-rngd/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"RUST_BACKTRACE\"] = \"full\"\n",
    "# Loading an artifact of Llama 3.1 8B Instruct model\n",
    "path = \"/home/elicer/nopro/gitRepo/shinhan/renegade/Llama-3.1-8B-Instruct\"\n",
    "llm = LLM.from_artifacts(path, devices=\"npu:1:*\")\n",
    "\n",
    "# You can specify various parameters for text generation\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_template(prompt):\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In Large Language Models (LLM), \"quantization\" refers to the process of reducing the precision of model weights from 32-bit floating-point numbers (e.g., float32) to lower-precision data types, such as 8-bit integers (e.g., int8) or 16-bit integers (e.g., int16). This technique is often used to reduce the memory footprint of large LLMs and accelerate inference on hardware with limited precision arithmetic capabilities.\n",
      "\n",
      "Quantization involves reducing the number of bits used to represent each model weight, effectively reducing the number of possible values each weight can take. This process relies on techniques such as:\n",
      "\n",
      "1. **Weight clustering**: Combining similar weights into a single set of representative values, which are then scaled and shifted to minimize the loss of accuracy.\n",
      "2. **Quantization aware training**: Training the model to learn the optimal values for quantized weights, rather than relying on a post-training quantization step.\n",
      "\n",
      "There are several quantization techniques"
     ]
    }
   ],
   "source": [
    "prompt = apply_template(\"What is quantization in LLM models?\")\n",
    "async for output_txt in llm.stream_generate(prompt, sampling_params):\n",
    "    print(output_txt, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LLM 모델의 quantization란 무엇입니까?"
     ]
    }
   ],
   "source": [
    "def apply_translation_template(source_lang, target_lang, source_text):\n",
    "    prompt = f\"\"\"This is an {source_lang} to {target_lang} translation, please provide the {target_lang} translation for this text in as polite a tone as possible. \\\n",
    "Do not provide any explanations or text apart from the translation.\n",
    "The translation result must be written in {target_lang}.\n",
    "\n",
    "{source_lang}: {source_text}\n",
    "\n",
    "{target_lang}:\"\"\"\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful assistant specialized in translation tasks.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "# 프롬프트 생성\n",
    "source_lang = \"English\"\n",
    "target_lang = \"Korean\"\n",
    "source_text = \"What is quantization in LLM models?\"\n",
    "\n",
    "prompt = apply_translation_template(source_lang, target_lang, source_text)\n",
    "\n",
    "# 실행 코드\n",
    "async for output_txt in llm.stream_generate(prompt, sampling_params):\n",
    "    print(output_txt, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LLM 모델의 quantization란 무엇입니까?\n"
     ]
    }
   ],
   "source": [
    "output_txt = llm.generate(prompt, sampling_params)\n",
    "print(output_txt.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(source_text, source_lang, target_lang):\n",
    "    prompt = apply_translation_template(source_lang, target_lang, source_text)\n",
    "    output_txt = llm.generate(prompt, sampling_params)\n",
    "    return output_txt.outputs[0].text[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 시스템 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언어별 토크나이저\n",
    "from nltk.tokenize import word_tokenize\n",
    "from janome.tokenizer import Tokenizer\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "def tokenize(text, lang):\n",
    "    if lang == \"eng\":\n",
    "        # 영어: NLTK word_tokenize\n",
    "        return word_tokenize(text)\n",
    "    elif lang == \"kor\":\n",
    "        # 한국어: Kiwi 형태소 분석기\n",
    "        kiwi = Kiwi()\n",
    "        tokens = kiwi.tokenize(text)\n",
    "        return [token.form for token in tokens]  # 형태소만 추출\n",
    "    elif lang == \"jpn\":\n",
    "        # 일본어: Janome 형태소 분석기\n",
    "        tokenizer = Tokenizer()\n",
    "        return [token.surface for token in tokenizer.tokenize(text)]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported language: {lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from bert_score import score\n",
    "import numpy as np\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate(\n",
    "    reference: str,\n",
    "    candidate: str,\n",
    "    tgt_lang: str,\n",
    "    bert_model=\"microsoft/deberta-xlarge-mnli\",\n",
    "):\n",
    "    # 텍스트를 토큰화\n",
    "    reference_tokens = tokenize(reference, tgt_lang)\n",
    "    candidate_tokens = tokenize(candidate, tgt_lang)\n",
    "\n",
    "    # BLEU 점수 계산 (스무딩 적용)\n",
    "    bleu = sentence_bleu(\n",
    "        [reference_tokens],\n",
    "        candidate_tokens,\n",
    "        smoothing_function=SmoothingFunction().method1,\n",
    "    )\n",
    "\n",
    "    # METEOR 점수 계산 (리스트 형태로 전달)\n",
    "    meteor = meteor_score([reference_tokens], candidate_tokens)\n",
    "\n",
    "    # BERTScore 계산\n",
    "    P, R, F1 = score(\n",
    "        [candidate],\n",
    "        [reference],\n",
    "        lang=tgt_lang,\n",
    "        model_type=bert_model,\n",
    "        device=\"cuda\",  # GPU 사용\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"BLEU\": round(bleu, 4),\n",
    "        \"METEOR\": round(meteor, 4),\n",
    "        \"BERT\": round(F1.item(), 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 FLORES-Plus 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.\n",
      "\"그는 \"\"현재 4개월 된 당뇨병에서 치료된 생쥐가 있다\"\"고 덧붙였다.\"\n",
      "「我々が飼っている生後4か月のマウスはかつて糖尿病でしたが現在は糖尿病ではない、」と彼は付け加えました。\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 파일 로더\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "# 데이터 위치 \n",
    "data_dir = \"/home/elicer/Jun/llm-rag-chatbot/data/flores/\"\n",
    "\n",
    "data_eng = load_text_file(f\"{data_dir}/devtest.eng_Latn\")\n",
    "data_kor = load_text_file(f\"{data_dir}/devtest.kor_Hang\")\n",
    "data_jpn = load_text_file(f\"{data_dir}/devtest.jpn_Jpan\")\n",
    "\n",
    "print(data_eng[0])\n",
    "print(data_kor[0])\n",
    "print(data_jpn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: 노바스코샤주 핼리팩스의 댈하우지대학교 의과 교수이자 캐나다 당뇨 협회 임상과학부 의장인 Ehud Ur 박사는 이 연구가 아직 초기 단계라고 경고했습니다.\n",
      "translated: \n",
      "\n",
      "Dr. Ehud Ur, a professor of medicine at Dalhousie University in Halifax, Nova Scotia, and a member of the Clinical Science Committee of the Canadian Diabetes Association, cautioned that the study is still in its early stages.\n"
     ]
    }
   ],
   "source": [
    "test_idx = 1\n",
    "print(f'original: {data_kor[test_idx]}')\n",
    "result = translate(data_kor[test_idx], 'Korean', 'Enlgish')\n",
    "print(f'translated: {result}')\n",
    "\n",
    "# eval_result = evaluate(data_eng[test_idx], result, 'eng')\n",
    "# print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dr. Ehud Ur, a professor of medicine at Dalhousie University in Halifax, Nova Scotia, and a member of the Clinical Science Committee of the Canadian Diabetes Association, cautioned that the study is still in its early stages.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: Dr. Ehud Ur, professor of medicine at Dalhousie University in Halifax, Nova Scotia and chair of the clinical and scientific division of the Canadian Diabetes Association cautioned that the research is still in its early days.\n",
      "translated: \n",
      "\n",
      "Dr. Ehud Ur, Halifax의 Dalhousie University의 의학 교수와 캐나다糖尿病協會의临床과과 과학부의 의장은 연구는 아직 초기 단계에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에\n"
     ]
    }
   ],
   "source": [
    "print(f'original: {data_eng[1]}')\n",
    "result = translate(data_eng[1], 'Enlgish', 'Korean')\n",
    "print(f'translated: {result}')\n",
    "\n",
    "# eval_result = evaluate(data_kor[1], result, 'Korean')\n",
    "# print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: Dr. Ehud Ur, professor of medicine at Dalhousie University in Halifax, Nova Scotia and chair of the clinical and scientific division of the Canadian Diabetes Association cautioned that the research is still in its early days.\n",
      "translated: ドクター・エフード・ウール博士は、ハリファックスにあるノバスコティア州のダルハウス大学医学部教授であり、カナダ糖尿病協会臨床科学部門委員長として、研究がまだ初期段階であることを注意した。\n",
      "{'BLEU': 0.2401, 'METEOR': 0.5821, 'BERT': 0.867}\n"
     ]
    }
   ],
   "source": [
    "print(f'original: {data_eng[1]}')\n",
    "result = translate(data_eng[1], 'eng', 'jpn', prompt)\n",
    "print(f'translated: {result}')\n",
    "\n",
    "eval_result = evaluate(data_jpn[1], result, 'jpn')\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번역 평가 코드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 번역 및 시간 측정\u001b[39;00m\n\u001b[1;32m     26\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 27\u001b[0m candidate \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time  \u001b[38;5;66;03m# 번역 소요 시간\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 번역 결과 저장\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m, in \u001b[0;36mtranslate\u001b[0;34m(source_text, source_lang, target_lang)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate\u001b[39m(source_text, source_lang, target_lang):\n\u001b[1;32m      2\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m apply_translation_template(source_lang, target_lang, source_text)\n\u001b[0;32m----> 3\u001b[0m     output_txt \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_txt\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext[\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/envs/jun-rngd/lib/python3.10/site-packages/furiosa_llm/api.py:2278\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, tokenizer_kwargs)\u001b[0m\n\u001b[1;32m   2273\u001b[0m     prompt_token_ids \u001b[38;5;241m=\u001b[39m encode_auto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, prompts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n\u001b[1;32m   2275\u001b[0m LLM\u001b[38;5;241m.\u001b[39m__verify_sampling_params_with_generator_config(\n\u001b[1;32m   2276\u001b[0m     sampling_params, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator_config\n\u001b[1;32m   2277\u001b[0m )\n\u001b[0;32m-> 2278\u001b[0m native_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_token_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_postprocess(native_outputs, prompts, prompt_token_ids)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "nTest = 200\n",
    "\n",
    "# 언어 쌍 데이터 (명확히 영어, 일본어, 한국어로 명시)\n",
    "pairs = [\n",
    "    (\"English\", \"Korean\", data_eng[:nTest], data_kor[:nTest]),\n",
    "    (\"English\", \"Japanese\", data_eng[:nTest], data_jpn[:nTest]),\n",
    "    (\"Korean\", \"English\", data_kor[:nTest], data_eng[:nTest]),\n",
    "    (\"Korean\", \"Japanese\", data_kor[:nTest], data_jpn[:nTest]),\n",
    "    (\"Japanese\", \"English\", data_jpn[:nTest], data_eng[:nTest]),\n",
    "    (\"Japanese\", \"Korean\", data_jpn[:nTest], data_kor[:nTest]),\n",
    "]\n",
    "\n",
    "# 평가 수행 및 저장 디렉토리\n",
    "save_dir = \"../../data/translate_rngd_flores\"\n",
    "all_results = []  # 모든 결과를 모아서 저장할 리스트\n",
    "\n",
    "for source_lang, target_lang, sources, references in pairs:\n",
    "    pair_results = []  # 현재 언어 쌍의 결과 저장\n",
    "    ic = 0\n",
    "    for source in sources:\n",
    "        if ic % 20 == 0:\n",
    "            print(ic)\n",
    "        ic += 1\n",
    "\n",
    "        # 번역 및 시간 측정\n",
    "        start_time = time.time()\n",
    "        candidate = translate(source, source_lang, target_lang)\n",
    "        elapsed_time = time.time() - start_time  # 번역 소요 시간\n",
    "\n",
    "        # 번역 결과 저장\n",
    "        pair_results.append({\n",
    "            \"Source Language\": source_lang,\n",
    "            \"Target Language\": target_lang,\n",
    "            \"Source\": source,\n",
    "            \"Candidate\": candidate,\n",
    "            \"Translation Time (s)\": round(elapsed_time, 4),  # 번역 소요 시간 추가\n",
    "        })\n",
    "\n",
    "    # 현재 언어 쌍 결과를 데이터프레임으로 변환 및 저장\n",
    "    pair_df = pd.DataFrame(pair_results)\n",
    "    pair_filename = f\"{save_dir}/Translate_results_{source_lang}_to_{target_lang}.csv\"\n",
    "    pair_df.to_csv(pair_filename, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Results for {source_lang} to {target_lang} saved: {pair_filename}\")\n",
    "\n",
    "    # 전체 결과 통합\n",
    "    all_results.extend(pair_results)\n",
    "\n",
    "\n",
    "# 모든 결과를 통합한 데이터프레임 생성 및 저장\n",
    "final_df = pd.DataFrame(all_results)\n",
    "final_filename = f\"{save_dir}/Translate_results_flores_all.csv\"\n",
    "final_df.to_csv(final_filename, index=False, encoding=\"utf-8\")\n",
    "print(f\"All results saved: {final_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jun-rngd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
